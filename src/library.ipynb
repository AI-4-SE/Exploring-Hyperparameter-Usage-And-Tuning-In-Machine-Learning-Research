{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def df_to_latex(df: pd.DataFrame) -> None:\n",
    "    print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_library_classes(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, value in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                if \"base_class_0\" in value:\n",
    "                                    continue\n",
    "                                classes.append(class_name)\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options\n",
    "    class_options = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class)\n",
    "            continue\n",
    "            #raise StopIteration()\n",
    "\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "\n",
    "                                        if len(data.keys()) == 1 and \"**kwargs\" in data:\n",
    "                                            print(project)\n",
    "                                            print(file)\n",
    "                                            print(data)\n",
    "\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if not param == \"variable\" and not param == \"params\":\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "                                \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            option_str = \", \".join(options)\n",
    "            most_used_class_option.append(option_str)\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "    \n",
    "    return df_classes\n",
    "\n",
    "\n",
    "#df_sklearn_classes = get_library_classes(\"sklearn\", \"modules/sklearn_default_values.json\" , \"statistics/*\")\n",
    "#df_sklearn_classes = df_sklearn_classes[:10]\n",
    "#df_tf_classes = get_library_classes(\"tensorflow\", \"modules/tensorflow_default_values.json\" , \"statistics/*\")\n",
    "#df_tf_classes = df_tf_classes[:10]\n",
    "#df_pytorch_classes = get_library_classes(\"torch\", \"modules/torch_default_values.json\" , \"statistics/*\")\n",
    "#df_pytorch_classes = df_pytorch_classes[:10]\n",
    "#df_all_classes = pd.concat([df_sklearn_classes, df_tf_classes, df_pytorch_classes])\n",
    "\n",
    "#df_sklearn = get_library_classes(\"sklearn\", \"modules/sklearn_ml_algorithms.json\" , \"statistics/*\")\n",
    "#df_sklearn = df_sklearn[:30]\n",
    "\n",
    "\n",
    "#df_to_latex(df=df_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_library_methods(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].islower():\n",
    "                                method_name_parts = key.split(\"_\")[:-1]\n",
    "                                method_name = \"_\".join(method_name_parts)\n",
    "                                methods.append(method_name)\n",
    "\n",
    "    method_data = Counter(methods)\n",
    "    df_methods = pd.DataFrame.from_dict(method_data, orient=\"index\").reset_index()\n",
    "    df_methods = df_methods.rename(columns={'index':'Method', 0:'Count'})\n",
    "    df_methods = df_methods.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Compute number of args that can be set regarding the API data\n",
    "    method_options = []\n",
    "    methods = df_methods[\"Method\"].to_list()\n",
    "\n",
    "    for method in methods:\n",
    "        try:\n",
    "            method_data = next(filter(lambda x: x[\"name\"] == method, library_data))\n",
    "            method_options.append(len(method_data[\"params\"]))\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", method, library_name)\n",
    "            method_options.append(0)\n",
    "            continue\n",
    "\n",
    "    df_methods[\"#Args\"] = method_options\n",
    "\n",
    "    # Compute average number of args used per method\n",
    "    avg_method_args = []\n",
    "    most_used_method_args = []\n",
    "\n",
    "    for method in methods:\n",
    "        avg_method_args_used = []\n",
    "        method_args = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].islower():\n",
    "                                    method_name_parts = module_name.split(\"_\")[:-1]\n",
    "                                    method_name = \"_\".join(method_name_parts)\n",
    "                                    if method == method_name:\n",
    "                                        counter = 0\n",
    "                                        for arg in data.keys():\n",
    "                                            if arg == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if not arg == \"variable\" and not arg == \"params\":\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if arg == \"params\":\n",
    "                                                method_args.append(\"default\")\n",
    "                                            else:\n",
    "                                                method_args.append(arg)\n",
    "                                        \n",
    "                                        avg_method_args_used.append(counter)\n",
    "                                \n",
    "        avg_method_args.append(round((sum(avg_method_args_used) / len(avg_method_args_used)),2))\n",
    "        method_arg_data = Counter(method_args)\n",
    "        try:\n",
    "            most_common_num = method_arg_data.most_common(1)[0][1]\n",
    "            args = [x[0] for x in method_arg_data.most_common() if x[1] == most_common_num]\n",
    "            arg_str = \", \".join(args)\n",
    "            most_used_method_args.append(arg_str)\n",
    "        except IndexError:\n",
    "            most_used_method_args.append(\"None\")\n",
    "\n",
    "    df_methods[\"AvgArgsUsed\"] = avg_method_args\n",
    "    df_methods[\"Most Used Args\"] = most_used_method_args\n",
    "\n",
    "    return df_methods\n",
    "\n",
    "#df_sklearn_methods = get_library_methods(\"sklearn\", \"modules/sklearn_default_values.json\", \"statistics/*\")\n",
    "#df_sklearn_methods = df_sklearn_methods[:10]\n",
    "#df_tf_methods = get_library_methods(\"tensorflow\", \"modules/tensorflow_default_values.json\", \"statistics/*\")\n",
    "#df_tf_methods = df_tf_methods[:10]\n",
    "#df_torch_methods = get_library_methods(\"torch\", \"modules/torch_default_values.json\", \"statistics/*\")\n",
    "#df_torch_methods = df_torch_methods[:10]\n",
    "\n",
    "#df_all_methods = pd.concat([df_sklearn_methods, df_tf_methods, df_torch_methods])\n",
    "\n",
    "#df_to_latex(df=df_all_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_api_and_project_classes(library_names: List, library_data_dirs: List, project_dir: str):\n",
    "    api_class_count = []\n",
    "    project_class_count = []\n",
    "\n",
    "    for library_data_dir in library_data_dirs:\n",
    "        class_counter = 0\n",
    "        with open(library_data_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "            library_data = json.load(library_file)\n",
    "\n",
    "        for module in library_data:\n",
    "            if module[\"name\"][0].isupper():\n",
    "                class_counter += 1\n",
    "\n",
    "        api_class_count.append(class_counter)\n",
    "\n",
    "\n",
    "    for library_name in library_names:\n",
    "        class_counter = 0\n",
    "        classes_visited = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for key, value in module_data.items():\n",
    "                                if key[0].isupper():\n",
    "                                    class_name = key.split(\"_\")[0]\n",
    "                                    if \"base_class_0\" in value:\n",
    "                                        continue\n",
    "                                    if class_name not in classes_visited:\n",
    "                                        class_counter += 1\n",
    "                                        classes_visited.append(class_name)\n",
    "        \n",
    "        project_class_count.append(class_counter)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"ML Library\"] = library_names\n",
    "    df[\"API Class Count\"] = api_class_count\n",
    "    df[\"Project Class Count\"] = project_class_count\n",
    "    return df     \n",
    "\n",
    "#library_data_dirs = [\"modules/sklearn_default_values.json\", \"modules/tensorflow_default_values.json\",  \"modules/torch_default_values.json\"]\n",
    "#library_names = [\"sklearn\", \"tensorflow\", \"torch\"]\n",
    "#df_class_count = compare_api_and_project_classes(library_names, library_data_dirs, \"statistics/*\")\n",
    "#df_class_count.head()\n",
    "#df_class_count.to_latex(index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameter_count(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    total_count = 0\n",
    "    default_count = 0\n",
    "    customized_count = 0\n",
    "    not_match_api_count = 0\n",
    "\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for module_name, module_value in module_data.items():\n",
    "                            if module_name[0].isupper():\n",
    "                                class_name = module_name.split(\"_\")[0]\n",
    "                                if \"base_class_0\" in module_value:\n",
    "                                    continue\n",
    "                                #print(\"Module Name: \", class_name)\n",
    "                                for param, param_value in module_value.items():\n",
    "                                    if param in (\"variable\", \"params\"):\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        total_count += 1\n",
    "\n",
    "                                        api_module = next(filter(lambda x: x[\"name\"] == class_name, library_data))\n",
    "\n",
    "                                        if param in api_module[\"params\"]:\n",
    "                                            api_param_value = api_module[\"params\"].get(param)\n",
    "                                            if api_param_value == param_value[\"value\"]:\n",
    "                                                default_count +=1\n",
    "                                            else:\n",
    "                                                customized_count += 1\n",
    "                                        else:\n",
    "                                            not_match_api_count += 1\n",
    "\n",
    "    #assert total_count == default_count + customized_count + not_match_api_count                                        \n",
    "\n",
    "    data = {\n",
    "        \"Library\": [library_name],\n",
    "        \"Total Count\": [total_count],\n",
    "        \"Default Count\": [default_count],\n",
    "        \"Customized Count\": [customized_count],\n",
    "        \"Not API Match Count\": [not_match_api_count]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "#df_sklearn = get_hyperparameter_count(\"sklearn\", \"modules/sklearn_default_values.json\", \"statistics/*\")\n",
    "#df_tf = get_hyperparameter_count(\"tensorflow\", \"modules/tensorflow_default_values.json\", \"statistics/*\")\n",
    "#df_torch = get_hyperparameter_count(\"torch\", \"modules/torch_default_values.json\", \"statistics/*\")\n",
    "\n",
    "#df_all = pd.concat([df_sklearn, df_tf, df_torch])\n",
    "#df_all.head()\n",
    "#df_all.to_latex(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrl}\n",
      "\\toprule\n",
      "                        Class &  Count &  \\#HP &  AvgOptionsUsed &                                                                 Most Used HP \\\\\n",
      "\\midrule\n",
      "           LogisticRegression &     40 &   15 &            1.12 &                                                                      default \\\\\n",
      "                       KMeans &     30 &    9 &            2.30 &                                                                   n\\_clusters \\\\\n",
      "             LinearRegression &     13 &    5 &            0.08 &                                                                      default \\\\\n",
      "                          PCA &     12 &    9 &            1.00 &                                                                 n\\_components \\\\\n",
      "                          SVC &      7 &   15 &            2.00 &                                                                       kernel \\\\\n",
      "             NearestNeighbors &      7 &    8 &            1.29 &                                                                  n\\_neighbors \\\\\n",
      "       RandomForestClassifier &      6 &   18 &            2.33 &                                                                 n\\_estimators \\\\\n",
      "        DecisionTreeRegressor &      5 &   11 &            0.80 &                                                                      default \\\\\n",
      "       DecisionTreeClassifier &      4 &   12 &            1.25 & criterion, random\\_state, default, min\\_samples\\_leaf, max\\_leaf\\_nodes, **kwargs \\\\\n",
      "         KNeighborsClassifier &      4 &    8 &            1.50 &                                                          n\\_neighbors, metric \\\\\n",
      "                 SGDRegressor &      4 &   19 &            0.75 &                                                                     **kwargs \\\\\n",
      "              RANSACRegressor &      4 &   13 &            1.00 &                                                               base\\_estimator \\\\\n",
      "                       DBSCAN &      3 &    8 &            2.00 &                                                             eps, min\\_samples \\\\\n",
      "          KNeighborsRegressor &      3 &    8 &            0.67 &                                               default, **kwargs, n\\_neighbors \\\\\n",
      "                SGDClassifier &      3 &   21 &            2.00 &                                                                     **kwargs \\\\\n",
      "    LatentDirichletAllocation &      3 &   16 &            1.00 &                                                                 n\\_components \\\\\n",
      "                KernelDensity &      3 &    9 &            2.00 &                                                            kernel, bandwidth \\\\\n",
      "                    KernelPCA &      3 &   16 &            1.00 &                                      n\\_components, kernel, default, **kwargs \\\\\n",
      "      AgglomerativeClustering &      3 &    8 &            2.00 &                                                                   n\\_clusters \\\\\n",
      "                   GaussianNB &      3 &    2 &            0.00 &                                                                      default \\\\\n",
      "            ColumnTransformer &      2 &    7 &            1.50 &                                                                 transformers \\\\\n",
      "                        Lasso &      2 &   11 &            1.00 &                                                                        alpha \\\\\n",
      "QuadraticDiscriminantAnalysis &      2 &    4 &            0.50 &                                                            default, **kwargs \\\\\n",
      "   LinearDiscriminantAnalysis &      2 &    7 &            0.50 &                                                            default, **kwargs \\\\\n",
      "                      FastICA &      2 &    9 &            0.50 &                                                            default, **kwargs \\\\\n",
      "              IsolationForest &      2 &    9 &            0.50 &                                                            default, **kwargs \\\\\n",
      "                       KDTree &      2 &    4 &            1.50 &                                                                            X \\\\\n",
      "                        Ridge &      2 &    9 &            1.00 &                                                                        alpha \\\\\n",
      "                  BernoulliNB &      2 &    4 &            1.00 &                                                 default, binarize, fit\\_prior \\\\\n",
      "        RandomForestRegressor &      2 &   17 &            1.50 &                                                                 n\\_estimators \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_ml_algorithms(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, value in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                if \"base_class_0\" in value:\n",
    "                                    continue\n",
    "                                for item in library_data:\n",
    "                                    if item[\"name\"] == class_name:\n",
    "                                        classes.append(class_name)\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options\n",
    "    class_options = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class)\n",
    "            continue\n",
    "            #raise StopIteration()\n",
    "\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "\n",
    "                                        if len(data.keys()) == 1 and \"**kwargs\" in data:\n",
    "                                            print(project)\n",
    "                                            print(file)\n",
    "                                            print(data)\n",
    "\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if not param == \"variable\" and not param == \"params\":\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "                                \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            option_str = \", \".join(options)\n",
    "            most_used_class_option.append(option_str)\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "    \n",
    "    return df_classes\n",
    "\n",
    "\n",
    "\n",
    "df_sklearn = get_ml_algorithms(\"sklearn\", \"modules/sklearn_ml_algorithms.json\", \"statistics/*\")\n",
    "df_sklearn = df_sklearn[:30]\n",
    "\n",
    "df_to_latex(df=df_sklearn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baf69434822515d6eb5706280ed4fdf1abcb6899576a97bde367cb051faeb565"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
