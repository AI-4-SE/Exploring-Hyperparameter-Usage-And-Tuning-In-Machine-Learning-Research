{
    "open_spiel/contrib/python/export_graph.py": {
        "tensorflow": {
            "placeholder_44": {
                "variable": {
                    "value": "net_input",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, flat_info_state_length]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "input",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_48": {
                "variable": {
                    "value": "output",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "output",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_49": {
                "variable": {
                    "value": "legals_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "legals_mask",
                    "type": "str",
                    "possible_values": []
                }
            },
            "dense_52": {
                "variable": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "net_input",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(tf.float32, [None, flat_info_state_length], name='input')",
                            "Call"
                        ]
                    ]
                },
                "units": {
                    "value": "128",
                    "type": "int",
                    "possible_values": []
                },
                "activation": {
                    "value": "tf.nn.relu",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "dense_53": {
                "variable": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.layers.dense(net_input, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, num_actions)",
                            "Call"
                        ],
                        [
                            "policy_net - tf.reduce_max(policy_net, axis=-1, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "units": {
                    "value": "128",
                    "type": "int",
                    "possible_values": []
                },
                "activation": {
                    "value": "tf.nn.relu",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "dense_54": {
                "variable": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.layers.dense(net_input, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, num_actions)",
                            "Call"
                        ],
                        [
                            "policy_net - tf.reduce_max(policy_net, axis=-1, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "units": {
                    "value": "num_actions",
                    "type": "variable",
                    "possible_values": [
                        [
                            "game.num_distinct_actions()",
                            "Call"
                        ]
                    ]
                }
            },
            "multiply_65": {
                "variable": {
                    "value": "masked_exp_logit",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "tf.exp(policy_net)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "legals_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(tf.float32, [None, num_actions], name='legals_mask')",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_sum_66": {
                "variable": {
                    "value": "renormalizing_factor",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "masked_exp_logit",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(tf.exp(policy_net), legals_mask)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "where_69": {
                "variable": {
                    "value": "policy_softmax",
                    "type": "variable",
                    "possible_values": []
                },
                "condition": {
                    "value": "tf.equal(legals_mask, 0.0)",
                    "type": "Call",
                    "possible_values": []
                },
                "x": {
                    "value": "tf.zeros_like(masked_exp_logit)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "tf.divide(masked_exp_logit, renormalizing_factor)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "policy_softmax",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_75": {
                "variable": {
                    "value": "policy_targets",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "reduce_mean_77": {
                "variable": {
                    "value": "policy_cost",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.nn.softmax_cross_entropy_with_logits_v2(logits=policy_net, labels=policy_targets)",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "categorical_83": {
                "variable": {
                    "value": "sampled_actions",
                    "type": "variable",
                    "possible_values": []
                },
                "logits": {
                    "value": "tf.log(policy_softmax)",
                    "type": "Call",
                    "possible_values": []
                },
                "num_samples": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                },
                "name": {
                    "value": "sampled_actions",
                    "type": "str",
                    "possible_values": []
                }
            },
            "AdamOptimizer_87": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "0.0001",
                    "type": "float",
                    "possible_values": []
                }
            },
            "variables_initializer_91": {
                "variable": {
                    "value": "init",
                    "type": "variable",
                    "possible_values": []
                },
                "var_list": {
                    "value": "tf.global_variables()",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "init_all_vars_op",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Session_43": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "write_graph_95": {
                "graph_or_graph_def": {
                    "value": "sess.graph_def",
                    "type": "Attribute",
                    "possible_values": []
                },
                "logdir": {
                    "value": "FLAGS.dir",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "FLAGS.filename",
                    "type": "Attribute",
                    "possible_values": []
                },
                "as_text": {
                    "value": "False",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "reduce_max_63": {
                "input_tensor": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.layers.dense(net_input, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, num_actions)",
                            "Call"
                        ],
                        [
                            "policy_net - tf.reduce_max(policy_net, axis=-1, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "exp_65": {
                "x": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.layers.dense(net_input, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, num_actions)",
                            "Call"
                        ],
                        [
                            "policy_net - tf.reduce_max(policy_net, axis=-1, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                }
            },
            "equal_70": {
                "x": {
                    "value": "legals_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(tf.float32, [None, num_actions], name='legals_mask')",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "0.0",
                    "type": "float",
                    "possible_values": []
                }
            },
            "zeros_like_71": {
                "input": {
                    "value": "masked_exp_logit",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(tf.exp(policy_net), legals_mask)",
                            "Call"
                        ]
                    ]
                }
            },
            "divide_72": {
                "x": {
                    "value": "masked_exp_logit",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(tf.exp(policy_net), legals_mask)",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "renormalizing_factor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_sum(masked_exp_logit, axis=-1, keepdims=True)",
                            "Call"
                        ]
                    ]
                }
            },
            "softmax_cross_entropy_with_logits_v2_78": {
                "logits": {
                    "value": "policy_net",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.layers.dense(net_input, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, 128, activation=tf.nn.relu)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(policy_net, num_actions)",
                            "Call"
                        ],
                        [
                            "policy_net - tf.reduce_max(policy_net, axis=-1, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "labels": {
                    "value": "policy_targets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(shape=[None, num_actions], dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "log_84": {
                "x": {
                    "value": "policy_softmax",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.where(tf.equal(legals_mask, 0.0), tf.zeros_like(masked_exp_logit), tf.divide(masked_exp_logit, renormalizing_factor), name='policy_softmax')",
                            "Call"
                        ]
                    ]
                }
            },
            "global_variables_91": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/data/paper_data/routing_game_experiments/utils.py": {
        "tensorflow": {
            "global_variables_initializer_838": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/alpha_zero/model.py": {
        "tensorflow": {
            "Graph_171": {
                "variable": {
                    "value": "g",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_180": {
                "variable": {
                    "value": "session",
                    "type": "variable",
                    "possible_values": []
                },
                "graph": {
                    "value": "g",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.Graph()",
                            "Call"
                        ],
                        [
                            "tf.Graph()",
                            "Call"
                        ]
                    ]
                }
            },
            "Graph_199": {
                "variable": {
                    "value": "g",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_202": {
                "variable": {
                    "value": "session",
                    "type": "variable",
                    "possible_values": []
                },
                "graph": {
                    "value": "g",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.Graph()",
                            "Call"
                        ],
                        [
                            "tf.Graph()",
                            "Call"
                        ]
                    ]
                }
            },
            "placeholder_217": {
                "variable": {
                    "value": "observations",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, input_size]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "input",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_218": {
                "variable": {
                    "value": "legals_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.bool",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, output_size]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "legals_mask",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_220": {
                "variable": {
                    "value": "training",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.bool",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "training",
                    "type": "str",
                    "possible_values": []
                }
            },
            "where_267": {
                "variable": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "condition": {
                    "value": "legals_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(tf.bool, [None, output_size], name='legals_mask')",
                            "Call"
                        ]
                    ]
                },
                "x": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tfkl.Dense(output_size, name='policy')(policy_head)",
                            "Call"
                        ],
                        [
                            "tf.where(legals_mask, policy_logits, -1e+32 * tf.ones_like(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "-1e+32 * tf.ones_like(policy_logits)",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "identity_269": {
                "variable": {
                    "value": "unused_policy_softmax",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tfkl.Softmax()(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "policy_softmax",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_271": {
                "variable": {
                    "value": "policy_targets",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, output_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "policy_targets",
                    "type": "str",
                    "possible_values": []
                }
            },
            "reduce_mean_273": {
                "variable": {
                    "value": "policy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.nn.softmax_cross_entropy_with_logits_v2(logits=policy_logits, labels=policy_targets)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "policy_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "identity_295": {
                "variable": {
                    "value": "value_out",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "value_out",
                    "type": "variable",
                    "possible_values": [
                        [
                            "cascade(value_head, [tfkl.Dense(nn_width, name='value_dense'), tfkl.Activation('relu'), tfkl.Dense(1, name='value'), tfkl.Activation('tanh')])",
                            "Call"
                        ],
                        [
                            "tf.identity(value_out, name='value_out')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "value_out",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_296": {
                "variable": {
                    "value": "value_targets",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, 1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "value_targets",
                    "type": "str",
                    "possible_values": []
                }
            },
            "identity_298": {
                "variable": {
                    "value": "value_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tf.losses.mean_squared_error(value_out, value_targets)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "value_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "add_n_301": {
                "variable": {
                    "value": "l2_reg_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "[weight_decay * tf.nn.l2_loss(var) for var in tf.trainable_variables() if '/bias:' not in var.name]",
                    "type": "ListComp",
                    "possible_values": []
                },
                "name": {
                    "value": "l2_reg_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "AdamOptimizer_308": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "variables_initializer_175": {
                "variable": {
                    "value": "init",
                    "type": "variable",
                    "possible_values": []
                },
                "var_list": {
                    "value": "tf.global_variables()",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "init_all_vars_op",
                    "type": "str",
                    "possible_values": []
                }
            },
            "import_meta_graph_201": {
                "variable": {
                    "value": "saver",
                    "type": "variable",
                    "possible_values": []
                },
                "meta_graph_or_file": {
                    "value": "metagraph",
                    "type": "variable",
                    "possible_values": [
                        [
                            "metagraph + '.meta'",
                            "BinOp"
                        ]
                    ]
                }
            },
            "trainable_variables_317": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "export_meta_graph_322": {
                "graph_def": {
                    "value": "self._session.graph_def",
                    "type": "Attribute",
                    "possible_values": []
                },
                "saver_def": {
                    "value": "self._saver.saver_def",
                    "type": "Attribute",
                    "possible_values": []
                },
                "filename": {
                    "value": "full_path",
                    "type": "variable",
                    "possible_values": [
                        [
                            "os.path.join(self._path, filename)",
                            "Call"
                        ]
                    ]
                },
                "as_text": {
                    "value": "False",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Saver_178": {
                "variable": {
                    "value": "saver",
                    "type": "variable",
                    "possible_values": []
                },
                "max_to_keep": {
                    "value": "10000",
                    "type": "int",
                    "possible_values": []
                },
                "sharded": {
                    "value": "False",
                    "type": "bool",
                    "possible_values": []
                },
                "name": {
                    "value": "saver",
                    "type": "str",
                    "possible_values": []
                }
            },
            "softmax_cross_entropy_with_logits_v2_274": {
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tfkl.Dense(output_size, name='policy')(policy_head)",
                            "Call"
                        ],
                        [
                            "tf.where(legals_mask, policy_logits, -1e+32 * tf.ones_like(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "labels": {
                    "value": "policy_targets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(shape=[None, output_size], dtype=tf.float32, name='policy_targets')",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_squared_error_298": {
                "y_true": {
                    "value": "value_out",
                    "type": "variable",
                    "possible_values": [
                        [
                            "cascade(value_head, [tfkl.Dense(nn_width, name='value_dense'), tfkl.Activation('relu'), tfkl.Dense(1, name='value'), tfkl.Activation('tanh')])",
                            "Call"
                        ],
                        [
                            "tf.identity(value_out, name='value_out')",
                            "Call"
                        ]
                    ]
                },
                "y_pred": {
                    "value": "value_targets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(shape=[None, 1], dtype=tf.float32, name='value_targets')",
                            "Call"
                        ]
                    ]
                }
            },
            "control_dependencies_309": {
                "control_inputs": {
                    "value": "bn_updates",
                    "type": "variable",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                }
            },
            "global_variables_175": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "device_177": {
                "device_name": {
                    "value": "/cpu:0",
                    "type": "str",
                    "possible_values": []
                }
            },
            "ones_like_268": {
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tfkl.Dense(output_size, name='policy')(policy_head)",
                            "Call"
                        ],
                        [
                            "tf.where(legals_mask, policy_logits, -1e+32 * tf.ones_like(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "l2_loss_302": {
                "t": {
                    "value": "var",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "trainable_variables_303": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "trainable_variables_314": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/deep_cfr.py": {
        "tensorflow": {
            "disable_v2_behavior_36": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "placeholder_175": {
                "variable": {
                    "value": "self._info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, self._embedding_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_179": {
                "variable": {
                    "value": "self._info_state_action_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, self._embedding_size + 1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_action_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_183": {
                "variable": {
                    "value": "self._action_probs_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "action_probs_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_187": {
                "variable": {
                    "value": "self._iter_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, 1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "iter_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "softmax_205": {
                "variable": {
                    "value": "self._action_probs",
                    "type": "Attribute",
                    "possible_values": []
                },
                "logits": {
                    "value": "action_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._policy_network(self._info_state_ph)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_mean_206": {
                "variable": {
                    "value": "self._loss_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.losses.mean_squared_error(labels=tf.math.sqrt(self._iter_ph) * self._action_probs_ph, predictions=tf.math.sqrt(self._iter_ph) * self._action_probs)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "AdamOptimizer_210": {
                "variable": {
                    "value": "self._optimizer_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "mean_squared_error_207": {
                "labels": {
                    "value": "tf.math.sqrt(self._iter_ph) * self._action_probs_ph",
                    "type": "BinOp",
                    "possible_values": []
                },
                "predictions": {
                    "value": "tf.math.sqrt(self._iter_ph) * self._action_probs",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "group_258": {
                "*inputs": {
                    "value": "*[var.initializer for var in self._advantage_networks[player].variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "placeholder_192": {
                "shape": {
                    "value": "[None, self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "'advantage_ph_' + str(p)",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "reduce_mean_230": {
                "input_tensor": {
                    "value": "tf.losses.mean_squared_error(labels=tf.math.sqrt(self._iter_ph) * self._advantage_ph[p], predictions=tf.math.sqrt(self._iter_ph) * self._advantage_outputs[p])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "AdamOptimizer_236": {
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "mean_squared_error_231": {
                "labels": {
                    "value": "tf.math.sqrt(self._iter_ph) * self._advantage_ph[p]",
                    "type": "BinOp",
                    "possible_values": []
                },
                "predictions": {
                    "value": "tf.math.sqrt(self._iter_ph) * self._advantage_outputs[p]",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "sqrt_208": {
                "x": {
                    "value": "self._iter_ph",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "sqrt_209": {
                "x": {
                    "value": "self._iter_ph",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "sqrt_232": {
                "x": {
                    "value": "self._iter_ph",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "sqrt_233": {
                "x": {
                    "value": "self._iter_ph",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/deep_cfr_test.py": {
        "tensorflow": {
            "disable_v2_behavior_26": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "main_75": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_34": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_53": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_46": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_65": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/deep_cfr_tf2.py": {
        "tensorflow": {
            "Dense_117": {
                "variable": {
                    "value": "self.hidden",
                    "type": "Attribute",
                    "possible_values": []
                },
                "units": {
                    "value": "units",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_network_layers[:-1]",
                            "Subscript"
                        ],
                        [
                            "adv_network_layers[:-1]",
                            "Subscript"
                        ]
                    ]
                },
                "kernel_initializer": {
                    "value": "he_normal",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Softmax_147": {
                "variable": {
                    "value": "self.softmax",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "LayerNormalization_158": {
                "variable": {
                    "value": "self.normalization",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Dense_159": {
                "variable": {
                    "value": "self.lastlayer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "units": {
                    "value": "policy_network_layers[-1]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "kernel_initializer": {
                    "value": "he_normal",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Dense_162": {
                "variable": {
                    "value": "self.out_layer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "units": {
                    "value": "num_actions",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "where_183": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "condition": {
                    "value": "mask == 1",
                    "type": "Compare",
                    "possible_values": []
                },
                "x": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": [
                        [
                            "layer(x)",
                            "Call"
                        ],
                        [
                            "self.activation(x)",
                            "Call"
                        ],
                        [
                            "self.normalization(x)",
                            "Call"
                        ],
                        [
                            "self.lastlayer(x)",
                            "Call"
                        ],
                        [
                            "self.activation(x)",
                            "Call"
                        ],
                        [
                            "self.out_layer(x)",
                            "Call"
                        ],
                        [
                            "tf.where(mask == 1, x, -1e+21)",
                            "Call"
                        ],
                        [
                            "self.softmax(x)",
                            "Call"
                        ],
                        [
                            "layer(x)",
                            "Call"
                        ],
                        [
                            "self.activation(x)",
                            "Call"
                        ],
                        [
                            "self.normalization(x)",
                            "Call"
                        ],
                        [
                            "self.lastlayer(x)",
                            "Call"
                        ],
                        [
                            "self.activation(x)",
                            "Call"
                        ],
                        [
                            "self.out_layer(x)",
                            "Call"
                        ],
                        [
                            "mask * x",
                            "BinOp"
                        ]
                    ]
                },
                "y": {
                    "value": "-1e+21",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "LayerNormalization_221": {
                "variable": {
                    "value": "self.normalization",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Dense_222": {
                "variable": {
                    "value": "self.lastlayer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "units": {
                    "value": "adv_network_layers[-1]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "kernel_initializer": {
                    "value": "he_normal",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Dense_225": {
                "variable": {
                    "value": "self.out_layer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "units": {
                    "value": "num_actions",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Example_491": {
                "variable": {
                    "value": "example",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "tf.train.Features(feature={'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'action_probs': tf.train.Feature(float_list=tf.train.FloatList(value=strategy_action_probs)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))})",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "parse_example_512": {
                "variable": {
                    "value": "tups",
                    "type": "variable",
                    "possible_values": []
                },
                "serialized": {
                    "value": "serialized",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "self._strategy_feature_description",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Example_519": {
                "variable": {
                    "value": "example",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "tf.train.Features(feature={'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'samp_regret': tf.train.Feature(float_list=tf.train.FloatList(value=samp_regret)), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))})",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "parse_example_539": {
                "variable": {
                    "value": "tups",
                    "type": "variable",
                    "possible_values": []
                },
                "serialized": {
                    "value": "serialized",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "self._advantage_feature_description",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "maximum_595": {
                "variable": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "advs",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._adv_networks[player]((tf.expand_dims(info_state, axis=0), legal_actions_mask), training=False)[0]",
                            "Subscript"
                        ]
                    ]
                },
                "y": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "reduce_sum_596": {
                "variable": {
                    "value": "summed_regret",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.maximum(advs, 0)",
                            "Call"
                        ]
                    ]
                }
            },
            "constant_616": {
                "variable": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "state.information_state_tensor(player)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_618": {
                "variable": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "state.legal_actions_mask(player)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_628": {
                "variable": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "state.legal_actions_mask(cur_player)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_630": {
                "variable": {
                    "value": "info_state_vector",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "state.information_state_tensor()",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "LeakyReLU_141": {
                "variable": {
                    "value": "self.activation",
                    "type": "Attribute",
                    "possible_values": []
                },
                "alpha": {
                    "value": "0.2",
                    "type": "float",
                    "possible_values": []
                }
            },
            "LeakyReLU_206": {
                "variable": {
                    "value": "self.activation",
                    "type": "Attribute",
                    "possible_values": []
                },
                "alpha": {
                    "value": "0.2",
                    "type": "float",
                    "possible_values": []
                }
            },
            "Adam_378": {
                "variable": {
                    "value": "self._optimizer_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "self._learning_rate",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "MeanSquaredError_380": {
                "variable": {
                    "value": "self._loss_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_388": {
                "variable": {
                    "value": "self._optimizer_advantages[player]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "self._learning_rate",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "one_hot_600": {
                "variable": {
                    "value": "matched_regrets",
                    "type": "variable",
                    "possible_values": []
                },
                "indices": {
                    "value": "tf.argmax(tf.where(legal_actions_mask == 1, advs, -1e+21))",
                    "type": "Call",
                    "possible_values": []
                },
                "depth": {
                    "value": "self._num_actions",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "expand_dims_633": {
                "variable": {
                    "value": "info_state_vector",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "info_state_vector",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.information_state_tensor(), dtype=tf.float32)",
                            "Call"
                        ],
                        [
                            "tf.expand_dims(info_state_vector, axis=0)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "constant_682": {
                "variable": {
                    "value": "tfit",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "self._iteration",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "TFRecordDataset_694": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "filenames": {
                    "value": "self._memories_tfrecordpath",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "ReLU_143": {
                "variable": {
                    "value": "self.activation",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "ReLU_208": {
                "variable": {
                    "value": "self.activation",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "device_374": {
                "device_name": {
                    "value": "self._train_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "device_384": {
                "device_name": {
                    "value": "self._train_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_412": {
                "shape": {
                    "value": "[self._embedding_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_413": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_414": {
                "shape": {
                    "value": "[1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_415": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_418": {
                "shape": {
                    "value": "[self._embedding_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_419": {
                "shape": {
                    "value": "[1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_420": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_421": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "device_427": {
                "device_name": {
                    "value": "self._infer_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "add_n_660": {
                "variable": {
                    "value": "loss",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "[main_loss]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "model.losses",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "device_681": {
                "device_name": {
                    "value": "self._train_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "add_n_720": {
                "variable": {
                    "value": "loss",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "[main_loss]",
                    "type": "List",
                    "possible_values": []
                },
                "name": {
                    "value": "model.losses",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "device_726": {
                "device_name": {
                    "value": "self._train_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "device_360": {
                "device_name": {
                    "value": "self._train_device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Features_492": {
                "feature": {
                    "value": "{'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'action_probs': tf.train.Feature(float_list=tf.train.FloatList(value=strategy_action_probs)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))}",
                    "type": "Dict",
                    "possible_values": []
                }
            },
            "Features_520": {
                "feature": {
                    "value": "{'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'samp_regret': tf.train.Feature(float_list=tf.train.FloatList(value=samp_regret)), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))}",
                    "type": "Dict",
                    "possible_values": []
                }
            },
            "argmax_601": {
                "input": {
                    "value": "tf.where(legal_actions_mask == 1, advs, -1e+21)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "GradientTape_656": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "GradientTape_716": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Dense_156": {
                "units": {
                    "value": "units",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_network_layers[:-1]",
                            "Subscript"
                        ],
                        [
                            "adv_network_layers[:-1]",
                            "Subscript"
                        ]
                    ]
                },
                "kernel_initializer": {
                    "value": "he_normal",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Dense_219": {
                "units": {
                    "value": "units",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_network_layers[:-1]",
                            "Subscript"
                        ],
                        [
                            "adv_network_layers[:-1]",
                            "Subscript"
                        ]
                    ]
                },
                "kernel_initializer": {
                    "value": "he_normal",
                    "type": "str",
                    "possible_values": []
                }
            },
            "MeanSquaredError_364": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_366": {
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "expand_dims_593": {
                "input": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.information_state_tensor(player), dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "where_601": {
                "condition": {
                    "value": "legal_actions_mask == 1",
                    "type": "Compare",
                    "possible_values": []
                },
                "x": {
                    "value": "advs",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._adv_networks[player]((tf.expand_dims(info_state, axis=0), legal_actions_mask), training=False)[0]",
                            "Subscript"
                        ]
                    ]
                },
                "y": {
                    "value": "-1e+21",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "TFRecordWriter_431": {
                "path": {
                    "value": "self._memories_tfrecordpath",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Feature_495": {
                "float_list": {
                    "value": "tf.train.FloatList(value=info_state)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_498": {
                "float_list": {
                    "value": "tf.train.FloatList(value=strategy_action_probs)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_502": {
                "float_list": {
                    "value": "tf.train.FloatList(value=[iteration])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_505": {
                "float_list": {
                    "value": "tf.train.FloatList(value=legal_actions_mask)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_523": {
                "float_list": {
                    "value": "tf.train.FloatList(value=info_state)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_526": {
                "float_list": {
                    "value": "tf.train.FloatList(value=[iteration])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_529": {
                "float_list": {
                    "value": "tf.train.FloatList(value=samp_regret)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_532": {
                "float_list": {
                    "value": "tf.train.FloatList(value=legal_actions_mask)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "FloatList_496": {
                "value": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.information_state_tensor(player), dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_499": {
                "value": {
                    "value": "strategy_action_probs",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "FloatList_503": {
                "value": {
                    "value": "[iteration]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "FloatList_506": {
                "value": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.legal_actions_mask(player), dtype=tf.float32)",
                            "Call"
                        ],
                        [
                            "tf.constant(state.legal_actions_mask(cur_player), dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_524": {
                "value": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.information_state_tensor(player), dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_527": {
                "value": {
                    "value": "[iteration]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "FloatList_530": {
                "value": {
                    "value": "samp_regret",
                    "type": "variable",
                    "possible_values": [
                        [
                            "(exp_payoff - ev) * state.legal_actions_mask(player)",
                            "BinOp"
                        ]
                    ]
                }
            },
            "FloatList_533": {
                "value": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(state.legal_actions_mask(player), dtype=tf.float32)",
                            "Call"
                        ],
                        [
                            "tf.constant(state.legal_actions_mask(cur_player), dtype=tf.float32)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/algorithms/deep_cfr_tf2_test.py": {
        "tensorflow": {
            "main_66": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/dqn.py": {
        "tensorflow": {
            "disable_v2_behavior_28": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "placeholder_98": {
                "variable": {
                    "value": "self._info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, state_representation_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_102": {
                "variable": {
                    "value": "self._action_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.int32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "action_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_104": {
                "variable": {
                    "value": "self._reward_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "reward_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_106": {
                "variable": {
                    "value": "self._is_final_step_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "is_final_step_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_108": {
                "variable": {
                    "value": "self._next_info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, state_representation_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "next_info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_112": {
                "variable": {
                    "value": "self._legal_actions_mask_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "legal_actions_mask_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "stop_gradient_126": {
                "variable": {
                    "value": "self._target_q_values",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input": {
                    "value": "self._target_q_values",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "reduce_max_136": {
                "variable": {
                    "value": "max_next_q",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.math.add(tf.stop_gradient(self._target_q_values), illegal_logits)",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "stack_143": {
                "variable": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": []
                },
                "values": {
                    "value": "[tf.range(tf.shape(self._q_values)[0]), self._action_ph]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "gather_nd_145": {
                "variable": {
                    "value": "predictions",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "self._q_values",
                    "type": "Attribute",
                    "possible_values": []
                },
                "indices": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stack([tf.range(tf.shape(self._q_values)[0]), self._action_ph], axis=-1)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_mean_158": {
                "variable": {
                    "value": "self._loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "loss_class(labels=target, predictions=predictions)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "group_414": {
                "variable": {
                    "value": "initialization_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_416": {
                "variable": {
                    "value": "initialization_target_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._target_variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_418": {
                "variable": {
                    "value": "initialization_opt",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._optimizer.variables()]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "AdamOptimizer_162": {
                "variable": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "group_264": {
                "*inputs": {
                    "value": "[tf.assign(target_v, v) for (target_v, v) in zip(self._target_variables, self._variables)]",
                    "type": "ListComp",
                    "possible_values": []
                }
            },
            "group_452": {
                "variable": {
                    "value": "copy_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(q_network.variables, self._q_network.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_458": {
                "variable": {
                    "value": "copy_target_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(target_q_network.variables, self._target_q_network.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "add_137": {
                "x": {
                    "value": "tf.stop_gradient(self._target_q_values)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "illegal_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "illegal_actions * ILLEGAL_ACTION_LOGITS_PENALTY",
                            "BinOp"
                        ]
                    ]
                }
            },
            "GradientDescentOptimizer_164": {
                "variable": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "group_422": {
                "*inputs": {
                    "value": "*[initialization_weights, initialization_target_weights, initialization_opt]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "stop_gradient_137": {
                "input": {
                    "value": "self._target_q_values",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_147": {
                "var_list": {
                    "value": "self._q_network.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_149": {
                "var_list": {
                    "value": "self._target_q_network.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "assign_265": {
                "ref": {
                    "value": "target_v",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "v",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "latest_checkpoint_364": {
                "checkpoint_dir": {
                    "value": "self._full_checkpoint_name(checkpoint_dir, name)",
                    "type": "Call",
                    "possible_values": []
                },
                "latest_filename": {
                    "value": "os.path.join(checkpoint_dir, self._latest_checkpoint_filename(name))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "shape_144": {
                "input": {
                    "value": "self._q_values",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_453": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_459": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/dqn_test.py": {
        "tensorflow": {
            "disable_v2_behavior_24": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "main_133": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_52": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_79": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_121": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/eva.py": {
        "tensorflow": {
            "disable_v2_behavior_36": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "placeholder_152": {
                "variable": {
                    "value": "self._info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, self._info_state_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/eva_test.py": {
        "tensorflow": {
            "disable_v2_behavior_24": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "main_106": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_36": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_54": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/exploitability_descent.py": {
        "tensorflow": {
            "disable_v2_behavior_49": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "py_func_99": {
                "variable": {
                    "value": "(nash_conv, q_values, cf_reach_probabilities)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "func": {
                    "value": "evaluate_policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "_create_policy_evaluator(self.tabular_policy, self.q_value_calculator)",
                            "Call"
                        ],
                        [
                            "_create_policy_evaluator(self.tabular_policy, self.q_value_calculator)",
                            "Call"
                        ]
                    ]
                },
                "inp": {
                    "value": "[policy_values]",
                    "type": "List",
                    "possible_values": []
                },
                "Tout": {
                    "value": "[tf.float64, tf.float64, tf.float64]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "reduce_sum_101": {
                "variable": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "policy_values * q_values",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "py_func_111": {
                "variable": {
                    "value": "(nash_conv, real_q_values, cf_reach_probabilities)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "func": {
                    "value": "evaluate_policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "_create_policy_evaluator(self.tabular_policy, self.q_value_calculator)",
                            "Call"
                        ],
                        [
                            "_create_policy_evaluator(self.tabular_policy, self.q_value_calculator)",
                            "Call"
                        ]
                    ]
                },
                "inp": {
                    "value": "[policy_values]",
                    "type": "List",
                    "possible_values": []
                },
                "Tout": {
                    "value": "[tf.float64, tf.float64, tf.float64]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "reduce_sum_113": {
                "variable": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "policy_values * q_values",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "gather_117": {
                "variable": {
                    "value": "policy_values",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "policy_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.gather(policy_values, indices)",
                            "Call"
                        ]
                    ]
                },
                "indices": {
                    "value": "indices",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "gather_118": {
                "variable": {
                    "value": "advantage",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "advantage",
                    "type": "variable",
                    "possible_values": [
                        [
                            "q_values - tf.stop_gradient(baseline)",
                            "BinOp"
                        ],
                        [
                            "q_values - baseline",
                            "BinOp"
                        ],
                        [
                            "tf.gather(advantage, indices)",
                            "Call"
                        ]
                    ]
                },
                "indices": {
                    "value": "indices",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "gather_119": {
                "variable": {
                    "value": "cf_reach_probabilities",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "cf_reach_probabilities",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.concatenate([np.array(evaluations[p].counterfactual_reach_probs_vs_br, np.float64) for p in [0, 1]])",
                            "Call"
                        ],
                        [
                            "tf.gather(cf_reach_probabilities, indices)",
                            "Call"
                        ]
                    ]
                },
                "indices": {
                    "value": "indices",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "reduce_mean_124": {
                "variable": {
                    "value": "q_value_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "(q_values - real_q_values) ** 2",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "gather_125": {
                "variable": {
                    "value": "q_value_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "q_value_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_mean((q_values - real_q_values) ** 2, axis=1)",
                            "Call"
                        ],
                        [
                            "tf.gather(q_value_loss, indices)",
                            "Call"
                        ],
                        [
                            "tf.reduce_sum(q_value_loss * cf_reach_probabilities)",
                            "Call"
                        ]
                    ]
                },
                "indices": {
                    "value": "indices",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "reduce_sum_126": {
                "variable": {
                    "value": "q_value_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "q_value_loss * cf_reach_probabilities",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "reduce_sum_127": {
                "variable": {
                    "value": "policy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "loss_per_state * cf_reach_probabilities",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "Variable_137": {
                "variable": {
                    "value": "self._logits",
                    "type": "Attribute",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "np.ones_like(self._loss_calculator.tabular_policy.action_probability_array, dtype=np.float64)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "logits",
                    "type": "str",
                    "possible_values": []
                },
                "use_resource": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "placeholder_146": {
                "variable": {
                    "value": "self._learning_rate",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float64",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "()",
                    "type": "Tuple",
                    "possible_values": []
                },
                "name": {
                    "value": "learning_rate",
                    "type": "str",
                    "possible_values": []
                }
            },
            "GradientDescentOptimizer_147": {
                "variable": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "self._learning_rate",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "stop_gradient_102": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_sum(policy_values * q_values, axis=-1, keepdims=True)",
                            "Call"
                        ],
                        [
                            "tf.reduce_sum(policy_values * q_values, axis=-1, keepdims=True)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_sum_103": {
                "input_tensor": {
                    "value": "policy_values * advantage",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "reduce_sum_104": {
                "input_tensor": {
                    "value": "loss_per_state * cf_reach_probabilities",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "reduce_sum_122": {
                "input_tensor": {
                    "value": "policy_values * tf.stop_gradient(advantage)",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "stop_gradient_123": {
                "input": {
                    "value": "advantage",
                    "type": "variable",
                    "possible_values": [
                        [
                            "q_values - tf.stop_gradient(baseline)",
                            "BinOp"
                        ],
                        [
                            "q_values - baseline",
                            "BinOp"
                        ],
                        [
                            "tf.gather(advantage, indices)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/algorithms/exploitability_descent_test.py": {
        "tensorflow": {
            "disable_v2_behavior_24": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "main_59": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_32": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_47": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/losses/rl_losses.py": {
        "tensorflow": {
            "disable_v2_behavior_31": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "TensorShape_38": {
                "variable": {
                    "value": "union_of_shapes",
                    "type": "variable",
                    "possible_values": []
                },
                "dims": {
                    "value": "None",
                    "type": "NoneType",
                    "possible_values": []
                }
            },
            "softmax_54": {
                "variable": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": []
                },
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "stop_gradient_56": {
                "variable": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_sum_60": {
                "variable": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.nn.relu(action_values - tf.expand_dims(baseline, 1))",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "softmax_69": {
                "variable": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": []
                },
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "stop_gradient_71": {
                "variable": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ]
                    ]
                }
            },
            "sparse_softmax_cross_entropy_with_logits_85": {
                "variable": {
                    "value": "cross_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "labels": {
                    "value": "actions",
                    "type": "variable",
                    "possible_values": []
                },
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "stop_gradient_87": {
                "variable": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "reduce_sum_47": {
                "input_tensor": {
                    "value": "tf.multiply(policy, tf.stop_gradient(action_values))",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "relu_77": {
                "variable": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "reduce_sum_81": {
                "input_tensor": {
                    "value": "policy_advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "-tf.multiply(policy, tf.stop_gradient(advantages))",
                            "UnaryOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "multiply_89": {
                "x": {
                    "value": "cross_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.nn.sparse_softmax_cross_entropy_with_logits(labels=actions, logits=policy_logits)",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "reduce_sum_93": {
                "input_tensor": {
                    "value": "-tf.nn.softmax(policy_logits) * tf.nn.log_softmax(policy_logits)",
                    "type": "BinOp",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "reduce_mean_117": {
                "variable": {
                    "value": "total_adv",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "reduce_mean_150": {
                "variable": {
                    "value": "total_adv",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "reduce_mean_183": {
                "variable": {
                    "value": "total_regret",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_sum(tf.nn.relu(action_values - tf.expand_dims(baseline, 1)), axis=1)",
                            "Call"
                        ],
                        [
                            "compute_regrets(policy_logits, action_values)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "reduce_mean_220": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "policy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_a2c_loss(policy_logits, actions, advantages)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "multiply_48": {
                "x": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.nn.softmax(policy_logits, axis=1)",
                            "Call"
                        ],
                        [
                            "tf.nn.softmax(policy_logits, axis=1)",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "tf.stop_gradient(action_values)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "relu_61": {
                "features": {
                    "value": "action_values - tf.expand_dims(baseline, 1)",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "expand_dims_75": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ],
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "multiply_80": {
                "x": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.nn.softmax(policy_logits, axis=1)",
                            "Call"
                        ],
                        [
                            "tf.nn.softmax(policy_logits, axis=1)",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "tf.stop_gradient(advantages)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "reduce_mean_121": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "multiply_122": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "entropy_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "add_124": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(policy_loss, axis=0)",
                            "Call"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "total_loss_with_entropy",
                    "type": "str",
                    "possible_values": []
                }
            },
            "reduce_mean_154": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "multiply_155": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "entropy_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "add_157": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(policy_loss, axis=0)",
                            "Call"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "total_loss_with_entropy",
                    "type": "str",
                    "possible_values": []
                }
            },
            "reduce_mean_187": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "multiply_188": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "entropy_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "add_190": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(policy_loss, axis=0)",
                            "Call"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "total_loss_with_entropy",
                    "type": "str",
                    "possible_values": []
                }
            },
            "reduce_mean_222": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "multiply_223": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "y": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "entropy_loss",
                    "type": "str",
                    "possible_values": []
                }
            },
            "add_225": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ],
                        [
                            "tf.reduce_mean(policy_loss, axis=0)",
                            "Call"
                        ],
                        [
                            "tf.add(total_loss, entropy_loss, name='total_loss_with_entropy')",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ],
                        [
                            "tf.multiply(float(self._entropy_cost), policy_entropy, name='entropy_loss')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "total_loss_with_entropy",
                    "type": "str",
                    "possible_values": []
                }
            },
            "stop_gradient_48": {
                "input": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(action_values)",
                            "Call"
                        ]
                    ]
                }
            },
            "stop_gradient_80": {
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - tf.expand_dims(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "tf.nn.relu(advantages)",
                            "Call"
                        ],
                        [
                            "tf.stop_gradient(advantages)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "log_softmax_94": {
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "expand_dims_61": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ],
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "softmax_94": {
                "logits": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/losses/rl_losses_test.py": {
        "tensorflow": {
            "disable_v2_behavior_24": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "constant_33": {
                "variable": {
                    "value": "q_values",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[0.0, -1.0, 1.0], [1.0, -1.0, 0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_34": {
                "variable": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[1.0, 1.0, 1.0], [1.0, 1.0, 4.0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_51": {
                "variable": {
                    "value": "q_values",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[0.0, -1.0, 1.0], [1.0, -1.0, 0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_52": {
                "variable": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[1.0, 1.0, 1.0], [1.0, 1.0, 4.0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_70": {
                "variable": {
                    "value": "q_values",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[0.0, -1.0, 1.0], [1.0, -1.0, 0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_71": {
                "variable": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[1.0, 1.0, 1.0], [1.0, 1.0, 4.0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_88": {
                "variable": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[[1.0, 1.0, 1.0], [1.0, 1.0, 4.0]]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_89": {
                "variable": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[1.0 / 3, 0.5]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_90": {
                "variable": {
                    "value": "actions",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[1, 2]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.int32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "constant_91": {
                "variable": {
                    "value": "returns",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "[0.0, 1.0]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "main_108": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/masked_softmax.py": {
        "tensorflow": {
            "disable_v2_behavior_21": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "cast_39": {
                "variable": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.cast(legal_actions_mask, dtype=logits.dtype)",
                            "Call"
                        ]
                    ]
                },
                "dtype": {
                    "value": "logits.dtype",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "reduce_max_41": {
                "variable": {
                    "value": "max_logit",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "masked_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "logits + tf.log(legal_actions_mask)",
                            "BinOp"
                        ],
                        [
                            "logits + np.log(legal_actions_mask)",
                            "BinOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "exp_42": {
                "variable": {
                    "value": "exp_logit",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "masked_logits - max_logit",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "log_40": {
                "x": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.cast(legal_actions_mask, dtype=logits.dtype)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_sum_43": {
                "input_tensor": {
                    "value": "exp_logit",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.exp(masked_logits - max_logit)",
                            "Call"
                        ],
                        [
                            "np.exp(masked_logits - max_logit)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/masked_softmax_test.py": {
        "tensorflow": {
            "disable_v2_behavior_27": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Variable_117": {
                "variable": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "np_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.asarray([[[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]])",
                            "Call"
                        ]
                    ]
                },
                "trainable": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Variable_118": {
                "variable": {
                    "value": "mask",
                    "type": "variable",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "np_legal_actions",
                    "type": "variable",
                    "possible_values": []
                },
                "trainable": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Variable_137": {
                "variable": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "np_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.asarray([[[1.0, 1.0, 1.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]])",
                            "Call"
                        ]
                    ]
                },
                "trainable": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Variable_143": {
                "variable": {
                    "value": "mask",
                    "type": "variable",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "np_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.asarray([[[1.0, 1.0, 1.0], [1.0, 0.0, 1.0], [0.0, 0.0, 0.0]]])",
                            "Call"
                        ]
                    ]
                },
                "trainable": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Session_122": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_153": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_123": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_154": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/neurd.py": {
        "tensorflow": {
            "disable_v2_behavior_34": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "cast_39": {
                "variable": {
                    "value": "can_decrease",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "tf.greater(logits, -threshold)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "cast_40": {
                "variable": {
                    "value": "can_increase",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "tf.less(logits, threshold)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "minimum_41": {
                "variable": {
                    "value": "regrets_negative",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stop_gradient(thresholded(logits, regrets, threshold=threshold))",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "0.0",
                    "type": "float",
                    "possible_values": []
                }
            },
            "maximum_42": {
                "variable": {
                    "value": "regrets_positive",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stop_gradient(thresholded(logits, regrets, threshold=threshold))",
                            "Call"
                        ]
                    ]
                },
                "y": {
                    "value": "0.0",
                    "type": "float",
                    "possible_values": []
                }
            },
            "zeros_149": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[1, rcfr.num_features(game)]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "greater_39": {
                "x": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - tf.reduce_mean(logits, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "y": {
                    "value": "-threshold",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "less_40": {
                "x": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - tf.reduce_mean(logits, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "y": {
                    "value": "threshold",
                    "type": "variable",
                    "possible_values": [
                        [
                            "2.0",
                            "Method Argument"
                        ],
                        [
                            "2.0",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "stop_gradient_67": {
                "variable": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "thresholded(logits, regrets, threshold=threshold)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "reduce_mean_69": {
                "variable": {
                    "value": "utility",
                    "type": "variable",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "logits * regrets",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "squeeze_219": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._models[player](self._root_wrapper.sequence_features[player])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "exp_222": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "x": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                            "Call"
                        ],
                        [
                            "tensor - tf.reduce_max(tensor, keepdims=True)",
                            "BinOp"
                        ],
                        [
                            "tf.math.exp(tensor)",
                            "Call"
                        ]
                    ]
                }
            },
            "expand_dims_275": {
                "variable": {
                    "value": "targets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "regrets.astype('float32')",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "GradientTape_62": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Dense_143": {
                "units": {
                    "value": "1 + self._autoencode * rcfr.num_features(game)",
                    "type": "BinOp",
                    "possible_values": []
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "reduce_mean_65": {
                "input_tensor": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - tf.reduce_mean(logits, keepdims=True)",
                            "BinOp"
                        ]
                    ]
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Dense_136": {
                "units": {
                    "value": "num_hidden_units",
                    "type": "variable",
                    "possible_values": []
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "activation": {
                    "value": "hidden_activation",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.nn.relu",
                            "Method Argument"
                        ]
                    ]
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "reduce_max_221": {
                "input_tensor": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                            "Call"
                        ],
                        [
                            "tensor - tf.reduce_max(tensor, keepdims=True)",
                            "BinOp"
                        ],
                        [
                            "tf.math.exp(tensor)",
                            "Call"
                        ]
                    ]
                },
                "keepdims": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Dense_130": {
                "units": {
                    "value": "num_hidden_factors",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0",
                            "Method Argument"
                        ]
                    ]
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/algorithms/neurd_test.py": {
        "tensorflow": {
            "disable_v2_behavior_23": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "enable_eager_execution_25": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "set_random_seed_44": {
                "seed": {
                    "value": "42",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/nfsp.py": {
        "tensorflow": {
            "disable_v2_behavior_34": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "placeholder_98": {
                "variable": {
                    "value": "self._info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, state_representation_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_103": {
                "variable": {
                    "value": "self._action_probs_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "action_probs_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_106": {
                "variable": {
                    "value": "self._legal_actions_mask_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "legal_actions_mask_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "softmax_115": {
                "variable": {
                    "value": "self._avg_policy_probs",
                    "type": "Attribute",
                    "possible_values": []
                },
                "logits": {
                    "value": "self._avg_policy",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "reduce_mean_123": {
                "variable": {
                    "value": "self._loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.stop_gradient(self._action_probs_ph), logits=self._avg_policy)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "AdamOptimizer_129": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "sl_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "softmax_cross_entropy_with_logits_v2_124": {
                "labels": {
                    "value": "tf.stop_gradient(self._action_probs_ph)",
                    "type": "Call",
                    "possible_values": []
                },
                "logits": {
                    "value": "self._avg_policy",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "GradientDescentOptimizer_131": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "sl_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "Saver_118": {
                "var_list": {
                    "value": "self._rl_agent._q_network.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_119": {
                "var_list": {
                    "value": "self._avg_network.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "latest_checkpoint_299": {
                "checkpoint_dir": {
                    "value": "self._full_checkpoint_name(checkpoint_dir, name)",
                    "type": "Call",
                    "possible_values": []
                },
                "latest_filename": {
                    "value": "os.path.join(checkpoint_dir, self._latest_checkpoint_filename(name))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "stop_gradient_125": {
                "input": {
                    "value": "self._action_probs_ph",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/nfsp_test.py": {
        "tensorflow": {
            "disable_v2_behavior_23": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "main_93": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_44": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/policy_gradient.py": {
        "tensorflow": {
            "disable_v2_behavior_86": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "placeholder_173": {
                "variable": {
                    "value": "self._info_state_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None, info_state_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "info_state_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_175": {
                "variable": {
                    "value": "self._action_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.int32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "action_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "placeholder_177": {
                "variable": {
                    "value": "self._return_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "[None]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                },
                "name": {
                    "value": "return_ph",
                    "type": "str",
                    "possible_values": []
                }
            },
            "softmax_195": {
                "variable": {
                    "value": "self._policy_probs",
                    "type": "Attribute",
                    "possible_values": []
                },
                "logits": {
                    "value": "self._policy_logits",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "group_469": {
                "variable": {
                    "value": "initialization_torso",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._net_torso.variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_471": {
                "variable": {
                    "value": "initialization_logit",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._policy_logits_layer.variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_479": {
                "variable": {
                    "value": "initialization_crit_opt",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._critic_optimizer.variables()]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_481": {
                "variable": {
                    "value": "initialization_pi_opt",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._pi_optimizer.variables()]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "squeeze_203": {
                "variable": {
                    "value": "self._baseline",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input": {
                    "value": "self._baseline_layer(torso_out)",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "reduce_mean_215": {
                "variable": {
                    "value": "self._critic_loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.losses.mean_squared_error(labels=self._return_ph, predictions=self._baseline)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "stack_220": {
                "variable": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": []
                },
                "values": {
                    "value": "[tf.range(tf.shape(self._q_values)[0]), self._action_ph]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "gather_nd_222": {
                "variable": {
                    "value": "value_predictions",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "self._q_values",
                    "type": "Attribute",
                    "possible_values": []
                },
                "indices": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.stack([tf.range(tf.shape(self._q_values)[0]), self._action_ph], axis=-1)",
                            "Call"
                        ]
                    ]
                }
            },
            "reduce_mean_223": {
                "variable": {
                    "value": "self._critic_loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "input_tensor": {
                    "value": "tf.losses.mean_squared_error(labels=self._return_ph, predictions=value_predictions)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "AdamOptimizer_227": {
                "variable": {
                    "value": "self._critic_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "critic_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "AdamOptimizer_259": {
                "variable": {
                    "value": "self._pi_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "pi_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.001",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "group_474": {
                "variable": {
                    "value": "initialization_baseline_or_q_val",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._baseline_layer.variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_477": {
                "variable": {
                    "value": "initialization_baseline_or_q_val",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[var.initializer for var in self._q_values_layer.variables]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_524": {
                "variable": {
                    "value": "copy_mlp_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(net_torso.variables, self._net_torso.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_530": {
                "variable": {
                    "value": "copy_logit_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(policy_logits_layer.variables, self._policy_logits_layer.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "GradientDescentOptimizer_230": {
                "variable": {
                    "value": "self._critic_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "critic_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "clip_by_global_norm_239": {
                "variable": {
                    "value": "(grads, _)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "t_list": {
                    "value": "grads",
                    "type": "variable",
                    "possible_values": []
                },
                "clip_norm": {
                    "value": "max_global_gradient_norm",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "GradientDescentOptimizer_262": {
                "variable": {
                    "value": "self._pi_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "pi_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.001",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "group_485": {
                "*inputs": {
                    "value": "*[initialization_torso, initialization_logit, initialization_baseline_or_q_val, initialization_crit_opt, initialization_pi_opt]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_537": {
                "variable": {
                    "value": "copy_q_value_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(q_values_layer.variables, self._q_values_layer.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "group_543": {
                "variable": {
                    "value": "copy_baseline_weights",
                    "type": "variable",
                    "possible_values": []
                },
                "*inputs": {
                    "value": "*[va.assign(vb * (1 + sigma * tf.random.normal(vb.shape))) for (va, vb) in zip(baseline_layer.variables, self._baseline_layer.variables)]",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "mean_squared_error_216": {
                "labels": {
                    "value": "self._return_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "predictions": {
                    "value": "self._baseline",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "mean_squared_error_224": {
                "labels": {
                    "value": "self._return_ph",
                    "type": "Attribute",
                    "possible_values": []
                },
                "predictions": {
                    "value": "value_predictions",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.gather_nd(self._q_values, action_indices)",
                            "Call"
                        ]
                    ]
                }
            },
            "latest_checkpoint_365": {
                "checkpoint_dir": {
                    "value": "self._full_checkpoint_name(checkpoint_dir, name)",
                    "type": "Call",
                    "possible_values": []
                },
                "latest_filename": {
                    "value": "os.path.join(checkpoint_dir, self._latest_checkpoint_filename(name))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Saver_490": {
                "var_list": {
                    "value": "self._net_torso.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_492": {
                "var_list": {
                    "value": "self._policy_logits_layer.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_495": {
                "var_list": {
                    "value": "self._baseline_layer.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Saver_498": {
                "var_list": {
                    "value": "self._q_values_layer.variables",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "shape_221": {
                "input": {
                    "value": "self._q_values",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_525": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_531": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_538": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "normal_544": {
                "shape": {
                    "value": "vb.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/policy_gradient_test.py": {
        "tensorflow": {
            "main_143": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_53": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_100": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/algorithms/rcfr.py": {
        "tensorflow": {
            "disable_v2_behavior_44": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "convert_to_tensor_59": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.convert_to_tensor(tensor)",
                            "Call"
                        ],
                        [
                            "tf.reshape(tensor, [1, num_columns])",
                            "Call"
                        ],
                        [
                            "tf.nn.relu(tf.squeeze(self._models[player](self._root_wrapper.sequence_features[player])))",
                            "Call"
                        ]
                    ]
                }
            },
            "one_hot_97": {
                "variable": {
                    "value": "action_features",
                    "type": "variable",
                    "possible_values": []
                },
                "indices": {
                    "value": "[action]",
                    "type": "List",
                    "possible_values": []
                },
                "depth": {
                    "value": "num_distinct_actions",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "tile_98": {
                "variable": {
                    "value": "action_features",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "action_features",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.one_hot([action], num_distinct_actions)",
                            "Call"
                        ],
                        [
                            "tf.tile(action_features, [tf.shape(state_features)[0], 1])",
                            "Call"
                        ]
                    ]
                },
                "multiples": {
                    "value": "[tf.shape(state_features)[0], 1]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "concat_99": {
                "variable": {
                    "value": "all_features",
                    "type": "variable",
                    "possible_values": []
                },
                "values": {
                    "value": "[state_features, action_features]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "concat_101": {
                "values": {
                    "value": "with_action_features",
                    "type": "variable",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "zeros_609": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[1, num_features(game)]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "reshape_67": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "tensor": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.convert_to_tensor(tensor)",
                            "Call"
                        ],
                        [
                            "tf.reshape(tensor, [1, num_columns])",
                            "Call"
                        ],
                        [
                            "tf.nn.relu(tf.squeeze(self._models[player](self._root_wrapper.sequence_features[player])))",
                            "Call"
                        ]
                    ]
                },
                "shape": {
                    "value": "[1, num_columns]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "relu_670": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "tf.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "expand_dims_774": {
                "variable": {
                    "value": "targets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._regret_targets[regret_player]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "concat_168": {
                "values": {
                    "value": "rows",
                    "type": "variable",
                    "possible_values": []
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "zeros_539": {
                "variable": {
                    "value": "zeros",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[tf.shape(x)[0], padding]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "concat_540": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "values": {
                    "value": "[x, zeros]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Dense_606": {
                "units": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "strided_slice_542": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "input_": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tensor_to_matrix(x)",
                            "Call"
                        ],
                        [
                            "y",
                            "variable"
                        ],
                        [
                            "tf.concat([x, zeros], axis=1)",
                            "Call"
                        ],
                        [
                            "tf.strided_slice(x, [0, 0], [tf.shape(x)[0], y.shape[1].value])",
                            "Call"
                        ],
                        [
                            "tf.zeros([1, num_features(game)])",
                            "Call"
                        ],
                        [
                            "layer(x)",
                            "Call"
                        ]
                    ]
                },
                "begin": {
                    "value": "[0, 0]",
                    "type": "List",
                    "possible_values": []
                },
                "end": {
                    "value": "[tf.shape(x)[0], y.shape[1].value]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "Dense_599": {
                "units": {
                    "value": "num_hidden_units",
                    "type": "variable",
                    "possible_values": []
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "activation": {
                    "value": "hidden_activation",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.nn.relu",
                            "Method Argument"
                        ]
                    ]
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "squeeze_671": {
                "input": {
                    "value": "self._models[player](self._root_wrapper.sequence_features[player])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "shape_98": {
                "input": {
                    "value": "state_features",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tensor_to_matrix(state_features)",
                            "Call"
                        ]
                    ]
                }
            },
            "Dense_593": {
                "units": {
                    "value": "num_hidden_factors",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0",
                            "Method Argument"
                        ]
                    ]
                },
                "use_bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "expand_dims_861": {
                "input": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.zeros(self.num_player_sequences[regret_player])",
                            "Call"
                        ],
                        [
                            "np.maximum(-relu(self._regret_targets[regret_player]), regrets)",
                            "Call"
                        ],
                        [
                            "np.maximum(-relu(sequence_weights[regret_player]), regrets)",
                            "Call"
                        ]
                    ]
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "stack_868": {
                "values": {
                    "value": "a",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "shape_539": {
                "input": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tensor_to_matrix(x)",
                            "Call"
                        ],
                        [
                            "y",
                            "variable"
                        ],
                        [
                            "tf.concat([x, zeros], axis=1)",
                            "Call"
                        ],
                        [
                            "tf.strided_slice(x, [0, 0], [tf.shape(x)[0], y.shape[1].value])",
                            "Call"
                        ],
                        [
                            "tf.zeros([1, num_features(game)])",
                            "Call"
                        ],
                        [
                            "layer(x)",
                            "Call"
                        ]
                    ]
                }
            },
            "shape_542": {
                "input": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tensor_to_matrix(x)",
                            "Call"
                        ],
                        [
                            "y",
                            "variable"
                        ],
                        [
                            "tf.concat([x, zeros], axis=1)",
                            "Call"
                        ],
                        [
                            "tf.strided_slice(x, [0, 0], [tf.shape(x)[0], y.shape[1].value])",
                            "Call"
                        ],
                        [
                            "tf.zeros([1, num_features(game)])",
                            "Call"
                        ],
                        [
                            "layer(x)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/algorithms/rcfr_test.py": {
        "tensorflow": {
            "disable_v2_behavior_28": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "enable_eager_execution_30": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "ones_94": {
                "variable": {
                    "value": "info_state_features",
                    "type": "variable",
                    "possible_values": []
                },
                "shape": {
                    "value": "[1, 1, 1]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "set_random_seed_49": {
                "seed": {
                    "value": "42",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Adam_479": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Adam_507": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Adam_568": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "expand_dims_474": {
                "input": {
                    "value": "cumulative_regrets[regret_player]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "huber_loss_483": {
                "labels": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "predictions": {
                    "value": "models[regret_player](x)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "huber_loss_511": {
                "labels": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "predictions": {
                    "value": "model(x)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "huber_loss_572": {
                "labels": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "predictions": {
                    "value": "model(x)",
                    "type": "Call",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/breakthrough_dqn.py": {
        "tensorflow": {
            "Session_92": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_105": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/deep_cfr.py": {
        "tensorflow": {
            "disable_v2_behavior_30": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_42": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_57": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/eva.py": {
        "tensorflow": {
            "Session_53": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_71": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/exploitability_descent.py": {
        "tensorflow": {
            "disable_v2_behavior_42": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "constant_67": {
                "variable": {
                    "value": "layer",
                    "type": "variable",
                    "possible_values": []
                },
                "value": {
                    "value": "loss_calculator.tabular_policy.state_in",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float64",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "dense_74": {
                "variable": {
                    "value": "layer",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "layer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(loss_calculator.tabular_policy.state_in, tf.float64)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(layer, num_hidden, activation=tf.nn.relu, kernel_regularizer=regularizer)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(layer, game.num_distinct_actions(), kernel_regularizer=regularizer)",
                            "Call"
                        ]
                    ]
                },
                "units": {
                    "value": "game.num_distinct_actions()",
                    "type": "Call",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.keras.regularizers.l2(l=FLAGS.regularizer_scale)",
                            "Call"
                        ],
                        [
                            "tf.keras.regularizers.l2(l=FLAGS.regularizer_scale)",
                            "Call"
                        ]
                    ]
                }
            },
            "placeholder_83": {
                "variable": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float64",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shape": {
                    "value": "()",
                    "type": "Tuple",
                    "possible_values": []
                },
                "name": {
                    "value": "learning_rate",
                    "type": "str",
                    "possible_values": []
                }
            },
            "GradientDescentOptimizer_84": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "learning_rate": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.placeholder(tf.float64, (), name='learning_rate')",
                            "Call"
                        ]
                    ]
                }
            },
            "dense_70": {
                "variable": {
                    "value": "layer",
                    "type": "variable",
                    "possible_values": []
                },
                "inputs": {
                    "value": "layer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.constant(loss_calculator.tabular_policy.state_in, tf.float64)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(layer, num_hidden, activation=tf.nn.relu, kernel_regularizer=regularizer)",
                            "Call"
                        ],
                        [
                            "tf.layers.dense(layer, game.num_distinct_actions(), kernel_regularizer=regularizer)",
                            "Call"
                        ]
                    ]
                },
                "units": {
                    "value": "num_hidden",
                    "type": "variable",
                    "possible_values": [
                        [
                            "FLAGS.num_hidden",
                            "Attribute"
                        ]
                    ]
                },
                "activation": {
                    "value": "tf.nn.relu",
                    "type": "Attribute",
                    "possible_values": []
                },
                "kernel_regularizer": {
                    "value": "regularizer",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.keras.regularizers.l2(l=FLAGS.regularizer_scale)",
                            "Call"
                        ],
                        [
                            "tf.keras.regularizers.l2(l=FLAGS.regularizer_scale)",
                            "Call"
                        ]
                    ]
                }
            },
            "get_regularization_loss_80": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "MonitoredTrainingSession_88": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/kuhn_nfsp.py": {
        "tensorflow": {
            "Session_90": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_99": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/kuhn_policy_gradient.py": {
        "tensorflow": {
            "Session_71": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_84": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/leduc_nfsp.py": {
        "tensorflow": {
            "Session_138": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_146": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/lewis_signaling_dqn.py": {
        "tensorflow": {
            "Session_100": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/neurd_example.py": {
        "tensorflow": {
            "enable_eager_execution_30": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/psro_v2_example.py": {
        "tensorflow": {
            "Session_303": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_310": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/rcfr_example.py": {
        "tensorflow": {
            "enable_eager_execution_24": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_90": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "lr": {
                    "value": "FLAGS.step_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "huber_loss_96": {
                "labels": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "predictions": {
                    "value": "model(x)",
                    "type": "Call",
                    "possible_values": []
                },
                "delta": {
                    "value": "0.01",
                    "type": "float",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/rl_response.py": {
        "tensorflow": {
            "set_random_seed_176": {
                "seed": {
                    "value": "FLAGS.seed",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Session_205": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_211": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/single_agent_catch.py": {
        "tensorflow": {
            "Session_58": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_101": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/skat_dqn.py": {
        "tensorflow": {
            "FileWriter_98": {
                "variable": {
                    "value": "summary_writer",
                    "type": "variable",
                    "possible_values": []
                },
                "logdir": {
                    "value": "summaries_dir",
                    "type": "variable",
                    "possible_values": [
                        [
                            "os.path.join(FLAGS.checkpoint_dir, 'random_eval')",
                            "Call"
                        ]
                    ]
                },
                "graph": {
                    "value": "tf.get_default_graph()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Saver_112": {
                "variable": {
                    "value": "saver",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Session_96": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "get_default_graph_99": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_113": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Summary_121": {
                "variable": {
                    "value": "summary",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/examples/tic_tac_toe_dqn_vs_tabular.py": {
        "tensorflow": {
            "Session_101": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "global_variables_initializer_113": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/jax/deep_cfr.py": {
        "tensorflow": {
            "Example_353": {
                "variable": {
                    "value": "example",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "tf.train.Features(feature={'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'samp_regret': tf.train.Feature(float_list=tf.train.FloatList(value=samp_regret)), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))})",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "parse_example_373": {
                "variable": {
                    "value": "tups",
                    "type": "variable",
                    "possible_values": []
                },
                "serialized": {
                    "value": "serialized",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "self._advantage_feature_description",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Example_380": {
                "variable": {
                    "value": "example",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "tf.train.Features(feature={'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'action_probs': tf.train.Feature(float_list=tf.train.FloatList(value=strategy_action_probs)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))})",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "parse_example_401": {
                "variable": {
                    "value": "tups",
                    "type": "variable",
                    "possible_values": []
                },
                "serialized": {
                    "value": "serialized",
                    "type": "variable",
                    "possible_values": []
                },
                "features": {
                    "value": "self._strategy_feature_description",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_322": {
                "shape": {
                    "value": "[self._embedding_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_323": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_324": {
                "shape": {
                    "value": "[1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_325": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_328": {
                "shape": {
                    "value": "[self._embedding_size]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_329": {
                "shape": {
                    "value": "[1]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_330": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "FixedLenFeature_331": {
                "shape": {
                    "value": "[self._num_actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dtype": {
                    "value": "tf.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Features_354": {
                "feature": {
                    "value": "{'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'samp_regret': tf.train.Feature(float_list=tf.train.FloatList(value=samp_regret)), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))}",
                    "type": "Dict",
                    "possible_values": []
                }
            },
            "Features_381": {
                "feature": {
                    "value": "{'info_state': tf.train.Feature(float_list=tf.train.FloatList(value=info_state)), 'action_probs': tf.train.Feature(float_list=tf.train.FloatList(value=strategy_action_probs)), 'iteration': tf.train.Feature(float_list=tf.train.FloatList(value=[iteration])), 'legal_actions': tf.train.Feature(float_list=tf.train.FloatList(value=legal_actions_mask))}",
                    "type": "Dict",
                    "possible_values": []
                }
            },
            "Feature_357": {
                "float_list": {
                    "value": "tf.train.FloatList(value=info_state)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_360": {
                "float_list": {
                    "value": "tf.train.FloatList(value=[iteration])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_363": {
                "float_list": {
                    "value": "tf.train.FloatList(value=samp_regret)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_366": {
                "float_list": {
                    "value": "tf.train.FloatList(value=legal_actions_mask)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_384": {
                "float_list": {
                    "value": "tf.train.FloatList(value=info_state)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_387": {
                "float_list": {
                    "value": "tf.train.FloatList(value=strategy_action_probs)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_391": {
                "float_list": {
                    "value": "tf.train.FloatList(value=[iteration])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Feature_394": {
                "float_list": {
                    "value": "tf.train.FloatList(value=legal_actions_mask)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "FloatList_358": {
                "value": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": [
                        [
                            "jnp.array(state.information_state_tensor(player), dtype=jnp.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_361": {
                "value": {
                    "value": "[iteration]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "FloatList_364": {
                "value": {
                    "value": "samp_regret",
                    "type": "variable",
                    "possible_values": [
                        [
                            "(exp_payoff - ev) * state.legal_actions_mask(player)",
                            "BinOp"
                        ]
                    ]
                }
            },
            "FloatList_367": {
                "value": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "jnp.array(state.legal_actions_mask(player), dtype=jnp.float32)",
                            "Call"
                        ],
                        [
                            "jnp.array(state.legal_actions_mask(cur_player), dtype=jnp.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_385": {
                "value": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": [
                        [
                            "jnp.array(state.information_state_tensor(player), dtype=jnp.float32)",
                            "Call"
                        ]
                    ]
                }
            },
            "FloatList_388": {
                "value": {
                    "value": "strategy_action_probs",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "FloatList_392": {
                "value": {
                    "value": "[iteration]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "FloatList_395": {
                "value": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "jnp.array(state.legal_actions_mask(player), dtype=jnp.float32)",
                            "Call"
                        ],
                        [
                            "jnp.array(state.legal_actions_mask(cur_player), dtype=jnp.float32)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/simple_nets.py": {
        "tensorflow": {
            "disable_v2_behavior_21": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Variable_48": {
                "variable": {
                    "value": "self._weights",
                    "type": "Attribute",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "tf.random.truncated_normal([in_size, out_size], mean=0.0, stddev=stddev)",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "weights",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Variable_52": {
                "variable": {
                    "value": "self._bias",
                    "type": "Attribute",
                    "possible_values": []
                },
                "initial_value": {
                    "value": "tf.zeros([out_size])",
                    "type": "Call",
                    "possible_values": []
                },
                "name": {
                    "value": "bias",
                    "type": "str",
                    "possible_values": []
                }
            },
            "truncated_normal_49": {
                "shape": {
                    "value": "[in_size, out_size]",
                    "type": "List",
                    "possible_values": []
                },
                "mean": {
                    "value": "0.0",
                    "type": "float",
                    "possible_values": []
                },
                "stddev": {
                    "value": "stddev",
                    "type": "variable",
                    "possible_values": [
                        [
                            "1.0 / math.sqrt(in_size)",
                            "BinOp"
                        ]
                    ]
                }
            },
            "zeros_52": {
                "shape": {
                    "value": "[out_size]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "matmul_55": {
                "a": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "layer(tensor)",
                            "Call"
                        ]
                    ]
                },
                "b": {
                    "value": "self._weights",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "relu_56": {
                "features": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tf.matmul(tensor, self._weights) + self._bias",
                            "BinOp"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/deep_cfr.py": {
        "torch": {
            "linear_76": {
                "variable": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "weight": {
                    "value": "self._weight",
                    "type": "Attribute",
                    "possible_values": []
                },
                "bias": {
                    "value": "self._bias",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Parameter_88": {
                "variable": {
                    "value": "self._weight",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.Tensor(stats.truncnorm.rvs(lower, upper, loc=mean, scale=stddev, size=[self._out_size, self._in_size]))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Parameter_96": {
                "variable": {
                    "value": "self._bias",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.zeros([self._out_size])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "ModuleList_129": {
                "variable": {
                    "value": "self.model",
                    "type": "Attribute",
                    "possible_values": []
                },
                "modules": {
                    "value": "self._layers",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Softmax_269": {
                "variable": {
                    "value": "self._policy_sm",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "MSELoss_270": {
                "variable": {
                    "value": "self._loss_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_271": {
                "variable": {
                    "value": "self._optimizer_policy",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._policy_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "MSELoss_282": {
                "variable": {
                    "value": "self._loss_advantages",
                    "type": "Attribute",
                    "possible_values": []
                },
                "reduction": {
                    "value": "mean",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Adam_304": {
                "variable": {
                    "value": "self._optimizer_advantages[player]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "params": {
                    "value": "self._advantage_networks[player].parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "self._learning_rate",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "relu_77": {
                "input": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.linear(tensor, self._weight, self._bias)",
                            "Call"
                        ]
                    ]
                }
            },
            "zeros_96": {
                "*size": {
                    "value": "[self._out_size]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "no_grad_405": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "no_grad_432": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_286": {
                "params": {
                    "value": "self._advantage_networks[p].parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/deep_cfr_pytorch_test.py": {
        "torch": {
            "manual_seed_69": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/dqn.py": {
        "torch": {
            "Parameter_60": {
                "variable": {
                    "value": "self._weight",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.Tensor(stats.truncnorm.rvs(lower, upper, loc=mean, scale=stddev, size=[out_size, in_size]))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Parameter_65": {
                "variable": {
                    "value": "self._bias",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.zeros([out_size])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "linear_68": {
                "variable": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "weight": {
                    "value": "self._weight",
                    "type": "Attribute",
                    "possible_values": []
                },
                "bias": {
                    "value": "self._bias",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "ModuleList_102": {
                "variable": {
                    "value": "self.model",
                    "type": "Attribute",
                    "possible_values": []
                },
                "modules": {
                    "value": "self._layers",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Tensor_318": {
                "variable": {
                    "value": "rewards",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_319": {
                "variable": {
                    "value": "next_info_states",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_320": {
                "variable": {
                    "value": "are_final_steps",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_321": {
                "variable": {
                    "value": "legal_actions_mask",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "stack_334": {
                "variable": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[torch.arange(self._q_values.shape[0], dtype=torch.long), actions]",
                    "type": "List",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Adam_183": {
                "variable": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._q_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "Tensor_284": {
                "variable": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "save_415": {
                "obj": {
                    "value": "self._q_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "f": {
                    "value": "data_path",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "load_430": {
                "f": {
                    "value": "self._q_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "map_location": {
                    "value": "data_path",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "load_431": {
                "f": {
                    "value": "self._target_q_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "map_location": {
                    "value": "data_path",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_61": {},
            "zeros_65": {
                "*size": {
                    "value": "[out_size]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "relu_69": {
                "input": {
                    "value": "y",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.linear(tensor, self._weight, self._bias)",
                            "Call"
                        ]
                    ]
                }
            },
            "SGD_186": {
                "variable": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._q_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "max_330": {
                "input": {
                    "value": "legal_target_q_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._target_q_values.masked_fill(illegal_actions_mask, ILLEGAL_ACTION_LOGITS_PENALTY)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "save_417": {
                "obj": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "f": {
                    "value": "optimizer_data_path",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ],
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "load_433": {
                "f": {
                    "value": "self._optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "map_location": {
                    "value": "optimizer_data_path",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ],
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "argmax_287": {
                "input": {
                    "value": "legal_q_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "q_values[legal_actions]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "arange_335": {
                "start": {
                    "value": "self._q_values.shape[0]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.long",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "no_grad_397": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "randn_399": {
                "*size": {
                    "value": "q_model.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "randn_401": {
                "*size": {
                    "value": "tq_model.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/dqn_pytorch_test.py": {
        "torch": {
            "manual_seed_136": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24261711",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/eva.py": {
        "torch": {
            "Tensor_249": {
                "variable": {
                    "value": "self._info_state",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Tensor_334": {
                "variable": {
                    "value": "self._agent.info_state",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Tensor_325": {
                "variable": {
                    "value": "self._agent.info_state",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/eva_pytorch_test.py": {
        "torch": {
            "manual_seed_104": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/losses/rl_losses.py": {
        "torch": {
            "softmax_62": {
                "variable": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "sum_68": {
                "variable": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "F.relu(action_values - torch.unsqueeze(baseline, 1))",
                    "type": "Call",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "softmax_80": {
                "variable": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "cross_entropy_102": {
                "variable": {
                    "value": "cross_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "target": {
                    "value": "actions",
                    "type": "variable",
                    "possible_values": []
                },
                "reduction": {
                    "value": "none",
                    "type": "str",
                    "possible_values": []
                }
            },
            "sum_56": {
                "input": {
                    "value": "torch.mul(policy, action_values.detach())",
                    "type": "Call",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "relu_88": {
                "variable": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - torch.unsqueeze(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "F.relu(advantages)",
                            "Call"
                        ],
                        [
                            "threshold_fn(policy_logits, advantages)",
                            "Call"
                        ],
                        [
                            "advantages.detach()",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, threshold_fn=thresholded)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "sum_98": {
                "input": {
                    "value": "policy_advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "-torch.mul(policy_logits, advantages.detach())",
                            "UnaryOp"
                        ],
                        [
                            "-torch.mul(policy, advantages.detach())",
                            "UnaryOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mul_107": {
                "input": {
                    "value": "cross_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.cross_entropy(policy_logits, actions, reduction='none')",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - torch.unsqueeze(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "F.relu(advantages)",
                            "Call"
                        ],
                        [
                            "threshold_fn(policy_logits, advantages)",
                            "Call"
                        ],
                        [
                            "advantages.detach()",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, threshold_fn=thresholded)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                }
            },
            "sum_111": {
                "input": {
                    "value": "-F.softmax(policy_logits, dim=1) * F.log_softmax(policy_logits, dim=1)",
                    "type": "BinOp",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "mean_136": {
                "variable": {
                    "value": "total_adv",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - torch.unsqueeze(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "F.relu(advantages)",
                            "Call"
                        ],
                        [
                            "threshold_fn(policy_logits, advantages)",
                            "Call"
                        ],
                        [
                            "advantages.detach()",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, threshold_fn=thresholded)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_168": {
                "variable": {
                    "value": "total_adv",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - torch.unsqueeze(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "F.relu(advantages)",
                            "Call"
                        ],
                        [
                            "threshold_fn(policy_logits, advantages)",
                            "Call"
                        ],
                        [
                            "advantages.detach()",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, threshold_fn=thresholded)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_199": {
                "variable": {
                    "value": "total_adv",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "advantages",
                    "type": "variable",
                    "possible_values": [
                        [
                            "action_values - torch.unsqueeze(baseline, 1)",
                            "BinOp"
                        ],
                        [
                            "F.relu(advantages)",
                            "Call"
                        ],
                        [
                            "threshold_fn(policy_logits, advantages)",
                            "Call"
                        ],
                        [
                            "advantages.detach()",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, threshold_fn=thresholded)",
                            "Call"
                        ],
                        [
                            "compute_advantages(policy_logits, action_values, use_relu=True)",
                            "Call"
                        ],
                        [
                            "returns - baseline",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_230": {
                "variable": {
                    "value": "total_regret",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.sum(F.relu(action_values - torch.unsqueeze(baseline, 1)), dim=1)",
                            "Call"
                        ],
                        [
                            "compute_regrets(policy_logits, action_values)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_265": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "policy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_a2c_loss(policy_logits, actions, advantages)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mul_56": {
                "input": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.softmax(policy_logits, dim=1)",
                            "Call"
                        ],
                        [
                            "F.softmax(policy_logits, dim=1)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "action_values.detach()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "relu_69": {
                "input": {
                    "value": "action_values - torch.unsqueeze(baseline, 1)",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "unsqueeze_86": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ],
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_140": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_141": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "other": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "add_142": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "torch.mean(policy_loss, dim=0)",
                            "Call"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_172": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_173": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "other": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "add_174": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "torch.mean(policy_loss, dim=0)",
                            "Call"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_203": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_204": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "other": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "add_205": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "torch.mean(policy_loss, dim=0)",
                            "Call"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_234": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_235": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "other": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "add_236": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "torch.mean(policy_loss, dim=0)",
                            "Call"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_267": {
                "variable": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "compute_entropy(policy_logits)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_268": {
                "variable": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "float(self._entropy_cost)",
                    "type": "Call",
                    "possible_values": []
                },
                "other": {
                    "value": "policy_entropy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ],
                        [
                            "torch.mean(compute_entropy(policy_logits))",
                            "Call"
                        ]
                    ]
                }
            },
            "add_269": {
                "variable": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "total_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_adv",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "total_regret",
                            "variable"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ],
                        [
                            "torch.mean(policy_loss, dim=0)",
                            "Call"
                        ],
                        [
                            "torch.add(total_loss, entropy_loss)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "entropy_loss",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ],
                        [
                            "torch.mul(float(self._entropy_cost), policy_entropy)",
                            "Call"
                        ]
                    ]
                }
            },
            "mul_94": {
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "other": {
                    "value": "advantages.detach()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "mul_97": {
                "input": {
                    "value": "policy",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.softmax(policy_logits, dim=1)",
                            "Call"
                        ],
                        [
                            "F.softmax(policy_logits, dim=1)",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "advantages.detach()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "log_softmax_112": {
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "unsqueeze_69": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ],
                        [
                            "compute_baseline(policy, action_values)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "softmax_112": {
                "input": {
                    "value": "policy_logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "policy_logits - policy_logits.mean(-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/losses/rl_losses_pytorch_test.py": {
        "torch": {
            "manual_seed_101": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/neurd.py": {
        "torch": {
            "gt_36": {
                "variable": {
                    "value": "can_decrease",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - torch.mean(logits)",
                            "BinOp"
                        ]
                    ]
                },
                "other": {
                    "value": "-threshold",
                    "type": "UnaryOp",
                    "possible_values": []
                }
            },
            "lt_37": {
                "variable": {
                    "value": "can_increase",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - torch.mean(logits)",
                            "BinOp"
                        ]
                    ]
                },
                "other": {
                    "value": "threshold",
                    "type": "variable",
                    "possible_values": [
                        [
                            "2.0",
                            "Method Argument"
                        ],
                        [
                            "2.0",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "minimum_38": {
                "variable": {
                    "value": "regrets_negative",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "thresholded(logits, regrets, threshold=threshold).detach()",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "torch.Tensor([0.0])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "maximum_39": {
                "variable": {
                    "value": "regrets_positive",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "regrets",
                    "type": "variable",
                    "possible_values": [
                        [
                            "thresholded(logits, regrets, threshold=threshold).detach()",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "torch.Tensor([0.0])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "DataLoader_50": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.TensorDataset(player_seq_features[regret_player], targets)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "batch_size",
                    "type": "variable",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "mean_58": {
                "variable": {
                    "value": "utility",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "logits * regrets",
                    "type": "BinOp",
                    "possible_values": []
                }
            },
            "ModuleList_112": {
                "variable": {
                    "value": "self.layers",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "squeeze_190": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._models[player](self._root_wrapper.sequence_features[player])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "exp_193": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                            "Call"
                        ],
                        [
                            "tensor - torch.max(tensor, dim=0)[0]",
                            "BinOp"
                        ],
                        [
                            "torch.exp(tensor)",
                            "Call"
                        ]
                    ]
                }
            },
            "unsqueeze_245": {
                "variable": {
                    "value": "targets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.Tensor(regrets)",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "TensorDataset_246": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "*tensors": {
                    "value": "player_seq_features[regret_player]",
                    "type": "Subscript",
                    "possible_values": []
                }
            },
            "mean_55": {
                "input": {
                    "value": "logits",
                    "type": "variable",
                    "possible_values": [
                        [
                            "output[:, :1]",
                            "Subscript"
                        ],
                        [
                            "logits - torch.mean(logits)",
                            "BinOp"
                        ]
                    ]
                }
            },
            "no_grad_64": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Linear_127": {
                "in_features": {
                    "value": "self.input_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1 + self._autoencode * rcfr.num_features(game)",
                    "type": "BinOp",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Linear_121": {
                "in_features": {
                    "value": "self.input_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_features": {
                    "value": "num_hidden_units",
                    "type": "variable",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Linear_117": {
                "in_features": {
                    "value": "self.input_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_features": {
                    "value": "num_hidden_factors",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0",
                            "Method Argument"
                        ]
                    ]
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "max_192": {
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                            "Call"
                        ],
                        [
                            "tensor - torch.max(tensor, dim=0)[0]",
                            "BinOp"
                        ],
                        [
                            "torch.exp(tensor)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/neurd_pytorch_test.py": {
        "torch": {
            "manual_seed_39": {
                "seed": {
                    "value": "42",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/nfsp.py": {
        "torch": {
            "softmax_131": {
                "variable": {
                    "value": "action_probs",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "action_values",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._avg_network(torch.Tensor(info_state))",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Tensor_232": {
                "variable": {
                    "value": "action_probs",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "cross_entropy_235": {
                "variable": {
                    "value": "loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._avg_network(info_states)",
                    "type": "Call",
                    "possible_values": []
                },
                "target": {
                    "value": "torch.max(action_probs, dim=1)[1]",
                    "type": "Subscript",
                    "possible_values": []
                }
            },
            "Adam_104": {
                "variable": {
                    "value": "self.optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._avg_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "sl_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "SGD_107": {
                "variable": {
                    "value": "self.optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._avg_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "sl_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.01",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "Tensor_130": {},
            "save_260": {
                "obj": {
                    "value": "model.state_dict()",
                    "type": "Call",
                    "possible_values": []
                },
                "f": {
                    "value": "path",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ],
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ]
                    ]
                }
            },
            "max_236": {
                "input": {
                    "value": "action_probs",
                    "type": "variable",
                    "possible_values": [
                        [
                            "F.softmax(action_values, dim=1).detach()",
                            "Call"
                        ],
                        [
                            "torch.Tensor([t.action_probs for t in transitions])",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "load_282": {
                "f": {
                    "value": "full_checkpoint_dir",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/nfsp_pytorch_test.py": {
        "torch": {
            "manual_seed_91": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/policy_gradient.py": {
        "torch": {
            "ModuleList_110": {
                "variable": {
                    "value": "self.model",
                    "type": "Attribute",
                    "possible_values": []
                },
                "modules": {
                    "value": "self._layers",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Sequential_204": {
                "variable": {
                    "value": "self.policy_logits_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "self._net_torso",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Sequential_233": {
                "variable": {
                    "value": "self._pi_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "self._net_torso",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Tensor_265": {
                "variable": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "softmax_268": {
                "variable": {
                    "value": "policy_probs",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._policy_logits",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Tensor_412": {
                "variable": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_414": {
                "variable": {
                    "value": "return_",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_443": {
                "variable": {
                    "value": "info_state",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_445": {
                "variable": {
                    "value": "return_",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Sequential_220": {
                "variable": {
                    "value": "self._critic_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "self._net_torso",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Sequential_225": {
                "variable": {
                    "value": "self._critic_network",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "self._net_torso",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Adam_235": {
                "variable": {
                    "value": "self._pi_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._pi_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "pi_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.001",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "squeeze_420": {
                "variable": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._baseline_layer(torso_out)",
                    "type": "Call",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_421": {
                "variable": {
                    "value": "critic_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "F.mse_loss(baseline, return_)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "stack_427": {
                "variable": {
                    "value": "action_indices",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[torch.arange(q_values.shape[0], dtype=torch.long), action]",
                    "type": "List",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "mean_430": {
                "variable": {
                    "value": "critic_loss",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "F.mse_loss(value_predictions, return_)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "squeeze_450": {
                "variable": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "self._baseline_layer(torso_out)",
                    "type": "Call",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "SGD_238": {
                "variable": {
                    "value": "self._pi_optimizer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self._pi_network.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "pi_learning_rate",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0.001",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "clip_grad_norm__259": {
                "parameters": {
                    "value": "model.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "max_norm": {
                    "value": "self._max_global_gradient_norm",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "save_342": {
                "obj": {
                    "value": "model.state_dict()",
                    "type": "Call",
                    "possible_values": []
                },
                "f": {
                    "value": "path",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ],
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ]
                    ]
                }
            },
            "load_356": {
                "f": {
                    "value": "full_checkpoint_dir",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self._full_checkpoint_name(checkpoint_dir, name)",
                            "Call"
                        ]
                    ]
                }
            },
            "mse_loss_421": {
                "input": {
                    "value": "baseline",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.squeeze(self._baseline_layer(torso_out), dim=1)",
                            "Call"
                        ],
                        [
                            "torch.squeeze(self._baseline_layer(torso_out), dim=1)",
                            "Call"
                        ]
                    ]
                },
                "target": {
                    "value": "return_",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.Tensor(self._dataset['returns'])",
                            "Call"
                        ],
                        [
                            "torch.Tensor(self._dataset['returns'])",
                            "Call"
                        ]
                    ]
                }
            },
            "mse_loss_430": {
                "input": {
                    "value": "value_predictions",
                    "type": "variable",
                    "possible_values": [
                        [
                            "q_values[list(action_indices)]",
                            "Subscript"
                        ]
                    ]
                },
                "target": {
                    "value": "return_",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.Tensor(self._dataset['returns'])",
                            "Call"
                        ],
                        [
                            "torch.Tensor(self._dataset['returns'])",
                            "Call"
                        ]
                    ]
                }
            },
            "no_grad_500": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "arange_428": {
                "start": {
                    "value": "q_values.shape[0]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.long",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "randn_505": {
                "*size": {
                    "value": "policy_logits_layer.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "randn_502": {
                "*size": {
                    "value": "layer.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "randn_509": {
                "*size": {
                    "value": "q_values_layer.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "randn_513": {
                "*size": {
                    "value": "baseline_layer.weight.shape",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "open_spiel/python/pytorch/policy_gradient_pytorch_test.py": {
        "torch": {
            "manual_seed_177": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            }
        }
    },
    "open_spiel/python/pytorch/rcfr.py": {
        "torch": {
            "one_hot_98": {
                "variable": {
                    "value": "action_features",
                    "type": "variable",
                    "possible_values": []
                },
                "tensor": {
                    "value": "torch.tensor([action])",
                    "type": "Call",
                    "possible_values": []
                },
                "num_classes": {
                    "value": "num_distinct_actions",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "cat_100": {
                "variable": {
                    "value": "all_features",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[state_features, action_features]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "cat_102": {
                "tensors": {
                    "value": "with_action_features",
                    "type": "variable",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "ModuleList_612": {
                "variable": {
                    "value": "self.layers",
                    "type": "Attribute",
                    "possible_values": []
                },
                "modules": {
                    "value": "self.layers",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "zeros_614": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "*size": {
                    "value": "[1, num_features(game)]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "reshape_68": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.Tensor(tensor)",
                            "Call"
                        ],
                        [
                            "torch.reshape(tensor, [1, num_columns])",
                            "Call"
                        ],
                        [
                            "F.relu(torch.squeeze(self._models[player](self._root_wrapper.sequence_features[player])))",
                            "Call"
                        ]
                    ]
                },
                "shape": {
                    "value": "[1, num_columns]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "relu_668": {
                "variable": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.squeeze(self._models[player](self._root_wrapper.sequence_features[player]))",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "unsqueeze_767": {
                "variable": {
                    "value": "targets",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.Tensor(self._regret_targets[regret_player])",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "TensorDataset_769": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "*tensors": {
                    "value": "player_seq_features[regret_player]",
                    "type": "Subscript",
                    "possible_values": []
                }
            },
            "TensorDataset_860": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "*tensors": {
                    "value": "*my_buffer",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "tensor_99": {
                "data": {
                    "value": "[action]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "cat_169": {
                "tensors": {
                    "value": "rows",
                    "type": "variable",
                    "possible_values": []
                },
                "axis": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "zeros_546": {
                "variable": {
                    "value": "zeros",
                    "type": "variable",
                    "possible_values": []
                },
                "*size": {
                    "value": "[x.shape[0], padding]",
                    "type": "List",
                    "possible_values": []
                }
            },
            "cat_547": {
                "variable": {
                    "value": "x",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[x, zeros]",
                    "type": "List",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "Linear_610": {
                "in_features": {
                    "value": "num_hidden_units",
                    "type": "variable",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "Linear_603": {
                "in_features": {
                    "value": "num_hidden_factors if self._hidden_are_factored else input_size",
                    "type": "IfExp",
                    "possible_values": []
                },
                "out_features": {
                    "value": "num_hidden_units",
                    "type": "variable",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "squeeze_669": {
                "input": {
                    "value": "self._models[player](self._root_wrapper.sequence_features[player])",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Linear_600": {
                "in_features": {
                    "value": "input_size",
                    "type": "variable",
                    "possible_values": [
                        [
                            "num_features(game)",
                            "Call"
                        ]
                    ]
                },
                "out_features": {
                    "value": "num_hidden_factors",
                    "type": "variable",
                    "possible_values": [
                        [
                            "0",
                            "Method Argument"
                        ]
                    ]
                },
                "bias": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "unsqueeze_851": {
                "input": {
                    "value": "torch.Tensor(regrets)",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "stack_858": {
                "tensors": {
                    "value": "a",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_851": {}
        }
    },
    "open_spiel/python/pytorch/rcfr_pytorch_test.py": {
        "torch": {
            "manual_seed_568": {
                "seed": {
                    "value": "SEED",
                    "type": "variable",
                    "possible_values": [
                        [
                            "24984617",
                            "int"
                        ]
                    ]
                }
            },
            "TensorDataset_447": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "*tensors": {
                    "value": "root.sequence_features[regret_player]",
                    "type": "Subscript",
                    "possible_values": []
                }
            },
            "DataLoader_451": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.utils.data.TensorDataset(root.sequence_features[regret_player], torch.unsqueeze(torch.Tensor(cumulative_regrets[regret_player]), axis=1))",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "_BATCH_SIZE",
                    "type": "variable",
                    "possible_values": [
                        [
                            "12",
                            "int"
                        ]
                    ]
                },
                "shuffle": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "SmoothL1Loss_454": {
                "variable": {
                    "value": "loss_fn",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_455": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "models[regret_player].parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "DataLoader_481": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.utils.data.TensorDataset(root.sequence_features[regret_player], torch.unsqueeze(torch.Tensor(cumulative_regrets[regret_player]), axis=1))",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "_BATCH_SIZE",
                    "type": "variable",
                    "possible_values": [
                        [
                            "12",
                            "int"
                        ]
                    ]
                },
                "shuffle": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "SmoothL1Loss_484": {
                "variable": {
                    "value": "loss_fn",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_485": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "model.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "DataLoader_544": {
                "variable": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "data",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.utils.data.TensorDataset(root.sequence_features[regret_player], torch.unsqueeze(torch.Tensor(cumulative_regrets[regret_player]), axis=1))",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ],
                        [
                            "torch.utils.data.DataLoader(data, batch_size=_BATCH_SIZE, shuffle=True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "_BATCH_SIZE",
                    "type": "variable",
                    "possible_values": [
                        [
                            "12",
                            "int"
                        ]
                    ]
                },
                "shuffle": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "SmoothL1Loss_547": {
                "variable": {
                    "value": "loss_fn",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "Adam_548": {
                "variable": {
                    "value": "optimizer",
                    "type": "variable",
                    "possible_values": []
                },
                "params": {
                    "value": "model.parameters()",
                    "type": "Call",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.005",
                    "type": "float",
                    "possible_values": []
                },
                "amsgrad": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            },
            "unsqueeze_449": {
                "input": {
                    "value": "torch.Tensor(cumulative_regrets[regret_player])",
                    "type": "Call",
                    "possible_values": []
                },
                "axis": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    }
}