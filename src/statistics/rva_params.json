{
    "evaluate.py": {
        "torch": {
            "DataLoader_152": {
                "variable": {
                    "value": "val_dataloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "val_dataset",
                    "possible_values": [
                        [
                            "VisDialDataset(config['dataset'], args.val_json, args.val_dense_json, overfit=args.overfit, in_memory=args.in_memory, return_options=True, add_boundary_toks=False if config['model']['decoder'] == 'disc' else True)",
                            "Call"
                        ],
                        [
                            "VisDialDataset(config['dataset'], args.test_json, overfit=args.overfit, in_memory=args.in_memory, return_options=True, add_boundary_toks=False if config['model']['decoder'] == 'disc' else True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "config['solver']['batch_size'] if config['model']['decoder'] == 'disc' else 5",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "args.cpu_workers",
                    "possible_values": []
                }
            },
            "manual_seed_97": {
                "seed": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "manual_seed_all_98": {
                "seed": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "DataParallel_172": {
                "variable": {
                    "value": "model",
                    "possible_values": []
                },
                "module": {
                    "value": "model",
                    "possible_values": [
                        [
                            "EncoderDecoderModel(encoder, decoder).to(device)",
                            "Call"
                        ],
                        [
                            "nn.DataParallel(model, args.gpu_ids)",
                            "Call"
                        ]
                    ]
                },
                "device_ids": {
                    "value": "args.gpu_ids",
                    "possible_values": []
                }
            },
            "device_114": {
                "type": {
                    "value": "cuda",
                    "possible_values": []
                },
                "index": {
                    "value": "args.gpu_ids[0]",
                    "possible_values": []
                }
            },
            "device_116": {
                "type": {
                    "value": "cpu",
                    "possible_values": []
                }
            },
            "no_grad_195": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "arange_227": {
                "start": {
                    "value": "output.size(0)",
                    "possible_values": []
                }
            }
        }
    },
    "train.py": {
        "torch": {
            "DataLoader_138": {
                "variable": {
                    "value": "train_dataloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "train_dataset",
                    "possible_values": [
                        [
                            "VisDialDataset(config['dataset'], args.train_json, overfit=args.overfit, in_memory=args.in_memory, return_options=True if config['model']['decoder'] == 'disc' else False, add_boundary_toks=False if config['model']['decoder'] == 'disc' else True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "config['solver']['batch_size']",
                    "possible_values": [
                        [
                            "yaml.load(open(args.config_yml))",
                            "Call"
                        ]
                    ]
                },
                "num_workers": {
                    "value": "args.cpu_workers",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "DataLoader_154": {
                "variable": {
                    "value": "val_dataloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "val_dataset",
                    "possible_values": [
                        [
                            "VisDialDataset(config['dataset'], args.val_json, args.val_dense_json, overfit=args.overfit, in_memory=args.in_memory, return_options=True, add_boundary_toks=False if config['model']['decoder'] == 'disc' else True)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "config['solver']['batch_size'] if config['model']['decoder'] == 'disc' else 5",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "args.cpu_workers",
                    "possible_values": []
                }
            },
            "Adamax_214": {
                "variable": {
                    "value": "optimizer",
                    "possible_values": []
                },
                "params": {
                    "value": "model.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "config['solver']['initial_lr']",
                    "possible_values": [
                        [
                            "yaml.load(open(args.config_yml))",
                            "Call"
                        ]
                    ]
                }
            },
            "LambdaLR_215": {
                "variable": {
                    "value": "scheduler",
                    "possible_values": []
                },
                "optimizer": {
                    "value": "optimizer",
                    "possible_values": [
                        [
                            "optim.Adamax(model.parameters(), lr=config['solver']['initial_lr'])",
                            "Call"
                        ]
                    ]
                },
                "lr_lambda": {
                    "value": "lr_lambda_fun",
                    "possible_values": []
                }
            },
            "manual_seed_97": {
                "seed": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "manual_seed_all_98": {
                "seed": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "from_numpy_170": {
                "variable": {
                    "value": "encoder.word_embed.weight.data",
                    "possible_values": []
                },
                "ndarray": {
                    "value": "np.load(config['dataset']['glove_npy'])",
                    "possible_values": []
                }
            },
            "DataParallel_179": {
                "variable": {
                    "value": "model",
                    "possible_values": []
                },
                "module": {
                    "value": "model",
                    "possible_values": [
                        [
                            "EncoderDecoderModel(encoder, decoder).to(device)",
                            "Call"
                        ],
                        [
                            "nn.DataParallel(model, args.gpu_ids)",
                            "Call"
                        ]
                    ]
                },
                "device_ids": {
                    "value": "args.gpu_ids",
                    "possible_values": []
                }
            },
            "CrossEntropyLoss_183": {
                "variable": {
                    "value": "criterion",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "device_115": {
                "type": {
                    "value": "cuda",
                    "possible_values": []
                },
                "index": {
                    "value": "args.gpu_ids[0]",
                    "possible_values": []
                }
            },
            "device_117": {
                "type": {
                    "value": "cpu",
                    "possible_values": []
                }
            },
            "CrossEntropyLoss_185": {
                "variable": {
                    "value": "criterion",
                    "possible_values": []
                },
                "ignore_index": {
                    "value": "train_dataset.vocabulary.PAD_INDEX",
                    "possible_values": []
                }
            },
            "empty_cache_293": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "empty_cache_344": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "no_grad_325": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "arange_330": {
                "start": {
                    "value": "output.size(0)",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/data/dataset.py": {
        "torch": {
            "VisDialDataset_16": {
                "base_class_0": {
                    "value": "torch.utils.data.Dataset",
                    "possible_values": []
                },
                "self.config": {
                    "value": "config",
                    "possible_values": []
                },
                "self.return_options": {
                    "value": "return_options",
                    "possible_values": []
                },
                "self.add_boundary_toks": {
                    "value": "add_boundary_toks",
                    "possible_values": []
                }
            },
            "tensor_79": {
                "variable": {
                    "value": "image_features",
                    "possible_values": []
                },
                "data": {
                    "value": "image_features",
                    "possible_values": [
                        [
                            "self.hdf_reader[image_id]",
                            "Subscript"
                        ],
                        [
                            "torch.tensor(image_features)",
                            "Call"
                        ],
                        [
                            "normalize(image_features, dim=0, p=2)",
                            "Call"
                        ]
                    ]
                }
            },
            "tensor_144": {
                "variable": {
                    "value": "item[img_ids]",
                    "possible_values": []
                },
                "data": {
                    "value": "image_id",
                    "possible_values": [
                        [
                            "self.image_ids[index]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "tensor_150": {
                "variable": {
                    "value": "item[ques_len]",
                    "possible_values": []
                },
                "data": {
                    "value": "question_lengths",
                    "possible_values": []
                }
            },
            "tensor_151": {
                "variable": {
                    "value": "item[hist_len]",
                    "possible_values": []
                },
                "data": {
                    "value": "history_lengths",
                    "possible_values": [
                        [
                            "[len(round_history) for round_history in history]",
                            "ListComp"
                        ]
                    ]
                }
            },
            "tensor_152": {
                "variable": {
                    "value": "item[ans_len]",
                    "possible_values": []
                },
                "data": {
                    "value": "answer_lengths",
                    "possible_values": []
                }
            },
            "tensor_153": {
                "variable": {
                    "value": "item[num_rounds]",
                    "possible_values": []
                },
                "data": {
                    "value": "visdial_instance['num_rounds']",
                    "possible_values": []
                }
            },
            "full_246": {
                "variable": {
                    "value": "maxpadded_sequences",
                    "possible_values": []
                },
                "size": {
                    "value": "(len(sequences), self.config['max_sequence_length'])",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.vocabulary.PAD_INDEX",
                    "possible_values": []
                }
            },
            "pad_sequence_250": {
                "variable": {
                    "value": "padded_sequences",
                    "possible_values": []
                },
                "sequences": {
                    "value": "[torch.tensor(sequence) for sequence in sequences]",
                    "possible_values": []
                },
                "batch_first": {
                    "value": "True",
                    "possible_values": []
                },
                "padding_value": {
                    "value": "self.vocabulary.PAD_INDEX",
                    "possible_values": []
                }
            },
            "full_301": {
                "variable": {
                    "value": "maxpadded_history",
                    "possible_values": []
                },
                "size": {
                    "value": "(len(history), max_history_length)",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.vocabulary.PAD_INDEX",
                    "possible_values": []
                }
            },
            "pad_sequence_305": {
                "variable": {
                    "value": "padded_history",
                    "possible_values": []
                },
                "sequences": {
                    "value": "[torch.tensor(round_history) for round_history in history]",
                    "possible_values": []
                },
                "batch_first": {
                    "value": "True",
                    "possible_values": []
                },
                "padding_value": {
                    "value": "self.vocabulary.PAD_INDEX",
                    "possible_values": []
                }
            },
            "normalize_83": {
                "variable": {
                    "value": "image_features",
                    "possible_values": []
                },
                "input": {
                    "value": "image_features",
                    "possible_values": [
                        [
                            "self.hdf_reader[image_id]",
                            "Subscript"
                        ],
                        [
                            "torch.tensor(image_features)",
                            "Call"
                        ],
                        [
                            "normalize(image_features, dim=0, p=2)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                },
                "p": {
                    "value": "2",
                    "possible_values": []
                }
            },
            "tensor_208": {
                "variable": {
                    "value": "item[gt_relevance]",
                    "possible_values": []
                },
                "data": {
                    "value": "dense_annotations['gt_relevance']",
                    "possible_values": []
                }
            },
            "tensor_211": {
                "variable": {
                    "value": "item[round_id]",
                    "possible_values": []
                },
                "data": {
                    "value": "dense_annotations['round_id']",
                    "possible_values": []
                }
            },
            "stack_179": {
                "variable": {
                    "value": "answer_options_in",
                    "possible_values": []
                },
                "tensors": {
                    "value": "answer_options_in",
                    "possible_values": [
                        [
                            "torch.stack(answer_options_in, 0)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "stack_180": {
                "variable": {
                    "value": "answer_options_out",
                    "possible_values": []
                },
                "tensors": {
                    "value": "answer_options_out",
                    "possible_values": [
                        [
                            "torch.stack(answer_options_out, 0)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "tensor_184": {
                "variable": {
                    "value": "item[opt_len]",
                    "possible_values": []
                },
                "data": {
                    "value": "answer_option_lengths",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "[]",
                            "List"
                        ]
                    ]
                }
            },
            "stack_194": {
                "variable": {
                    "value": "answer_options",
                    "possible_values": []
                },
                "tensors": {
                    "value": "answer_options",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "torch.stack(answer_options, 0)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "tensor_197": {
                "variable": {
                    "value": "item[opt_len]",
                    "possible_values": []
                },
                "data": {
                    "value": "answer_option_lengths",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "[]",
                            "List"
                        ]
                    ]
                }
            },
            "tensor_203": {
                "variable": {
                    "value": "item[ans_ind]",
                    "possible_values": []
                },
                "data": {
                    "value": "answer_indices",
                    "possible_values": [
                        [
                            "[dialog_round['gt_index'] for dialog_round in dialog]",
                            "ListComp"
                        ]
                    ]
                }
            },
            "tensor_251": {
                "data": {
                    "value": "sequence",
                    "possible_values": []
                }
            },
            "tensor_306": {
                "data": {
                    "value": "round_history",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/decoders/disc.py": {
        "torch": {
            "DiscriminativeDecoder_7": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.config": {
                    "value": "config",
                    "possible_values": []
                },
                "Embedding_12": {
                    "variable": {
                        "value": "self.word_embed",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "len(vocabulary)",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "vocabulary.PAD_INDEX",
                        "possible_values": []
                    }
                },
                "LSTM_17": {
                    "variable": {
                        "value": "self.option_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                }
            },
            "zeros_70": {
                "variable": {
                    "value": "options_embed",
                    "possible_values": []
                },
                "*size": {
                    "value": "batch_size * num_rounds * num_options",
                    "possible_values": []
                },
                "out": {
                    "value": "nonzero_options_embed.size(-1)",
                    "possible_values": []
                },
                "device": {
                    "value": "nonzero_options_embed.device",
                    "possible_values": []
                }
            },
            "sum_91": {
                "variable": {
                    "value": "scores",
                    "possible_values": []
                },
                "input": {
                    "value": "options_embed * encoder_output",
                    "possible_values": []
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/decoders/gen.py": {
        "torch": {
            "GenerativeDecoder_5": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.config": {
                    "value": "config",
                    "possible_values": []
                },
                "Embedding_10": {
                    "variable": {
                        "value": "self.word_embed",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "len(vocabulary)",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "vocabulary.PAD_INDEX",
                        "possible_values": []
                    }
                },
                "LSTM_15": {
                    "variable": {
                        "value": "self.answer_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                },
                "Linear_23": {
                    "variable": {
                        "value": "self.lstm_to_words",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "self.config['lstm_hidden_size']",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "len(vocabulary)",
                        "possible_values": []
                    }
                },
                "Dropout_27": {
                    "variable": {
                        "value": "self.dropout",
                        "possible_values": []
                    },
                    "p": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                },
                "LogSoftmax_28": {
                    "variable": {
                        "value": "self.logsoftmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "zeros_like_61": {
                "variable": {
                    "value": "init_cell",
                    "possible_values": []
                },
                "input": {
                    "value": "init_hidden",
                    "possible_values": [
                        [
                            "encoder_output.view(1, batch_size * num_rounds, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(self.config['lstm_num_layers'], 1, 1)",
                            "Call"
                        ],
                        [
                            "encoder_output.view(batch_size, num_rounds, 1, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(1, 1, num_options, 1)",
                            "Call"
                        ],
                        [
                            "init_hidden.view(1, batch_size * num_rounds * num_options, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(self.config['lstm_num_layers'], 1, 1)",
                            "Call"
                        ]
                    ]
                }
            },
            "zeros_like_101": {
                "variable": {
                    "value": "init_cell",
                    "possible_values": []
                },
                "input": {
                    "value": "init_hidden",
                    "possible_values": [
                        [
                            "encoder_output.view(1, batch_size * num_rounds, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(self.config['lstm_num_layers'], 1, 1)",
                            "Call"
                        ],
                        [
                            "encoder_output.view(batch_size, num_rounds, 1, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(1, 1, num_options, 1)",
                            "Call"
                        ],
                        [
                            "init_hidden.view(1, batch_size * num_rounds * num_options, -1)",
                            "Call"
                        ],
                        [
                            "init_hidden.repeat(self.config['lstm_num_layers'], 1, 1)",
                            "Call"
                        ]
                    ]
                }
            },
            "gather_121": {
                "variable": {
                    "value": "ans_word_scores",
                    "possible_values": []
                },
                "input": {
                    "value": "ans_word_scores",
                    "possible_values": [
                        [
                            "self.lstm_to_words(ans_out)",
                            "Call"
                        ],
                        [
                            "self.logsoftmax(self.lstm_to_words(ans_out))",
                            "Call"
                        ],
                        [
                            "torch.gather(ans_word_scores, -1, target_ans_out.unsqueeze(-1)).squeeze()",
                            "Call"
                        ],
                        [
                            "ans_word_scores * (target_ans_out > 0).float().cuda()",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                },
                "index": {
                    "value": "target_ans_out.unsqueeze(-1)",
                    "possible_values": []
                }
            },
            "squeeze_121": {
                "variable": {
                    "value": "ans_word_scores",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "sum_128": {
                "variable": {
                    "value": "ans_scores",
                    "possible_values": []
                },
                "input": {
                    "value": "ans_word_scores",
                    "possible_values": [
                        [
                            "self.lstm_to_words(ans_out)",
                            "Call"
                        ],
                        [
                            "self.logsoftmax(self.lstm_to_words(ans_out))",
                            "Call"
                        ],
                        [
                            "torch.gather(ans_word_scores, -1, target_ans_out.unsqueeze(-1)).squeeze()",
                            "Call"
                        ],
                        [
                            "ans_word_scores * (target_ans_out > 0).float().cuda()",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "-1",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/encoders/lf.py": {
        "torch": {
            "LateFusionEncoder_8": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.config": {
                    "value": "config",
                    "possible_values": []
                },
                "Embedding_13": {
                    "variable": {
                        "value": "self.word_embed",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "len(vocabulary)",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "vocabulary.PAD_INDEX",
                        "possible_values": []
                    }
                },
                "LSTM_18": {
                    "variable": {
                        "value": "self.hist_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                },
                "LSTM_25": {
                    "variable": {
                        "value": "self.ques_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                },
                "Dropout_32": {
                    "variable": {
                        "value": "self.dropout",
                        "possible_values": []
                    },
                    "p": {
                        "value": "config['dropout']",
                        "possible_values": []
                    }
                },
                "Linear_40": {
                    "variable": {
                        "value": "self.image_features_projection",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "config['img_feature_size']",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "config['lstm_hidden_size']",
                        "possible_values": []
                    }
                },
                "Linear_45": {
                    "variable": {
                        "value": "self.attention_proj",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "config['lstm_hidden_size']",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "1",
                        "possible_values": []
                    }
                },
                "Linear_51": {
                    "variable": {
                        "value": "self.fusion",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "fusion_size",
                        "possible_values": [
                            [
                                "config['img_feature_size'] + config['lstm_hidden_size'] * 2",
                                "BinOp"
                            ]
                        ]
                    },
                    "out_features": {
                        "value": "config['lstm_hidden_size']",
                        "possible_values": []
                    }
                }
            },
            "softmax_104": {
                "variable": {
                    "value": "image_attention_weights",
                    "possible_values": []
                },
                "input": {
                    "value": "image_attention_weights",
                    "possible_values": [
                        [
                            "self.attention_proj(projected_ques_image).squeeze()",
                            "Call"
                        ],
                        [
                            "F.softmax(image_attention_weights, dim=-1)",
                            "Call"
                        ],
                        [
                            "image_attention_weights.unsqueeze(-1).repeat(1, 1, self.config['img_feature_size'])",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "cat_129": {
                "variable": {
                    "value": "fused_vector",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(img, ques_embed, hist_embed)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "tanh_132": {
                "variable": {
                    "value": "fused_embedding",
                    "possible_values": []
                },
                "input": {
                    "value": "self.fusion(fused_vector)",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/encoders/modules.py": {
        "torch": {
            "ATT_MODULE_6": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_11": {
                    "variable": {
                        "value": "self.V_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_18": {
                    "variable": {
                        "value": "self.Q_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_25": {
                    "variable": {
                        "value": "self.att",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Softmax_33": {
                    "variable": {
                        "value": "self.softmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "PAIR_MODULE_68": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_73": {
                    "variable": {
                        "value": "self.H_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_79": {
                    "variable": {
                        "value": "self.Q_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_85": {
                    "variable": {
                        "value": "self.MLP",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Linear_96": {
                    "variable": {
                        "value": "self.att",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "2",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "1",
                        "possible_values": []
                    }
                }
            },
            "INFER_MODULE_144": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_149": {
                    "variable": {
                        "value": "self.embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_155": {
                    "variable": {
                        "value": "self.att",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Softmax_163": {
                    "variable": {
                        "value": "self.softmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "RvA_MODULE_197": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                }
            },
            "normalize_62": {
                "variable": {
                    "value": "att_embed",
                    "possible_values": []
                },
                "input": {
                    "value": "img_embed * ques_embed",
                    "possible_values": []
                },
                "p": {
                    "value": "2",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "cat_120": {
                "variable": {
                    "value": "att_embed",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(hist_embed, ques_embed)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "tril_123": {
                "variable": {
                    "value": "delta_t",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.ones(size=[num_rounds, num_rounds], requires_grad=False)",
                    "possible_values": []
                }
            },
            "cumsum_123": {
                "variable": {
                    "value": "delta_t",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "cat_126": {
                "variable": {
                    "value": "att_embed",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(score, delta_t)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "zeros_like_131": {
                "variable": {
                    "value": "hist_gs_set",
                    "possible_values": []
                },
                "input": {
                    "value": "hist_logits",
                    "possible_values": [
                        [
                            "self.att(att_embed).squeeze(-1)",
                            "Call"
                        ]
                    ]
                }
            },
            "normalize_182": {
                "variable": {
                    "value": "ques_embed",
                    "possible_values": []
                },
                "input": {
                    "value": "ques_embed",
                    "possible_values": [
                        [
                            "ques.view(-1, ques.size(-1))",
                            "Call"
                        ],
                        [
                            "self.Q_embed(ques_embed)",
                            "Call"
                        ],
                        [
                            "ques_embed.view(batch_size, num_rounds, ques_embed.size(-1))",
                            "Call"
                        ],
                        [
                            "ques_embed.unsqueeze(2).repeat(1, 1, num_proposals, 1)",
                            "Call"
                        ],
                        [
                            "self.Q_embed(ques)",
                            "Call"
                        ],
                        [
                            "ques_embed.unsqueeze(2).repeat(1, 1, num_rounds, 1)",
                            "Call"
                        ],
                        [
                            "self.embed(ques)",
                            "Call"
                        ],
                        [
                            "F.normalize(ques_embed, p=2, dim=-1)",
                            "Call"
                        ]
                    ]
                },
                "p": {
                    "value": "2",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "Tensor_226": {
                "variable": {
                    "value": "ques_prob_single",
                    "possible_values": []
                },
                "data": {
                    "value": "[1, 0]",
                    "possible_values": []
                }
            },
            "gumbel_softmax_187": {
                "variable": {
                    "value": "ques_gs",
                    "possible_values": []
                },
                "logits": {
                    "value": "logits",
                    "possible_values": [
                        [
                            "hist_logits[:, i, :i + 1]",
                            "Subscript"
                        ],
                        [
                            "ques_logits.view(-1, 2)",
                            "Call"
                        ]
                    ]
                },
                "hard": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "cat_238": {
                "variable": {
                    "value": "img_att_cat",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(img_att_ques[:, i, :].unsqueeze(1), img_att_temp.unsqueeze(1))",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_241": {
                "variable": {
                    "value": "ques_prob",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(ques_prob_single, ques_prob_pair)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "bmm_243": {
                "variable": {
                    "value": "ques_prob_refine",
                    "possible_values": []
                },
                "input": {
                    "value": "ques_gs[:, i, :].view(-1, 1, 2)",
                    "possible_values": []
                },
                "mat2": {
                    "value": "ques_prob",
                    "possible_values": [
                        [
                            "torch.cat((ques_prob_single, ques_prob_pair), dim=-1)",
                            "Call"
                        ],
                        [
                            "ques_prob.view(-1, 2, 2)",
                            "Call"
                        ]
                    ]
                }
            },
            "bmm_245": {
                "variable": {
                    "value": "img_att_refined[:, i, :]",
                    "possible_values": []
                },
                "input": {
                    "value": "ques_prob_refine",
                    "possible_values": [
                        [
                            "torch.bmm(ques_gs[:, i, :].view(-1, 1, 2), ques_prob).view(-1, 1, 2)",
                            "Call"
                        ]
                    ]
                },
                "mat2": {
                    "value": "img_att_cat",
                    "possible_values": [
                        [
                            "torch.cat((img_att_ques[:, i, :].unsqueeze(1), img_att_temp.unsqueeze(1)), dim=1)",
                            "Call"
                        ]
                    ]
                }
            },
            "Dropout_12": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_19": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_26": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_27": {
                "in_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "Dropout_74": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_80": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_86": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_87": {
                "in_features": {
                    "value": "config['lstm_hidden_size'] * 2",
                    "possible_values": []
                },
                "out_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                }
            },
            "Dropout_90": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_91": {
                "in_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "gumbel_softmax_136": {
                "variable": {
                    "value": "hist_gs",
                    "possible_values": []
                },
                "logits": {
                    "value": "logits",
                    "possible_values": [
                        [
                            "hist_logits[:, i, :i + 1]",
                            "Subscript"
                        ],
                        [
                            "ques_logits.view(-1, 2)",
                            "Call"
                        ]
                    ]
                },
                "hard": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "Dropout_150": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_156": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_157": {
                "in_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "2",
                    "possible_values": []
                }
            },
            "cat_236": {
                "variable": {
                    "value": "img_att_temp",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(img_att_cap, img_att_refined[:, :i, :])",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "sum_237": {
                "variable": {
                    "value": "img_att_temp",
                    "possible_values": []
                },
                "input": {
                    "value": "hist_gs.unsqueeze(-1) * img_att_temp",
                    "possible_values": []
                },
                "dim": {
                    "value": "-2",
                    "possible_values": []
                }
            },
            "ones_123": {
                "size": {
                    "value": "[num_rounds, num_rounds]",
                    "possible_values": []
                },
                "requires_grad": {
                    "value": "False",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/encoders/rva.py": {
        "torch": {
            "RvAEncoder_9": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.config": {
                    "value": "config",
                    "possible_values": []
                },
                "Embedding_14": {
                    "variable": {
                        "value": "self.word_embed",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "len(vocabulary)",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "vocabulary.PAD_INDEX",
                        "possible_values": []
                    }
                },
                "LSTM_20": {
                    "variable": {
                        "value": "self.hist_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    },
                    "bidirectional": {
                        "value": "True",
                        "possible_values": []
                    }
                },
                "LSTM_28": {
                    "variable": {
                        "value": "self.ques_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['word_embedding_size']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "config['dropout']",
                        "possible_values": []
                    },
                    "bidirectional": {
                        "value": "True",
                        "possible_values": []
                    }
                },
                "Sequential_52": {
                    "variable": {
                        "value": "self.fusion",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Softmax_60": {
                    "variable": {
                        "value": "self.softmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "bmm_101": {
                "variable": {
                    "value": "img_feat",
                    "possible_values": []
                },
                "input": {
                    "value": "img_att",
                    "possible_values": []
                },
                "mat2": {
                    "value": "img",
                    "possible_values": [
                        [
                            "batch['img_feat']",
                            "Subscript"
                        ]
                    ]
                }
            },
            "cat_113": {
                "variable": {
                    "value": "fused_vector",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(img_ans_feat, ques_ans_feat, hist_ans_feat)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "tanh_115": {
                "variable": {
                    "value": "fused_embedding",
                    "possible_values": []
                },
                "input": {
                    "value": "self.fusion(fused_vector)",
                    "possible_values": []
                }
            },
            "cat_136": {
                "variable": {
                    "value": "ques_encoded",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(ques_encoded_forawrd, ques_encoded_backward)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "cat_154": {
                "variable": {
                    "value": "hist_encoded",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(hist_encoded_forward, hist_encoded_backward)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "Dropout_53": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_54": {
                "in_features": {
                    "value": "config['img_feature_size'] + config['word_embedding_size'] + config['lstm_hidden_size'] * 2",
                    "possible_values": []
                },
                "out_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/metrics.py": {
        "torch": {
            "sum_128": {
                "variable": {
                    "value": "k",
                    "possible_values": []
                },
                "input": {
                    "value": "target_relevance != 0",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "sort_131": {
                "variable": {
                    "value": "(_, rankings)",
                    "possible_values": []
                },
                "input": {
                    "value": "predicted_ranks",
                    "possible_values": [
                        [
                            "scores_to_ranks(predicted_scores)",
                            "Call"
                        ],
                        [
                            "predicted_ranks.view(batch_size * num_rounds, num_options)",
                            "Call"
                        ],
                        [
                            "scores_to_ranks(predicted_scores)",
                            "Call"
                        ],
                        [
                            "predicted_ranks.squeeze()",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "sort_133": {
                "variable": {
                    "value": "(_, best_rankings)",
                    "possible_values": []
                },
                "input": {
                    "value": "target_relevance",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                },
                "descending": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "log2_156": {
                "variable": {
                    "value": "discounts",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.arange(len(rankings)).float() + 2",
                    "possible_values": []
                }
            },
            "tensor_78": {
                "variable": {
                    "value": "__rank_list",
                    "possible_values": []
                },
                "data": {
                    "value": "self._rank_list",
                    "possible_values": []
                }
            },
            "sum_157": {
                "input": {
                    "value": "sorted_relevance / discounts",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "arange_70": {
                "start": {
                    "value": "batch_size * num_rounds",
                    "possible_values": []
                }
            },
            "mean_80": {
                "input": {
                    "value": "(__rank_list <= 1).float()",
                    "possible_values": []
                }
            },
            "mean_81": {
                "input": {
                    "value": "(__rank_list <= 5).float()",
                    "possible_values": []
                }
            },
            "mean_82": {
                "input": {
                    "value": "(__rank_list <= 10).float()",
                    "possible_values": []
                }
            },
            "mean_83": {
                "input": {
                    "value": "__rank_list",
                    "possible_values": [
                        [
                            "torch.tensor(self._rank_list).float()",
                            "Call"
                        ]
                    ]
                }
            },
            "mean_84": {
                "input": {
                    "value": "__rank_list.reciprocal()",
                    "possible_values": []
                }
            },
            "arange_156": {
                "start": {
                    "value": "len(rankings)",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/model.py": {
        "torch": {
            "EncoderDecoderModel_4": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.encoder": {
                    "value": "encoder",
                    "possible_values": []
                },
                "self.decoder": {
                    "value": "decoder",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/utils/checkpointing.py": {
        "torch": {
            "load_178": {
                "variable": {
                    "value": "components",
                    "possible_values": []
                },
                "f": {
                    "value": "checkpoint_pthpath",
                    "possible_values": [
                        [
                            "Path(checkpoint_pthpath)",
                            "Call"
                        ]
                    ]
                }
            },
            "save_111": {
                "obj": {
                    "value": "{'model': self._model_state_dict(), 'optimizer': self.optimizer.state_dict()}",
                    "possible_values": []
                },
                "f": {
                    "value": "self.ckpt_dirpath / f'checkpoint_{self.last_epoch}.pth'",
                    "possible_values": []
                }
            }
        }
    },
    "visdialch/utils/dynamic_rnn.py": {
        "torch": {
            "DynamicRNN_6": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.rnn_model": {
                    "value": "rnn_model",
                    "possible_values": []
                }
            },
            "pack_padded_sequence_33": {
                "variable": {
                    "value": "packed_seq_input",
                    "possible_values": []
                },
                "input": {
                    "value": "sorted_seq_input",
                    "possible_values": [
                        [
                            "seq_input.index_select(0, fwd_order)",
                            "Call"
                        ]
                    ]
                },
                "lengths": {
                    "value": "sorted_len",
                    "possible_values": [
                        [
                            "list(sorted_len)",
                            "Call"
                        ]
                    ]
                },
                "batch_first": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "index_select_51": {
                "variable": {
                    "value": "outputs",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                },
                "index": {
                    "value": "bwd_order",
                    "possible_values": []
                }
            },
            "sort_59": {
                "variable": {
                    "value": "(sorted_len, fwd_order)",
                    "possible_values": []
                },
                "input": {
                    "value": "lens.contiguous().view(-1)",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                },
                "descending": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "sort_62": {
                "variable": {
                    "value": "(_, bwd_order)",
                    "possible_values": []
                },
                "input": {
                    "value": "fwd_order",
                    "possible_values": []
                }
            },
            "pad_packed_sequence_51": {
                "sequence": {
                    "value": "outputs",
                    "possible_values": [
                        [
                            "pad_packed_sequence(outputs, batch_first=True, total_length=max_sequence_length)[0].index_select(dim=0, index=bwd_order)",
                            "Call"
                        ]
                    ]
                },
                "batch_first": {
                    "value": "True",
                    "possible_values": []
                },
                "total_length": {
                    "value": "max_sequence_length",
                    "possible_values": [
                        [
                            "seq_input.size(1)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "visdialch/utils/layers.py": {
        "torch": {
            "GatedTrans_5": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_10": {
                    "variable": {
                        "value": "self.embed_y",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Linear(in_dim, out_dim)",
                        "possible_values": []
                    }
                },
                "Sequential_17": {
                    "variable": {
                        "value": "self.embed_g",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Linear(in_dim, out_dim)",
                        "possible_values": []
                    }
                }
            },
            "Q_ATT_32": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_37": {
                    "variable": {
                        "value": "self.embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_44": {
                    "variable": {
                        "value": "self.att",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Softmax_51": {
                    "variable": {
                        "value": "self.softmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "H_ATT_80": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_85": {
                    "variable": {
                        "value": "self.H_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_92": {
                    "variable": {
                        "value": "self.Q_embed",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Sequential_99": {
                    "variable": {
                        "value": "self.att",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                },
                "Softmax_106": {
                    "variable": {
                        "value": "self.softmax",
                        "possible_values": []
                    },
                    "dim": {
                        "value": "-1",
                        "possible_values": []
                    }
                }
            },
            "V_Filter_138": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_143": {
                    "variable": {
                        "value": "self.filter",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "nn.Dropout(p=config['dropout_fc'])",
                        "possible_values": []
                    }
                }
            },
            "normalize_69": {
                "variable": {
                    "value": "ques_norm",
                    "possible_values": []
                },
                "input": {
                    "value": "ques_embed",
                    "possible_values": [
                        [
                            "self.embed(ques_word_encoded)",
                            "Call"
                        ],
                        [
                            "self.Q_embed(ques)",
                            "Call"
                        ],
                        [
                            "ques_embed.unsqueeze(2).repeat(1, 1, num_rounds, 1)",
                            "Call"
                        ],
                        [
                            "self.filter(ques)",
                            "Call"
                        ]
                    ]
                },
                "p": {
                    "value": "2",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "sum_76": {
                "variable": {
                    "value": "feat",
                    "possible_values": []
                },
                "input": {
                    "value": "att.unsqueeze(-1) * ques_word",
                    "possible_values": []
                },
                "dim": {
                    "value": "-2",
                    "possible_values": []
                }
            },
            "normalize_127": {
                "variable": {
                    "value": "att_embed",
                    "possible_values": []
                },
                "input": {
                    "value": "hist_embed * ques_embed",
                    "possible_values": []
                },
                "p": {
                    "value": "2",
                    "possible_values": []
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "tril_130": {
                "variable": {
                    "value": "att_not_pad",
                    "possible_values": []
                },
                "input": {
                    "value": "torch.ones(size=[num_rounds, num_rounds], requires_grad=False)",
                    "possible_values": []
                }
            },
            "sum_134": {
                "variable": {
                    "value": "feat",
                    "possible_values": []
                },
                "input": {
                    "value": "att_masked.unsqueeze(-1) * hist.unsqueeze(1)",
                    "possible_values": []
                },
                "dim": {
                    "value": "-2",
                    "possible_values": []
                }
            },
            "Linear_11": {
                "in_features": {
                    "value": "in_dim",
                    "possible_values": []
                },
                "out_features": {
                    "value": "out_dim",
                    "possible_values": []
                }
            },
            "Tanh_15": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Linear_18": {
                "in_features": {
                    "value": "in_dim",
                    "possible_values": []
                },
                "out_features": {
                    "value": "out_dim",
                    "possible_values": []
                }
            },
            "Sigmoid_22": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Dropout_38": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_45": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_46": {
                "in_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "sum_75": {
                "input": {
                    "value": "att",
                    "possible_values": [
                        [
                            "self.att(ques_norm).squeeze(-1)",
                            "Call"
                        ],
                        [
                            "self.softmax(att)",
                            "Call"
                        ],
                        [
                            "att * ques_not_pad",
                            "BinOp"
                        ],
                        [
                            "att / torch.sum(att, dim=-1, keepdim=True)",
                            "BinOp"
                        ],
                        [
                            "self.softmax(att_embed)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                },
                "keepdim": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "Dropout_86": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_93": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Dropout_100": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_101": {
                "in_features": {
                    "value": "config['lstm_hidden_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "ones_130": {
                "size": {
                    "value": "[num_rounds, num_rounds]",
                    "possible_values": []
                },
                "requires_grad": {
                    "value": "False",
                    "possible_values": []
                }
            },
            "sum_133": {
                "input": {
                    "value": "att_masked",
                    "possible_values": [
                        [
                            "att * att_not_pad",
                            "BinOp"
                        ],
                        [
                            "att_masked / torch.sum(att_masked, dim=-1, keepdim=True)",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "-1",
                    "possible_values": []
                },
                "keepdim": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "Dropout_144": {
                "p": {
                    "value": "config['dropout_fc']",
                    "possible_values": []
                }
            },
            "Linear_145": {
                "in_features": {
                    "value": "config['word_embedding_size']",
                    "possible_values": []
                },
                "out_features": {
                    "value": "config['img_feature_size']",
                    "possible_values": []
                }
            },
            "Sigmoid_149": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    }
}