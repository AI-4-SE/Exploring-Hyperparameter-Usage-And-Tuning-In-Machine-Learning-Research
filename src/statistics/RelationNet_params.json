{
    "Comparator.py": {
        "torch": {
            "RelationNetwork_9": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Linear_14": {
                    "variable": {
                        "value": "self.fc1",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "input_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "hidden_size",
                        "possible_values": []
                    }
                },
                "Linear_15": {
                    "variable": {
                        "value": "self.fc2",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "hidden_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "1",
                        "possible_values": []
                    }
                }
            },
            "TextCNN_31": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.filter_sizes": {
                    "value": "filter_sizes",
                    "possible_values": []
                },
                "Embedding_36": {
                    "variable": {
                        "value": "self.embedding",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "vocab_size",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "embed_dim",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "Constants.PAD",
                        "possible_values": []
                    }
                }
            },
            "Classifier_72": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Linear_75": {
                    "variable": {
                        "value": "self.linear",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "vocab_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "n_labels",
                        "possible_values": []
                    }
                }
            },
            "cosine_similarity_22": {
                "variable": {
                    "value": "cosine",
                    "possible_values": []
                },
                "x1": {
                    "value": "a",
                    "possible_values": [
                        [
                            "a.reshape(n_examples, -1)",
                            "Call"
                        ]
                    ]
                },
                "x2": {
                    "value": "b",
                    "possible_values": [
                        [
                            "b.reshape(n_examples, -1)",
                            "Call"
                        ]
                    ]
                }
            },
            "cat_26": {
                "variable": {
                    "value": "x",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(a, b)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "relu_27": {
                "variable": {
                    "value": "x",
                    "possible_values": []
                },
                "input": {
                    "value": "self.fc1(x)",
                    "possible_values": []
                }
            },
            "sigmoid_28": {
                "variable": {
                    "value": "x",
                    "possible_values": []
                },
                "input": {
                    "value": "self.fc2(x)",
                    "possible_values": []
                }
            },
            "cat_67": {
                "variable": {
                    "value": "cat",
                    "possible_values": []
                },
                "tensors": {
                    "value": "pooled",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "squeeze_67": {
                "variable": {
                    "value": "cat",
                    "possible_values": []
                },
                "input": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "relu_57": {
                "variable": {
                    "value": "conved",
                    "possible_values": []
                },
                "input": {
                    "value": "self.convs[i](embedded)",
                    "possible_values": []
                }
            },
            "max_pool2d_58": {
                "variable": {
                    "value": "conved",
                    "possible_values": []
                },
                "input": {
                    "value": "conved",
                    "possible_values": [
                        [
                            "F.relu(self.convs[i](embedded))",
                            "Call"
                        ],
                        [
                            "F.max_pool2d(conved, (seq_len - self.filter_sizes[i] + 1, 1))",
                            "Call"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "(seq_len - self.filter_sizes[i] + 1, 1)",
                    "possible_values": []
                }
            },
            "log_softmax_78": {
                "input": {
                    "value": "self.linear(vector)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "Conv2d_40": {
                "in_channels": {
                    "value": "1",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_filters",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "(filter_size, embed_dim)",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "possible_values": []
                }
            }
        }
    },
    "Encoder.py": {
        "torch": {
            "device_8": {
                "variable": {
                    "value": "device",
                    "possible_values": []
                },
                "type": {
                    "value": "cuda:0 if torch.cuda.is_available() else cpu",
                    "possible_values": []
                }
            },
            "StructuredSelfAttention_12": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "GRU_27": {
                    "variable": {
                        "value": "self.gru",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "config['emb_dim']",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "bidirectional": {
                        "value": "True",
                        "possible_values": []
                    }
                },
                "Linear_31": {
                    "variable": {
                        "value": "self.linear_first",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "2 * config['lstm_hid_dim']",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "config['d_a']",
                        "possible_values": []
                    }
                },
                "Linear_34": {
                    "variable": {
                        "value": "self.linear_second",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "config['d_a']",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "self.r",
                        "possible_values": []
                    }
                },
                "Dropout_35": {
                    "variable": {
                        "value": "self.dropout",
                        "possible_values": []
                    },
                    "p": {
                        "value": "0.1",
                        "possible_values": []
                    }
                }
            },
            "Classifier_88": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Linear_91": {
                    "variable": {
                        "value": "self.linear",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "vocab_size",
                        "possible_values": [
                            [
                                "config['vocab_size']",
                                "Subscript"
                            ]
                        ]
                    },
                    "out_features": {
                        "value": "n_labels",
                        "possible_values": []
                    }
                }
            },
            "TEXTCNN_96": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.filter_sizes": {
                    "value": "filter_sizes",
                    "possible_values": []
                },
                "Embedding_101": {
                    "variable": {
                        "value": "self.embedding",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "vocab_size",
                        "possible_values": [
                            [
                                "config['vocab_size']",
                                "Subscript"
                            ]
                        ]
                    },
                    "embedding_dim": {
                        "value": "embed_dim",
                        "possible_values": []
                    },
                    "padding_idx": {
                        "value": "Constants.PAD",
                        "possible_values": []
                    }
                },
                "Linear_108": {
                    "variable": {
                        "value": "self.fc",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "len(filter_sizes) * n_filters",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "output_dim",
                        "possible_values": []
                    }
                },
                "Dropout_109": {
                    "variable": {
                        "value": "self.dropout",
                        "possible_values": []
                    },
                    "p": {
                        "value": "dropout",
                        "possible_values": []
                    }
                }
            },
            "tanh_77": {
                "variable": {
                    "value": "x",
                    "possible_values": []
                },
                "input": {
                    "value": "self.linear_first(outputs)",
                    "possible_values": []
                }
            },
            "softmax_80": {
                "variable": {
                    "value": "x",
                    "possible_values": []
                },
                "input": {
                    "value": "x",
                    "possible_values": [
                        [
                            "x.to(device)",
                            "Call"
                        ],
                        [
                            "torch.tanh(self.linear_first(outputs))",
                            "Call"
                        ],
                        [
                            "self.linear_second(x)",
                            "Call"
                        ],
                        [
                            "F.softmax(x, dim=1)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "is_available_8": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Embedding_53": {
                "variable": {
                    "value": "word_embeddings",
                    "possible_values": []
                },
                "num_embeddings": {
                    "value": "vocab_size",
                    "possible_values": [
                        [
                            "config['vocab_size']",
                            "Subscript"
                        ]
                    ]
                },
                "embedding_dim": {
                    "value": "emb_dim",
                    "possible_values": []
                },
                "padding_idx": {
                    "value": "Constants.PAD",
                    "possible_values": []
                }
            },
            "log_softmax_94": {
                "input": {
                    "value": "self.linear(vector)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "relu_122": {
                "variable": {
                    "value": "conved",
                    "possible_values": []
                },
                "input": {
                    "value": "self.convs[i](embedded)",
                    "possible_values": []
                }
            },
            "max_pool2d_123": {
                "variable": {
                    "value": "conved",
                    "possible_values": []
                },
                "input": {
                    "value": "conved",
                    "possible_values": [
                        [
                            "F.relu(self.convs[i](embedded))",
                            "Call"
                        ],
                        [
                            "F.max_pool2d(conved, (seq_len - self.filter_sizes[i] + 1, 1))",
                            "Call"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "(seq_len - self.filter_sizes[i] + 1, 1)",
                    "possible_values": []
                }
            },
            "no_grad_43": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Embedding_56": {
                "variable": {
                    "value": "word_embeddings",
                    "possible_values": []
                },
                "num_embeddings": {
                    "value": "weights.size(0)",
                    "possible_values": []
                },
                "embedding_dim": {
                    "value": "weights.size(1)",
                    "possible_values": []
                }
            },
            "Parameter_57": {
                "variable": {
                    "value": "word_embeddings.weight",
                    "possible_values": []
                },
                "data": {
                    "value": "weights",
                    "possible_values": [
                        [
                            "load_weights(config['word2index'], 'data/char_vector.txt')",
                            "Call"
                        ]
                    ]
                }
            },
            "sum_83": {
                "input": {
                    "value": "sentence_embeddings",
                    "possible_values": [
                        [
                            "attention @ outputs",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "Conv2d_105": {
                "in_channels": {
                    "value": "1",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_filters",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "(filter_size, embed_dim)",
                    "possible_values": []
                },
                "bias": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "cat_130": {
                "tensors": {
                    "value": "pooled",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            }
        }
    },
    "Util.py": {
        "torch": {
            "device_11": {
                "variable": {
                    "value": "device",
                    "possible_values": []
                },
                "type": {
                    "value": "cuda if torch.cuda.is_available() else cpu",
                    "possible_values": []
                }
            },
            "is_available_11": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "fewshot_main.py": {
        "torch": {
            "device_12": {
                "variable": {
                    "value": "device",
                    "possible_values": []
                },
                "type": {
                    "value": "cuda:0 if torch.cuda.is_available() else cpu",
                    "possible_values": []
                }
            },
            "Adam_42": {
                "variable": {
                    "value": "feature_encoder_optim",
                    "possible_values": []
                },
                "params": {
                    "value": "feature_encoder.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "config['LEARNING_RATE']",
                    "possible_values": [
                        [
                            "{'CLASS_NUM': 5, 'SAMPLE_NUM_PER_CLASS': 5, 'BATCH_NUM_PER_CLASS': 15, 'EPISODE': 1000000, 'TEST_EPISODE': 100, 'LEARNING_RATE': 0.0001, 'FEATURE_DIM': 256, 'RELATION_DIM': 8, 'use_bert': False, 'max_len': 30, 'emb_dim': 300, 'lstm_hid_dim': 128, 'd_a': 64, 'r': 1, 'n_classes': 5, 'num_layers': 1, 'dropout': 0.1, 'type': 1, 'use_pretrained_embeddings': True, 'word2index': word2index, 'vocab_size': len(word2index)}",
                            "Dict"
                        ]
                    ]
                },
                "weight_decay": {
                    "value": "0.0001",
                    "possible_values": []
                }
            },
            "StepLR_43": {
                "variable": {
                    "value": "feature_encoder_scheduler",
                    "possible_values": []
                },
                "optimizer": {
                    "value": "feature_encoder_optim",
                    "possible_values": [
                        [
                            "torch.optim.Adam(feature_encoder.parameters(), lr=config['LEARNING_RATE'], weight_decay=0.0001)",
                            "Call"
                        ]
                    ]
                },
                "step_size": {
                    "value": "100000",
                    "possible_values": []
                },
                "gamma": {
                    "value": "0.5",
                    "possible_values": []
                }
            },
            "Adam_44": {
                "variable": {
                    "value": "relation_network_optim",
                    "possible_values": []
                },
                "params": {
                    "value": "relation_network.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "config['LEARNING_RATE']",
                    "possible_values": [
                        [
                            "{'CLASS_NUM': 5, 'SAMPLE_NUM_PER_CLASS': 5, 'BATCH_NUM_PER_CLASS': 15, 'EPISODE': 1000000, 'TEST_EPISODE': 100, 'LEARNING_RATE': 0.0001, 'FEATURE_DIM': 256, 'RELATION_DIM': 8, 'use_bert': False, 'max_len': 30, 'emb_dim': 300, 'lstm_hid_dim': 128, 'd_a': 64, 'r': 1, 'n_classes': 5, 'num_layers': 1, 'dropout': 0.1, 'type': 1, 'use_pretrained_embeddings': True, 'word2index': word2index, 'vocab_size': len(word2index)}",
                            "Dict"
                        ]
                    ]
                },
                "weight_decay": {
                    "value": "0.0001",
                    "possible_values": []
                }
            },
            "StepLR_45": {
                "variable": {
                    "value": "relation_network_scheduler",
                    "possible_values": []
                },
                "optimizer": {
                    "value": "relation_network_optim",
                    "possible_values": [
                        [
                            "torch.optim.Adam(relation_network.parameters(), lr=config['LEARNING_RATE'], weight_decay=0.0001)",
                            "Call"
                        ]
                    ]
                },
                "step_size": {
                    "value": "100000",
                    "possible_values": []
                },
                "gamma": {
                    "value": "0.5",
                    "possible_values": []
                }
            },
            "is_available_12": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "save_82": {
                "obj": {
                    "value": "feature_encoder.state_dict()",
                    "possible_values": []
                },
                "f": {
                    "value": "save/encoder.pkl",
                    "possible_values": []
                }
            },
            "save_83": {
                "obj": {
                    "value": "relation_network.state_dict()",
                    "possible_values": []
                },
                "f": {
                    "value": "save/relation.pkl",
                    "possible_values": []
                }
            }
        }
    },
    "scripts.py": {
        "torch": {
            "tensor_48": {
                "variable": {
                    "value": "input_ids",
                    "possible_values": []
                },
                "data": {
                    "value": "[tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")]",
                    "possible_values": []
                }
            },
            "trace_53": {
                "variable": {
                    "value": "traced_model",
                    "possible_values": []
                },
                "input": {
                    "value": "model",
                    "possible_values": [
                        [
                            "model_class.from_pretrained(pretrained_weights)",
                            "Call"
                        ],
                        [
                            "model_class.from_pretrained('bert-base-uncased')",
                            "Call"
                        ],
                        [
                            "model_class.from_pretrained(pretrained_weights, output_hidden_states=True, output_attentions=True)",
                            "Call"
                        ],
                        [
                            "model_class.from_pretrained(pretrained_weights, torchscript=True)",
                            "Call"
                        ],
                        [
                            "model_class.from_pretrained('./directory/to/save/')",
                            "Call"
                        ]
                    ]
                }
            },
            "tensor_27": {
                "variable": {
                    "value": "input_ids",
                    "possible_values": []
                },
                "data": {
                    "value": "[tokenizer.encode('Here is some text to encode', add_special_tokens=True)]",
                    "possible_values": []
                }
            },
            "no_grad_28": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "softmax_classifer.py": {
        "torch": {
            "device_20": {
                "variable": {
                    "value": "device",
                    "possible_values": []
                },
                "type": {
                    "value": "cuda:0 if torch.cuda.is_available() else cpu",
                    "possible_values": []
                }
            },
            "SGD_95": {
                "variable": {
                    "value": "optimizer",
                    "possible_values": []
                },
                "params": {
                    "value": "classer.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.01",
                    "possible_values": []
                }
            },
            "is_available_20": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "task_generator.py": {
        "torch": {
            "FewShotDataset_79": {
                "base_class_0": {
                    "value": "torch.utils.data.Dataset",
                    "possible_values": []
                },
                "self.task": {
                    "value": "task",
                    "possible_values": []
                },
                "self.split": {
                    "value": "split",
                    "possible_values": [
                        [
                            "'train'",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.word2index": {
                    "value": "word2index",
                    "possible_values": []
                },
                "self.max_len": {
                    "value": "max_len",
                    "possible_values": []
                }
            },
            "ClassBalancedSampler_110": {
                "base_class_0": {
                    "value": "torch.utils.data.Sampler",
                    "possible_values": []
                },
                "self.num_per_class": {
                    "value": "num_per_class",
                    "possible_values": [
                        [
                            "1",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.num_cl": {
                    "value": "num_cl",
                    "possible_values": []
                },
                "self.num_inst": {
                    "value": "num_inst",
                    "possible_values": []
                },
                "self.shuffle": {
                    "value": "shuffle",
                    "possible_values": [
                        [
                            "True",
                            "MethodArgument"
                        ],
                        [
                            "True",
                            "MethodArgument"
                        ]
                    ]
                }
            },
            "DataLoader_146": {
                "variable": {
                    "value": "loader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "dataset",
                    "possible_values": [
                        [
                            "Omniglot(task, split=split, word2index=config['word2index'], max_len=config['max_len'])",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "num_per_class * task.num_classes",
                    "possible_values": []
                },
                "sampler": {
                    "value": "sampler",
                    "possible_values": [
                        [
                            "ClassBalancedSampler(num_per_class, task.num_classes, task.train_num, shuffle=shuffle)",
                            "Call"
                        ],
                        [
                            "ClassBalancedSampler(num_per_class, task.num_classes, task.test_num, shuffle=shuffle)",
                            "Call"
                        ]
                    ]
                }
            },
            "tensor_107": {
                "data": {
                    "value": "image",
                    "possible_values": [
                        [
                            "sentence2indices(line, self.word2index, self.max_len, Constants.PAD)",
                            "Call"
                        ]
                    ]
                }
            },
            "randperm_123": {
                "n": {
                    "value": "self.num_inst",
                    "possible_values": []
                }
            }
        }
    },
    "trainer.py": {
        "torch": {
            "sum_27": {
                "variable": {
                    "value": "sample_features",
                    "possible_values": []
                },
                "input": {
                    "value": "sample_features1",
                    "possible_values": [
                        [
                            "sample_features0.view(config['CLASS_NUM'], config['SAMPLE_NUM_PER_CLASS'], -1)",
                            "Call"
                        ]
                    ]
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "squeeze_27": {
                "variable": {
                    "value": "sample_features",
                    "possible_values": []
                },
                "input": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "transpose_33": {
                "variable": {
                    "value": "batch_features_ext",
                    "possible_values": []
                },
                "input": {
                    "value": "batch_features_ext",
                    "possible_values": [
                        [
                            "batch_features.unsqueeze(0).repeat(config['CLASS_NUM'], 1, 1)",
                            "Call"
                        ],
                        [
                            "torch.transpose(batch_features_ext, 0, 1)",
                            "Call"
                        ]
                    ]
                },
                "dim0": {
                    "value": "0",
                    "possible_values": []
                },
                "dim1": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "MSELoss_37": {
                "variable": {
                    "value": "mse",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "zeros_39": {
                "variable": {
                    "value": "one_hot_labels",
                    "possible_values": []
                },
                "*size": {
                    "value": "config['BATCH_NUM_PER_CLASS'] * config['CLASS_NUM']",
                    "possible_values": []
                },
                "out": {
                    "value": "config['CLASS_NUM']",
                    "possible_values": []
                }
            },
            "sum_72": {
                "variable": {
                    "value": "sample_features",
                    "possible_values": []
                },
                "input": {
                    "value": "sample_features",
                    "possible_values": [
                        [
                            "torch.sum(sample_features1, 1).squeeze(1)",
                            "Call"
                        ],
                        [
                            "feature_encoder(Variable(sample_images).to(device))",
                            "Call"
                        ],
                        [
                            "sample_features.view(config['CLASS_NUM'], config['SAMPLE_NUM_PER_CLASS'], -1)",
                            "Call"
                        ],
                        [
                            "torch.sum(sample_features, 1).squeeze(1)",
                            "Call"
                        ]
                    ]
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "squeeze_72": {
                "variable": {
                    "value": "sample_features",
                    "possible_values": []
                },
                "input": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "transpose_78": {
                "variable": {
                    "value": "test_features_ext",
                    "possible_values": []
                },
                "input": {
                    "value": "test_features_ext",
                    "possible_values": [
                        [
                            "test_features.unsqueeze(0).repeat(config['CLASS_NUM'], 1, 1)",
                            "Call"
                        ],
                        [
                            "torch.transpose(test_features_ext, 0, 1)",
                            "Call"
                        ]
                    ]
                },
                "dim0": {
                    "value": "0",
                    "possible_values": []
                },
                "dim1": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "max_83": {
                "variable": {
                    "value": "(_, predict_labels)",
                    "possible_values": []
                },
                "input": {
                    "value": "relations.data",
                    "possible_values": []
                }
            }
        }
    }
}