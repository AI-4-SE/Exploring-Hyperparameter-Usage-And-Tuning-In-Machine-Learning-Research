{
    "main.py": {
        "torch": {
            "is_available_19": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "device_19": {
                "type": {
                    "value": "cpu",
                    "type": "str",
                    "possible_values": []
                }
            }
        }
    },
    "model/conv_bn_relu.py": {
        "torch": {
            "Sequential_11": {
                "variable": {
                    "value": "self.layers",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "nn.Conv2d(channels_in, channels_out, 3, stride, padding=1)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Conv2d_12": {
                "in_channels": {
                    "value": "channels_in",
                    "type": "variable",
                    "possible_values": [
                        [
                            "channels_in",
                            "Method Argument"
                        ]
                    ]
                },
                "out_channels": {
                    "value": "channels_out",
                    "type": "variable",
                    "possible_values": [
                        [
                            "channels_out",
                            "Method Argument"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "3",
                    "type": "int",
                    "possible_values": []
                },
                "stride": {
                    "value": "stride",
                    "type": "variable",
                    "possible_values": [
                        [
                            "1",
                            "Method Argument"
                        ]
                    ]
                },
                "padding": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "BatchNorm2d_13": {
                "num_features": {
                    "value": "channels_out",
                    "type": "variable",
                    "possible_values": [
                        [
                            "channels_out",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "ReLU_14": {
                "inplace": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                }
            }
        }
    },
    "model/decoder.py": {
        "torch": {
            "Sequential_25": {
                "variable": {
                    "value": "self.layers",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "*layers",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "Linear_28": {
                "variable": {
                    "value": "self.linear",
                    "type": "Attribute",
                    "possible_values": []
                },
                "in_features": {
                    "value": "config.message_length",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_features": {
                    "value": "config.message_length",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "AdaptiveAvgPool2d_24": {
                "output_size": {
                    "value": "(1, 1)",
                    "type": "Tuple",
                    "possible_values": []
                }
            }
        }
    },
    "model/discriminator.py": {
        "torch": {
            "Sequential_17": {
                "variable": {
                    "value": "self.before_linear",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "*layers",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "Linear_18": {
                "variable": {
                    "value": "self.linear",
                    "type": "Attribute",
                    "possible_values": []
                },
                "in_features": {
                    "value": "config.discriminator_channels",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_features": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "AdaptiveAvgPool2d_16": {
                "output_size": {
                    "value": "(1, 1)",
                    "type": "Tuple",
                    "possible_values": []
                }
            }
        }
    },
    "model/encoder.py": {
        "torch": {
            "Sequential_25": {
                "variable": {
                    "value": "self.conv_layers",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "*layers",
                    "type": "Starred",
                    "possible_values": []
                }
            },
            "Conv2d_32": {
                "variable": {
                    "value": "self.final_layer",
                    "type": "Attribute",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "self.conv_channels",
                    "type": "Attribute",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "3",
                    "type": "int",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "cat_44": {
                "variable": {
                    "value": "concat",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[expanded_message, encoded_image, image]",
                    "type": "List",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            }
        }
    },
    "model/encoder_decoder.py": {
        "torch": {}
    },
    "model/hidden.py": {
        "torch": {
            "Adam_24": {
                "variable": {
                    "value": "self.optimizer_enc_dec",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self.encoder_decoder.parameters()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "Adam_25": {
                "variable": {
                    "value": "self.optimizer_discrim",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "self.discriminator.parameters()",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "BCEWithLogitsLoss_37": {
                "variable": {
                    "value": "self.bce_with_logits_loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "MSELoss_38": {
                "variable": {
                    "value": "self.mse_loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "full_74": {
                "variable": {
                    "value": "d_target_label_cover",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.cover_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "full_75": {
                "variable": {
                    "value": "d_target_label_encoded",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.encoded_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "full_76": {
                "variable": {
                    "value": "g_target_label_encoded",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.cover_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "full_149": {
                "variable": {
                    "value": "d_target_label_cover",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.cover_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "full_150": {
                "variable": {
                    "value": "d_target_label_encoded",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.encoded_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "full_151": {
                "variable": {
                    "value": "g_target_label_encoded",
                    "type": "variable",
                    "possible_values": []
                },
                "size": {
                    "value": "(batch_size, 1)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "fill_value": {
                    "value": "self.cover_label",
                    "type": "Attribute",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "enable_grad_68": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "no_grad_148": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            }
        }
    },
    "noise_layers/crop.py": {
        "torch": {}
    },
    "noise_layers/cropout.py": {
        "torch": {
            "zeros_like_21": {
                "variable": {
                    "value": "cropout_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "noised_image",
                    "type": "variable",
                    "possible_values": [
                        [
                            "noised_and_cover[0]",
                            "Subscript"
                        ]
                    ]
                }
            }
        }
    },
    "noise_layers/dropout.py": {
        "torch": {
            "tensor_23": {
                "variable": {
                    "value": "mask_tensor",
                    "type": "variable",
                    "possible_values": []
                },
                "data": {
                    "value": "mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.random.choice([0.0, 1.0], noised_image.shape[2:], p=[1 - mask_percent, mask_percent])",
                            "Call"
                        ]
                    ]
                },
                "device": {
                    "value": "noised_image.device",
                    "type": "Attribute",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.float",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "noise_layers/identity.py": {
        "torch": {}
    },
    "noise_layers/jpeg_compression.py": {
        "torch": {
            "tensor_70": {
                "variable": {
                    "value": "self.dct_conv_weights",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "gen_filters(8, 8, dct_coeff)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "tensor_72": {
                "variable": {
                    "value": "self.idct_conv_weights",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "gen_filters(8, 8, idct_coeff)",
                    "type": "Call",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.float32",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "cat_123": {
                "variable": {
                    "value": "image_conv_stacked",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "image_conv_channels",
                    "type": "variable",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "type": "int",
                    "possible_values": []
                }
            },
            "empty_like_138": {
                "variable": {
                    "value": "image_yuv",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "noised_image",
                    "type": "variable",
                    "possible_values": [
                        [
                            "noised_and_cover[0]",
                            "Subscript"
                        ],
                        [
                            "nn.ZeroPad2d((0, pad_width, 0, pad_height))(noised_image)",
                            "Call"
                        ]
                    ]
                }
            },
            "mul_149": {
                "variable": {
                    "value": "image_dct_mask",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "image_dct",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self.apply_conv(image_yuv, 'dct')",
                            "Call"
                        ]
                    ]
                },
                "other": {
                    "value": "mask",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.zeros((window_size, window_size), dtype=np.uint8)",
                            "Call"
                        ],
                        [
                            "torch.from_numpy(get_jpeg_yuv_filter_mask(requested_shape, 8, weights_to_keep))",
                            "Call"
                        ],
                        [
                            "self.get_mask(image_dct.shape[1:])",
                            "Call"
                        ]
                    ]
                }
            },
            "empty_like_154": {
                "variable": {
                    "value": "image_ret_padded",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "image_dct",
                    "type": "variable",
                    "possible_values": [
                        [
                            "self.apply_conv(image_yuv, 'dct')",
                            "Call"
                        ]
                    ]
                }
            },
            "empty_86": {
                "variable": {
                    "value": "self.jpeg_mask",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*size": {
                    "value": "(3,) + requested_shape",
                    "type": "BinOp",
                    "possible_values": []
                },
                "device": {
                    "value": "self.device",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "conv2d_110": {
                "variable": {
                    "value": "image_conv",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "image_yuv_ch",
                    "type": "variable",
                    "possible_values": [
                        [
                            "image[:, channel, :, :].unsqueeze_(1)",
                            "Call"
                        ]
                    ]
                },
                "weight": {
                    "value": "filters",
                    "type": "variable",
                    "possible_values": [
                        [
                            "np.zeros((size_x * size_y, size_x, size_y))",
                            "Call"
                        ],
                        [
                            "self.dct_conv_weights",
                            "Attribute"
                        ],
                        [
                            "self.idct_conv_weights",
                            "Attribute"
                        ]
                    ]
                },
                "stride": {
                    "value": "8",
                    "type": "int",
                    "possible_values": []
                }
            },
            "from_numpy_88": {
                "variable": {
                    "value": "mask",
                    "type": "variable",
                    "possible_values": []
                },
                "ndarray": {
                    "value": "get_jpeg_yuv_filter_mask(requested_shape, 8, weights_to_keep)",
                    "type": "Call",
                    "possible_values": []
                }
            },
            "ZeroPad2d_135": {
                "padding": {
                    "value": "(0, pad_width, 0, pad_height)",
                    "type": "Tuple",
                    "possible_values": []
                }
            }
        }
    },
    "noise_layers/noiser.py": {
        "torch": {}
    },
    "noise_layers/quantization.py": {
        "torch": {
            "tensor_25": {
                "variable": {
                    "value": "self.weights",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "[(-1) ** (n + 1) / (np.pi * (n + 1)) for n in range(self.N)]",
                    "type": "ListComp",
                    "possible_values": []
                }
            },
            "tensor_26": {
                "variable": {
                    "value": "self.scales",
                    "type": "Attribute",
                    "possible_values": []
                },
                "data": {
                    "value": "[2 * np.pi * (n + 1) for n in range(self.N)]",
                    "type": "ListComp",
                    "possible_values": []
                }
            },
            "mul_34": {
                "input": {
                    "value": "tensor",
                    "type": "variable",
                    "possible_values": [
                        [
                            "tensor",
                            "Method Argument"
                        ],
                        [
                            "tensor",
                            "Method Argument"
                        ]
                    ]
                },
                "other": {
                    "value": "self.scales",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "sum_35": {
                "variable": {
                    "value": "z",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "z",
                    "type": "variable",
                    "possible_values": [
                        [
                            "torch.mul(self.weights, torch.sin(torch.mul(tensor, self.scales)))",
                            "Call"
                        ],
                        [
                            "torch.sum(z, dim=0)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "is_available_20": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "device_20": {
                "type": {
                    "value": "cpu",
                    "type": "str",
                    "possible_values": []
                }
            },
            "sin_34": {
                "input": {
                    "value": "torch.mul(tensor, self.scales)",
                    "type": "Call",
                    "possible_values": []
                }
            }
        }
    },
    "noise_layers/resize.py": {
        "torch": {
            "interpolate_21": {
                "variable": {
                    "value": "noised_and_cover[0]",
                    "type": "Subscript",
                    "possible_values": []
                },
                "input": {
                    "value": "noised_image",
                    "type": "variable",
                    "possible_values": [
                        [
                            "noised_and_cover[0]",
                            "Subscript"
                        ]
                    ]
                },
                "scale_factor": {
                    "value": "(resize_ratio, resize_ratio)",
                    "type": "Tuple",
                    "possible_values": []
                },
                "mode": {
                    "value": "self.interpolation_method",
                    "type": "Attribute",
                    "possible_values": []
                }
            }
        }
    },
    "test_model.py": {
        "torch": {
            "load_45": {
                "variable": {
                    "value": "checkpoint",
                    "type": "variable",
                    "possible_values": []
                },
                "f": {
                    "value": "args.checkpoint_file",
                    "type": "Attribute",
                    "possible_values": []
                }
            },
            "Tensor_57": {
                "variable": {
                    "value": "message",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "is_available_25": {
                "params": {
                    "value": "default",
                    "type": null,
                    "possible_values": []
                }
            },
            "device_26": {
                "variable": {
                    "value": "device",
                    "type": "variable",
                    "possible_values": []
                },
                "type": {
                    "value": "cuda",
                    "type": "str",
                    "possible_values": []
                }
            },
            "device_28": {
                "variable": {
                    "value": "device",
                    "type": "variable",
                    "possible_values": []
                },
                "type": {
                    "value": "cpu",
                    "type": "str",
                    "possible_values": []
                }
            }
        }
    },
    "train.py": {
        "torch": {
            "Tensor_52": {
                "variable": {
                    "value": "message",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "Tensor_78": {
                "variable": {
                    "value": "message",
                    "type": "variable",
                    "possible_values": []
                }
            }
        }
    },
    "utils.py": {
        "torch": {
            "Tensor_25": {
                "variable": {
                    "value": "image_tensor",
                    "type": "variable",
                    "possible_values": []
                }
            },
            "cat_55": {
                "variable": {
                    "value": "stacked_images",
                    "type": "variable",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[images, watermarked_images]",
                    "type": "List",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "type": "int",
                    "possible_values": []
                }
            },
            "load_96": {
                "variable": {
                    "value": "checkpoint",
                    "type": "variable",
                    "possible_values": []
                },
                "f": {
                    "value": "last_checkpoint_file",
                    "type": "variable",
                    "possible_values": [
                        [
                            "last_checkpoint_from_folder(checkpoint_folder)",
                            "Call"
                        ]
                    ]
                }
            },
            "DataLoader_139": {
                "variable": {
                    "value": "train_loader",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "train_images",
                    "type": "variable",
                    "possible_values": [
                        [
                            "datasets.ImageFolder(train_options.train_folder, data_transforms['train'])",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "train_options.batch_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "type": "bool",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "4",
                    "type": "int",
                    "possible_values": []
                }
            },
            "DataLoader_143": {
                "variable": {
                    "value": "validation_loader",
                    "type": "variable",
                    "possible_values": []
                },
                "dataset": {
                    "value": "validation_images",
                    "type": "variable",
                    "possible_values": [
                        [
                            "datasets.ImageFolder(train_options.validation_folder, data_transforms['test'])",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "train_options.batch_size",
                    "type": "Attribute",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "False",
                    "type": "bool",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "4",
                    "type": "int",
                    "possible_values": []
                }
            },
            "interpolate_52": {
                "variable": {
                    "value": "images",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "images",
                    "type": "variable",
                    "possible_values": [
                        [
                            "original_images[:original_images.shape[0], :, :, :].cpu()",
                            "Call"
                        ],
                        [
                            "(images + 1) / 2",
                            "BinOp"
                        ],
                        [
                            "F.interpolate(images, size=resize_to)",
                            "Call"
                        ]
                    ]
                },
                "size": {
                    "value": "resize_to",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "interpolate_53": {
                "variable": {
                    "value": "watermarked_images",
                    "type": "variable",
                    "possible_values": []
                },
                "input": {
                    "value": "watermarked_images",
                    "type": "variable",
                    "possible_values": [
                        [
                            "watermarked_images[:watermarked_images.shape[0], :, :, :].cpu()",
                            "Call"
                        ],
                        [
                            "(watermarked_images + 1) / 2",
                            "BinOp"
                        ],
                        [
                            "F.interpolate(watermarked_images, size=resize_to)",
                            "Call"
                        ],
                        [
                            "watermarked_images",
                            "Method Argument"
                        ]
                    ]
                },
                "size": {
                    "value": "resize_to",
                    "type": "variable",
                    "possible_values": [
                        [
                            "None",
                            "Method Argument"
                        ]
                    ]
                }
            },
            "save_88": {
                "obj": {
                    "value": "checkpoint",
                    "type": "variable",
                    "possible_values": [
                        [
                            "{'enc-dec-model': model.encoder_decoder.state_dict(), 'enc-dec-optim': model.optimizer_enc_dec.state_dict(), 'discrim-model': model.discriminator.state_dict(), 'discrim-optim': model.optimizer_discrim.state_dict(), 'epoch': epoch}",
                            "Dict"
                        ],
                        [
                            "torch.load(last_checkpoint_file)",
                            "Call"
                        ],
                        [
                            "checkpoint",
                            "Method Argument"
                        ]
                    ]
                },
                "f": {
                    "value": "checkpoint_filename",
                    "type": "variable",
                    "possible_values": [
                        [
                            "f'{experiment_name}--epoch-{epoch}.pyt'",
                            "JoinedStr"
                        ],
                        [
                            "os.path.join(checkpoint_folder, checkpoint_filename)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    },
    "validate-trained-models.py": {
        "torch": {
            "device_28": {
                "variable": {
                    "value": "device",
                    "type": "variable",
                    "possible_values": []
                },
                "type": {
                    "value": "cpu",
                    "type": "str",
                    "possible_values": []
                }
            },
            "Tensor_74": {
                "variable": {
                    "value": "message",
                    "type": "variable",
                    "possible_values": []
                }
            }
        }
    },
    "vgg_loss.py": {
        "torch": {
            "Sequential_29": {
                "variable": {
                    "value": "self.vgg_loss",
                    "type": "Attribute",
                    "possible_values": []
                },
                "*args": {
                    "value": "*layers",
                    "type": "Starred",
                    "possible_values": []
                }
            }
        }
    }
}