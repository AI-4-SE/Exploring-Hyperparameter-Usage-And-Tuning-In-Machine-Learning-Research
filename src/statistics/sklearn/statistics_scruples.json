{
    "src/scruples/baselines/labels.py": {
        "Pipeline_9": {
            "steps": "[('classifier', DummyClassifier(strategy='prior'))]"
        },
        "Pipeline_20": {
            "steps": "[('classifier', DummyClassifier(strategy='stratified'))]"
        },
        "DummyClassifier_10": {
            "strategy": "prior"
        },
        "DummyClassifier_21": {
            "strategy": "stratified"
        }
    },
    "src/scruples/baselines/linear.py": {
        "Pipeline_15": {
            "steps": "[('concatenator', FunctionTransformer(func=utils.concat_title_and_text, validate=False)), ('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None)), ('tfidf', TfidfTransformer(smooth_idf=True)), ('scaler', MaxAbsScaler()), ('classifier', LogisticRegression(penalty='l2', dual=False, tol=0.0001, intercept_scaling=1.0, solver='lbfgs', max_iter=100, warm_start=True))]"
        },
        "Pipeline_80": {
            "steps": "[('featurizer', utils.ResourceTransformer(transformer=Pipeline([('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='featurizer', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None)), ('tfidf', TfidfTransformer(smooth_idf=True)), ('scaler', MaxAbsScaler())]))), ('classifier', LogisticRegression(penalty='l2', dual=False, tol=0.0001, fit_intercept=False, intercept_scaling=1.0, solver='lbfgs', max_iter=100, warm_start=True))]"
        },
        "FunctionTransformer_18": {
            "func": "utils.concat_title_and_text",
            "validate": "False"
        },
        "CountVectorizer_22": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None"
        },
        "TfidfTransformer_34": {
            "smooth_idf": "True"
        },
        "LogisticRegression_42": {
            "penalty": "l2",
            "dual": "False",
            "tol": "0.0001",
            "intercept_scaling": "1.0",
            "solver": "lbfgs",
            "max_iter": "100",
            "warm_start": "True"
        },
        "LogisticRegression_109": {
            "penalty": "l2",
            "dual": "False",
            "tol": "0.0001",
            "fit_intercept": "False",
            "intercept_scaling": "1.0",
            "solver": "lbfgs",
            "max_iter": "100",
            "warm_start": "True"
        },
        "Pipeline_84": {
            "steps": "[('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='featurizer', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None)), ('tfidf', TfidfTransformer(smooth_idf=True)), ('scaler', MaxAbsScaler())]"
        },
        "CountVectorizer_87": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "featurizer",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None"
        },
        "TfidfTransformer_99": {
            "smooth_idf": "True"
        }
    },
    "src/scruples/baselines/metrics.py": {},
    "src/scruples/baselines/naivebayes.py": {
        "Pipeline_16": {
            "steps": "[('concatenator', FunctionTransformer(func=utils.concat_title_and_text, validate=False)), ('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None, binary=True)), ('classifier', BernoulliNB(binarize=None, fit_prior=True))]"
        },
        "Pipeline_60": {
            "steps": "[('concatenator', FunctionTransformer(func=utils.concat_title_and_text, validate=False)), ('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None, binary=False)), ('classifier', MultinomialNB(fit_prior=True))]"
        },
        "Pipeline_104": {
            "steps": "[('concatenator', FunctionTransformer(func=utils.concat_title_and_text, validate=False)), ('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None, binary=False)), ('classifier', ComplementNB(fit_prior=True))]"
        },
        "FunctionTransformer_19": {
            "func": "utils.concat_title_and_text",
            "validate": "False"
        },
        "CountVectorizer_23": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None",
            "binary": "True"
        },
        "BernoulliNB_36": {
            "binarize": "None",
            "fit_prior": "True"
        },
        "FunctionTransformer_63": {
            "func": "utils.concat_title_and_text",
            "validate": "False"
        },
        "CountVectorizer_67": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None",
            "binary": "False"
        },
        "MultinomialNB_80": {
            "fit_prior": "True"
        },
        "FunctionTransformer_107": {
            "func": "utils.concat_title_and_text",
            "validate": "False"
        },
        "CountVectorizer_111": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None",
            "binary": "False"
        },
        "ComplementNB_124": {
            "fit_prior": "True"
        }
    },
    "src/scruples/baselines/style.py": {
        "Pipeline_315": {
            "steps": "[('featurizer', ColumnTransformer([('title_style', StyleFeaturizer(), 'title'), ('text_style', StyleFeaturizer(), 'text')], remainder='drop')), ('classifier', XGBClassifier(n_estimators=100, verbosity=0, objective='multi:softprob', booster='gbtree', n_jobs=1, max_delta_step=0, colsample_bylevel=1.0, colsample_bynode=1.0))]"
        },
        "Pipeline_365": {
            "steps": "[('classifier', LengthRanker(choose='shortest', length='words'))]"
        },
        "Pipeline_373": {
            "steps": "[('classifier', LengthRanker(choose='longest', length='words'))]"
        },
        "Pipeline_381": {
            "steps": "[('classifier', LengthRanker(choose='shortest', length='characters'))]"
        },
        "Pipeline_389": {
            "steps": "[('classifier', LengthRanker(choose='longest', length='characters'))]"
        },
        "Pipeline_400": {
            "steps": "[('featurizer', utils.ResourceTransformer(transformer=Pipeline([('featurizer', StyleFeaturizer()), ('scaler', MaxAbsScaler())]))), ('classifier', LogisticRegression(penalty='l2', dual=False, tol=0.0001, fit_intercept=False, intercept_scaling=1.0, solver='lbfgs', max_iter=100, warm_start=True))]"
        },
        "ColumnTransformer_318": {
            "transformers": "[('title_style', StyleFeaturizer(), 'title'), ('text_style', StyleFeaturizer(), 'text')]",
            "remainder": "drop"
        },
        "LogisticRegression_417": {
            "penalty": "l2",
            "dual": "False",
            "tol": "0.0001",
            "fit_intercept": "False",
            "intercept_scaling": "1.0",
            "solver": "lbfgs",
            "max_iter": "100",
            "warm_start": "True"
        },
        "Pipeline_404": {
            "steps": "[('featurizer', StyleFeaturizer()), ('scaler', MaxAbsScaler())]"
        }
    },
    "src/scruples/baselines/trees.py": {
        "Pipeline_15": {
            "steps": "[('concatenator', FunctionTransformer(func=utils.concat_title_and_text, validate=False)), ('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', preprocessor=None, tokenizer=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', max_features=None, vocabulary=None)), ('tfidf', TfidfTransformer(smooth_idf=True)), ('classifier', RandomForestClassifier(n_estimators=100, max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, oob_score=False, n_jobs=1, verbose=0, warm_start=False))]"
        },
        "FunctionTransformer_18": {
            "func": "utils.concat_title_and_text",
            "validate": "False"
        },
        "CountVectorizer_22": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "preprocessor": "None",
            "tokenizer": "None",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "max_features": "None",
            "vocabulary": "None"
        },
        "TfidfTransformer_34": {
            "smooth_idf": "True"
        },
        "RandomForestClassifier_38": {
            "n_estimators": "100",
            "max_depth": "None",
            "max_features": "auto",
            "max_leaf_nodes": "None",
            "min_impurity_decrease": "0.0",
            "oob_score": "False",
            "n_jobs": "1",
            "verbose": "0",
            "warm_start": "False"
        }
    },
    "src/scruples/baselines/utils.py": {},
    "src/scruples/demos/scoracle/app.py": {},
    "src/scruples/scripts/analyze/corpus/extractions.py": {},
    "src/scruples/scripts/analyze/corpus/predictions.py": {},
    "src/scruples/scripts/analyze/oracle_estimator.py": {},
    "src/scruples/scripts/analyze/resource/predictions.py": {},
    "src/scruples/scripts/analyze/resource/topics.py": {
        "Pipeline_46": {
            "steps": "[('vectorizer', CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words='english', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=2, max_features=None, vocabulary=None, binary=False)), ('topic_model', LatentDirichletAllocation(n_components=N_COMPONENTS))]"
        },
        "CountVectorizer_49": {
            "input": "content",
            "encoding": "utf-8",
            "decode_error": "strict",
            "strip_accents": "None",
            "lowercase": "True",
            "preprocessor": "None",
            "tokenizer": "None",
            "stop_words": "english",
            "token_pattern": "(?u)\\b\\w\\w+\\b",
            "ngram_range": "(1, 1)",
            "analyzer": "word",
            "max_df": "1.0",
            "min_df": "2",
            "max_features": "None",
            "vocabulary": "None",
            "binary": "False"
        },
        "LatentDirichletAllocation_70": {
            "n_components": "N_COMPONENTS"
        }
    },
    "src/scruples/scripts/evaluate/corpus/run_shallow.py": {},
    "src/scruples/scripts/evaluate/resource/run_shallow.py": {},
    "src/scruples/utils.py": {},
    "tests/scruples/baselines/utils.py": {},
    "tests/scruples/demos/scoracle/test_app.py": {}
}