#!/usr/bin/env python
# coding: utf-8

# ### HW Соколова Александра
# ---
# ### **Open ML Course: Линейные модели**
# Домашнее задание: разведовательный анализ по вопросам теста (EDA)
# 12.05.2022

# # 1. Установка библиотек и настройка их параметров 

# In[ ]:


import pickle
import numpy as np
import pandas as pd

pd.set_option('display.max_columns', 50)
np.warnings.filterwarnings('ignore')


# # 2. Импорт данных, препроцессинг, инициализация констант

# In[ ]:


# CURRENT_DIR = './'  # имя текущей директории для локальной машины 
CURRENT_DIR = '../'  # имя текущей директории для каггл

PATH_TO_WORKDIR = CURRENT_DIR + 'working/'

PATH_TO_TRAIN = CURRENT_DIR + 'input/open-ml-course-linear-models-spring22/'


# In[ ]:


# инициализируем константы для импорта и более удобного препроцессинга
times = ['time'+str(i) for i in range(1,11)]
sites = ['site'+str(i) for i in range(1,11)]


# In[ ]:


# импортируем тест и трейн, парсим дату
train_df = pd.read_csv(PATH_TO_TRAIN+'train.csv',
                       index_col='session_id', parse_dates=times)

# посмотрим на пример данных из трейна
train_df.sample(5)


# In[ ]:


train_df.info()


# ### Перевод описания признаков трейна и дополнительной информации из бейзлайна к соревнованию курса, которое может пригодится при выполнении домашнего задания:
# 
# Набор обучающих данных содержит следующие функции:
# 
# - **site1** – идентификатор первого посещенного веб-сайта в сеансе
# - **time1** – время посещения первого веб-сайта в сеансе
# - ...
# - **site10** – идентификатор десятого посещенного веб-сайта в сеансе
# - **time10** – время посещения десятого веб-сайта в сессии
# - ** target** – целевая переменная, равная 1 для сеансов Алисы и 0 в противном случае
# 
# **Сеансы пользователя заканчиваются в том случае, если пользователь посетил десять веб-сайтов, либо в том случае, если сеанс длился более тридцати минут.**
# 
# В таблице есть несколько пустых значений, это означает, что некоторые сеансы содержат менее десяти веб-сайтов. Рекомендации: Замените пустые значения на 0 и измените типы столбцов на целочисленные. Также загрузите словарь веб-сайтов и проверьте, как он выглядит

# In[ ]:


# загружаем словарь с названиями сайтов и их кодами, указанными в трейне
with open(PATH_TO_TRAIN + 'site_dic.pkl', 'rb') as input_file:
    site_dict = pickle.load(input_file)
    
# параметр rb означает r+b (read + bynary) read - для чтения. 
# По умолчанию используется текстовый режим, который может преобразовывать символы '\n' в специфичное для платформы 
# представление при записи и обратно при чтении. При открытии двоичного файла вы должны добавить «b» к значению режима, 
# чтобы открыть файл в двоичном режиме.

print('Websites total:', len(site_dict))


# In[ ]:


# посмотрим на несколько элементов словаря
[item for item in site_dict.items()][:5]


# In[ ]:


# инвертируем словарь (поменяем ключ и значение местами)
new_dict = {}
for key in site_dict:
    new_dict[site_dict[key]] = key


# # 3. Ответы на вопросы теста

# ### 1) Какой процент сессий Элис из всех сессий?

# In[ ]:


train_df['target'].value_counts(normalize=True)


# In[ ]:


print('Ответ на вопрос 1:', train_df['target'].value_counts(normalize=True)[1]*100, '%')


# ### 2) В какой день недели Элис появилась в первый раз?

# In[ ]:


# проверим по всем полям времени посещения на случай, 
# если вдруг по ошибке время не записалось в первую ячейку и записалось в остальные  
alice = train_df['target'] == 1
temp_df = train_df[alice][times].describe().T
temp_df


# In[ ]:


# выбираем первое время по всему датасету - first и выбираем из него минимальное
min_date = temp_df['first'].min()
min_date


# In[ ]:


# переводим в день недели (где 0 - Понедельник, 6 - воскресение)
min_date.weekday()


# In[ ]:


print('Ответ на вопрос 2:', '4 - Пятница:')


# ### 3) Какие года присутствуют в обучающей выборке?

# In[ ]:


# проверим года по всем полям времени посещения на случай, 
# если вдруг по ошибке время не записалось в первую ячейку и записалось в остальные  
unique_year_set = set()
for i_time in times:
    arr = np.array(train_df[i_time].dt.year.unique())
    arr1 = arr[~np.isnan(arr)]
    unique_year_set = unique_year_set.union(set(list(arr1)))
unique_year_set


# In[ ]:


print('Ответ на вопрос 3:', unique_year_set)


# ### 4) Какой сайт в обучающей выборке чаще всего посещает первым Элис и обычный пользователь?

# In[ ]:


temp_df = train_df[['site1', 'target']].copy()  # сделаем копию фрейма, чтобы случайно не испортить данные


# In[ ]:


temp_df_0 = temp_df[~alice]
temp_df_alice = temp_df[alice]


# In[ ]:


# выведем для наглядности первые пять популярных сайтов Элис
temp_df_alice.site1.value_counts()[:5]


# In[ ]:


# выведем для наглядности первые пять популярных сайтов обычных пользователей (не Элис)
temp_df_0.site1.value_counts()[:5]


# In[ ]:


# запишем коды самых популярных, чтобы вывести их названия из словаря
i0 = temp_df_0.site1.value_counts().index[0]
i1 = temp_df_alice.site1.value_counts().index[0]


# In[ ]:


print('Ответ на вопрос 4:', new_dict[i1], new_dict[i0])


# ### 5) Какой сайт в обучающей выборке является вторым по популярности в 2014 году и сколько раз его посещали в 2014 году?

# In[ ]:


# вытянем информацию по всем сайтам и времени посещения в длинный массив, который содержит сайт и время посещения
temp_df0 = pd.DataFrame()
for i in range(1,11,1):
    temp_df0 = pd.concat([temp_df0, train_df[[f'site{i}',f'time{i}']].rename(columns = {f'site{i}': 'site', f'time{i}' : 'time'})])
print('Проверим что длинна итогового массива верна:', temp_df0.shape[0]/10 == train_df.shape[0])


# In[ ]:


# проверим правильность составления итогового массива
temp_df0.head()  


# In[ ]:


# создадим новый признак - год
temp_df0['year'] = temp_df0['time'].dt.year
temp_df0.head()


# In[ ]:


temp_df_14 = temp_df0[temp_df0['year']==2014].copy()  # отберем только 2014 год


# In[ ]:


# выведем для наглядности пять самых популярных сайтов посещаемых в 2014 году
temp_df_14.site.value_counts().iloc[:5]


# In[ ]:


i_second = temp_df_14.site.value_counts().index[1]  # сохраним код второго в списке чтобы вывести его название и кол-во посещений
print('Ответ на вопрос 5:', new_dict[i_second], temp_df_14.site.value_counts().iloc[1])


# ### 6) Сколько всего сессий обучающей выборке содержат в себе меньше чем 10 сайтов?

# In[ ]:


# признаков не много можно обойтись простым перечислением логических выражений пустых значений
# перебираем все на случай если вдруг произошел сбой и записи расположились не попорядку
result = len(train_df[(train_df['site10'].isna()) | 
                      (train_df['site9'].isna())  |
                      (train_df['site8'].isna())  | 
                      (train_df['site7'].isna())  | 
                      (train_df['site6'].isna())  | 
                      (train_df['site5'].isna())  | 
                      (train_df['site4'].isna())  | 
                      (train_df['site3'].isna())  | 
                      (train_df['site2'].isna())  | 
                      (train_df['site1'].isna())  ])

print('Ответ на вопрос 6:', result)


# ### 7) Выберите верные утверждения для обучающей выборки:
# 
# 1. В среднем Элис проводит меньше времени на первой странице, чем другие пользователи
# 2. Медианная длительность посещения первой страницы у не-Элис больше, чем у Элис
# 3. Элис наиболее активна в марте и неактивна летом
# 4. Лето является самым малоактивным временем года вообще

# In[ ]:


train_df.loc[alice, 'time1'].dt.month.value_counts()  # посмотрим на активность Элис по месяцам


# вывод 1: Самый активный месяц - 12 - декабрь => пункт 3 не верен (Элис наиболее активна в марте и неактивна летом)  
# вывод 2: Летних месяцев в списке нет => Элис не посещает сайты летом => пункт 4 верен (Лето является самым малоактивным временем года вообще)  
# 

# In[ ]:


# теперь перейдем к расчету времение проведенному на 1-ом сайте 
# перед тем как вычитать первое время из второго вспомним, что во втором столбце были пропуски
len(train_df[train_df['time2'].isna()])


# In[ ]:


# как быть в этом случае, на помощь нам придет фраза из бейзлайна с описанием датасета
# Сеансы пользователя заканчиваются в том случае, если пользователь посетил десять веб-сайтов, 
# либо в том случае, если сеанс длился более тридцати минут.

# расчитаем время поесещения первой страницы с учетом этой информации

temp_df = train_df[['time1','time2']].copy()  # сделаем копию фрейма, чтобы случайно не испортить исходные данные

index_empty_time2 = temp_df['time2'].isna()  # сохраняем индексы, где второе время пустое

# заполним пустое время time2 +30 минутными дельтами
temp_df.loc[index_empty_time2, 'time2'] = temp_df.loc[index_empty_time2, 'time1'] + np.timedelta64(30, 'm')  

# проверим что пустых нет
print('Пустых time2 после заполнения:', temp_df['time2'].isna().sum())

temp_df['diff_time_2_1'] = temp_df['time2'] - temp_df['time1']  # создадим новый признак как разница второго и первого времени
temp_df['diff_time_2_1'] = temp_df['diff_time_2_1']  / np.timedelta64(1, 's')  # переведем в секунды
temp_df.head()


# In[ ]:


temp_df[alice]['diff_time_2_1'].median()  # проверим медианное время Элис


# In[ ]:


temp_df[~alice]['diff_time_2_1'].median()  # проверим медианное время обычных пользователей


# вывод 3: медианное время Элис и обычных пользователей совпаает => пункт 2 не верный (Медианная длительность посещения первой страницы у не-Элис больше, чем у Элис)

# In[ ]:


temp_df[alice]['diff_time_2_1'].mean()  # проверим среднее время Элис


# In[ ]:


temp_df[~alice]['diff_time_2_1'].mean()  # проверим среднее время обычных пользователей


# вывод 4: среднее время Элис (8.74) меньше чем у обычных пользователей (43.83) => пункт 1 верный (В среднем Элис проводит меньше времени на первой странице, чем другие пользователи)

# ### 8) Чему равное медианное значение количества уникальных сайтов в рамках одной сессии обучающей выборки?

# In[ ]:


# создадим функцию, которая подсчитывает кол-во уникальных сайтов
def unique_site (row):
    arr = np.array([row['site1'], row['site2'], row['site3'], row['site4'], row['site5'], row['site6'], 
                    row['site7'], row['site8'], row['site9'], row['site10']])
    arr1 = arr[~np.isnan(arr)]
    return len(np.unique(arr1))


# In[ ]:


temp_df = train_df.copy()  # скопируем чтобы не испортить исходный датафрейм


# In[ ]:


get_ipython().run_line_magic('time', '')
temp_df['unique_site'] = temp_df.apply(unique_site, axis=1)  # создадим новый признак с кол-вом уникальных сайтов


# In[ ]:


# проверим несколько случайных строк
temp_df[sites+['unique_site']].sample(5)


# In[ ]:


print('Ответ на вопрос 8:', temp_df.unique_site.median())


# ### 9) Чему равное медианное значение времени сессии в обучающей выборке (время посещения последнего сайта примем равным нулю)?

# In[ ]:


# создадим функцию для подсчета разницы между последним и первым временем сессии
def time_session (row):
    arr = np.array([row['time1'], row['time2'], row['time3'], row['time4'], row['time5'], row['time6'], 
                    row['time7'], row['time8'], row['time9'], row['time10']])
    arr1 = arr[~pd.isnull(arr)]
    return np.max(arr1)-np.min(arr1)


# In[ ]:


get_ipython().run_cell_magic('time', '', "temp_df['time_session'] = temp_df.apply(time_session, axis=1)  # создадим новый признак времени сессии\n")


# In[ ]:


# проверим несколько случайных строк
temp_df[times+['time_session']].sample(5)


# In[ ]:


# переведем в секунды и сохраним в новый признак
temp_df['time_session_sec'] = temp_df.time_session / np.timedelta64(1, 's')


# In[ ]:


print('Ответ на вопрос 9:', temp_df['time_session_sec'].median())


# ### 10) В каком месяце доля сессий Элис среди всех сессий наибольшая?

# In[ ]:


# делаем предположение что случаев когда Элис посещает первый сайт в последний день месяца в последние минуты перед полуночью 
# статистически незначимое кол-во и берем для анализа только время time1
temp_df = train_df[['time1', 'target']].copy()


# In[ ]:


temp_df['month'] = temp_df['time1'].dt.month  # создаем новый признак номер месяца


# In[ ]:


temp_df_alice = temp_df[alice]


# In[ ]:


group_0 = temp_df.groupby(['month'], as_index=False).target.count()  # сгруппируем по месяцам колво сессий  всех пользователей
group_0


# In[ ]:


group_alice = temp_df_alice.groupby(['month'], as_index=False).target.count() # сгруппируем по месяцам колво сессий Элис
group_alice


# In[ ]:


merge_group_a_0 = group_alice.merge(group_0, on='month')  # смержим датафреймы чтобы расчитать доли
merge_group_a_0


# In[ ]:


merge_group_a_0['share'] = merge_group_a_0['target_x']/merge_group_a_0['target_y']  # создадим новый признак - доля Элис
merge_group_a_0


# In[ ]:


merge_group_a_0[merge_group_a_0['share'] == merge_group_a_0['share'].max()]


# In[ ]:


print('Ответ на вопрос 10:', '11 месяц - Ноябрь')


# I hope you like my kernel. Thanks for upvote¶
# Надеюсь ноутбук вам понравился. Не стесняйтесь писать вопросы лично и в комментариях. Также буду рад любым замечаниям и предложениям по улучшению ноута.
# 
# Ну и если ноутбук показался вам интересным или полезным, то я буду благодарен вам за апвойт[1]
# 
# [1] Апвойт - кнопка со стрелкой вверх чуть левее от черной кнопки Edit в хедере (наверху с названием) этого кернела :)
