repo_url,tar_filename,title,decorators,imports,arxiv_categories,file location 1,file location 2,methods,paper_sections,model parameters,techniques,final values,notes,hyperparameter,annotator 1,annotator 2
https://github.com/ycd2016/acaioc2,ycd2016_acaioc2.tar.gz,Highly Efficient Memory Failure Prediction using Mcelog-based Data Mining and Machine Learning,"@app.errorhandler(500) @app.route(""/tccapi"", methods=[""GET"", ""POST""]) @app.route(""/shutdown"", methods=[""GET"", ""POST""])",scipy pandas sys json ai_hub _pickle logging sklearn os pytorch_tabnet numpy flask,cs.DB cs.LG cs.PF cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ycd2016_acaioc2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ycd2016_acaioc2.pdf,,,no,,,,no,Databases,Machine Learning
https://github.com/omni-us/research-GANwriting,omni-us_research-GANwriting.tar.gz,GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images,,utils PIL blocks glob recognizer tqdm modules_tro network_tro torchvision pathlib pairs_idx_wid_iam load_data torch os main_torch_latest string scipy marcal_augmentor_v4 loss_tro doc_augment_lib inception argparse tensorboardX numpy time vgg_tro_channel3_modi math loadData4_vgg cv2 random subprocess models Levenshtein,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/omni-us_research-GANwriting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\omni-us_research-GANwriting.pdf,,,no,,,,no,Computer Vision,Computer Vision
https://github.com/openskynetwork/aircraft-localization,openskynetwork_aircraft-localization.tar.gz,LocaRDS: A Localization Reference Data Set,@__check_index @jit(nopython=True) @__check_station @property @staticmethod,gc copy pandas sensorsparams warnings sys multiprocessing pdb tqdm lightgbm ast distutils libs Cython collections json src pwlf datetime pytorch_lightning splineaircraftpos pathos torch os csaps optimize matplotlib datautils common scipy itertools IPython numba aircraftpos pickle typing sklearn argparse joblib numpy time graph_tool round2_mlat math networkx random learnfilter pyproj bayes_opt,cs.NI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/openskynetwork_aircraft-localization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\openskynetwork_aircraft-localization.pdf,,,no,,no,,no,Miscellaneous,Software Engineering
https://github.com/ZJULearning/MaxSquareLoss,ZJULearning_MaxSquareLoss.tar.gz,Domain Adaptation for Semantic Segmentation with Maximum Squares Loss,,utils PIL copy imageio sys tools tqdm graphs distutils torchvision torch os scipy argparse tensorboardX numpy time shutil math datasets random logging,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZJULearning_MaxSquareLoss.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZJULearning_MaxSquareLoss.pdf,,,yes,SGD,yes,"learning rtae, momentum, weight decay, learning rate policy, ",no,Computer Vision,Computer Vision
https://github.com/BIT-DA/GDCAN,BIT-DA_GDCAN.tar.gz,Generalized Domain Conditioned Adaptation Network,,PIL loss data_list warnings lr_schedule pdb tqdm __future__ torchvision collections network torch os argparse numbers logger numpy tensorboardX time pre_process math random logging,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BIT-DA_GDCAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BIT-DA_GDCAN.pdf,squeeze-and-excitation networks  Max Pooling  Average Pooling  Dense Connections  Rectified Linear Units  Sigmoid Activation  Channel Attention Module,,yes,SGD,yes,"momentum, learning rate, batch size, p set via importance weighted cross validation",no,Computer Vision,Computer Vision
https://github.com/hitvoice/DrQA,hitvoice_DrQA.tar.gz,Reading Wikipedia to Answer Open-Domain Questions,,multiprocessing sys tqdm drqa prepro re datetime json collections torch os string functools spacy argparse numpy time shutil train msgpack math unicodedata random logging,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hitvoice_DrQA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hitvoice_DrQA.pdf,,,yes,Adamax,not all,"hidden units, mini batch size, dropout",no,NLP,NLP
https://github.com/nearai/program_synthesis,nearai_program_synthesis.tar.gz,NAPS: Natural Program Synthesis Dataset,"@pytest.fixture @parameterized.expand([ @watchable(""statement"") @cached_property @pytest.mark.parametrize('code', [ @ray.remote @property @staticmethod @watchable(""foreach_block"") @watchable(""if_block"") @watchable(""block"") @watchable('func_block') @contextmanager @watchable(""while_block"") @pytest.mark.parametrize('tokens,linearized', [ @classmethod @unittest.skip('TransposedPackedSequence is missing') @watchable(""ternary_expression"") @watchable(""expression"") @six.add_metaclass(MetaNode)","parameterized copy slicing_joining program_synthesis pandas contextlib warnings sys multiprocessing signal types glob tqdm glob, re, os __future__ pprint cached_property gym re tensorflow unittest collections json torchfold enum data tokenize torch os bleu pytest ray tempfile prompt_toolkit string six struct gzip functools IPython itertools codecs ply dataset pickle argparse pyparsing numbers numpy pylru time shutil math unicodedata sortedcontainers setuptools traceback mock errno random operator subprocess cPickle inspect Levenshtein",cs.LG cs.AI cs.PL stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nearai_program_synthesis.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nearai_program_synthesis.pdf,,,no,,,dataset is presented,no,Machine Learning,NLP
https://github.com/Phoenix1153/ViT_OOD_generalization,Phoenix1153_ViT_OOD_generalization.tar.gz,Delving Deep into the Generalization of Vision Transformers under Distribution Shifts,,einops PIL math json dataset collections cv2 tqdm torch os argparse numpy models torchvision io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Phoenix1153_ViT_OOD_generalization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Phoenix1153_ViT_OOD_generalization.pdf,,,yes,AdamW,yes,"extensive experiment details, image resolution, batch size, epochs, learning rate, momentum, weight decay",yes,Computer Vision,Computer Vision
https://github.com/allenxiangx/snowflakenet,allenxiangx_snowflakenet.tar.gz,SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer,@unique @classmethod @staticmethod,utils warnings sys glob config_pcn tqdm easydict pprint core chamfer_3D datetime json open3d h5py enum torch os matplotlib transforms3d importlib pointnet2_ops Chamfer3D typing argparse tensorboardX numpy time math setuptools config_c3d random logging cv2 mpl_toolkits mc models io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/allenxiangx_snowflakenet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\allenxiangx_snowflakenet.pdf,,,no,,,,no,Computer Vision,Computer Vision
https://github.com/pksvision/Deep-WaveNet-Underwater-Image-Restoration,pksvision_Deep-WaveNet-Underwater-Image-Restoration.tar.gz,Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration,,PIL sys imqual_utils glob tqdm __future__ torchvision re collections get_uiqm torch os scipy uiqm_utils dataset typing argparse numbers numpy time shutil math ntpath measure_ssim_psnr cv2 random misc uqim_utils options ssim vgg models,eess.IV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pksvision_Deep-WaveNet-Underwater-Image-Restoration.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pksvision_Deep-WaveNet-Underwater-Image-Restoration.pdf,Dilated Causal Convolution  Mixture of Logistic Distributions  WaveNet,,yes,Adam,not all,"learning rate, batch size",no,Computer Vision,Computer Vision
https://github.com/shiyongming/calib-dataset-eval,shiyongming_calib-dataset-eval.tar.gz,Quantizing deep convolutional networks for efficient inference: A whitepaper,,copy sys tools datasetapi mmcv re json collections torch os matplotlib itertools xml urllib sklearn argparse mmdet numpy time pycocotools setuptools cv2 random mpl_toolkits,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shiyongming_calib-dataset-eval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shiyongming_calib-dataset-eval.pdf,,,yes,SGD,no,,no,Machine Learning,Machine Learning
https://github.com/nash169/learn-diffeomorphism,nash169_learn-diffeomorphism.tar.gz,Euclideanizing Flows: Diffeomorphic Reduction for Learning Stable Dynamical Systems,@optimizer.setter @loss.setter @target.setter @input.setter @attractor.setter @model.setter @property @diffeomorphism.setter,setuptools learn_diffeomorphism torch argparse os mpl_toolkits numpy matplotlib,cs.RO cs.LG cs.SY eess.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nash169_learn-diffeomorphism.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nash169_learn-diffeomorphism.pdf,,,yes,Adam,yes,"default adam parameters, learning rate, l2-regularization",no,Robotic,Robotic
https://github.com/Soumyabrata/CloudSegNet,Soumyabrata_CloudSegNet.tar.gz,CloudSegNet: A Deep Network for Nychthemeron Cloud Image Segmentation,,calculate_score PIL scipy keras pandas csv sys h5py cv2 roc_items sklearn os score_card __future__ numpy create_dataset matplotlib,physics.ao-ph cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Soumyabrata_CloudSegNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Soumyabrata_CloudSegNet.pdf,,,yes,AdaDelta,yes,"epochs, batch size, learning rate",no,Physics,Computer Vision
https://github.com/genforce/sefa,genforce_sefa.tar.gz,Closed-Form Factorization of Latent Semantics in GANs,"@st.cache(allow_output_mutation=True, show_spinner=False) @staticmethod",utils SessionState cv2 tqdm torch os argparse subprocess base64 numpy models streamlit,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/genforce_sefa.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\genforce_sefa.pdf,,"Comparison with Supervised ApproachWe compare our closed-form algorithm with the stateof-the-art supervised method, InterFaceGAN [24]. We conduct experiments on face synthesis models due to the well definition of facial attributes. In particular, we make comparison between SeFa and InterFaceGAN on both the conventional generator (i.e., PGGAN [16]) and the stylebased generator (i.e., StyleGAN [17]). Qualitative Results. Fig. 5 visualizes some manipulation results by using the identified semantics. We can tell that SeFa achieves similar performance as InterFaceGAN from the perspective of editing pose, gender, eyeglasses, and expression (smile), suggesting its effectiveness. More importantly, InterFaceGAN requires sampling numerous data and pre-training attribute predictors. By contrast, SeFa is completely independent of data sampling and model training, which is more efficient and generalizable.Re-scoring Analysis. For quantitative analysis, we train an attribute predictor on CelebA dataset [19] with ResNet-50 structure [11], following [24]. With this predictor, we are able to perform re-scoring analysis to quantitatively evaluate whether the identified directions can properly represent the corresponding attributes. In particular, we randomly sample 2K images and manipulate them along a certain discovered direction. We then use the prepared predictor to check how the semantic score varies in such manipulation process. Tab. 2 shows the results where we have three observations. (i) SeFa can adequately control some attribute, such as pose and gender, similar to Inter-FaceGAN. (ii) When altering one semantic, InterFaceGAN shows stronger robustness to other attributes, benefiting from its supervised training manner. For example, the age and eyeglasses corresponding to the same latent direction identified by SeFa. That is because the training data is somewhat biased (i.e., older people are more likely to wear eyeglasses), as pointed out by [24]. By contrast, involving labels as the supervision can help learn a more accurate direction to some extent. (iii) SeFa fails to discover the direction corresponding to eyeglasses. The reason is that the presence of eyeglasses is not a large variation and hence does not meet the optimization objective in Eq. ( 4).Diversity Comparison. Supervised approach highly depends on the available attribute predictors. By contrast, our method is more general and can find more diverse semantics in the latent space. As shown in Fig. 6 (a), we successfully identify the directions corresponding to hair color, hair style, and brightness. This surpasses InterFaceGAN since predictors for these attributes are not easy to acquire in practice. Also, supervised methods are usually limited by the training objective. For example, InterFaceGAN is proposed to handle binary attributes [24]. In comparison, our method can identify more complex attributes, like the different hair styles shown in Fig. 6 (b).",no,,,,no,Computer Vision,Computer Vision
https://github.com/histocartography/histocartography,histocartography_histocartography.tar.gz,HistoCartography: A Toolkit for Graph Analytics in Digital Pathology,@classmethod @abstractmethod @property @staticmethod,requests PIL copy pandas mlflow csv yaml warnings sys multiprocessing glob skimage tqdm better_apidoc torchvision distutils pathlib re collections json unittest histocartography h5py torch os matplotlib importlib dgl scipy functools itertools hashlib bisect typing pickle sklearn numpy time abc shutil math setuptools traceback networkx cv2 logging random subprocess io,cs.CV eess.IV cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/histocartography_histocartography.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\histocartography_histocartography.pdf,,,no,,no,,no,Computer Vision,Biology
https://github.com/lephong/mulrel-nel,lephong_mulrel-nel.tar.gz,Improving Entity Linking by Modeling Latent Relations between Mentions,@staticmethod,re nel sys json random torch argparse numpy pprint time io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lephong_mulrel-nel.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lephong_mulrel-nel.pdf,,,yes,"Adam, manual tuning",yes,,yes,NLP,NLP
https://github.com/malteschilling/ddrl_hexapod,malteschilling_ddrl_hexapod.tar.gz,Decentralized Deep Reinforcement Learning for a Distributed and Adaptive Locomotion Controller of a Hexapod Robot,,importlib threading tensorflow baselines pandas seaborn sys multiprocessing statistics os numpy gym_gazebo2 matplotlib time gym,cs.RO cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/malteschilling_ddrl_hexapod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\malteschilling_ddrl_hexapod.pdf,,"Lower LevelMotor Control Environment a t < l a t e x i t s h a 1 _ b a s e 6 4 = "" y a V m I D 0 / V N h / v q a n S / e Q x O 4 4 W R w = "" > A A A C C n i c b V B L T g J B F H y D P 8 Q f 6 t J N R 2 L i i s w g i S 5 J 3 L j E K J 8 E J q S n p 4 E O P Z 9 0 v y G S C T f Q r Fig. 1: Visualization of influences for the biological inspired approach: On the left (a) the standard view of interaction with the environment in reinforcement learning [13] is extended to a hierarchical perspective [10] as advocated, for example in [6]. For higher level control (shown in blue) this is in agreement with what we know on the structure of motor control in mammals [14] about descending pathways and modulation of lower level control centers (shown in green) in the spinal cord (see schematic b) in blue and green). Such structures are shared not only in mammals, but also in invertebrates and insects [15], see c). Work in such simpler model systems allows more detailed analysis of interaction with the environment which has stressed the importance of very fast and local reflex activity controlled directly on the lowest level or that are even realized by passive properties as muscle elasticities or preflexes (shown in orange). One important characteristic emphasized by this work is the emergence of behavior as a result of decentralized and locally interacting concurrent control structures (bottom part of c), one example is given by the decentralized control structure found in stick insects [16], but this concurrency is as well assumed in primates [17]. In the presented approach, this decentralized architecture (bottom part of c) is used on a hexapod and learning of the six local control modules (realized as deep neural networks) is driven by a reward signal as in reinforcement learning.d 7 D n X H r J b y G J 7 C B W Q h Y S S e V q v f y q s u L p d B o 2 9 9 W b m N z a 3 s n v 1 v Y 2 z 8 4 P C o e n z R 1 l C j G G y y S k W p 7 V H M p Q t 5 A g Z K 3 Y 8 V p 4 E n e 8 k a 3 M 7 8 1 5 k q L K H z E S c z d g A 5 C 0 R e M o p E e a A 9 7 x Z J d t u c g 6 8 T J S A k y 1 H v F n 6 4 f s S T g I T J J t e 4 4 d o x u S h U K J v m 0 0 E 0 0 j y k b 0 Q H v G B r S g G s 3 n U e d k g u j + K Q f K f N C J H P 1 7 0 Z K A 6 0 n g W c m A 4 p D v e r N x P + 8 j j 8 W s c 5 u P S 2 O L S f B / o 2 b i j B O k I d s E a S f S I I R m f V C f K E 4 Q z k x h D I l z F 8 I G 1 J F G Z r 2 C q Y k Z 7 W S d d K s l J 2 r c u W + W q p V s 7 r y c A b n c A k O X E M N 7 q A O D W A w g B d 4 h T f r 2 X q 3 P q z P x W j O y n Z O Y Q n W 1 y + Z 0 5 u C < / l a t e x i t > S t < l a t e x i t s h a 1 _ b a s e 6 4 = "" 6 g X k k R n P r g 3 2 M S 5 l 8 q 0 c f h o L 4 a 8 = "" > A A A C C n i c b V B L T g J B F H z j F / G H u n T T k Z i 4 I j N I o k s S N y 4 x y C e B C e n p a a B D z y f d b 4 h k w g 1 0 q / d w Z 9 x 6 C a / h C W x g F g J W 0 k m l 6 r 2 8 6 v J i K T T a 9 r e 1 s b m 1 v b O b 2 8 v v H x w e H R d O T p s 6 S h T j D R b J S L U 9 q r k U I W + g Q M n b s e I 0 8 C R v e a O 7 m d 8 a c 6 V F F D 7 i J O Z u Q A e h 6 A t G 0 U j 1 e g 9 7 h a J d s u c g 6 8 T J S B E y 1 H q F n 6 4 f s S T g I T J J t e 4 4 d o x u S h U K J v k 0 3 0 0 0 j y k b 0 Q H v G B r S g G s 3 n U e d k k u j + K Q f K f N C J H P 1 7 0 Z K A 6 0 n g W c m A 4 p D v e r N x P + 8 j j 8 W s c 5 u P S 2 O L S f B / q 2 b i j B O k I d s E a S f S I I R m f V C f K E 4 Q z k x h D I l z F 8 I G 1 J F G Z r 2 8 q Y k Z 7 W S d d I s l 5 z r U v m h U q x W s r p y c A 4 X c A U O 3 E A V 7 q E G D W A w g B d 4 h T f r 2 X q 3 P q z P x e i G l e 2 c w R K s r 1 + C v 5 t 0 < / l a t e x i t > state R t < l a t e x i t s h a 1 _ b a s e 6 4 = "" c T e d c t B a I s Y C W 6 q a W 0 F l E / d E 4 V 8 = "" > A A A C C n i c b V D J T g J B F H z j i r i h H r 1 0 J C a e y A y S 6 J H E i 0 d c W B K Y k J 6 e B j r 0 L O l + Q y Q T / k C v + h / e j F d / w t / w C 2 x g D g J W 0 k m l 6 r 2 8 6 v J i K T T a 9 r e 1 t r 6 x u b W d 2 8 n v 7 u 0 f H B a O j h s 6 S h T j d R b J S L U 8 q r k U I a + j Q M l b s e I 0 8 C R v e s O b q d 8 c c a V F F D 7 i O O Z u Q P u h 6 A l G 0 U g P 9 1 3 s F o p 2 y Z 6 B r B I n I 0 X I U O s W f j p + x J K A h 8 g k 1 b r t 2 D G 6 K V U o m O S T f C f R P K Z s S P u 8 b W h I A 6 7 d d B Z 1 Q s 6 N 4 p N e p M w L k c z U v x s p D b Q e B 5 6 Z D C g O 9 L I 3 F f / z 2 v 5 I x D q 7 9 T Q / t p g E e 9 d u K s I 4 Q R 6 y e Z B e I g l G Z N o L 8 Y X i D O X Y E M q U M H 8 h b E A V Z W j a y 5 u S n O V K V k m j X H I u S + W 7 S r F a y e r K w S m c w Q U 4 c A V V u I U a 1 I F B H 1 7 g F d 6 s Z + v d + r A + 5 6 N r V r Z z A g u w v n 4 B g R m b c w = = < / l a t e x i t > reward action Rt+1 < l a t e x i t s h a 1 _ b a s e 6 4 = "" t f o S X 8 X n a W S 0 W + b a 3 1 M k 9 8 c R Y 9 Q = "" > A A A C D n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k U Q h J J U Q Z c F N y 6 r 2 A e 0 o U w m 0 3 b o Z B J m b o o l 9 B 9 0 q / / h T t z 6 C / 6 G X + C 0 z c K 2 H h g 4 n H M v 9 8 z x Y 8 E 1 O s 6 3 l V t b 3 9 j c y m 8 X d n b 3 9 g + K h 0 c N H S W K s j q N R K R a P t F M c M n q y F G w V q w Y C X 3 B m v 7 w d u o 3 R 0 x p H s l H H M f M C 0 l f 8 h 6 n B I 3 U f O i m e O F O u s W S U 3 Z m s F e J m 5 E S Z K h 1 i z + d I K J J y C R S Q b R u u 0 6 M X k o U c i r Y p N B J N I s J H Z I + a x s q S c i 0 l 8 7 i T u w z o w R 2 L 1 L m S b R n 6 t + N l I R a j 0 P f T I Y E B 3 r Z m 4 r / e e 1 g x G O d 3 X q a H 1 t M g r 0 b L + U y T p B J O g / S S 4 S N k T 3 t x g 6 4 Y h T F 2 B B C F T d / s e m A K E L R N F g w J b n L l a y S R q X s X p Y r 9 1 e l q p P V l Y c T O I V z c O E a q n A H N a g D h S G 8 w C u 8 W c / W u / V h f c 5 H c 1 a 2 c w w L s L 5 + A U i h n O s = < / l a t e x i t > St+1 < l a t e x i t s h a 1 _ b a s e 6 4 = "" u v k b b Y W g B T P y G V q 2 u I 8 6 E j 7 G p n w = "" > A A A C D n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k U Q h J J U Q Z c F N y 4 r 2 g e 0 o U w m 0 3 b o Z B J m b o o l 9 B 9 0 q / / h T t z 6 C / 6 G X + C 0 z c K 2 H h g 4 n H M v 9 8 z x Y 8 E 1 O s 6 3 l V t b 3 9 j c y m 8 X d n b 3 9 g + K h 0 c N H S W K s j q N R K R a P t F M c M n q y F G w V q w Y C X 3 B m v 7 w d u o 3 R 0 x p H s l H H M f M C 0 l f 8 h 6 n B I 3 U f O i m e O F O u s W S U 3 Z m s F e J m 5 E S Z K h 1 i z + d I K J J y C R S Q b R u u 0 6 M X k o U c i r Y p N B J N I s J H Z I + a x s q S c i 0 l 8 7 i T u w z o w R 2 L 1 L m S b R n 6 t + N l I R a j 0 P f T I Y E B 3 r Z m 4 r / e e 1 g x G O d 3 X q a H 1 t M g r 0 b L + U y T p B J O g / S S 4 S N k T 3 t x g 6 4 Y h T F 2 B B C F T d / s e m A K E L R N F g w J b n L l a y S R q X s X p Y r 9 1 e l q p P V l Y c T O I V z c O E a q n A H N a g D h S G 8 w C u 8 W c / W u / V h f c 5 H c 1 a 2 c w w L s L 5 + A U p L n O w = < / l a tcontrol principle into a control architecture for six-legged walking and want to analyze how learning in general and DRL -as a contribution of this publication -benefits from this structure and how in the end control performance is affected by such a structure. Therefore, we train and compare a decentralized control architecture and compare this to learning of a baseline centralized control approach. As a result, the decentralized controller learns faster and, importantly, is not only able to produce comparable walking behavior, but even results in significantly better performance and is able to generalize as well towards novel environments. Decentralization appears as a viable principle for DRL and an important aspect for adaptive behavior [6] as demonstrated here in our simulation on a six-legged robot. B. Deep Reinforcement Learning FrameworkReinforcement Learning is characterized by an agent that is interacting in an environment (Fig. 1 a). While the agent produces actions, it gets as a response an updated state of the environment and a reward signal. The goal of the agent is to learn a policy π(S) for sequential decision making that maximizes the accumulated long-term return. During learning the agent has to explore possible alternatives and while getting more confident should come up with a policy it can exploit (for an introduction see [1], [13]). Sequential decision making can be formalized as a Markov Decision Process which provides and is defined as a tuple of:• observable states S, • possible actions A, • a reward signal R providing the immediate reward after choosing an action from the current state,• transition probabilities P that describe the probability distribution over states after following an action when being in the current specific state, and• a discount factor γ that describes how to decrease the weights of future rewards. We are dealing with a continuous control task: the state space as given by the sensory signals is continuous as well as the action space which corresponds to the joint motor signals.Overall, these spaces are high dimensional and continuous which requires to rely on function approximation for learning As a framework for DRL we employed OpenAI's gym. The connection to the simulator through ROS was wrapped as a new environment through a connector using ROS2Learn [31]. This allowed to use the standard baseline implementations of DRL algorithms [32]. In this approach Proximal Policy Optimization (PPO) [33] was used as it has shown to work well on continuous tasks without the need of intensive hyperparameter tuning. As a further advantage, it has a comparably low sampling complexity (amount of training samples required to find usable policies).The reward function was defined as the mean velocity over an episode (1024 iteration steps, equals nearly 41 seconds). The agent uses the received reward R t+1 after executing an action A t in state S t in order to directly optimize its' policy π. A neural network was used as a function approximator for the learned policy. The policy networks consisted of two hidden layers of each 64 units with tanh activation functions. Updating the neural networks' weights is called an epoch. After training, the policy network should generalize to novel sensed states and modulate motor output in order to achieve a maximal external reward. The next section will describe in detail the two compared architectures that employ such policy networks.",no,,,,no,Robotic,Robotic
https://github.com/Dengsgithub/DRD-Net,Dengsgithub_DRD-Net.tar.gz,Detail-recovery Image Deraining via Context Aggregation Networks,,pathlib keras tensorflow model cv2 random skimage argparse os numpy string generator,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Dengsgithub_DRD-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Dengsgithub_DRD-Net.pdf,,,yes,Adam,yes,"batch size, learning rate, epochs",no,Computer Vision,Computer Vision
https://github.com/wangxiao5791509/SiamDW_tracker_revised,wangxiao5791509_SiamDW_tracker_revised.tar.gz,Deeper and Wider Siamese Networks for Real-Time Visual Tracking,@engine.fitness_register,utils yaml sys glob test_siamrpn easydict shapely gaft __future__ pprint torchvision matlab pathlib _init_paths core json collections torch os concurrent ray siamrpn tracker scipy dataset test_siamfc argparse tensorboardX numpy time shutil math hyperopt cv2 random logging mpi4py models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wangxiao5791509_SiamDW_tracker_revised.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wangxiao5791509_SiamDW_tracker_revised.pdf,1x1 Convolution  Convolution  Local Response Normalization  Grouped Convolution  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  AlexNet,,yes,SGD,yes,"epochs, learning rate, weight decay, momentum, mini batch size",no,Computer Vision,Computer Vision
https://github.com/shi27feng/transformers.satisfy,shi27feng_transformers.satisfy.tar.gz,Transformer-based Machine Learning for Fast SAT Solvers and Logic Synthesis,@property @staticmethod,einops utils copy loss sys cnf torch_sparse layers pdb tqdm models2 re collections json linalg models src data torch os matplotlib torch_geometric torch_scatter cdcl typing pickle argparse numpy cdcl_heuristic heapq formula2cnf time abc shutil train math optimizer networkx attentions random args inspect,cs.NE cs.AI cs.LG cs.LG cs.LO cs.NE stat.ML stat.ML cs.AI cs.LG cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shi27feng_transformers.satisfy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shi27feng_transformers.satisfy.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Label Smoothing  Residual Connection  Dense Connections  Softmax  Scaled Dot-Product Attention  Layer Normalization  Dropout  Adam  Multi-Head Attention  Byte Pair Encoding  Transformer  Graph Attention Network,,yes,Adam,yes,"betas, learning rate",yes,Computer Vision,Machine Learning
https://github.com/jkkummerfeld/irc-disentanglement,jkkummerfeld_irc-disentanglement.tar.gz,A Large-Scale Corpus for Conversation Disentanglement,,math unicodedata re sys collections traceback dynet dynet_config random logging reserved_words argparse sklearn string ortools __future__ numpy time,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jkkummerfeld_irc-disentanglement.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jkkummerfeld_irc-disentanglement.pdf,,,no,,no,,yes,NLP,NLP
https://github.com/supratikbanerjee/SubPixel-BackProjection_SuperResolution,supratikbanerjee_SubPixel-BackProjection_SuperResolution.tar.gz,Sub-Pixel Back-Projection Network For Lightweight Single Image Super-Resolution,,PIL yaml sys skimage tqdm torchvision util datetime collections data visdom torch os argparse numpy time math random cv2 models,eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/supratikbanerjee_SubPixel-BackProjection_SuperResolution.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\supratikbanerjee_SubPixel-BackProjection_SuperResolution.pdf,Convolution,,yes,Adam,yes,"minibatch size, betas, learning rate, decay",yes,Computer Vision,Computer Vision
https://github.com/jonathanventura/cylindricalsfmlearner,jonathanventura_cylindricalsfmlearner.tar.gz,Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video,,utils SfMLearner sys glob tqdm __future__ pprint tensorflow headcam os matplotlib nets scipy tf_cylindrical argparse joblib numpy time math data_loader cv2 random,cs.CV cs.LG cs.RO cs.CV cs.LG cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jonathanventura_cylindricalsfmlearner.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jonathanventura_cylindricalsfmlearner.pdf,Max Pooling  CARLA: An Open Urban Driving Simulator  Max Pooling,,no,,,,no,Computer Vision,Computer Vision
https://github.com/delemottelab/demystifying,delemottelab_demystifying.tar.gz,Molecular Insights from Conformational Ensembles via Machine Learning,,scipy re demystifying biopandas sys collections glob operator logging sklearn argparse os __future__ numpy mdtraj matplotlib benchmarking,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/delemottelab_demystifying.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\delemottelab_demystifying.pdf,Restricted Boltzmann Machine  Dense Connections  Feedforward Network  Principal Components Analysis  AutoEncoder,"CONCLUSIONSIn this study, we have applied various ML methods to identify important features from molecular simulations. We constructed a toy model that mimics real macromolecular behavior to perform a quantitative comparison between the methods and derive insights regarding their applicability for practical purposes.When optimizing the performance of different ML methods, we found that the choice of proper input features had larger effect than scrupulous hyperparameter tuning. Specifically, a great performance increase was observed when switching from raw Cartesian to internal distancebased features (potentially with a cutoff). Supervised learning methods, in particular, outperform unsupervised methods on a suboptimal Cartesian data set. Furthermore, an MLP classifier was shown to be significantly better at identifying all important features of a system compared to an RF or computing the KL divergence. This indicates that a NN is able to learn the transformation of input data to an optimal representation and that more complex deep learning approaches are robust analysis tools when little, or even no, prior knowledge is available to guide the choice of input features. However, MLP's output was arguably more difficult to interpret than that of the other supervised methods. For unsupervised learning on an optimal set of internal coordinates, all methods (PCA, RBM, and AE) identified important residues with similar accuracies on smaller data sets. On larger data sets, however, PCA outperformed both RBMs and AEs with regards to accuracy as well as computational efficiency.Finally, we successfully applied this protocol to derive key insights into three distinct types of biomolecular processes: the conformational rearrangements of the soluble protein calmodulin, the effect of ligand binding to a GPCR, and the allosteric coupling of an ion channel VSD to a transmembrane potential. Rather than trying to identify one optimal method for all practical applications, we condensed our learnings into a protocol in the form of a checklist. In short, the ML methods were designed for different purposes, and the best choice of analysis methods depends on the question at hand. Supervised learning should be used when applicable, for example, for studying the small allosteric switches induced by ligand binding to a receptor or comparing distinct metastable states, whereas unsupervised methods can derive ensemble features of unlabeled data, thus capturing the major conformational change of a protein.This work sheds, to our knowledge, new light on ML as a pillar in interpretation of biomolecular systems. The possible applications of our approach spans from interpreting large amounts of simulation or experimental ensemble data to selecting CVs for enhanced sampling simulations. Compared to the computationally expensive MD simulations, these insights are cheap: all results in this study were computed on commodity hardware using software published in the public repository ''demystifying'' on GitHub (Table 1).Humans did not evolve to interpret large sets of highdimensional data by mere visual inspection. ML methods were, on the contrary, specifically designed to process big data sets. Nevertheless, despite the increasing interest and faith put into ML, current state-of-the-art methods are not at a stage at which computers are able to set up, run, and analyze simulations autonomously. Instead, we have shown how statistical models and algorithms commonly used to solve ML problems provide a powerful toolbox to efficiently make data-driven interpretations of biomolecular systems. As the timescales accessible by simulations increase, and the popularity of ML tools continue to thrive across many scientific areas, we anticipate that our approach can be useful to aid many researchers in demystifying complex simulations.",yes,"Adam, cross validation",yes,,yes,Biology,Biology
https://github.com/thomaspinder/SteinGP,thomaspinder_SteinGP.tar.gz,Stein Variational Gaussian Processes,@property @staticmethod @add_regression @add_classficiation,steingp pandas tqdm tensorflow datetime os matplotlib scipy urllib ssl pickle typing sklearn numpy time shutil tarfile setuptools logging mpl_toolkits tensorflow_probability gpflow zipfile,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thomaspinder_SteinGP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thomaspinder_SteinGP.pdf,Gaussian Process,,no,,,,no,Machine Learning,Machine Learning
https://github.com/dilinwang820/fast-energy-aware-splitting,dilinwang820_fast-energy-aware-splitting.tar.gz,Energy-Aware Neural Architecture Optimization with Fast Splitting Steepest Descent,,"utils sys __future__ torchvision sp_mbnet json collections sp_conv compute_flops torch os matplotlib mobilenetv1 sp_mbnetv2 argparse, logging, os, math, time seaborn argparse numpy time config shutil math random logging dataloader",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dilinwang820_fast-energy-aware-splitting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dilinwang820_fast-energy-aware-splitting.pdf,,"|Splitting Neurons:(a) Computing the splitting index of each neuron using gradient-based approximation (Section 4); (b) Finding the optimal set of neurons to split by solving the energy-aware allocation problem in Equation ( 6); (c) For each neuron selected above (step 2a), split it into two equally weighted off-springs along their splitting gradients, following Equation ( 5).case of big data. However, we can conveniently address this by approximating {g } with subsampled mini-batches B. In the case of single-neuron networks, that is,Sv = ∇ η F (η), with F (η) = 1 ||B|| ||B|| i=1 Φ σ(θ, x i ) + η ∇ 2 θθ σ(θ, x i )v .Assume we sweep the training data T times to train the Rayleigh-Quotient to convergence (see Equation ( 7)). In this way, the splitting time complexity for approximating all splitting indexes and gradients would be only O(T nd 2 ) (T is often a small constant). More importantly, a significant advantage of our gradient-based approximation is that the space complexity is only O(nd). In this way, all calculation could be efficient performed on GPUs. This given us an algorithm for splitting that is almost as efficient as back-propagation.Overall Algorithm Our overall algorithm in shown in Algorithm 1, which improves over Liu et al. (2019b) by offering much lower time and space complexity, and the flexibility of incorporating energy and other costs of different neurons. It can be implemented easily using modern deep learning frameworks such as Pytorch (Paszke et al., 2017). Our code is available at https://github. com/dilinwang820/fast-energy-aware-splitting.|",yes,"Grid Search, SGD",yes,"sparsity hyperparameter, momentum, weight decay, batch size, learning rate, epochs, decay",yes,Machine Learning,Machine Learning
https://github.com/katiana22/GDM-PCE-surrogates,katiana22_GDM-PCE-surrogates.tar.gz,Manifold learning-based polynomial chaos expansions for high-dimensional surrogate models,@staticmethod,"electric_potential copy DimensionReduction sys skopt UQpy LoktaVoltera DiffusionEquation pylab os, subprocess rand_cmap matplotlib scipy functools itertools sklearn numpy time math datafold colorsys random mpl_toolkits",physics.data-an cs.LG physics.comp-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/katiana22_GDM-PCE-surrogates.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\katiana22_GDM-PCE-surrogates.pdf,,,no,,,,no,Physics,Physics
https://github.com/silvanmelchior/CBF-SSM,silvanmelchior_CBF-SSM.tar.gz,Structured Variational Inference in Partially Observable Unstable Gaussian Process State Space Models,@abc.abstractmethod @property @staticmethod @constrain.setter,warnings sys tqdm __future__ tensorflow cbfssm doctest os matplotlib importlib scipy functools sklearn numpy time shutil abc math setuptools random,cs.LG cs.SY eess.SY stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/silvanmelchior_CBF-SSM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\silvanmelchior_CBF-SSM.pdf,Gaussian Process,,no,,,,no,Machine Learning,Machine Learning
https://github.com/seyonechithrananda/selfies-mirror,seyonechithrananda_selfies-mirror.tar.gz,Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation,"@pytest.mark.skip(reason=""covered by round-trip test"") @pytest.fixture @pytest.mark.skip(reason=""eMolecules dataset not on GitHub"") @pytest.mark.parametrize(""test_name, column_name"", datasets) @pytest.fixture() @staticmethod","faulthandler pandas yaml GrammarVAE_grammar sys pdb __future__ os, sys examples selfies rdkit h5py nltk torch os pytest matplotlib deepsmiles GrammarVAE_codes six GPlus2S itertools adjusted_selfies_fcts typing tensorboardX numpy time math data_loader setuptools random one_hot_converter",cs.LG physics.chem-ph quant-ph stat.ML cs.NE cs.LG physics.chem-ph physics.comp-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/seyonechithrananda_selfies-mirror.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\seyonechithrananda_selfies-mirror.pdf,Interpretability  Genetic Algorithms,,yes,Experimental Tuning,no,,yes,Machine Learning,Biology
https://github.com/fiveai/making-better-mistakes,fiveai_making-better-mistakes.tar.gz,Making Better Mistakes: Leveraging Class Hierarchies with Deep Networks,@functools.lru_cache(maxsize=None),copy conditional csv yaml warnings glob tqdm pprint torchvision configargparse distutils re lzma datetime collections json nltk torch os tree_format matplotlib functools typing pickle hierarchies argparse tensorboardX numpy time shutil math networkx random operator better_mistakes,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fiveai_making-better-mistakes.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fiveai_making-better-mistakes.pdf,,"Experimental resultsIn the following, we analyse the performance of the two approaches described in Sec. 3.1 and Sec. 3.2, which we denote by HXE and soft labels, respectively. Besides a vanilla cross-entropy-based flat classifier, we also implemented and compared against the methods proposed by Redmon & Farhadi [33] (YOLO-v2) 3 , Frome et al. [14] (DeViSE), and Barz & Denzler [4]. As mentioned in Sec. 1, these methods represent, to the best of our knowledge, the only modern attempts to deliberately reduce the semantic severity of a classifier's mistakes that are generally applicable to any modern architecture. Note, though, that we do not run DeViSE on iNaturalist-19, as the class IDs of this dataset are alien to the corpus used by word2vec [24].Finally, we do not compare against the ""generalist/expert"" architectures surveyed in Sec. 2.3 for reasons explained in Appendix B.Since we are interested in understanding the mechanisms by which the above metrics can be improved, it is essential to use a simple configuration that is common between all of the algorithms taken into account. We use a ResNet-18 architecture (with weights pretrained on ImageNet) trained with Adam [32] for 200,000 steps and mini-batches of size 256. We use a learning rate of 1e−5 unless specified otherwise. Further implementation details are deferred to Appendix C. Main results. In Fig. 3 and 4 we show how it is possible to effectively trade off top-1 error to reduce hierarchical error, by simply adjusting the hyperparameters α and β in Eqn. 5 and 7. Specifically, increasing α corresponds to (exponentially) discounting information down the hierarchy, thus more severely penalising mistakes where the predicted class is further away from the ground truth. Similarly, decreasing β in the soft-label method amounts to progressively shifting the label mass away from the ground truth and towards the neighbouring classes. Both methods reduce to the cross-entropy in the respective limits α → 0 and β → ∞. Moreover, notice that varying β affects the entropy of the distribution representing a soft label, where the two limit cases are β=∞ for the standard one-hot case and β=0 for the uniform distribution. We experiment with 0.1 ≤ α ≤ 0.9 and 4 ≤ β ≤ 30.To limit noise in the evaluation procedure, for both of our methods and all of the competitors, we fit a 4th-degree polynomial to the validation loss (after having discarded the first 50,000 training steps) and pick the epoch corresponding to its minimum along with its four neighbours. Then, to produce the points reported in our plots, we average the results obtained from these five epochs on the validation set, while reserving the test set for the experiments of Table 1. Notice how, in Fig. 4, when considering the hierarchical distance with k=1, methods are almost perfectly aligned along the plot diagonal, which demonstrates the strong linear correlation between this metric and the top-1 error. This result is consistent with what is observed in [37], which in 2011 led the organisers of the ILSVRC workshop to discard rankings based on hierarchical distance.When considering the other metrics described in Sec. 4.2, a different picture emerges. In fact, a tradeoff between top-1 error and hierarchical distance is evident in Fig. 3 and in the plots of Fig. 4 with k=5 and k=20. Notice how the points on the plots belonging to our methods outline a set of tradeoffs that subsumes the prior art. For example, in Fig. 3, given any desired tradeoff betweeen top-1 error and hierarchical distance of mistakes on tieredImageNet-H, it is better to use HXE than any other method. A similar phenomenon is observable when considering the average hierarchical distance of top-5 and top-20 (Fig. 4), although in these cases it is better to use the soft labels. The only exception to this trend is represented by Barz & Denzler [4] on tieredImageNet-H, which can achieve slightly lower average hierarchical distance for k=5 or k=20 than soft labels with β=5 at a significant cost in terms of top-1 error.Using the results illustrated in Fig. 3 and 4, we pick two reasonable operating points for both of our proposals: one for the high-distance/low-top1-error regime, and one for the low-distance/high-top1-error regime. We then run both of these configurations on the test sets and report our results in Table 1. Means and 95% confidence intervals are obtained from the five best epochs.The trends observed on the validation set largely repeat themselves on the test set. When one desires to prioritise top-1 error, then soft labels with high β or HXE with low α are more appropriate, as they outperform the crossentropy on the hierarchical-distance-based metrics while being practically equivalent in terms of top-1 error. In cases where the hierarchical measures should be prioritised instead, it is preferable to use soft labels with low β or HXE with high α, depending on the particular choice of hierarchical metric. Although the method of Barz & Denzler is competitive in this regime, it also exhibits the worst deterioration in top-1 error with respect to the cross-entropy.Our experiments generally indicate, over all tested methods, an inherent tension between performance in the top-1 sense and in the hierarchical sense. We speculate that there may be a connection between this tension and observations proceeding from the study of adversarial examples indicating a tradeoff between robustness and (conventional) accuracy, as in e.g. [40,49]. Can hierarchies be arbitrary? Although the lexical Word-Net hierarchy and the biological taxonomy of iNaturalist are not visual hierarchies per se, they reflect visual relationships between the objects represented in the underlying datasets. Since deep networks leverage visual features, it is interesting to investigate the extent to which the structure of a particular hierarchy is important. The connection between visual and semantic proximity has also been explored in such works as [11,13]. But what happens if we impose an arbi-Table 1: Results on the test sets of tieredImageNet-H (top) and iNaturalist-19 (bottom), with 95% confidence intervals. For each column of each dataset, the best entry is hightlighted in yellow, while the worst is highlighted in gray. D. Pruning the WordNet hierarchyThe ImageNet dataset [37] was generated by populating the WordNet [26] hierarchy of nouns with images. WordNet is structured as a graph composed of a set of IS-A parentchild relationships. Similarly to the work of Morin & Bengio [27] and Redmon & Farhadi [33], our proposed hierarchical cross entropy loss (HXE, Sec. 3.1) also relies on the assumption that the hierarchy underpinning the data takes the form of a tree. Therefore, we modified the hierarchy to obtain a tree from the WordNet graph.First, for each class, we found the longest path from the corresponding node to the root. This amounts to selecting the paths with the highest discriminative power with respect to the image classes. When multiple such paths existed, we selected the one with the minimum number of new nodes and added it to the new hierarchy. Second, we removed the few non-leaf nodes with a single child, as they do not possess any discriminative power.Finally, we observed that the pruned hierarchy's root is not PHYSICAL ENTITY, as one would expect, but rather the more general ENTITY. This is problematic, since EN-TITY contains both physical objects and abstract concepts, while tieredImageNet classes only represent physical objects. Upon inspection, we found that this was caused by the classes BUBBLE, TRAFFIC SIGN, and TRAFFIC LIGHTS being connected to SPHERE and SIGN, which are considered abstract concepts in the WordNet hierarchy. Instead, we connected them to SPHERE, ARTIFACT and SIGNBOARD, respectively, thus connecting them to PHYSICAL ENTITY.Even though our second proposed method (soft labels), as well as the cross-entropy baseline, DeViSE [14] and Barz & Denzler [4], do not make any assumption regarding the structure of the hierarchy, we ran them using this obtained pruned hierarchy for consistency of the experimental setup. Note that though this distribution shares some qualitative similarities with the ones shown in Fig. 1, it is nonetheless substantially different. This suggests that the shapes of the mistake-severity distributions for the various DNN architectures studied cannot be explained by properties of the dataset alone.",yes,Adam,not all,"steps, mini batch size, learning rate, ",no,Computer Vision,Computer Vision
https://github.com/realsonalkumar/Mish-Mash-Hackathon,realsonalkumar_Mish-Mash-Hackathon.tar.gz,Content Enhanced BERT-based Text-to-SQL Generation,@classmethod @property,"copy tabulate bert ujson lib tqdm __future__ stanza sqlova torchvision tensorflow re os, sys, argparse, re, json wikisql records modeling json collections corenlp torch os matplotlib six sqlnet argparse babel numpy math unicodedata os, json random os, sys, json",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/realsonalkumar_Mish-Mash-Hackathon.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\realsonalkumar_Mish-Mash-Hackathon.pdf,,,no,,,,no,NLP,NLP
https://github.com/cjx0525/BGCN,cjx0525_BGCN.tar.gz,Bundle Recommendation with Graph Convolutional Networks,@property,utils loss csv setproctitle model json torch os matplotlib string scipy itertools dataset test tensorboardX numpy time config train metric math random,cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cjx0525_BGCN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cjx0525_BGCN.pdf,,,yes,"Adam, Search",yes,"mini batch size, learning rate, l2 regularization term, dropout",yes,Information Retrieval,Machine Learning
https://github.com/amitport/Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting,amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.tar.gz,Towards Federated Learning With Byzantine-Robust Client Weighting,"@computations.federated_computation() @computations.tf_computation(server_state_type) @computations.federated_computation( @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.int64), shakespeare_train.element_type_structure]) @computations.tf_computation(model_weights_type, model_weights_type.trainable, @computations.tf_computation(dataset_with_byzflag_type, model_weights_type) @tf.function @abc.abstractmethod @tff.tf_computation((tf.string, tf.bool)) @computations.tf_computation @staticmethod @property @computations.federated_computation(new_param_type) @classmethod @dataclass(frozen=True) @abc.abstractproperty @attr.s(eq=False, frozen=True)",utils tensorflow_federated pathlib tensorflow absl json collections enum dataclasses os matplotlib wquantiles functools itertools shared typing argparse numpy abc math optimization attr experiments random,cs.LG cs.CR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.pdf,,,no,,,"hyperparameters from other study, ",no,Machine Learning,Machine Learning
https://github.com/MLRG-CEFET-RJ/stconvs2s,MLRG-CEFET-RJ_stconvs2s.tar.gz,STCONVS2S: SPATIOTEMPORAL CONVOLUTIONAL SEQUENCE TO SEQUENCE NETWORK FOR WEATHER FORECASTING ACCEPTED MANUSCRIPT,@staticmethod,pandas warnings sys multiprocessing platform smtplib pathlib model datetime statsmodels os ml_builder torch tool pkgutil matplotlib configparser importlib itertools sklearn argparse numpy time math traceback random xarray inspect,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MLRG-CEFET-RJ_stconvs2s.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MLRG-CEFET-RJ_stconvs2s.pdf,,,yes,"RMSprop, Experimental Tuning",yes,"learning rate, number of layers, kernel size number of filters",yes,Machine Learning,Machine Learning
https://github.com/AI-NERC-NUPT/PFH-OSNet,AI-NERC-NUPT_PFH-OSNet.tar.gz,Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro,@property @staticmethod @torch.no_grad(),PIL copy warnings sys glob default_config __future__ torchvision torchreid distutils re datetime collections Cython json h5py torch os matplotlib six scipy functools itertools pickle sklearn argparse numpy time shutil timeit thop tarfile math torchsummary errno random cv2 gdown layer yacs zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AI-NERC-NUPT_PFH-OSNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AI-NERC-NUPT_PFH-OSNet.pdf,,,yes,Adam,yes,"momentum, epochs",yes,Computer Vision,Computer Vision
https://github.com/pykao/ISLES2017-mRS-prediction,pykao_ISLES2017-mRS-prediction.tar.gz,Predicting Clinical Outcome of Stroke Patients with Tractographic Feature,,utils csv multiprocessing nibabel skimage medpy SimpleITK os matplotlib scipy itertools sklearn argparse natsort xgboost paths numpy shutil math logging subprocess utils_40,eess.IV cs.LG q-bio.QM stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pykao_ISLES2017-mRS-prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pykao_ISLES2017-mRS-prediction.pdf,,,no,,,,no,Biology,Biology
https://github.com/FarnazAdib/Crash_course_on_RL,FarnazAdib_Crash_course_on_RL.tar.gz,A Crash Course on Reinforcement Learning,,scipy copy tensorflow warnings datetime collections setuptools random lq argparse os mpl_toolkits numpy matplotlib cartpole gym,cs.LG cs.SY eess.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/FarnazAdib_Crash_course_on_RL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\FarnazAdib_Crash_course_on_RL.pdf,,"Categorizing RL agentThere are many ways to categorize an RL agent, like model-free and model-based, online or offline agents, and so on. One possible approach is to categorize RL agents based on the main components that the RL agent is built upon. Then, we will have the following classification• Policy gradient.• Dynamic Programming (DP)-based solutions.• Model building.Policy gradient approaches are built upon defining a policy for the agent, DP-based solutions require estimating value functions and model-building approaches try to estimate a model of the environment. This is a coarse classification of approaches; indeed by combining different features of the approaches, we get many useful variations which we do not discuss in this handout.All aforementioned approaches reduce to some sort of function approximation from data obtained from the dynamical systems. In policy gradient, we fit a function to the policy; i.e. we consider policy as a function of state π = network(state). In DP-based approach, we fit a model to the value function to characterize the cost-to-go. In the model-building approach, we fit a model to the state transition of the environment.As you can see, in all approaches, there is a modeling assumption. The thing which makes one approach different from another is ""where"" to put the modeling assumption: policy, value function or dynamical system. The reader should not be confused by the term ""model-free"" and think that no model is built in RL. The term ""model-free"" in RL community is simply used to describe the situation where no model of the dynamical system is built.",no,,,,no,Machine Learning,Machine Learning
https://github.com/chrhenning/posterior_replay_cl,chrhenning_posterior_replay_cl.tar.gz,Posterior Meta-Replay for Continual Learning,"@fun(Y, T, data, allowed_outputs, empirical_fisher, batch_ids) @permutation.setter @abstractmethod @preprocess_fct.setter @shuffle_val_samples.setter @shuffle_test_samples.setter @property @staticmethod @bptt_depth.setter",utils PIL packaging copy these pandas contextlib warnings sys types glob _pickle the torchvision threading hpsearch tensorflow hnets re mnets datetime collections json queue data tempfile torch os matplotlib importlib struct scipy gzip itertools seaborn urllib pickle probabilistic sklearn argparse GPUtil tensorboardX numpy time abc shutil getpass tarfile math __init__ traceback bsub random logging subprocess select inspect,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/chrhenning_posterior_replay_cl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\chrhenning_posterior_replay_cl.pdf,,"D Supplementary Experiments and ResultsIn this section, we extend the results presented in the main text, and provide a more detailed discussion. Furthermore, we report additional baselines and supplementary experiments such as PermutedMNIST (cf. SM D.4).Baselines. In addition to the methods and baselines that have already been introduced, we consider the following variations. To investigate the role played by the shared meta-model, we consider independently trained models per task (SeparatePosteriors); a baseline that by design is not affected by catastrophic interference, and therefore leads to identical TGiven-During and TGiven-Final scores.In this setting, task-specific solutions cannot benefit from knowledge transfer, but they are also less limited because the capacity of the meta-model is not shared with previous tasks.We distinguish two cases for this baseline. In the first case, there is no TC network and θ is trained directly (denoted SeparatePosteriors-<method>, e.g., SeparatePosteriors-BbB). However, one has to keep in mind that the underlying architecture in combination with the chosen weight prior defines a prior in function space that will affect predictive uncertainty (cf. Sec. 3 and [77]), and it is therefore unclear how comparable task inference scores based on predictive uncertainty are in this case, e.g., between SeparatePosteriors-BbB and PosteriorReplay-BbB. For this reason, we include a second SeparatePosteriors baseline where the TC network remains but has no dedicated functional purpose (denoted SeparatePosteriors-TC-<method>, e.g., SeparatePosteriors-TC-BbB). Note that for this baseline, one TC network with a single task embedding is learned per task.As a potential upper-bound we also consider Fine-Tuning, which simply refers to the continuous deterministic training of a main network without undertaking any measures against catastrophic forgetting. Thus, the achieved TGiven-During score can benefit from transfer while not being restricted to finding any trade-off that accommodates past tasks. In this case, the hyperparameter configuration is selected based on the best TGiven-During score. We only searched hyperparameters for this baseline in the PermutedMNIST and SplitCIFAR experiments, because the TGiven-During scores of other experiments are often maxed out. Importantly, whenever applicable and unless noted otherwise, all hyperparameter configurations have been selected based on the TInfer-Final (Ent) criterion. Hence, reported task-given (TGiven) scores do not necessarily reflect the ability of a method to combat forgetting. However, in most cases, forgetting does not seem to be a major challenge in the experiments we consider, and the reported TGiven-During and TGiven-Final scores are often very close. Please refer to SM E for details on the experimental setup and hyperparameter searches.Tempering. All results involving Bayesian methods use a standard Gaussian prior p(W) = N (0, I).For high-dimensional problems and for methods trained with variational inference, we explore tempering the posterior to increase training stability (cf. SM E.3). Specifically, we downscale the prior-matching term, which effectively increases the emphasis on matching the data well (cf. SM E in Wenzel et al. [76]). Note, no such posterior tempering is used when studying the low-dimensional problems in SM D.1 and SM D.2.",yes,Adam,no,"width of RBF kernel, number of samples, ",yes,Machine Learning,Machine Learning
https://github.com/mikudehuane/FedLaAvg,mikudehuane_FedLaAvg.tar.gz,Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability,@property @staticmethod,utils copy csv sys multiprocessing bs4 torchvision threading gensim re _init_paths datetime collections torch os matplotlib string common pickle typing sklearn argparse numpy time config shutil train math traceback random,stat.ML cs.DC cs.LG math.OC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mikudehuane_FedLaAvg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mikudehuane_FedLaAvg.pdf,,,yes,SGD,yes,"learning rate, batch size",no,Machine Learning,Machine Learning
https://github.com/kpolsen/SIGAME,kpolsen_SIGAME.tar.gz,SIMULATOR OF GALAXY MILLIMETER/SUBMILLIMETER EMISSION (S ÍGAME): THE [Cii] −SFR RELATIONSHIP OF MASSIVE Z=2 MAIN SEQUENCE GALAXIES,,"sympy copy pandas socket, getpass sys multiprocessing glob pdb aux csv, subprocess re astropy os matplotlib numexpr scipy pickle sigame sklearn argparse numpy time shutil periodictable linecache subprocess mpl_toolkits swiftsimio",astro-ph.GA astro-ph.GA astro-ph.GA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kpolsen_SIGAME.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kpolsen_SIGAME.pdf,,,no,,,,no,Physics,Physics
https://github.com/esclear/ph-nn,esclear_ph-nn.tar.gz,Port-Hamiltonian Approach to Neural Network Training,@tf.function,timeit keras tensorflow itertools typing PHNetworks numpy matplotlib,cs.NE cs.LG cs.SY eess.SY stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/esclear_ph-nn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\esclear_ph-nn.pdf,,,no,,,,no,Miscellaneous,Machine Learning
https://github.com/lancopku/DeconvDec,lancopku_DeconvDec.tar.gz,Deconvolution-Based Global Decoding for Neural Machine Translation,@property,utils yaml sys collections models torch os matplotlib pyrouge codecs bisect pickle argparse opts numpy time math random linecache logging lr_scheduler,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lancopku_DeconvDec.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lancopku_DeconvDec.pdf,,,yes,Adam,yes,"batch size, dropout, hidden layers, learning rate, momentum",no,NLP,Machine Learning
https://github.com/HaohanWang/PAR_experiments,HaohanWang_PAR_experiments.tar.gz,Learning Robust Global Representations by Penalizing Local Predictive Power,@property @staticmethod,utils PIL csv warnings sys glob skimage __future__ torchvision util tensorflow zipfile json collections data torch os StringIO matplotlib scipy gzip itertools IPython datagenerator bisect pickle argparse numpy time math DFT optimizer cv2 random utility cPickle models io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HaohanWang_PAR_experiments.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HaohanWang_PAR_experiments.pdf,,,yes,manual tuning,yes,,yes,Computer Vision,Computer Vision
https://github.com/stanford-iris-lab/batch-exploration,stanford-iris-lab_batch-exploration.tar.gz,Batch Exploration with Examples for Scalable Robotic Reinforcement Learning,,utils PIL imageio copy pandas sys tensor2tensor glob manip_envs __future__ gym tensorflow absl json h5py torch os matplotlib gtimer replay_buffer typing pickle argparse numpy time envs cem pytorch_util random cv2 losses models franka_env configs,cs.RO cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stanford-iris-lab_batch-exploration.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stanford-iris-lab_batch-exploration.pdf,,,yes,Adam,not all,"default hyperparameters used, batch size, episodes",no,Robotic,Robotic
https://github.com/cvxgrp/cvxstrat,cvxgrp_cvxstrat.tar.gz,A Distributed Method for Fitting Laplacian Regularized Stratified Models,,utils scipy pandas strat_models warnings datetime sys setuptools networkx multiprocessing sklearn torch os cvxpy numpy matplotlib time,cs.LG math.OC stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cvxgrp_cvxstrat.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cvxgrp_cvxstrat.pdf,,"|Gaussian distribution.Here Y = R m , and we fit a density to the observed values of y, given z. For example θ can parametrize a Gaussian on R m , N (µ, Σ). The standard parametrization uses the parameters θ= (Σ −1 , Σ −1 µ), with Θ = S m ++ × R m (S n++ denotes the set of n × n positive definite matrices.) The probability density function of y isp(y || θ) = (2π) −m/2 det(Σ) −1/2 exp(− 1 2 (y − µ) T Σ −1 (y − µ))The loss function (in the standard parametrization) isl(θ, y) = − log det S + y T Sy − 2y T ν + ν T S −1 νwhere θ = (S, ν) = (Σ −1 , Σ −1 µ). This loss function is jointly convex in S and ν; the first three terms are evidently convex and the fourth is the matrix fractional function (see [10, p76]).Some common (convex) choices for the local regularization function include the trace of Σ −1 (encourages the harmonic mean of the eigenvalues of Σ, and hence volume, to be large), sum of squares of Σ −1 (shrinks the overall conditional dependence between the variables),1 -norm of Σ −1 (encourages conditional independence between the variables) [69], or a prior on Σ −1 or Σ −1 µ. When m = 1, this model corresponds to the standard normal distribution, with mean µ and variance σ 2 . A stratified Gaussian distribution model has a separate mean and covariance of y for each value of the stratification parameter z. This model can be validated by evaluating the average log density of the observed values y i , under the predicted distributions, over a test set of data. We fit a non-parametric discrete distribution to y, which has the form A common (convex) regularization function is the negative entropy, given by M i=1 θ i log(θ i ).p(y = k || θ) = θ k , k = 1, . . . , M, where θ ∈ Θ = {p ∈ R M || 1 T p = 1, p 0}, i.Exponential families. A probability distribution that can be expressed asp(y || θ) = e M (θ,y)where M : R n × Y → R is a concave function of θ, is an exponential family [70,71]. Some important special cases include the Bernoulli, multinomial, Gaussian, and Poisson distributions. A probability distribution that has this form naturally leads to the following (convex) loss function l(θ, y) = − log p(y || θ) = −M (θ, y). Star graph.A star graph has one vertex with edges to every other vertex. The vertex that is connected to every other vertex is sometimes called the internal vertex. In a stratified model with a star regularization graph, the parameters of all of the non-internal vertices are encouraged to be similar to the parameter of the internal vertex. We refer to the internal vertex as the common model. A common use of a star graph is when the stratification feature relates many vertices only to the internal vertex. It is possible to have no data associated with the common model or internal vertex. In this case the common model parameter is a weighted average of other model parameters; it serves to pull the other parameter values together.It is a simple exercise to show that a stratified model with the complete graph can also be found using a star graph, with a central or internal vertex that is connected to all others, but has no data. (That is, it corresponds to a new fictitious value of the stratification feature, that does not occur in the real data.) Path graph. A path graph, or linear/chain graph, is a graph whose vertices can be listed in order, with edges between adjacent vertices in that order. The first and last vertices only have one edge, whereas the other vertices have two edges. Path graphs are a natural choice for when the stratification feature corresponds to time, or location in one dimension. When the stratification feature corresponds to time, a stratified model with a path regularization graph correspond to a time-varying model, where the model varies smoothly with time.Cycle graph. A cycle graph or circular graph is a graph where the vertices are connected in a closed chain. Every vertex in a cycle graph has two edges. A common use of a cycle graph is when the stratification feature corresponds to a periodic variables, e.g., the day of the week; in this case, we fit a separate model for each day of the week, and the model parameter for Sunday is close to both the model parameter for Monday and the model parameter for Saturday. Other examples include days of the year, the season, and non-time variables such as angle.Tree graph. A tree graph is a graph where any two vertices are connected by exactly one path (which may consist of multiple edges). Tree graphs can be useful for stratification features that naturally have hierarchical structure, e.g., the location in a corporate hierarchy, or in finance, hierarchical classification of individual stocks into sectors, industries, and subindustries. As in the star graph, which is a special case of a tree graph, internal vertices need not have any data associated with them, i.e., the data is only at the leaves of the tree.Grid graph. A grid graph is a graph where the vertices correspond to points with integer coordinates, and two vertices are connected if their maximum distance in each coordinate is less than or equal to one. For example, a path graph is a one-dimensional grid graph.Grid graphs are useful for when the stratification feature corresponds to locations. We discretize the locations into bins, and connect adjacent bins in each dimension to create a grid graph. A space-stratified model with grid graph regularization is encouraged to have model parameters that vary smoothly through physical space.Entity graph. There can be a vertex for every value of some entity, e.g., a person. The entity graph has an edge between two entities if they are perceived to be similar. For example, two people could be connected in a (friendship) entity graph if they were friends on a social networking site. In this case, we would fit a separate model for each person, and encourage friends to have similar models. We can have multiple relations among the entities, each one associated with its own graph; these would typically be weighted by separate hyperparameters.|",no,,,,no,Machine Learning,Machine Learning
https://github.com/viscom-ulm/GINN,viscom-ulm_GINN.tar.gz,Deep-learning the Latent Space of Light Transport,,GIDataSet imageio OpenGLUtils sys MCConvModule MCNetworkUtils TFRealTimeImpFast ctypes pygame PyUtils tensorflow os importlib sklearn argparse OpenGL numpy time math MeshHelpers screenproc MCConvBuilder,cs.GR cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/viscom-ulm_GINN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\viscom-ulm_GINN.pdf,,,yes,Adam,not all,"learning rate, decay, epochs, batch size",no,Computer Vision,Computer Vision
https://github.com/utahnlp/consistency,utahnlp_consistency.tar.gz,A Logic-Driven Framework for Consistency of Neural Models,,locked_dropout fp16_optimizer linear_classifier sys bert_encoder pipeline ujson multiclass_loss bert_loader util re bert_adam collections json h5py data torch os embeddings string apex itertools holder spacy sklearn argparse transition_loss numpy time math unicodedata optimizer pytorch_pretrained_bert logging random operator io,cs.AI cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/utahnlp_consistency.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\utahnlp_consistency.pdf,,"ExperimentsIn this section, we evaluate our framework using (near) state-of-the-art approaches for NLI, primarily based on BERT, and also compare to an LSTM model. We use the SNLI and MultiNLI  datasets to define annotation consistency. Our LSTM model is based on the decomposable attention model with a BiLSTM encoder and GloVe embeddings (Pennington et al., 2014). Our BERT model is based on the pretrained BERT base , finetuned on SNLI/MultiNLI. The constrained models are initialized with the finetuned BERT base and finetuned again with inconsistency losses. 7 For fair comparison, we also show results of BERT base models finetuned twice.Our constrained models are trained on both labeled and unlabeled examples. We expect that the different inconsistencies do not conflict with each other. Hence, we select hyperparameters (e.g., the λ's) using development accuracy only (i.e., anno-7 This is critical when label supervision is limited. tation consistency). We refer the reader to the appendix for details of our experimental setup.",yes,Adam,not all,"epochs, learning rate, dropout",no,NLP,Machine Learning
https://github.com/yizhou-wang/RODNet,yizhou-wang_RODNet.tar.gz,RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization,,multiprocessing sys tqdm cruw __future__ ctypes re collections json rodnet torch os matplotlib importlib pickle argparse numpy time shutil math setuptools random,cs.CV eess.SP cs.CV eess.SP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yizhou-wang_RODNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yizhou-wang_RODNet.pdf,AutoEncoder,,no,,,"kernel, stride, channels, ",no,Computer Vision,Computer Vision
https://github.com/computational-imaging/DeepS3PR,computational-imaging_DeepS3PR.tar.gz,Deep S 3 PR: Simultaneous Source Separation and Phase Retrieval Using Deep Generative Models (Long Version),,xcorr2 torch_dct argparse os torch numpy matplotlib time torchvision,cs.LG eess.IV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/computational-imaging_DeepS3PR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\computational-imaging_DeepS3PR.pdf,,"|Recovering latent variables||AG(z l )|| 2 2 2 ,(6)using an alternating descent algorithm. That is, we iteratively compute the loss (6) and take a gradient step (with momentum) with respect to z 1 , then compute the loss and take a gradient step (with momentum) with respect to z 2 , etc. In practice, we found alternating descent (using the ADAM optimizer [71]) provided a near monotonic reduction of the loss and ran in two minutes on an Nvidia Titan RTX GPU.For time/resource-sensitive applications, one could also use an alternating projection algorithm [49,58] or more advanced methods like AMP [48,51] or ADMM [52], which can provide significantly faster convergence.Generative model Following [47] we use a variation of the DC-GAN [72] architecture as our deep generative model, G(z). Our generator network is illustrated in Figure 2. It takes a 100 dimensional latent vector, z ∈ R 100 , as an input and applies a fully connected layer followed by a reshaping operation and batch normalization to form an 8 × 8 × 128 dimensional feature map. This feature map is then upsampled 2×, convolved with 128 different 3 × 3 filters, batch normalized, and feed through a leaky ReLU to form a 16 × 16 × 128 feature map. This feature map is then upsampled, convolved with 64 different 3 × 3 filters, batch normalized, and feed through another leaky ReLU to form a 32 × 32 × 64 feature map. Finally, this feature map is convolved with a single 3 × 3 filter and passed through a tanh nonlinearity to form the generated image.We train one such DC-GAN network to produce MNIST digits and another to produce Fashion MNIST articles of clothing. Each network was based off the Pytorch implementation of DC-GAN from [73] and was trained using the code's default parameters. The networks were trained using the training portion of their respective datasets and tested, as described in the next section, on a subset of the testing portion.|",yes,Adam,yes,,no,Machine Learning,Computer Vision
https://github.com/exosports/marge,exosports_marge.tar.gz,Accurate Machine Learning Atmospheric Retrieval via a Neural Network Surrogate Model for Radiative Transfer,,"utils warnings multiprocessing onnx2keras signal glob stats keras2onnx transit_module onnx keras tensorflow sys, os loader matplotlib callbacks NN importlib configparser six scipy constants functools plotter sys, os, platform pickle sklearn argparse reader numpy time wine PT random subprocess mpi4py makeatm MCcubed io",astro-ph.IM astro-ph.EP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/exosports_marge.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\exosports_marge.pdf,,,yes,"Adam, Grid Search",yes,"epochs, learning rate, batch size, hidden layers, activation functions, kernel size, pooling layers size, learning rate policy, decay,",yes,Physics,Physics
https://github.com/data61/aboleth,data61_aboleth.tar.gz,Noise Contrastive Priors for Functional Uncertainty,"@pytest.mark.parametrize('likelihood', [(tf.distributions.Categorical, 2), @pytest.mark.parametrize('dense', [ab.Dense, ab.DenseVariational]) @pytest.mark.parametrize('reps', [1, 3, 10]) @pytest.fixture @pytest.mark.parametrize('kernels', [ @pytest.mark.parametrize('indep', [True, False]) @pytest.mark.parametrize('kernels', kernel_list) @pytest.mark.parametrize('layer', [ab.EmbedVariational, ab.Embed]) @pytest.mark.parametrize('layer_args', [ @pytest.mark.parametrize('lenscales', [ @pytest.mark.parametrize('scalar', [np.pi, @staticmethod @pytest.mark.parametrize( @pytest.mark.parametrize('stats', [(1., 1.), @pytest.mark.parametrize('conv2d', [ab.Conv2D, ab.Conv2DVariational])",pandas sys types the aboleth tensorflow tempfile os pytest six scipy functools urllib sklearn numpy bokeh setuptools logging tensorflow_probability,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/data61_aboleth.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\data61_aboleth.pdf,,"INTRODUCTIONMany successful applications of neural networks (Krizhevsky et al., 2012;Sutskever et al., 2014;van den Oord et al., 2016) are in restricted settings where predictions are only made for inputs similar to the training distribution. In real-world scenarios, neural networks can face truly novel data points during inference, and in these * Correspondence to mail@danijar.com.settings it can be valuable to have good estimates of the model's uncertainty. For example, in healthcare, reliable uncertainty estimates can prevent overconfident decisions for rare or novel patient conditions (Schulam and Saria, 2015). Another application are autonomous agents that should actively explore their environment, requiring uncertainty estimates to decide what data points will be most informative.Epistemic uncertainty describes the amount of missing knowledge about the data generating function. Uncertainty can in principle be completely reduced by observing more data points at the right locations and training on them. In contrast, the data generating function may also have inherent randomness, which we call aleatoric noise. This noise can be captured by models outputting a distribution rather than a point prediction. Obtaining more data points allows the noise estimate to move closer to the true value, which is usually different from zero. For active learning, it is crucial to separate the two types of randomness: we want to acquire labels in regions of high uncertainty but low noise (Lindley et al., 1956).Bayesian analysis provides a principled approach to modeling uncertainty in neural networks (Denker et al., 1987;MacKay, 1992b). Namely, one places a prior over the network's weights and biases. This induces a distribution over the functions that the network represents, capturing uncertainty about which function best fits the data. Specifying this prior remains an open challenge. Common practice is to use an independent normal prior in weight space, which is neither informative about the induced function class nor the data (e.g., it is sensitive to parameterization). This can cause the induced function posterior to generalize in unforeseen ways on out-of-distribution (OOD) inputs, which are inputs outside of the distribution that generated the training data.Motivated by these challenges, we introduce noise contrastive priors (NCPs), which encourage uncertainty outside of the training distribution through a loss in data space. NCPs are compatible with any model that represents functional uncertainty as a random variable, are easy to scale, and yield reliable uncertainty estimates that show significantly improved active learning performance. On the Bayesian neural network, NCP produces smooth uncertainty estimates that generalize well to unseen data points. Models trained with NCP also separate uncertainty and noise well. The experimental setup is described in Section 5.1. ROBUSTNESS TO NOISE PATTERNSThe choice of input noise might seem like a critical hyper parameter for NCP. In this experiment, we find that our method is robust to the choice of input noise. The experimental setup is the same as for the active learning experiment described in Section 5.2, but with uniform or normal input noise with different variance(σ 2 x ∈ {0.1, 0.2, • • • , 1.0}). For uniform input noise, this means noise is drawn from the interval [−2σ x , 2σ x ].We observe that BBB+NCP is robust to the size of the input noise. NCP consistently improves RMSE for the tested noise sizes and yields the best NLPD for all noise sizes below 0.6. For our ODC baseline, we observe an intuitive trade-off: smaller input noise increases the regularization strength, leading to better NLPD but reduced RMSE. Robustness to the choice of input noise is further supported by the analogous experiment on toy data set, where above a small threshold (BBB+NCP σ 2x ≥ 0.3 and ODC+NCP σ 2x ≥ 0.1), NCP consistently performs well (Figure 6). D RELATED ACTIVE LEARNING WORKActive learning is often employed in domains where data is cheap but labeling is expensive, and is motivated by the idea that not all data points are equally valuable when it comes to learning (Settles, 2009;Dasgupta, 2004). Active learning techniques can be coarsely grouped into three categories. Ensemble methods (Seung et al., 1992;McCallumzy and Nigamy, 1998;Freund et al., 1997) generate queries that have the greatest disagreement between a set of classifiers.Error reduction approaches incorporate the select data based on the predicted reduction in classifier error based on information (MacKay, 1992a), Monte Carlo estimation (Roy and McCallum, 2001), or hard-negative example mining (Sung, 1994;Rowley et al., 1998).Uncertainty-based techniques select samples for which the classifier is most uncertain. Approaches include maximum entropy (Joshi et al., 2009), distance from the decision boundary (Tong and Koller, 2001), pseudo labelling high confidence examples (Wang et al., 2017), and mixtures of information density and uncertainty measures (Li and Guo, 2013). Within this category, the area most related to our work are Bayesian methods. Kapoor et al. (2007) estimate expected improvement using a Gaussian process. Other approaches use classifier confidence (Lewis and Gale, 1994), predicted expected error (Roy and McCallum, 2001), or model disagreement (Houlsby et al., 2011). Recently, Gal et al. (2017) applied a convolutional neural network with dropout uncertainty to images.Figure 1 :1Figure 1: Predictive distributions on a low-dimensional active learning task. The predictive distributions are visualized as mean and two standard deviations shaded. They decompose into epistemic uncertainty and aleatoric noise . Data points are only available within two bands, and are selected using the expected information gain . (a) A deterministic network models no uncertainty but only noise, resulting in overconfidence outside of the data distribution. (b) A variational Bayesian neural network with independent normal prior represents uncertainty and noise separately but is overconfident outside of the training distribution. (c) On the OOD classifier model, NCP prevents overconfidence. (d)On the Bayesian neural network, NCP produces smooth uncertainty estimates that generalize well to unseen data points. Models trained with NCP also separate uncertainty and noise well. The experimental setup is described in Section 5.1.",yes,Adam,not all,"batch size, learning rate, epochs",no,Machine Learning,Machine Learning
https://github.com/JimLiu96/basConv,JimLiu96_basConv.tar.gz,BasConv: Aggregating Heterogeneous Interactions for Basket Recommendation with Graph Convolutional Neural Network,,scipy tensorflow re sys multiprocessing random pickle sklearn argparse os utility numpy heapq time,cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/JimLiu96_basConv.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\JimLiu96_basConv.pdf,,,yes,"Adam, Experimental Tuning",yes,learning rate,yes,Information Retrieval,Machine Learning
https://github.com/Wouter-VDP/nuecc_python,Wouter-VDP_nuecc_python.tar.gz,Combined Neyman-Pearson Chi-square: An Improved Approximation to the Poisson-likelihood Chi-square,,gc pandas glob pathlib col_load os helpers matplotlib scipy pickle sklearn joblib xgboost numpy awkward time uproot subprocess enum_sample,physics.data-an hep-ex nucl-ex,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Wouter-VDP_nuecc_python.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Wouter-VDP_nuecc_python.pdf,,,no,,,,no,Physics,Physics
https://github.com/Huangdebo/YOLOv4-MultiTask,Huangdebo_YOLOv4-MultiTask.tar.gz,Ultra Fast Structure-aware Deep Lane Detection,@staticmethod,"utils copy time,pdb sys glob tqdm torchvision pathlib pretrainedmodels torch os matplotlib scipy argparse numpy mish_cuda time shutil thop math cv2 random subprocess models",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Huangdebo_YOLOv4-MultiTask.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Huangdebo_YOLOv4-MultiTask.pdf,Rectified Linear Units  1x1 Convolution  Average Pooling  Batch Normalization  Residual Connection  Cosine Annealing  Adam  Max Pooling  Global Average Pooling  Bottleneck Residual Block  Residual Block  Kaiming Initialization  Convolution  Residual Network,,yes,Adam,not all,"learning rate, loss coefficients, batch size, epochs",no,Computer Vision,Computer Vision
https://github.com/gmum/3d-point-clouds-autocomplete,gmum_3d-point-clouds-autocomplete.tar.gz,HyperPocket: Generative Point Cloud Completion,@classmethod @property @staticmethod @ray.remote,utils requests pandas csv warnings sys glob pptk tqdm re core model datetime json collections h5py torch os ray matplotlib scipy itertools urllib typing sklearn argparse numpy shutil datasets setuptools random logging losses trimesh mpl_toolkits zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gmum_3d-point-clouds-autocomplete.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gmum_3d-point-clouds-autocomplete.pdf,HyperNetwork,,yes,Adam,yes,"learning rate, epochs",no,Computer Vision,Computer Vision
https://github.com/Colin97/DeepMetaHandles,Colin97_DeepMetaHandles.tar.gz,DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes with Biharmonic Coordinates,,pointnet_utils utils tqdm torch_batch_svd torchvision pathlib render datetime visdom network torch os pytorch3d typing argparse numpy time shutil datasets laplacian random logging losses,cs.CV cs.GR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Colin97_DeepMetaHandles.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Colin97_DeepMetaHandles.pdf,,"Target-Driven DeformationWe evaluate our methods on the ShapeNet dataset [3]. We choose 15,522 models from the dataset, which cover three categories: chair, table, and car. Shapes are normalized to fit in a unit sphere. For each shape, we sample c = 50 control-point handles by FPS, in order to generally cover most of the surface and allow flexible deformations. We uniformly sample point clouds of the size p = 4096 to represent the shapes. We set the number of meta-handles to be m = 15. This should be an upper bound since the network can use part of them by setting some ranges to zero. As tetrahedral meshes are required as input to compute the biharmonic weights [40], all the ShapeNet [3] triangular meshes are first fed into Huang et al.'s algorithm [14] to become watertight manifolds, and are then fed into TetWild [12] to produce tetrahedral meshes. We use libigl's [18] implementation to compute the biharmonic coordinates, which are then interpolated from the mesh vertices to the sampled point cloud. For the differentiable renderer, we use an implementation from Pytorch3D [32]. We reserve 10% models for testing and the rest for training. For Figure 6: Qualitative comparison of our method with other deformation methods [13,39,10,46]. Our method allows flexible deformation and fine-grained detail preservation. Our results are also more plausible, especially when the source-target pairs do not share the same structures (see the second and the fourth columns). Please zoom in for details. each category, we train a separate model and test it on 3, 000 randomly sampled source-target pairs. We compare our method to non-rigid ICP (NRICP) [13], a non-neural registration technique which aligns two point clouds by minimizing a smooth deformation energy; 3D deformation network (3DN) [39] and cycle-consistent deformation (CC) [10], two learning-based methods that directly infer per-vertex displacements; and Neural Cages [46], a learnable cage-based deformation method.Qualitative results are shown in Fig. 6. Although NRICP [13], 3DN [39], and CC [10] do align the source shape to the target shape in most cases, they fail to preserve fine-grained details of the source shape and introduce lots of distortions. The results of Neural Cages [46] look more pleasing, but the cage-based deformation is less flexible than our control-point based deformation. Compared to the Neural Cages [46], our method can achieve more detailed deformation of a local region, such as adjusting the thickness of chair seats (first and fifth columns) and armrests' height (third column). Also, most alternative methods produce unrealistic deformations when the source shape and the target shape do not share similar structures. For example, suppose the source shape has four chair legs, and the target shape is a swivel chair (second and fourth columns).In that case, the alternative methods tend to deform the four chair legs toward the center under the fitting loss's influence, resulting in undesirable deformations. Thanks to the adversarial regularization we employed, our method can avoid such implausible deformations while still aligning the output to the target. Inspired by Neural Cages [46], we also utilize Chamfer distance [5] between the deformed shape and the target shape (computed over 100,000 uniformly sampled points) to measure the alignment error; and use the difference between cotangent Laplacians of the source shape and the deformed shape (l1-norm) to measure the distortion. The  [13], 3DN [39], and CC [10] achieve lower alignment errors, the distortions are much higher. Compared to Neural Cages [46], our method achieves better Chamfer distance with a similar cotangent Laplacian.",yes,Adam,yes,"iterations, batch size, learning rate, empirically set weights",no,Computer Vision,Computer Vision
https://github.com/zacks417/faster-rcnn-tf,zacks417_faster-rcnn-tf.tar.gz,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,@roidb_handler.setter @property @staticmethod,"utils PIL yaml sys glob nms pdb easydict __future__ pprint ast tensorflow re uuid model json collections cProfile os matplotlib nets six scipy xml pickle argparse numpy time math os,cv2 datasets pycocotools time,os,sys cv2 layer_utils roi_data_layer subprocess cPickle",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zacks417_faster-rcnn-tf.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zacks417_faster-rcnn-tf.pdf,VGG-16  Region Proposal Network  Softmax  Convolution  RoIPool  Faster R-CNN  Fast R-CNN,"4-Step Alternating Training.In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN. As such, both networks share the same convolutional layers and form a unified network. A similar alternating training can be run for more iterations, but we have observed negligible improvements. Experiments on PASCAL VOCWe comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories. We also provide results on the PASCAL VOC 2012 benchmark for a few models. For the ImageNet pre-trained network, we use the ""fast"" version of ZF net [32] that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model 7 [3] that has 13 convolutional layers and 3 fully-connected layers. We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics).Table 2 (top) shows Fast R-CNN results when trained and tested using various region proposal methods. These results use the ZF net. For Selective Search (SS) [4], we generate about 2000 proposals by the ""fast"" mode. For EdgeBoxes (EB) [6], we generate the proposals by the default EB setting tuned for 0.7 7. www.Robotic.ox.ac.uk/ ∼ vgg/research/very deep/ IoU. SS has an mAP of 58.7% and EB has an mAP of 58.6% under the Fast R-CNN framework. RPN with Fast R-CNN achieves competitive results, with an mAP of 59.9% while using up to 300 proposals 8 . Using RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers' cost (Table 5).Ablation Experiments on RPN. To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2). We observe that this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.Next, we disentangle the RPN's influence on training the Fast R-CNN detection network. For this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector.Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons.Somewhat surprisingly, the RPN still leads to a competitive result (55.1%) when using the top-ranked 8. For RPN, the number of proposals (e.g., 300) is the maximum number for an image. RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller.  Next, we separately investigate the roles of RPN's cls and reg outputs by turning off either of them at test-time. When the cls layer is removed at testtime (thus no NMS/ranking is used), we randomly sample N proposals from the unscored regions. The mAP is nearly unchanged with N = 1000 (55.8%), but degrades considerably to 44.6% when N = 100. This shows that the cls scores account for the accuracy of the highest ranked proposals.On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%. This suggests that the highquality proposals are mainly due to the regressed box bounds. The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection.We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.Performance of VGG-16. Table 3 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is predefined, the RPN is actively trained and benefits from better networks. For the feature-shared variant, the result is 69.9%-better than the strong SS baseline, yet with nearly cost-free proposals. We further train the RPN and detection network on the union set of PAS-CAL VOC 2007 trainval and 2012 trainval. The mAP is 73.2%. Figure 5 shows some results on the PASCAL VOC 2007 test set. On the PASCAL VOC 2012 test set (Table 4), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Table 6 and Table 7 show the detailed numbers.   In Table 5 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average about 1.5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers [2]). Our system with VGG-16 takes in total 198ms for both proposal and detection. With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers. Our regionwise computation is also lower, thanks to fewer proposals (300 per image). Our system has a frame-rate of 17 fps with the ZF net.Sensitivities to Hyper-parameters. In Table 8 we investigate the settings of anchors. By default we use 3 scales and 3 aspect ratios (69.9% mAP in Table 8). If using just one anchor at each position, the mAP drops by a considerable margin of 3-4%. The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.In Table 9 we compare different values of λ in Equation (1). By default we use λ = 10 which makes the two terms in Equation (1) roughly equally weighted after normalization. Table 9 shows that our result is impacted just marginally (by ∼ 1%) when λ is within a scale of about two orders of magnitude (1 to 100). This demonstrates that the result is insensitive to λ in a wide range.Analysis of Recall-to-IoU. Next we compute the recall of proposals at different IoU ratios with groundtruth boxes. It is noteworthy that the Recall-to-IoU metric is just loosely [19], [20], [21] related to the ultimate detection accuracy. It is more appropriate to use this metric to diagnose the proposal method than to evaluate it.In Figure 4, we show the results of using 300, 1000, and 2000 proposals. We compare with SS and EB, and the N proposals are the top-N ranked ones based on the confidence generated by these methods. The plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300. This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals. As we analyzed before, this property is mainly attributed to the cls term of the RPN. The recall of SS and EB drops more quickly than RPN when the proposals are fewer.   One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3×3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN-the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the ""proposals"" are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1], [2].Table 10 compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%. This experiment justifies the effectiveness of cascaded region proposals and object detection. Similar observations are reported in [2], [39], where replacing SS region proposals with sliding windows leads to ∼6% degradation in both papers. We also note that the onestage system is slower as it has considerably more proposals to process.",yes,,not all,"learning rate, momentum, mini batch size, weight decay, iterations",no,Computer Vision,Computer Vision
https://github.com/userbehavioranalysis/SR-GNN_PyTorch-Geometric,userbehavioranalysis_SR-GNN_PyTorch-Geometric.tar.gz,Session-based Recommendation with Graph Neural Networks,@property,train math csv model datetime dataset operator pickle logging tqdm argparse os torch tensorboardX numpy time torch_geometric,cs.IR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/userbehavioranalysis_SR-GNN_PyTorch-Geometric.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\userbehavioranalysis_SR-GNN_PyTorch-Geometric.pdf,,,yes,Adam,yes,Adam to optimize hyperparameters,yes,Information Retrieval,Machine Learning
https://github.com/Maluuba/nlg-eval,Maluuba_nlg-eval.tar.gz,Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,@property,"copy sys pdb psutil __future__ xdg sys, math, re threading atexit gensim re click collections unittest json nltk os six scipy sklearn numpy math pip nlgeval setuptools theano logging subprocess",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Maluuba_nlg-eval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Maluuba_nlg-eval.pdf,,,no,,,,no,NLP,NLP
https://github.com/ricsinaruto/gutenberg-dialog,ricsinaruto_gutenberg-dialog.tar.gz,The Gutenberg Dialogue Dataset,,utils warnings sys pipeline pprint re languages datetime collections json gutenberg transformers tempfile nltk torch os socket importlib apex itertools ignite argparse shutil train math tarfile unicodedata random logging,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ricsinaruto_gutenberg-dialog.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ricsinaruto_gutenberg-dialog.pdf,,"|TrainingsWe conduct experiments with Transformer 11 and GPT2 12 models. The Transformer is trained on utterance pairs, and we use the base version of roughly 50M parameters (further training details are given in Appendix A.1). The vocabulary is set to the top 100 000 words for Gutenberg and Opensubtitles trainings, and 32 768 and 16 384, for PersonaChat and DailyDialog, respectively. The Transformer is trained for 21 epochs on Gutenberg and Opensubtitles, because of time and hardware constraints, but the validation loss was still decreasing. Training took about 80 hours on a single RTX 2080 Ti, with batch size set to the memory limit. We used the Adam optimizer (Kingma and Ba, 2014). For generating test outputs greedy decoding is used.For the GPT2 trainings (117M pretrained version) we set the maximum number of previous utterances to be used as history to 3 (parameter details in Appendix A.1). The huggingface repository leverages GPT2 for dialogue modeling with an additional personality input and a random candidate classification loss (Wolf et al., 2018). We set the personality field to empty and use a single random candidate response from the training set for each example. We use the nucleus sampling implementation in the repository with default parameters to sample outputs (Holtzman et al., 2020). All GPT2 trainings are trained with a batch size of 2 and evaluated at the minimum of the validation loss. The English GPT2 Gutenberg training  We evaluate Gutenberg and Opensubtitles pretrained models in zero-shot and finetuning scenarios on DailyDialog and PersonaChat. The same amount of training data and train/test/dev ratio is used for both Gutenberg and Opensubtitles. Models are finetuned until the validation loss minimum is reached. Finetuning experiments are only done in English, due to the lack of additional datasets in other languages. For Transformer trainings, we remove overlapping utterance pairs between the official train and test sets from the DailyDialog training set. We observed that inflated results reported on DailyDialog (Csáky et al., 2019) are partly due to this overlap. For all datasets we use lowercase input text and NLTK 13 word tokenization as preprocessing. We use the official DailyDialog splits and we employ a random train/dev/test split of 80/10/10 for PersonaChat, which we make publicly available along all the datasets used in this paper 14 .||U || H u w H b w H u u H b u D u kl D b kl AVG EXT GRE COH d1 d2 b1 b2 b3 b4Gutenberg pre-training performs better than Opensubtitles on DailyDialog across nearly all metrics in both zero-shot and finetuned settings (Table 4a). Gutenberg pre-training outperforms even the model trained only on DailyDialog on some metrics. All GPT2 models are pretrained as language models on web text. Thus it comes as no surprise that the additional pretraining on i 'm from baltimore and i 'm also from florida Table 5: Random test samples from PersonaChat. TRF is the base Transformer and GPT2 is the non-pretrained GPT2 model. GUT and OPEN refer to Gutenberg and Opensubtitles, respectively, and ZS and FT refer to zero-shot and finetuned settings, respectively. EOU means ""End Of Utterance"".Gutenberg does not lead to the same relative improvement as with the Transformer models, which are trained from scratch. Gutenberg pre-training achieves better results than Opensubtitles in all metrics after finetuning on PersonaChat (Table 4b). In the Transformer zero-shot scenario, Opensubtitles achieves better BLEU scores, however, zero-shot BLEU scores are generally much lower than randomly selected responses, questioning the validity of this comparison. Gutenberg pre-training outperforms the baseline PersonaChat training on some metrics after finetuning. Considering the domain mismatch between the older Gutenberg books and the modern chit-chat style datasets this is especially impressive. Since the metrics are all very similar it is also important to look at responses qualitatively. Table 5 presents 5 random test samples. More samples from both DailyDialog and PersonaChat can be found in Appendix A.3. It is clear that the Transformer and the zero-shot GPT2 scenario perform the worst, followed by the finetuned Opensubtitles training. This shows some anecdotal support for the effectiveness of pre-training on Gutenberg.Table 6 compares Gutenberg and Opensubtitles trainings across all seven languages, using roughly the same amount of data. In absence of a third independent data source we create mixed test datasets for each language that include the same amount of data from Gutenberg and Opensubtitles, by limiting the larger of the two to the size of the smaller. Except for Hungarian, models trained on Gutenberg perform better on more metrics than Opensubtitles trainings. On some metrics, models perform worse than random responses from the training set. This is expected for entropy and distinct metrics, but we believe that BLEU scores would be higher after further training since overfitted models have been shown to perform better on these metrics (Csáky et al., 2019). This lack of stopping criteria also makes a fair comparison challenging. Example responses from all models are shown in Appendix A.3. To our knowledge, this is the first work to use non-English languages from the Opensubtitles dataset for dialogue modeling, and there are very few chatbot models in non-English languages in general.|",yes,Adam,yes,"epochs, vocabulary size, hidden size, number hidden layers, label smoothing, filter size, number of attention heads, layer dropout, relu dropout, attention dropout, learning rate, warumup steps",no,NLP,NLP
https://github.com/YadiLao/MM-Tag,YadiLao_MM-Tag.tar.gz,A Tree Search algorithm For Sequence Labeling,,gc sys util tensorflow torch_process_data datetime json collections treelib new_value torch os matplotlib cnn_lstm functools pg codecs pickle sklearn argparse numpy time math operator logging random,cs.CL cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YadiLao_MM-Tag.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YadiLao_MM-Tag.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Conditional Random Field,,no,,,"learning rate, tree search trade off, hidden units",no,NLP,Machine Learning
https://github.com/oval-group/ali-g,oval-group_ali-g.tar.gz,Training Neural Networks for and by Interpolation,@torch.autograd.no_grad(),utils loss l4pytorch yaml warnings waitGPU sys tqdm cli cuda mlogger torchvision tensorflow unittest collections data torch os socket dfw argparse optim numpy time epoch math scheduling alig setuptools random subprocess models,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oval-group_ali-g.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oval-group_ali-g.pdf,Adam  Stochastic Gradient Descent,,yes,multiple,yes,paper on training methods,no,Machine Learning,Machine Learning
https://github.com/yu20103983/FOTS,yu20103983_FOTS.tar.gz,FOTS: Fast Oriented Text Spotting with a Unified Network,@slim.add_arg_scope,"csv multiprocessing sys glob recognizer shapely threading tensorflow collections queue FOTS os icdar matplotlib nets RoiRotate scipy xml numpy Queue time sharedConv traceback data_util os, sys, csv cv2 random locality_aware_nms detector",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yu20103983_FOTS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yu20103983_FOTS.pdf,Convolution,,no,,,,no,Computer Vision,Computer Vision
https://github.com/pecu/FinancialVision,pecu_FinancialVision.tar.gz,ENCODING CANDLESTICKS AS IMAGES FOR PATTERN CLASSIFICATION USING CONVOLUTIONAL NEURAL NETWORKS,@abstractmethod @functools.wraps(generator) @call_decorator @abc.abstractmethod @property @staticmethod @functools.total_ordering @generator_decorator @classmethod,"utils requests PIL lasagne pandas warnings sys git multiprocessing lib tqdm __future__ os, sys threading keras tensorflow datetime collections json queue mpl_finance os concurrent caffe pkg_resources torch matplotlib mxnet importlib scipy foolbox functools itertools hashlib imageio, os randomgen pickle sklearn numbers numpy tensorforce time abc shutil math tarfile operator logging theano random talib zipfile",cs.CE cs.LG cs.CV stat.ML q-fin.ST cs.CR cs.LG stat.ML cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pecu_FinancialVision.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pecu_FinancialVision.pdf,,,yes,Adam,yes,"epochs, batch size, learning rate, betas, early stopping",no,Computer Vision,Finance
https://github.com/wilson1yan/rlpyt,wilson1yan_rlpyt.tar.gz,Learning to Manipulate Deformable Objects without Demonstrations,@torch.no_grad()  # Hint: apply this decorator on overriding method. @torch.no_grad() @contextmanager @property @staticmethod,imageio copy csv contextlib sys multiprocessing tqdm psutil base64 ctypes platform distutils gym re posix_ipc datetime json collections queue enum rlpyt os torch string importlib pyprind functools itertools pydoc pickle shlex argparse skvideo numpy time math rllab setuptools errno random subprocess inspect mmap,cs.RO cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wilson1yan_rlpyt.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wilson1yan_rlpyt.pdf,,"|B. Learning with Composite Action SpacesThe straightforward approach to learning with a pick-place action space is to learn a policy π joint that directly outputs the optimal locations to pick and to place [a pick , a place ], i.e. π joint ≡ p(a pick ||o) • p(a place ||o) where o is the observation of the deformable object (Fig. 2(a)). However, this approach fails to capture the underlying composite and conditional nature of the action space, where the location to place a place is strongly dependent on the pick point a pick .One way to learn with conditional output spaces is to explicitly factor the output space during learning. This has provided benefits in several other learning problems from generating images [64] to predicting large dimensional robotic actions [40,62]. Hence instead of learning the joint policy, we factor the policy as:π f actor ≡ π pick (a pick ||o) • π place (a place ||o, a pick )(1)This factorization will allow the policy to reason about the conditional dependence of placing on picking (Fig. 2(b)). However, in the context of RL, we face another challenge: action credit assignment. Using RL, the reward for a specific behavior comes through the cumulative discounted reward at the end of an episode. This results in the temporal credit assignment problem where attributing the reward to a specific action is difficult. With our factored action spaces, we now have an additional credit assignment problem on the different factors of the action space. This means that if an action receives high reward, we do not know if it is due to π pick or π place . Due to this, training π f actor jointly is inefficient and often leads to the policy selecting a suboptimal pick location. This suboptimal π pick then does not allow π place to learn, since π place (a place ||o, a pick ) only sees suboptimal picking locations a pick during early parts of training. Thus, this leads to a mode collapse as shown in Sec. V-D.To overcome the action credit assignment problem, we propose a two-stage learning scheme. Here the key insight is that training a placing policy can be done given a fullsupport picking policy and the picking policy can be obtained from the placing policy by accessing the Value approximator for placing. Algorithmically, this is done by first training π place conditioned on picking actions from the uniform random distribution U pick . Using SAC, we train and obtain π place (a place ||o, a pick ), s.t. a pick ∼ U pick as well as the place value approximator V π place place (o, a pick ). Since the value is also conditioned on pick point a pick , we can use this to obtain our picking policy as:π pick ≡ arg max a pick V π place place (o, a pick )(2)We call this picking policy: Maximum Value under Placing (MVP). The arg max is computed by searching over all available pick location from the image of the object being manipulated. MVP allows us get an informed picking policy without having to explicitly train for picking. This makes training efficient for off-policy learning with conditional action spaces especially in the context of deformable object manipulation.|",no,,,network architecture,no,Robotic,Robotic
https://github.com/xvjiarui/GCNet,xvjiarui_GCNet.tar.gz,GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond,"@auto_fp16() @DETECTORS.register_module @BACKBONES.register_module @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses')) @weighted_loss @force_fp32(apply_to=('cls_scores', 'bbox_preds')) @functools.wraps(loss_func) @force_fp32(apply_to=('feats',), out_fp16=True) @NECKS.register_module @SHARED_HEADS.register_module @property @staticmethod @once_differentiable @abstractmethod @force_fp32(apply_to=('mask_pred', )) @LOSSES.register_module @force_fp32( @functools.wraps(old_func) @auto_fp16(apply_to=('img', )) @HEADS.register_module @force_fp32(apply_to=('mask_pred',)) @ROI_EXTRACTORS.register_module",copy warnings sys roi_align __future__ mmcv re Cython collections json resource tempfile torch os matplotlib six functools xml seaborn terminaltables argparse mmdet roi_pool numpy time abc shutil math pycocotools setuptools logging subprocess inspect,cs.CV cs.AI cs.LG cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xvjiarui_GCNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xvjiarui_GCNet.pdf,Cosine Annealing  Non-Local Operation  Non-Local Block  Sigmoid Activation  Average Pooling  ResNeXt Block  Squeeze-and-Excitation Block  Dense Connections  SENet  Softmax  Mask R-CNN  Layer Normalization  Cascade Mask R-CNN  RoIAlign  Region Proposal Network  Cascade R-CNN  Deformable Convolution  Linear Warmup With Cosine Annealing  Random Horizontal Flip  Random Resized Crop  Step Decay  SGD with Momentum  Weight Decay  Max Pooling  Bottleneck Residual Block  Residual Block  Residual Network  Feature Pyramid Network  GCNet  Global Context Block  Grouped Convolution  Global Average Pooling  Residual Connection  Rectified Linear Units  Kaiming Initialization  1x1 Convolution  Convolution  Batch Normalization  ResNeXt  Softmax  1x1 Convolution  Rectified Linear Units  Non-Local Operation  Layer Normalization  Residual Connection  Non-Local Block  Global Context Block  GCNet,,yes,SGD,yes,"epochs, batch size, learning rate, decay, momentum, weight decay",no,Computer Vision,Computer Vision
https://github.com/ofersp/wlenet,ofersp_wlenet.tar.gz,Weak lensing shear estimation beyond the shape-noise limit: a machine learning approach,@tf.RegisterGradient(grad_name) @staticmethod,"sympy copy pandas galsim multiprocessing sys argcomplete, argparse glob warnings skimage __future__ pprint keras tensorflow json astropy os matplotlib_scalebar matplotlib sewpy importlib scipy itertools photutils numpy time shutil wlenet math random subprocess pyfits",astro-ph.CO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ofersp_wlenet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ofersp_wlenet.pdf,,,yes,,no,,no,Physics,Physics
https://github.com/gpernelle/spindles_with_gap_junctions,gpernelle_spindles_with_gap_junctions.tar.gz,Gap Junction Plasticity can Lead to Spindle Oscillations,,fns tensorflow numpy matplotlib time,q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gpernelle_spindles_with_gap_junctions.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gpernelle_spindles_with_gap_junctions.pdf,,,no,,,,no,Biology,Biology
https://github.com/ehsanhajiramezanali/BayReL,ehsanhajiramezanali_BayReL.tar.gz,BayReL: Bayesian Relational Learning for Multi-omics Data Integration,,math scipy tensorflow itertools pandas sklearn os tensorflow_probability __future__ numpy time,cs.LG q-bio.MN stat.AP stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ehsanhajiramezanali_BayReL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ehsanhajiramezanali_BayReL.pdf,,"Table S1 :S1Enriched GO terms for the top 200 interactions in AML data. Details on the experimental setups, hyper-parameter selection, and run time BayReL. In all cystic fibrosis (CF) and acute myeloid leukemia (AML) experiments, the learning rate is set to be 0.01. For the TCGA breast cancer (BRCA) dataset, the learning rates are 0.0005 when using all and half of samples and 0.005 when using 25% of all samples. We learn the model for 1000 training epochs and use the validation set for early stopping. All of the experiments are run on a single GPU node GeForce RTX 2080. Each training epoch for CF, BRCA, and AML took 0.01, 0.42, and 0.23 seconds, respectively.GO IDGO class Descriptionp-valueGO:0006084 BPacetyl-CoA metabolic process0.00051GO:0009404 BPtoxin metabolic process0.00099GO:0032395 MFMHC class II receptor activity0.00099GO:0033004 BPnegative regulation of mast cell activation0.00099GO:0048240 BPsperm capacitation0.00099GO:0008009 MFchemokine activity0.00175GO:0042613 CCMHC class II protein complex0.00273GO:0042379 MFchemokine receptor binding0.00404C.",yes,Adam,,"learning rate, epochs, default hyperparameter",no,Machine Learning,Biology
https://github.com/tatp22/linformer-pytorch,tatp22_linformer-pytorch.tar.gz,Linformer: Self-Attention with Linear Complexity,,math torchtext sys collections linformer_pytorch tqdm torch os numpy matplotlib,cs.CL cs.LG cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tatp22_linformer-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tatp22_linformer-pytorch.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer  Absolute Position Encodings  Position-Wise Feed-Forward Layer  Gaussian Error Linear Units  Linear Layer  Multi-Head Linear Attention  Linformer  Residual Connection  Label Smoothing  Multi-Head Attention  Adam  Dropout  Byte Pair Encoding  Dense Connections  Layer Normalization  Softmax  Scaled Dot-Product Attention  Transformer,,no,,,,no,NLP,NLP
https://github.com/Majeed7/L1LR,Majeed7_L1LR.tar.gz,An efficient projection neural network for ℓ 1 -regularized logistic regression,@staticmethod,timeit scipy model_LR_NN_PR sklearn torch numpy matplotlib,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Majeed7_L1LR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Majeed7_L1LR.pdf,Logistic Regression,"C. Comparison of real data setsWe now compare the performance of the proposed neural solution with other state-of-the-art solvers on several real data sets. The comparison is made in terms of execution time, classifier accuracy, ROC, as well as AUROC. We first introduce the methods and data sets being used.Methods: For the comparison study, we select several methods, each with a different approach to handle the ℓ 1 norm. The methods are Gauss-Seidel [25], Shooting [26], Gauss-Southwell [27], Grafting [28], Subgradient [7], epsL1 [29], Log-Barrier [7], SmoothL1 [5], SQP [29], ProjectionL1 [7], InteriorPoint [4], Orthant-wise [7], and Pattern-Search [7], all implemented in MATLAB and are freely available [30]. We also use the implementation for logistic regression in scikit-learn, and refer to it as sklearn [31]. The core of scikit-learn methods have been implemented in C/C + +, and the implementation of logistic regression takes advantage of an inherent feature and subset selection, which makes it significantly faster.Data sets and experimental setup: We subject the proposed method and other methods to eight data sets: leukemia, live-disorders, madelon, splice, gisette, ijcnn1, a1a, and a9a. The data sets have their own training and test partitions, which are used for the training and testing of all the methods. The information regarding these data sets is tabulated in Table I.Execution time comparison: We first compare the methods in terms of the time they consumed to solve problem (1). For doing so, we set the stopping criterion of all the methods to 10 −6 . Table II shows the execution time of the methods to solve problem (1) for different data sets. For those of data sets with a limited number of features and training samples, all the methods behave efficiently and converge to an optimal in a relatively fast manner, e.g., the performance of methods over the data sets splice, madelon, liver-disorders, and a1a. Even for a data set with a large number of the training samples like ijcnn1, the performance of the solvers in terms of their execution time is quite competitive, mainly because the dimensions of the solvers are typically linear in the number of features, which is 22 for the ijcnn1 data set. But when the number of features and training samples increases together, many of the solvers fail to produce a solution in a reasonable time frame. For example, it takes more than three and seven hours for the InteriorPoint method to generate a result on the leukemia and gisette data sets, respectively. Or it takes even more for the Orthant-Wise method to generate results for the same data sets. Among other methods, SubGradient has a very acceptable performance in terms of execution time in generating acceptable results, but we also need to investigate how good its result is regarding accuracy and AUROC. The proposed neural solution, on the other hand, shows a significantly better performance compared to other methods in terms of execution time and demonstrates a fast performance regardless of the size of the data set. In particular, it is especially competitive with sklearn despite its efficient implementation (which includes feature and subset selection in C/C + +). A surprising point is also on the execution time of sklearn for the a9a data set, which shows that a significant increase in both feature and training samples number would adversely affect the performance of this solver. The proposed method demonstrates a stable performance on different data sets with a different number of features and/or training samples.Accuracy comparison: While the execution time of the proposed method is quite superior to other solvers, it is also essential to compare its performance in terms of accuracy. Table III shows the accuracy of different methods on different data sets 2 . According to this table, the accuracy of the proposed method is better than or equivalent to other solvers on five data sets: splice, liver-disorders, ijcnn1, a9a, and leukemia. For the remaining three data sets, the accuracy of the proposed method is quite competitive with the best-performing solvers. In particular, the accuracy difference between the proposed neural solution and best-performing solvers for the three data sets (i.e., Gauss-Seidel for madelon, Gauss-Southwell for a1a, and Orthant-Wise for gisette) is less than 1%, making the performance of the proposed neural solution very competitive with the state-of-the-art solvers in terms of accuracy. At the same, these methods require plenty of time to generate a result. For example, Orthant-wise takes more than 15 hours to generate results on the gisette data set, while its accuracy is only 0.3% higher than that of the proposed neural solution.ROC curve and AUROC comparison: We now compare the proposed neural solution in terms of ROC curve and AUROC. Figure 5 plots the ROC curves of the methods over eight data sets, the legend of which shows the AUROC of each method as well. As in the ROC curve, a deviation from diagonal shows a better performance, this figure also supports that the performance of the proposed neural solution is competitive with other solvers. In particular, the proposed neural network is the best-performing method on a9a and ijcnn1. In addition, its difference with the top-performing method on a1a, splice, and gisette data sets are very infinitesimal with a difference in AUROC less than 0.004. However, Gauss-Seidel has shown better performance on the madelon data set with a higher margin. This experiment also upholds the reasonable performance of the proposed neural network compared to the state-of-the-art solvers. Therefore, the conclusion can be drawn that the proposed neural solution can provide a reliable solution to the logistic regression with ℓ 1 regularization, while it is significantly fast and is scalable for large-scale problems. TABLE II :IIThe methods' execution time (in seconds) over eight data sets.Method's namesplicemadelonliver-disordersijcnn1a1aa9aleukemiagisetteGauss-Seidel4.1327.670.057.792.9058.4221.90977.58Shooting2.4120.640.171.531.9543.7314.37698.52Gauss-Southwell3.5510.890.121.682.3859.2520.11846.80Grafting1.0820.640.252.612.3150.8221.63649.00SubGradient0.081.460.080.650.100.2561.0618.57epsL10.041.070.060.360.185.6825.80401.36Log-Barrier49.752.660.170.870.162.20457.35398.98SmoothL10.144.640.011.472.197.21144.022214.89SQP0.063.400.191.200.621.061591.701585.94ProjectionL10.045.200.041.512.5145.251951.456483.31InteriorPoint0.2212.340.090.530.422.6711677.1227362.16Orthant-Wise0.0520.090.061.833.1646.15143543.2554686.79Pattern-Search0.164.330.211.340.576.0329.52700.61sklearn0.014.630.010.530.6257.320.045.94Proposed method0.220.790.010.170.401.790.141.5 TABLE III :IIIAccuracy of the methods over eight data sets.Method's namesplicemadelonliver-disordersijcnn1a1aa9aleukemiagisetteGauss-Seidel85.4759.3359.589.2882.9483.1991.1884.8Shooting85.4752.5059.090.5083.4582.4664.7168Gauss-Southwell85.4758.1759.090.4483.9085.0185.2995.2Grafting85.4757.8359.091.0083.8484.9982.3594.4SubGradient85.4756.6759.09.5024.0523.6258.8250.0epsL185.4756.6759.091.3483.8484.9976.4798.0Log-Barrier85.4756.3359.091.3483.8484.9885.2997.9SmoothL185.4756.3359.091.2083.8479.6361.7695.7SQP85.4756.3359.091.3483.8484.9985.2997.9ProjectionL185.4756.3359.091.3383.8484.9885.2998.1InteriorPoint85.4756.3359.091.3483.8484.9985.2997.9Orthant-Wise85.4756.3359.091.3383.8484.9785.2998.3Pattern-Search85.4756.3359.091.3483.8484.9988.2498.2sklearn85.4756.6759.0088.2383.8384.9988.2397.9Proposed method85.4759.061.5091.9083.6285.0394.1198.0proposed method. Figure",no,,,,no,Machine Learning,Machine Learning
https://github.com/shamangary/FSA-Net,shamangary_FSA-Net.tar.gz,FSA-Net: Learning Fine-Grained Structure Aggregation for Head Pose Estimation from a Single Image,@register_keras_custom_object,mtcnn utils pandas sys loupe_keras lib tqdm TYY_generators keras tensorflow re os matplotlib importlib scipy TYY_callbacks sklearn argparse numpy math cv2 logging moviepy capsulelayers,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shamangary_FSA-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shamangary_FSA-Net.pdf,,,yes,Adam,yes,"learning rate, epochs",no,Computer Vision,Computer Vision
https://github.com/christopher-w-murphy/Class-Imbalance-in-WW-Polarization,christopher-w-murphy_Class-Imbalance-in-WW-Polarization.tar.gz,Determination of the W W polarization fractions in pp → W ± W ± jj using a deep machine learning technique,@classmethod,scipy tensorflow pandas xml csv pickle sklearn subprocess os numpy pylhe time,hep-ph hep-ex hep-ph hep-ex stat.ML hep-ph hep-ex,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/christopher-w-murphy_Class-Imbalance-in-WW-Polarization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\christopher-w-murphy_Class-Imbalance-in-WW-Polarization.pdf,,,yes,SGD,yes,hyperparameters tuned by hand and confirmed by grid search,yes,Physics,Physics
https://github.com/ariel415el/ALAE,ariel415el_ALAE.tar.gz,Adversarial Latent Autoencoders,,utils tarfile pathlib imageio datasets collections cv2 tqdm torch os dnn argparse matplotlib numpy pprint kaggle torchvision,cs.LG cs.CV cs.NE cs.LG stat.ML cs.NE cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ariel415el_ALAE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ariel415el_ALAE.pdf,Instance Normalization  Adversarial Latent Autoencoder  StyleALAE  Adaptive Instance Normalization  R1 Regularization  Leaky ReLU  Dense Connections  Feedforward Network  StyleGAN  AutoEncoder  Convolution  Generative Adversarial Network  Leaky ReLU  WGAN-GP Loss  Local Response Normalization  1x1 Convolution  Dense Connections  Adam  Progressively Growing GAN  Convolution  Convolution  Adaptive Instance Normalization  R1 Regularization  Leaky ReLU  Dense Connections  Feedforward Network  WGAN-GP Loss  Adam  StyleGAN,,yes,Adam,yes,"betas, learning rate, batch size",yes,Machine Learning,Computer Vision
https://github.com/INK-USC/CommonGen,INK-USC_CommonGen.tar.gz,CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning,"@register_model_architecture(""asr_vggtransformer_encoder"", ""vggtransformer_enc_1"") @sep_token.setter @app.route('/unload_model/<int:model_id>', methods=['GET']) @_with_metaclass @lru_cache() @record_results @register_model(""gru_transformer"") @cython.locals(start=cython.Py_ssize_t, end=cython.Py_ssize_t) @require_torch @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_2"") @add_start_docstrings_to_callable(GPT2_INPUTS_DOCSTRING) @register_model(""asr_w2l_conv_glu_encoder"") @add_start_docstrings(AutoModel.__doc__) @cls_token.setter @magic_arguments.argument( @add_metaclass(TraceMethodCallMeta) @register_criterion(""cross_entropy_acc"") @unittest.skip(""This is just too slow"") @PreTrainedTokenizer.mask_token.setter @add_start_docstrings_to_callable(CTRL_INPUTS_DOCSTRING) @register_model(""asr_vggtransformer_encoder"") @add_start_docstrings_to_callable(ROBERTA_INPUTS_DOCSTRING) @app.route('/clone_model/<int:model_id>', methods=['POST']) @app.route('/to_gpu/<int:model_id>', methods=['GET']) @metrics.aggregate('train') @add_start_docstrings(AutoModelForSequenceClassification.__doc__) @include_dirs.setter @n_token.setter @dont_suppress_errors @add_start_docstrings_to_callable(T5_INPUTS_DOCSTRING) @tf.function @register_task('translation_moe') @add_start_docstrings_to_callable(DISTILBERT_INPUTS_DOCSTRING) @app.route('/to_cpu/<int:model_id>', methods=['GET']) @register_model(""asr_vggtransformer"") @cython.ccall @dispatch_on_frame(c_command='info variables', python_command='py-globals') @slow @magic_arguments.magic_arguments() @add_start_docstrings_to_callable(TRANSFO_XL_INPUTS_DOCSTRING) @app.route('/health', methods=['GET']) @system_filename_pattern.setter @bos_token.setter @keras_serializable @add_start_docstrings_to_callable(XLNET_INPUTS_DOCSTRING) @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING) @cached_method @cython.locals(start=cython.Py_ssize_t, q=cython.Py_ssize_t, @cached_property @cname('the_cname') @register_model_architecture(""gru_transformer"", ""gru_transformer"") @app.route('/models', methods=['GET']) @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_1"") @app.route('/translate', methods=['POST']) @add_start_docstrings( @additional_special_tokens.setter @register_task('commonsense_qa') @s3_request @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING) @config_file.setter @unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"") @eos_token.setter @cython.locals(create_from='CCodeWriter') @lru_cache(maxsize=1) @register_task('winogrande') @functools.wraps(initializer) @decorator           # NameNode @register_criterion(""asg_loss"") @register_criterion(""ctc_loss"") @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_base"") @require_running_program @torch_required @custom_tokenizers @functools.wraps(func) @default_selected_gdb_frame(err=False) @require_tf @default_selected_gdb_frame() @register_task(""speech_recognition"") @dataclass @magics_class @register_model_architecture(""gru_transformer"", ""gru_transformer_big"") @property @add_start_docstrings(""""""RoBERTa Model with a `language modeling` head on top. """""", ROBERTA_START_DOCSTRING) @cython.locals(idx=int) @abstractmethod @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU') @classmethod @wraps(func) @register_task('wsc') @cython.final @unk_token.setter @dataclass(frozen=True) @register_model_architecture(""asr_w2l_conv_glu_encoder"", ""w2l_conv_glu_enc"") @register_criterion('wsc') @functools.wraps(function) @add_start_docstrings_to_callable(OPENAI_GPT_INPUTS_DOCSTRING) @n_words.setter @add_start_docstrings_to_callable(XLM_INPUTS_DOCSTRING) @add_start_docstrings_to_callable(ELECTRA_INPUTS_DOCSTRING) @unittest.skipIf( @torch.no_grad() @add_start_docstrings_to_callable(FLAUBERT_INPUTS_DOCSTRING) @unittest.skipIf(torch_device == ""cpu"", ""Cant do half precision"") @pad_token.setter @staticmethod @register_criterion('winogrande') @model_filename_pattern.setter @unittest.skip(""Passing inputs_embeds not implemented for Bart."") @add_start_docstrings_to_callable(BART_INPUTS_DOCSTRING) @contextmanager @cell_magic @dispatch_on_frame(c_command='info locals', python_command='py-locals') @cached_function @add_start_docstrings(AutoConfig.__doc__) @funccontext_property @mask_token.setter @cython.cfunc @critical","fastapi utils_hans copy tensorflow_datasets CompilationOptions csv meteor git multiprocessing warnings types run_ner seqeval cider s2s_ft re, sys, time __main__ torchvision threading lm_seqs_dataset en_core_web_lg re tensorflow run_generation pythran pygments h5py dataclasses torch StringIO configparser rouge importlib nn six spacy fairseq_cli codecs a pyonmttok pickle boto3 textwrap tensorboardX numpy transformer_base sentencepiece tarfile configuration_bertabs unicodedata traceback networkx random linecache starlette utils_mmimdb inspect requests utils source cnndm signal generate pdb validate zlib __future__ psutil spice pprint uvicorn distutils MacOS examples pathlib unittest pythainlp botocore tests transformers fnmatch nltk os socket trace elementtree string ftfy os, tempfile interactive gzip functools xml hashlib utils_squad imp pkuseg pyximport time shutil abc cider_scorer getpass difflib coverage bs_pyrouge run_glue snownlp pydantic operator cElementTree recommonmark subprocess utils_classification cgi models bleu_scorer Mykytea gc run_squad sys utils_seq_labeling model_bertabs biunilm regex tqdm the onmt tokenizers evaluations configargparse sys, math, re run_language_modeling jedityper datetime cython distiller absl enum tempfile tokenize pkg_resources runtests gigaword eval_lm sphinx_rtd_theme scipy itertools IPython torchtext pyhanlp typing shlex sklearn filelock traceback, sys preprocess timeit train sys, os, copy librosa utils_ner setuptools ConfigParser sacrebleu errno md5 io hans_processors utils_multiple_choice PIL packaging zipfile modeling_bertabs scripts contextlib sacremoses glob fairseq gdb rouge_score zipimport platform ast fileinput locale fastprogress grouped_batch_sampler soundfile uuid wav2letter glob, os, argparse json collections Cython pretrainedmodels pytorch_lightning bleu concurrent utils_squad_evaluate sys, os, re, inspect pyrouge apex modeling_layoutlm bisect optparse ptvsd builtins py3nvml urllib jieba argparse lxml flask math torchaudio MeCab pytorch_pretrained_bert logging cv2 torch_xla __builtin__ eval",cs.CL cs.AI cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/INK-USC_CommonGen.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\INK-USC_CommonGen.pdf,,,no,manual tuning,yes,"batch size, learning rate, maximum updates, warmup steps, dropout rate, wieght decay, adam_eplison",yes,NLP,Machine Learning
https://github.com/ecrows/l2-reddit-experiment,ecrows_l2-reddit-experiment.tar.gz,TOWARDS ETHICAL CONTENT-BASED DETECTION OF ONLINE INFLUENCE CAMPAIGNS,@classmethod,math scipy bert tensorflow pandas csv sys collections json glob errno statistics sklearn os __future__ numpy pprint,cs.CY cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ecrows_l2-reddit-experiment.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ecrows_l2-reddit-experiment.pdf,,,no,,no,,no,NLP,NLP
https://github.com/dougsm/egad,dougsm_egad.tar.gz,EGAD! an Evolved Grasping Analysis Dataset for diversity and reproducibility in robotic manipulation,@classmethod @property @staticmethod @diversity.setter,"pytorch_neat copy imageio sys multiprocessing autolab_core pathlib collections json tempfile torch os pathlib2 matplotlib progressbar scipy java itertools dexnet pickle neat jpype argparse trimesh, trimesh numpy time shutil setuptools random trimesh subprocess egad graphviz",cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dougsm_egad.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dougsm_egad.pdf,,,no,,,,no,Robotic,Robotic
https://github.com/houda96/imagi-filter,houda96_imagi-filter.tar.gz,ImagiFilter: A resource to enable the semi-automatic mining of images at scale,,"PIL json glob load_data tqdm torch argparse glob, os numpy models matplotlib torchvision",cs.CV cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/houda96_imagi-filter.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\houda96_imagi-filter.pdf,,,yes,Adam,not all,"learning rate, beta, epsilon, batch size, dropout",no,Computer Vision,Information Retrieval
https://github.com/montescene/demo,montescene_demo.tar.gz,Monte Carlo Scene Search for 3D Scene Understanding,@staticmethod,utils copy csv sys multiprocessing glob shapely open3d absl json collections enum tempfile torch os LayoutStruct pytorch3d urllib pickle argparse monte_carlo_model_search numpy preprocessing shutil quaternion random cv2 zipfile,cs.CV cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/montescene_demo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\montescene_demo.pdf,Monte-Carlo Tree Search,"retrieve objects and room layouts from noisy RGB-D scans. While MCTS was developed as a game-playing algorithm, we show it can also be used for complex perception problems. Our adapted MCTS algorithm has few easy-to-tune hyperparameters and can optimise general losses. We use it to optimise the posterior prob-ability of objects and room layout hypotheses given the RGB-D data. This results in an analysis-by-synthesis approach that explores the solution space by rendering the current solution and comparing it to the RGB-D observations. To perform this exploration even more efficiently, we propose simple changes to the standard MCTS' tree construction and exploration policy. We demonstrate our approach on the ScanNet dataset. Our method often retrieves configurations that are better than some manual annotations, especially on layouts. Introduction3D scene understanding is a fundamental problem in Computer Vision [41,53]. In the case of indoor scenes, one usually aims at recognizing the objects and their properties such as their 3D pose and geometry [2,3,15], or the room layouts [57,31,62,59,30,36,50,60,62,54,55], or both [4,18,35,45,51,56]. With the development of deep learning approaches, the field has made a remarkable progress. Unfortunately, all recent methods are trained in a supervised way on 3D annotated data. Such a supervised approach has several drawbacks: 3D manual annotations are particularly cumbersome to create and creating realistic virtual 3D scenes also has a high cost [42]. Moreover, supervised methods also tend to generalize poorly to other datasets. Even more importantly, they can only be as good as the training 3D annotations, and mistakes in manual annotations are actually common in existing datasets, as we will show. If one wants to go further and consider more scenes without creating real or synthetic training datasets, it seems important to be able to develop methods that do not rely too much on 3D scenes for training.Over the history of 3D scene understanding, many nonsupervised approaches have already been proposed, including recently to leverage deep learning object detection methods. They typically combine generative models and the optimization of their parameters. Generative methods for 3D scene understanding indeed often involve optimization problems with high complexity, and many optimization tools have thus been investigated, including Markov Random Fields (MRFs) and Conditional Random Fields (CRFs) [22,52,32], Markov Chains Monte Carlo (MCMCs) [9,19,10,58], tree search [28], or hill climbing [61,21]. However, there does not seem to be a clear method of choice: MRFs and CRFs impose strong constraints on the objective function; MCMCs depend on many hyperparameters that are difficult to tune and can result in slow convergence; hill climbing can easily get stuck in a local optimum. The tree search method used by [28] uses a fixed width search tree that can miss good solutions.In this paper, we advocate for the use of Monte Carlo Tree Search (MCTS) [12,5], which is a general discrete AI algorithm for learning to play games [46], for optimization in 3D scene understanding problems. We propose to see perception as a (single-player) game, where the goal is to identify the right 3D elements that explain the scene. In such cases where the search problem can be organized into a tree structure which is too large for exhaustive evaluation, MCTS becomes a very attractive option. It also depends on very few easy-to-tune hyperparameters. Moreover, it can be interrupted at any time to return the best solution found so far, which can be useful for robotics applications. A parallel implementation is also possible for high efficiency [8]. In short, MCTS is a powerful optimization algorithm, but to the best of our knowledge, it has never been applied to 3D perception problems.To apply MCTS to 3D scene understanding, as shown in Fig. 1, we generate proposals for possible objects and layout components using the point cloud generated from the RGB-D sequence, as previous works do from a single RGB-D frame [28,61]. MCTS can be used to optimize general loss functions, which do not even have to be differentiable. This allows us to rely on a loss function based on an analysis-bysynthesis (or ""render-and-compare"") approach to select the proposals that correspond best to the observations. Our loss function compares (non-realistic) renderings of a set of proposals to the input images and can incorporate constraints between the proposals. This turns MCTS into an analysisby-synthesis method that explores possible sets of proposals for the observations, possibly back-tracking to better solutions when an exploration does not appear promising.We adapted the original MCTS algorithm to the 3D scene understanding problem to guide it towards the correct solution faster, and call the resulting method ""MCSS"", for Monte Carlo Scene Search. First, it is possible to structure the search tree so that it does not contain any impossible solutions, for example, solutions with intersecting proposals. We also enforce the exploration of proposals which are close spatially to proposals in the same path to the root node. Second, we introduce a score based on how the proposal improves the solution locally to increase the efficiency of search.In practice, we first run MCSS only on the layout proposals to recover the layout. We then run MCSS on the object proposals using the recovered layout. The recovery of the objects thus exploits constraints from the layout, which we found useful as shown in our experiments. In principle, it is possible to run a single MCSS on both the object and layout component proposals, but constraints from the objects did not appear useful to constrain the recovery of the layout for the scenes in ScanNet, which we use to evaluate our approach. We therefore used this two-step approach for simplicity. It is, however, possible that more complex scenes would benefit from a single MCSS running on all the proposals.Running our method takes a few minutes per scene. This is the same order of magnitude as the time required to acquire an RGB-D sequence covering the scene, but definitively slower than supervised methods. However, our direction could lead to a solution that automatically generates annotations, which could be used to train supervised methods for fast inference. We show in the experiments that our method already retrieves annotations that are sometimes more accurate than existing manual annotations, and that it can be applied to new data without tuning any parameters. Beyond that, MCTS is a very general algorithm, and the approach we propose could be transposed to other percep-tion problems and even lead to an integrated architecture between perception and control, as MCTS has also already been applied to robot motion planning control [25].",yes,Grid Search,no,,no,Computer Vision,Computer Vision
https://github.com/oya163/nepali-ner,oya163_nepali-ner.tar.gz,Named Entity Recognition for Nepali Language,"@app.route('/post', methods=['GET', 'POST']) @app.route('/') @property",utils pandas csv warnings sys tqdm uniseg __future__ re datetime collections app nltk torch os configparser utilities torchtext pickle sklearn argparse numpy flask time shutil config train logging models io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oya163_nepali-ner.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oya163_nepali-ner.pdf,Support Vector Machine,,yes,"Experimental Tuning, Adam",yes,"dropout, hidden size, filter size, filter number, learning rate, weight decay, batch size",yes,NLP,NLP
https://github.com/rahuln/lm-bio-kgc,rahuln_lm-bio-kgc.tar.gz,Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study,@staticmethod,requests copy pandas sys glob pdb bs4 tqdm __future__ umls util authentication re model datetime json collections transformers Bio torch os matplotlib ogb itertools seaborn argparse lxml preprocess tensorboardX numpy time networkx random logging subprocess dataloader,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rahuln_lm-bio-kgc.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rahuln_lm-bio-kgc.pdf,,"IntroductionUnderstanding complex diseases such as cancer, HIV, and COVID-19 requires rich biological, chemical, and medical knowledge. This knowledge plays a vital role in the process of discovering therapies for these diseases -for example, identifying targets for drugs [Lindsay, 2003] requires knowing what genes or proteins are involved in a disease, and designing drugs requires predicting whether a drug molecule will interact with specific target proteins. In addition, to alleviate the great costs of designing new drugs, drug repositioning [Luo et al., 2021] involves identification of existing drugs that can be re-purposed for other diseases. Due to the challenging combinatorial nature of these tasks, there is need for automation with machine learning techniques. Given the many links between biomedical entities, recent work [Bonner et al., 2021a,b] has highlighted the potential benefits of knowledge graph (KG) data representations, formulating the associated tasks as KG completion problemspredicting missing links between drugs and diseases, diseases and genes, and so forth.The focus of KG completion work -in the general domain, as well as in biomedical applications -is on using graph structure to make predictions, such as with KG embedding (KGE) models and graph neural networks [Zitnik et al., 2018, Chang et al., 2020. In parallel, recent work in the general domain has explored the use of pre-trained language models (LMs) as ""soft"" knowledge bases, holding factual knowledge latently encoded in their parameters [Petroni et al., 2020]. An emerging direction for utilizing this information for the task of KG completion involves fine-tuning LMs to predict relations between pairs of entities based on their textual descriptions [Yao et al., 2019, Daza et al., 2021. In the scientific domain, this raises the prospect of using LMs trained on millions of research papers to tap into scientific knowledge embedded in LMs. While this text-based approach has been evaluated on general domain benchmarks derived from WordNet [Miller, 1995] and Freebase [Bollacker et al., 2008], to our knowledge it has not been applied to the task of scientific KG completion.Our contributions. We perform an extensive study of LM-based KG completion in the biomedical domain, focusing on datasets centered on drugs and diseases, two of which have not been addressed to date as KGs. To enable exploration of LM-based models, we collect missing entity descriptions, obtaining them for over 35k entities across all datasets. We evaluate a range of KGE models and domain-specific scientific LMs pre-trained on different biomedical corpora [Beltagy et al., 2019, Lee et al., 2020, Gu et al., 2020. We conduct analyses of predictions made by both types of models and find them to have complementary strengths, echoing similar observations made in recent work in the general domain  and motivating integration of both text and graph modalities. Unlike previous work, we train a router that selects for each input instance which type of model is likely to do better, finding it to overall outperform average-based ensembles. Integration of text and graph modalities generally provides substantial relative improvements of 13-36% in Mean Reciprocal Rank (MRR), and routing across multiple LM-based models further boosts results. Finally, we demonstrate the utility of LM-based models when applied to entities unseen during training, an important scenario in the rapidly evolving scientific domain. Our hope is that this work will encourage further research into using scientific LMs for biomedical KG completion, tapping into knowledge embedded in these models and making relational inferences between complex scientific concepts. We make our datasets and code publicly available to advance research in this area. MethodsRanking-based KG completion. Each KG completion model in our experiments learns a function f that computes a ranking score s = f (x) for a given triple x = (h, r, t). Models are trained to assign a high ranking score to correct positive triples from the set of known facts T and a low ranking score to triples that are likely to be incorrect. To do so, we use the max-margin loss functionL rank (x) = 1 N N i=1 max(0, λ − f (x) + f (x i )), where λ is a margin hyperparameter, x ∈ T is a known positive triple in the KG, and x i is a negative triple constructed by randomly corrupting either the head or tail entity of x with an entity of the same type.KG embedding (KGE) models. For each entity e ∈ E and each relation r ∈ R, KG embedding (KGE) models learn a vector representation E(e) ∈ R m and R(r) ∈ R n . For a given triple (h, r, t), each model computes the ranking score f (h, r, t) as a simple function of these embeddings (Figure 1b). We include a variety of different KGE models in our experiments, including ComplEx [Trouillon et al., 2016], DistMult [Yang et al., 2015], RotatE [Sun et al., 2019], and TransE [Bordes et al., 2013].LM-based models. KGE methods do not capture the rich information available from textual descriptions of nodes. To address this limitation, previous KG completion ap-proaches have incorporated textual representations [Toutanova et al., 2015, Wang andLi, 2016], most recently with approaches such as KG-BERT [Yao et al., 2019] that fine-tune the BERT language model (LM) [Devlin et al., 2019] for the task of KG completion. Our focus in this work is on LMs pretrained on corpora of biomedical documents (e.g., Pub-MedBERT [Gu et al., 2020]; see Appendix B.1.2 for full details). To score a triple using an LM, we use a cross-encoder approach [Yao et al., 2019 (Fig. 1a), where we encode the text of the head and tail entity along with the appropriate special tokens. Specifically, a triple (h, r, t) is encoded as v = LM([CLS] text(h) [SEP] text(t) [SEP]),where v is the contextualized representation of the [CLS] token at the last layer. 2 We then apply an additional linear layer with a single output dimension to v to compute the ranking score for the triple (f (x) = W rank v ∈ R), and train the LM with the same max-margin loss. Recent work on applying BERT for KG completion on general domain benchmarks has shown multi-task training to improve performance . We use the approach of  and incorporate two additional losses for each LM: a binary triple classification loss to identify if a triple is positive or negative, and a multi-class relation classification loss. 3 2.3 Integrating KGE and LM: Model Averaging vs. Routing Previous work using text for KG completion on general domain benchmarks has demonstrated the benefit of combining KGE and text-based models [Xie et al., 2016. We study integration of graph-based and text-based methods (Figure 1c), exploring whether learning to route input instances adaptively to a single model can improve performance over previous work that compute a weighted average of ranking scores . We also explore the more general setup of combining more than two models.More formally, for a given triple x = (h, r, t), let φ(x) be its feature vector. We can learn a function g(φ(x)) that outputs a set of weights α = [α 1 , . . . , α k ], i α i = 1, α i > 0 ∀i. These weights can be used to perform a weighted average of the ranking scores {s 1 , . . . , s k } for a set of k models we wish to combine, such that the final ranking score is s = i α i s i . We use a variety of graph-, triple-, and text-based features to construct the feature vector φ(x) such as node degree, entity and relation type, string edit distance between head and tail entity names, and overlap in graph neighbors of head and tail nodes. We explore these features further in Section 4.1, and provide a full list in Appendix B.1.3 (Table 7).For the function g(•), we experiment with an input-dependent weighted average that outputs arbitrary weights α and a router that outputs a constrained α such that α i = 1 for some i and α j = 0, ∀j = i (i.e., α is a one-hot vector). 4 In practice, we implement the router as a classifier which selects a single KG completion model for each example by training it to predict which model will perform better. 5 For the input-dependent weighted average we train a multilayer perceptron (MLP) using the max-margin ranking loss. We train all models on the validation set and evaluate on the test set for each dataset.  B.1.3 Integrated ModelsGlobal weighted average. For the global weighted average, we compute ranking scores for positives and negative examples as the weighted average of ranking scores output by all KG completion models in the combination. Specifically, for a set of ranking scores s 1 , . . . , s k output by k models for an example, we learn a set of weights α = [α 1 , . . . , α k ] to compute the final ranking score as s = k i=1 α i s i , where the same weight vector α is used for all examples. We search for each α i over the grid [0.05, 0.95] with steps of 0.05, ensuring that all α i 's sum to 1. We choose values that maximize validation set MRR, then apply them to the test set.Router. For the router-based method, we train a classifier to select a single model out of a set of KG completion models to use for computing ranking scores for a positive example and its associated negatives. The class for a particular example corresponds to which model performs best on that example (i.e., gives the best rank), with an additional class if all models perform the same. We explore a number of different classifiers, including logistic regression, decision tree, gradient boosted decision tree (GBDT), and multilayer perceptron (MLP), finding that GBDT and MLP classifiers perform the best. We use a diverse set of features computed from each positive example, listed in Table 7. Classifiers are trained on the validation set and evaluated on the test set for each dataset. We additionally perform hyperparameter tuning over the following values for each classifier: Logistic regression:• Penalty: L1, L2• Regularization parameter: 9 values evenly log-spaced between 1e-5 and 1e3 Decision tree:• Max depth: 2, 4, 8• Learning rate: 1e-1, 1e-2, 1e-3 GBDT:• Number of boosting rounds: 100, 500, 1000• Max depth: 2, 4, 8• Learning rate: 1e-1, 1e-2, 1e-3 MLP:• Number of hidden layers: 1, 2• Hidden layer size: 128, 256• Batch size: 64, 128, 256• Learning rate: 1e-1, 1e-2, 1e-3We perform five-fold cross-validation on the validation set and use validation set accuracy to choose the best set of hyperparameters for each classifier. We use Scikit-Learn [Pedregosa et al., 2011] to implement the logistic regression and MLP classifiers, and XGBoost [Chen and Guestrin, 2016] to implement the decision tree and GBDT classifiers, using default parameters other than the ones listed above.Input-dependent weighted average. The input-dependent weighted average method of integrating KG completion models operates similarly to the global weighted average, except that the set of weights can vary for each positive example (the same set of weights is used for all negative examples used to rank against each positive example). We train an MLP to output a set of weights that are then used to compute a weighted average of ranking scores for a set of KG completion models. The MLP is trained on the validation set and evaluated on the test set for each dataset. We use the max-margin ranking loss with a margin of 1. In order to match the MLP trained as a router, we train the model using the Adam optimizer [Kingma and Ba, 2015] for 200 epochs with early stopping on the training loss with a patience of 10 epochs. We perform a hyperparameter search over the following values (matching the values for the MLP router where applicable):• Number of hidden layers: 1, 2• Hidden layer size: 128, 256• Batch size: 64, 128, 256• Learning rate: 1e-1, 1e-2, 1e-4• Number of negatives (for max-margin loss): 16, 32We select the best hyperparameters by MRR on a held-out portion of the validation set. B.2.1 Knowledge Graph Embeddings and Language ModelsFor the KGE and LM models, we follow the same training procedure for the inductive splits as for the transductive splits. We perform hyperparameter tuning over the same grids of hyperparameters, periodically evaluate on the validation set and save the checkpoint with the best validation set MRR, and use the set of hyperparameters corresponding to the highest validation set MRR to evaluate on the test set.",yes,"Grid Search, Adam",yes,"batch size, steps, epochs, embedding dimension, max margin loss, learning rate, number of negative samples, parameter for l3 reguarlization of embeddings, penalty l1 and l2, max depth, number boosting rounds, number hidden layers, hidden layer size",yes,NLP,Biology
https://github.com/mkolodny/funit,mkolodny_funit.tar.gz,Few-Shot Unsupervised Image-to-Image Translation,,utils PIL copy blocks yaml sys torchvision data torch os trainer networks argparse tensorboardX numpy time shutil math funit_model,cs.CV cs.AI cs.GR cs.MM stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mkolodny_funit.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mkolodny_funit.pdf,,,yes,RMSProp,yes,"learning rate, alphas",no,Computer Vision,Computer Vision
https://github.com/CDAMICCAI2019/CDA,CDAMICCAI2019_CDA.tar.gz,Constrained Domain Adaptation for Image Segmentation,"@return (labelmap1, labelmap2, n_lables1, n_labels2, labelmapping2to1) @staticmethod",utils imageio PIL pandas csv warnings sys multiprocessing dice3d nibabel layers skimage tqdm torchvision pathlib MySampler bounds re datetime imgaug binary torch os matplotlib scipy functools itertools typing networks argparse numpy shutil math viewer random operator dataloader io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CDAMICCAI2019_CDA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CDAMICCAI2019_CDA.pdf,,,yes,"Experimental Tuning, grid search, Adam",not all,"epochs, weighting parameter, learning rate",yes,Computer Vision,Computer Vision
https://github.com/b-fg/SANSpy,b-fg_SANSpy.tar.gz,Deep learning the spanwise-averaged Navier-Stokes equations,,scipy keras tensorflow pandas setuptools json postproc sanspy os numpy matplotlib,physics.flu-dyn,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/b-fg_SANSpy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\b-fg_SANSpy.pdf,,,yes,Adam,no,,no,Physics,Physics
https://github.com/akanimax/BMSG-GAN,akanimax_BMSG-GAN.tar.gz,MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,@staticmethod,timeit PIL copy math scipy data_processing datetime sagemaker_containers generate_multi_scale_samples torch os MSG_GAN argparse tqdm numpy matplotlib time torchvision,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/akanimax_BMSG-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\akanimax_BMSG-GAN.pdf,,,yes,RMSProp,yes,"learning rate, parameters initiliazed according the standard normal distribution",no,Computer Vision,Computer Vision
https://github.com/simon555/LM_word,simon555_LM_word.tar.gz,Efficient softmax approximation for GPUs,,data_utils copy csv sys infoToTrack tqdm local_models fileinput distutils torchvision keras tensorflow model Cython collections visdom nltk torch os matplotlib string progressbar itertools torchtext arguments pickle argparse dill visualisation numpy time shutil math lazyContiguousDataset log_uniform random losses io,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/simon555_LM_word.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\simon555_LM_word.pdf,Adaptive Softmax,,yes,Adagrad,not all,"hidden units, weight decay, word embeddings, steps, step size, gradient clipping, batch size",no,NLP,Machine Learning
https://github.com/IvLabs/stagewise-knowledge-distillation,IvLabs_stagewise-knowledge-distillation.tar.gz,Data Efficient Stagewise Knowledge Distillation,@property,utils PIL image_classification warnings sys tqdm comet_ml torchvision pathlib re efficientnet_pytorch json collections pretrainedmodels torch os trainer matplotlib functools arguments custom_resnet semantic_segmentation typing argparse inplace_abn numpy shutil thop math setuptools random cv2 fastai models,cs.LG cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IvLabs_stagewise-knowledge-distillation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IvLabs_stagewise-knowledge-distillation.pdf,Knowledge Distillation,Experimental SetupIn this section we detail the experimental setup. We consider two tasks namely (a) Image Classification and (b) Semantic Segmentation.,yes,Adam,yes,"learning rate, epochs",no,Machine Learning,Machine Learning
https://github.com/yelusaleng/RRU-Net,yelusaleng_RRU-Net.tar.gz,RRU-Net: The Ringed Residual U-Net for Image Splicing Forgery Detection,,utils dice_loss PIL unet random tqdm torch os numpy matplotlib time eval,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yelusaleng_RRU-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yelusaleng_RRU-Net.pdf,,,yes,SGD,yes,"batch size, momentum, weight decay, learning rate",no,Computer Vision,Computer Vision
https://github.com/cbschaff/nlimb,cbschaff_nlimb.tar.gz,Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning,@abstractmethod @U.in_session @functools.wraps(f) @classmethod @tf.custom_gradient,"os, subprocess, sys copy component_chopper imageio pandas csv sys multiprocessing cloudpickle glob os, argparse, json os, time gym threading tensorflow algorithm model datetime json collections os, pickle tempfile init sys, os, shutil, glob, json, time os xml_parser Robotic rl deeplearning six os, random gym, roboschool scipy functools argparse, os, json, shutil, gym xml pickle numpy time shutil abc envs argparse, os, json, time os, os time, sys, os random cv2 operator subprocess mpi4py",cs.RO cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cbschaff_nlimb.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cbschaff_nlimb.pdf,,"B. BaselinesIn addition to reporting results from our framework (run eight times for each morphology and environment to gauge consistency), we evaluate and compare it to three baselines:Default Designs: This baseline simply involves training a control policy for the standard, hand-crafted Roboschool designs. We use the same policy architecture as with our method and train until convergence.Random Sampling: This baseline samples designs uniformly at random within the parameter ranges, and trains a separate control policy for each sample. We use the same policy architecture as with our method and train until convergence. Since this approach is inherently more parallelizable than our method, we allow this baseline to sample designs and train control policies for three times the number of timesteps used by our method. The reward of the best design-control pair found across samples can then be compared to the performance of a single run of our experiment.Bayesian Optimization: This baseline employs Bayesian optimization (BayesOpt) to search jointly over the physical design space, using the implementation from [47]. For each sampled design, we train a control policy until convergence and return the average episode reward to the BayesOpt routine. BayesOpt then samples a new design and the process repeats. We use the same policy architecture as our experiments and allow BayesOpt the same number of environment timesteps used by our method. However, this timestep limit only allows for roughly ten designs to be evaluated. To account for variance in outcomes, we run four copies of this baseline with different random seeds and report each outcome.",yes,"Bayesian Optimization, Random Sampling",no,,yes,Robotic,Machine Learning
https://github.com/vikolss/DACS,vikolss_DACS.tar.gz,DACS: Domain Adaptation via Cross-domain Mixed Sampling,@functools.wraps(old_replicate) @contextlib.contextmanager @property,"utils PIL contextlib sys multiprocessing types tqdm torchvision os, sys threading model datetime json collections jactorch queue unittest data torch os matplotlib scipy functools kornia pickle argparse numbers numpy evaluateUDA time timeit math random cv2",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vikolss_DACS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vikolss_DACS.pdf,,"ExperimentsIn order to validate the proposed DACS algorithm, we evaluate it in two popular datasets for UDA and compare to the state of the art for these tasks. We also describe additional experiments related to the source of the class conflation problem, as well as the effect of the choice of mixing strategy. This section details the experimental setup and provides the qualitative and quantitative results found.Implementation Details. For all the experiments in this paper, we adopt the widely used [35, 24, 25, 46, 36, 37, 53,    [5] with a ResNet101 backbone [13] as our model. The backbone is pretrained on ImageNet [9] and on MSCOCO [22]. Most hyper-parameters are identical to those used in [35]. We use Stochastic Gradient Descent with Nesterov acceleration, and an initial learning rate of 2.5×10 −4 , which is then decreased using polynomial decay with exponent 0.9 as in [5]. Weight decay is set to 5×10 −4 and momentum to 0.9. Source images are rescaled to 760 × 1280 and target images to 512 × 1024, after which random crops of size 512 × 512 are extracted. For our main results we use ClassMix unless otherwise stated and in addition we also apply Color jittering and Gaussian blurring on the mixed images. We train using batches with 2 source images and 2 mixed images for 250k iterations. The code is implemented using PyTorch, and is available at https://github.com/vikolss/DACS. Experiments were performed using a GTX 1080 Ti GPU with 12 GB memory.Datasets. We present results for two synthetic-to-real benchmarks common for UDA for semantic segmentation. Namely GTA5 → Cityscapes and SYNTHIA → Cityscapes. The target dataset Cityscapes has 2,975 training images taken from a car in urban environments and is labelled with 19 classes [8]. The source datasets GTA5 [30] and SYN-THIA [31] contain 24,966 and 9,400 synthetic training images respectively. Example images of all three datasets are shown in Figure 4 together with ground-truth semantic maps. The GTA5 images are labelled with the same 19 classes as Cityscapes whereas the SYNTHIA data is labelled with 16 of the 19 classes. All results are reported with the Intersection over Union (IoU) metric per class and the mean Intersection over Union (mIoU) over all classes, the standard performance metric for semantic segmentation.",yes,SGD,yes,"most hyperparameter similar to other study, nesterov acceleration, learning rate, decay, weight decay, momentum, iterations",no,Computer Vision,Computer Vision
https://github.com/rsheth80/pmf-automl,rsheth80_pmf-automl.tar.gz,Probabilistic Matrix Factorization for Automated Machine Learning,@property,utils bo gplvm scipy pandas sklearn torch kernels numpy matplotlib time,stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rsheth80_pmf-automl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rsheth80_pmf-automl.pdf,,"Related workThe concept of leveraging experiments performed in previous problem instances has been explored in different ways by two different communities. In the Bayesian optimization community, most of the work revolves around either casting this problem as an instance of multi-task learning or by selecting the first parameter settings to evaluate on a new dataset by looking at what worked in related datasets (we will refer to this as meta-learning for cold-start). In the multi-task setting, Swersky et al. (2013) have proposed a multi-task Bayesian optimization approach leveraging multiple related datasets in order to find the best hyperparameter setting for a new task. For instance, they suggested using a smaller dataset to tune the hyperparameters of a bigger dataset that is more expensive to evaluate. Schilling et al. (2015) also treat this problem as an instance of multi-task learning, but instead of treating each dataset as a separate task (or output), they effectively consider the tasks as conditionally independent given an indicator variable specifying which dataset was used to run a given experiment. Springenberg et al. (2016) do something similar with Bayesian neural networks, but instead of passing an indicator variable, their approach learns a dataset-specific embedding vector. Perrone et al. (2017) also effectively learn a task-specific embedding, but instead of using Bayesian neural networks end-to-end like in (Springenberg et al., 2016), they use feedforward neural networks to learn the basis functions of a Bayesian linear regression model.Other approaches address the cold-start problem by evaluating parameter settings that worked well in previous datasets. The most successful attempt to do so for automated machine learning problems (i.e. in very high-dimensional and structured parameter spaces) is the work by Feurer et al. (2015). In their paper, the authors compute meta-features of both the dataset under examination as well as a vari-ety of OpenML (Vanschoren et al., 2013) datasets. These meta-features include for example the number of classes or the number of samples in each dataset. They measure similarity between datasets by computing the L1 norm of the meta-features and use the optimization runs from the nearest datasets to warm-start the optimization. Reif et al. (2012) also use meta-features of the dataset to cold-start the optimization performed by a genetic algorithm. Wistuba et al. (2015) focus on hyperparameter optimization and extend the approach presented in (Feurer et al., 2015) by also taking into account the performance of hyperparameter configurations evaluated on the new dataset. In the same paper, they also propose to carefully pick these evaluations such that the similarity between datasets is more accurately represented, although they found that this doesn't result in improved performance in their experiments.Other related work has been produced in the context of algorithm selection for satisfiability problems. In particular, Stern et al. (2010) tackled constraint solving problems and combinatorial auction winner determination problems using a latent variable model to select which algorithm to use. Their model performs a joint linear embedding of problem instances and experts (e.g. different SAT solvers) based on their meta-features and a sparse matrix containing the results of previous algorithm runs. Malitsky & O'Sullivan (2014) also proposed to learn a latent variable model by decomposing the matrix containing the performance of each solver on each problem. They then develop a model to project commonly used hand-crafted meta-features used to select algorithms onto the latent space identified by their model.They use this last model to do one-shot (i.e. non-iterative) algorithm selection. This is similar to what was done by Mısır & Sebag (2017), but they do not use the second regression model and instead perform one-shot algorithm selection directly.Our work is most related to (Feurer et al., 2015) in terms of scope (i.e. joint automated pre-processing, model selection and hyperparameter tuning), but we discretize the space and set up a multi-task model, while they capture continuity in parameter space in a single-task model with a smart initialization. Our approach is also loosely related to the work of Stern et al. (2010), but we perform sequential model based optimization with a non-linear mapping between latent and observed space in an unsupervised model, while they use a supervised linear model trained on ranks for one-shot algorithm selection. The application domain of their model also required a different utility function and a time-based feedback model. ExperimentsIn this section, we compare our method to a series of baselines as well as to auto-sklearn (Feurer et al., 2015), the current state-of-the-art approach and overall winner of the ChaLearn AutoML competition (Guyon et al., 2016).We ran all of the experiments on 553 OpenML ( Vanschoren et al., 2013) datasets, selected by filtering for binary and multi-class classification problems with no more than 10, 000 samples and no missing values, although our method is capable of handling datasets which cause ML pipeline runs to be unsuccessful (described below). Generation of training dataWe generated training data for our method by splitting each OpenML dataset in 80% training data, 10% validation data and 10% test data, running 42, 000 ML pipelines on each dataset and measuring the balanced accuracy (i.e. accuracy rescaled such that random performance is 0 and perfect performance is 1.0).We generated the pipelines by sampling a combination of pre-processors P = {P 1 , P 2 , ..., P n }, machine learning models M = {M 1 , M 2 , ..., M m }, and their corresponding hyperparameters Θ P = {θ 1 P , ..., θ n P } and Θ M = {θ 1 M , ..., θ m M } from the entries in Supplementary Table 1. All the models and pre-processing methods we considered were implemented in scikit-learn (Pedregosa et al., 2011). We sampled the parameter space by using functions provided in the auto-sklearn library (Feurer et al., 2015). Similar to what was done in (Feurer et al., 2015), we limited the maximum training time of each individual model within a pipeline to 30 seconds and its memory consumption to 16GB. Because of network failures and the cluster occasionally running out of memory, the resulting matrix Y was not fully sampled and had approximately 21% missing entries. As pointed out in the previous section, this is expected in realistic applications and is not a problem for our method, since it can easily handle sparse data.Out of the 553 total datasets, we selected 100 of them as a held out test set. We found that some of the OpenML datasets are so easy to model, that most of the machine learning pipelines we tried worked equally well. Since this could swamp any difference between the different methods we were evaluating, we chose our test set taking into consideration the difficulty of each dataset. We did so by randomly drawing without replacement each dataset with probabilities proportional to how poorly random selection performed on it. Specifically, for each dataset, we ran random search for 300 iterations and recorded the regret. The probability of selecting a dataset was then proportional to the regret on that dataset, averaged over 100 trials of random selection. After removing OpenML datasets that were used to train auto-sklearn, the final size of the held out test set was 89.The training set consisted of the remaining 464 datasets (the IDs of both training and test sets are provided in the supplementary material).  ResultsWe compared the model described in this paper, PMF, to the following methods:• Random. For each test dataset, we performed a random search by sampling each pipeline to be evaluated from the set of 42,000 at random without replacement.• Random 2x. Same as above, but with twice the budget. This simulates parallel evaluation of pipelines and is a strong baseline (Li et al., 2016a).• Random 4x. Same as a above but with 4 times the budget.• auto-sklearn (Feurer et al., 2015). We ran autosklearn for 4 hours per dataset and set to optimize balanced accuracy on a holdout set. We disabled the automated ensembling of models in order to obtain a fair comparison to the other non-ensembling methods.Our method uses the same procedure used in (Feurer et al., 2015) to ""warm-start"" the process by selecting the first 5 pipelines, after which the acquisition function selects subsequent pipelines.Probabilistic Matrix Factorization for Automated Machine Learning Figure 4 shows the average rank for each method as a function of the number of iterations (i.e. the number of pipelines evaluated). Starting from the first iteration, our approach consistently achieves the best average rank. Auto-sklearn is the second best model, outperforming random 2x and almost matched by random 4x. Please note that random 2x and random 4x are only intended as baselines that are easy to understand and interpret, but that in no way can be considered practical solutions, since they both have a much larger computational budget than the non-baseline methods.Rank plots such as Figure 4 are useful to understand the relative performance of a set of models, but they don't give any information about the magnitude of the difference in performance. For this reason, we measured the difference between the maximum balanced accuracy obtained by any pipeline in each dataset and the one obtained by the pipeline selected at each iteration. The results summarized in Figure 5 show that our method still outperforms all the others. We also investigated how well our method performs when fewer observations/training datasets are available. In the first experiment, we ran our method in the setting where 90% of the entries in Y are missing. Supplementary Figures 1  and 2 demonstrate our method degrades in performance only slightly, but still results in the best performance amongst competitors. In the second experiment, we matched the number (and the identity, for the most part) of datasets that auto-sklearn uses to initialize its Bayesian optimization procedure. The results, shown in Supplementary Figures 3 and 4, confirm that our model outperforms competing approaches even when trained on a subset of the data.Next, we investigated how quickly our model is able to improve its predictions as more pipelines are evaluated. Figure 6a shows the mean squared error computed across the test datasets as a function of the number of evaluations. As expected the error monotonically decreases and appears to asymptote after 200 iterations. Figure 6b shows the uncertainty of the model (specifically, the posterior variance) as a function of the number of evaluations. Overall, Figure 6 a and b support that as more evaluations are performed, the model becomes less uncertain and the accuracy of the predictions increases.Including pipeline metadata. Our approach can easily incorporate information about the composition and the hyperparameters of the pipelines considered. This metadata could for example include information about which model is used within each pipeline or which pre-processor is applied to the data before passing it to the model. Empirically, we found that including this information in our model didn't improve performance (data not shown). Indeed, our model is able to effectively capture most of this information in a completely unsupervised fashion, just by observing the sparse pipelines-dataset matrix Y. This is visible in Figure 2, where we show the latent embedding colored according to which model was included in which pipeline. On a finer scale, the latent space can also capture different settings of an individual hyperparameter. This is shown in Figure 3, where each pipeline is embedded in a 2-dimensional space and colored by the value of the hyperparameter of interest, in this case the percent of variance retained by a PCA preprocessor. Overall, our findings indicate that pipeline metadata is not needed by our model if enough experimental data (i.e. enough entries in matrix Y) is available. DiscussionWe have presented a new approach to automatically build predictive ML pipelines for a given dataset, automating the selection of data pre-processing method and machine learning model as well as the tuning of their hyperparameters.Our approach combines techniques from collaborative filtering and ideas from Bayesian optimization to intelligently explore the space of ML pipelines, exploiting experiments performed in previous datasets. We have benchmarked our approach against the state-of-the-art in 89 OpenML datasets with different sample sizes, number of features and number of classes. Overall, our results show that our approach outperforms both the state-of-the-art as well as a set of strong baselines.One potential concern with our method is that it requires sampling (i.e. instantiating pipelines) from a potentially high-dimensional space and thus could require exponentially many samples in order to explore all areas of this space. We have found this not to be a problem for three reasons. First, many of the dimensions in the space of pipelines are conditioned on the choice of other dimensions. For example, the number of trees or depth of a random forest are parameters that are only relevant if a random forest is chosen in the ""model"" dimension. This reduces the effective search space significantly. Second, in our model we treat every pipeline as an additional sample, so increasing the sampling density also results in an increase in sample size (and similarly, adding a dataset also increases the effective sample size). Finally, very dense sampling of the pipeline space is only needed if the performance is very sensitive to small parameter changes, something that we haven't observed in practice. If this is a concern, we advise using our approach in conjunction with traditional Bayesian optimization methods (such as (Snoek et al., 2012)) to further fine-tune the parameters.We are currently investigating several extensions of this work. First, we would like to include dataset-specific information in our model. As discussed in section 3, the only data taken into account by our model is the performance of each method in each dataset. Similarity between different pipelines is induced by having correlated performance across multiple datasets, and ignores potentially relevant metadata about datasets, such as the sample size or number of classes. We are currently working on including such information by extending our model using additional kernels and dual embeddings (i.e. embedding both pipelines and dataset in separate latent spaces). Second, we are interested in using acquisition functions that include a factor representing the computational cost of running a given pipeline (Snoek et al., 2012) to handle instances when datasets have a large number of samples. The machine learning models we used for our experiments were constrained not to exceed a certain runtime, but this could be impractical in real applications. Finally, we are planning to experiment with different probabilistic matrix factorization models based on variational autoencoders.For the first additional experiment, we add results from (i) the Factorized MLP of (Schilling et al., 2015), and (ii) training the proposed method on an observation matrix with 90% of the entries missing. For the latter, we start with the original observation matrix for the training data, which has 20% missing entries, drop an additional 70% uniformly at random, and train with Adam using a learning rate of 10 −2 and Q = 5 for the latent dimensionality. The test set remains unchanged from the experiment in the main paper. Figures 1 and 2 show the results overlaid onto the results from the experiment in the main paper. We can see the performance of the factorized MLP to be between random and random-2x, even after tuning its hyperparameters to improve performance. This is significantly worse than both auto-sklearn and the proposed method. Our method degrades in performance only slightly when 10% of the observations are available versus when training on 80% available observations, but still out-performs auto-sklearn demonstrating very good robustness to missing data. Although not shown, we also ran the proposed method with ξ = 0 in the acquisition function, and this did not produce any distinguishable effect in our results. Our method even with ξ = 0 still achieves the best regret and rank.  For the second additional experiment, we trained the proposed method on 93 of the datasets used to train auto-sklearn and an additional 47 datasets selected uniformly at random from the training set of the main paper. The model and training parameters were 20 latent dimensions and a learning rate of 10 −5 . The evaluation was performed on the same held out test set. Figures 3 and 4 show the outcome of this experiment and demonstrate that our method still outperforms auto-sklearn. For this experiment, the list of OpenML dataset IDs used to train our method was [3,6,12,14,16,18,21,22,26,28,30,31,32,36,44,46,60,180,181,182,184,300,389,391,392,395,401,679,715,718,720,722,723,727,728,734,735,737,741,743,751,752,761,772,797,803,807,813,816,819,821,823,833,837,843,845,846,847,849,866,871,881,901,903,910,912,913,914,917,923,934,953,958,959,962,971,976,977,978,979,980,991,995,1019,1020,1021,1040,1041,1056,1068,1069,1116,1120,310,1132,685,824,1015,1541,50,890,1014,1446,747,875,1459,721,900,878,1236,40478,1562,1079,1496,1449,988,796,162,811,1145,776,457,476,1482,1529,1127,952,740,1043,1546,4135,1022,853,1237,758,827,814,450,155,462]   Figure 1 :1Figure 1: Two-dimensional embedding of 5,000 ML pipelines across 576 OpenML datasets. Each point corresponds to a pipeline and is colored by the AUROC obtained by that pipeline in one of the OpenML datasets (OpenML dataset id 943).",yes,"Bayesian Optimization, Random Search",yes,"learning rate, latent dimension, batch size, epochs",yes,Machine Learning,Machine Learning
https://github.com/BlurryLight/DD-Net-Pytorch,BlurryLight_DD-Net-Pytorch.tar.gz,"Make Skeleton-based Action Recognition Model Smaller, Faster and Better",,utils pathlib math scipy torchsummary sys pickle logging tqdm sklearn torch argparse dataloader numpy models matplotlib time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BlurryLight_DD-Net-Pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BlurryLight_DD-Net-Pytorch.pdf,,,yes,Adam,yes,"betas, decay, learning rate",no,Computer Vision,Computer Vision
https://github.com/Quiff789/Mapper-instability,Quiff789_Mapper-instability.tar.gz,A NUMERICAL MEASURE OF THE INSTABILITY OF MAPPER-TYPE ALGORITHMS,,math copy itertools re sys random sklearn kmapper os mpl_toolkits numpy matplotlib,math.AT cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Quiff789_Mapper-instability.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Quiff789_Mapper-instability.pdf,,"Table 11For the values of epsilon of 0.06 and 0.09, the instability decreases due to the disappearance of noise represented by spurious small connected components in the Mapper graph. The major part of the structure of the Mapper graph remains the same, revealing both the inner and outer circle. Above the epsilon values of 0.1, there is a spike in the instability value corresponding to a loss of detail in the inner circle within the Mapper graph. A similar spike occurs around the 0.32 value of epsilon, corresponding to the loss of the inner circle from the Mapper graph. The final large increase in instability occurs around the 0.45 value of epsilon, and it corresponds to the gradual merging of the two circles in the Mapper graph.We now pass to experiments that explore the dependence of the Mapper graph on the values of resolution and gain. Figure2presents a contour plot of the instability of Mapper on another dataset consisting of noisy concentric circles created by varying the percentage overlap between bins (gain) and the number of bins (resolution).considers a dataset with two noisy concentric circles. We produce a family of Mapper graphs using the -neighbourhood clustering with varying values of epsilon. The specific clustering procedure used was DBSCAN from the sklearn python package.Similarly to the discussion on Table1, it is possible to identify a number of global features within the plot with structural changes in the Mapper graph.",no,,no,,no,Math,Machine Learning
https://github.com/thu-media/FedCL,thu-media_FedCL.tar.gz,CONTINUAL LOCAL TRAINING FOR BETTER INITIALIZATION OF FEDERATED MODELS,@property @abstractmethod,PIL copy sys torchvision core model datetime collections queue data torch os itertools pickle argparse tensorboardX numpy time abc shutil config prefetch_generator,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thu-media_FedCL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thu-media_FedCL.pdf,,,yes,SGD,yes,"epochs, batch size, learning rate, decay, ",no,Machine Learning,Machine Learning
https://github.com/cyberpunk317/Action_detection,cyberpunk317_Action_detection.tar.gz,Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos,@staticmethod,utils pathlib PIL warnings model collections dataset glob setuptools pdb cv2 torch numpy matplotlib torchvision,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cyberpunk317_Action_detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cyberpunk317_Action_detection.pdf,3D Convolution  Convolution,,no,,,"learning rate, batch size",no,Computer Vision,Computer Vision
https://github.com/avivga/lord-pytorch,avivga_lord-pytorch.tar.gz,DEMYSTIFYING INTER-CLASS DISENTANGLEMENT,@abstractmethod @staticmethod,imageio tqdm torchvision re model h5py torch os assets itertools dataset pickle argparse numpy dlib abc config shutil cv2,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/avivga_lord-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\avivga_lord-pytorch.pdf,,"|LATENT OPTIMIZATION FOR CLASS SUPERVISIONWe make explicit the assumption that inter-class variation is significantly larger than intra-class variation. This allows us to model images as a combination of class and content codes:x i = G θ (e yi , 0, c i )(5)Shared Latent Optimization: We model the class representation as an embedding e y that is shared between all images belonging to the same class {x i ||y i = y}. Instead of using amortized inference (learning a mapping from the image to the class codes using an encoder), we optimize over the class embeddings directly using latent optimization. This has several important benefits: i) As the code is shared exactly between all images belonging to the same class (each having different content), it is impossible to include any content information in the class code. ii) As we learn per-class representations directly rather than using previous techniques as group averaging, each mini-batch can contain images randomly sampled from all classes allowing maximal diversity.We learn the content representation by optimizing over per-sample content embeddings directly using latent optimization and not in an amortized fashion using an image to content encoder. As we show in the experimental section, a model trained with latent optimization preserves a very high degree of disentanglement along the training and is less sensitive to hyperparameter choices.Asymmetric Noise Regularization: Latent optimization over the class embeddings ensures that no content information is present in the class representation. To ensure that class information does not leak into the content representation, we regularize the content code to enforce minimality of information. Previous approaches attempted to minimize content information by setting a bottleneck of a small content code or by matching the content distribution to a prior normal distribution using KLdivergence. Using a small noiseless bottleneck, does not however reduce information significantly.A continuous variable may in fact store an infinite amount of information (although the amount of information the generator may extract is limited by other factors). In our experiments, we found that regularizing with KL-divergence (as done by previous works) led to a partial posterior collapse i.e. nearly all means and standard deviations learned by the encoder defaulted to 0 and 1 respectively, satisfying a perfect standard normal distribution. For a few components, the encoder learned large means and very small standard deviations. The KL-divergence therefore learned behavior similar to a small-size bottleneck. This phenomenon implies that regularizing the distribution of the content codes with KL-divergence may require additional attention and a careful hyperparameter tuning. We present the experimental evidence in the Appendix A.3.In our approach, we regularize the content code with an additive Gaussian noise of a fixed variance, and an activation decay penalty. In contrast to a variational auto-encoder, we do not learn the variance, but rather keep it fixed. This prevents the possibility of the variance decreasing to a small value, ensuring that noise is applied equally on all components. Our objective function becomes: L = n i=1 G θ (e yi , 0, c i + z i ) − x i + λ c i 2 z i ∼ N (0, σ 2 I)(6)The first loss terms uses a VGG perceptual loss as implemented by Hoshen & Malik (2019). Unless stated otherwise, we optimize over class and content codes (e yi and c i ) directly using latent optimization. All latent codes and the parameters of the generator are learned end-to-end using stochastic gradient descent:{e * 1 , .., e * k , c * 1 .., c * n , θ * } = arg min e,c,θ L (7) 3.2 AMORTIZATION FOR ONE-SHOT INFERENCELatent optimization, which is used effectively for training, requires optimization for every image (including at inference time). In the training set, a class embedding is shared across multiple images, which prevents the embedding from including content information. However, at inference time, a single image from an unknown class is observed. Optimizing over the latent codes for a single image leads to overfitting which results in entangled representations. Moreover, it requires iterative test-time inference since it does not perform amortized inference.To this end, we introduce a second stage which learns class and content encoders that directly infer class e yi and content c i representations from a single image x i . The second stage effectively amortizes the results of the first stage and generalizes well to unseen classes and images. We train encoders E y : X − → Y and E c : X − → C, which take as input an image x i and output its class and content embeddings that were learned by our method in the first stage. We also use a reconstruction loss, to ensure the representations learned in the second stage must reconstruct the original image x i . The optimization objective is presented in Eq. 8. The optimization is over the parameters of encoders E y and E c (which are randomly initialized) and the parameters of the generator G (which are initialized from the first stage). Note that the given e yi and c i are the representations we have learned during the previous stage. tions and regenerating them as follows:L E = n i=1 G θ (E y (x i ), 0, E c (x i )) − x i + α 1 • E y (x i ) − e yi 2 + α 2 • E c (x i ) − c i 2(x1← −2 = G θ (E y ( x1 ), 0, E c ( x2 ))(9) Quantitative Experiments:Content transfer experiments: To test the quality of disentanglement, we measure the quality of content transfer in terms of perceptual similarity by LPIPS (Zhang et al., 2018). We use the content labels available in the Cars3D and SmallNorb datasets as ground truth for content transfer. For given test images x i and x j , we measure the similarity between x 1← −2 = G θ (E y (x i ), 0, E c (x j )) and another image from the same class of x i matching the same content of x j . For CelebA, given Classification experiments: To assess the disentanglement of our learned representations, we follow the protocol in Harsh Jha et al. ( 2018) and train a classifier to classify class labels from content codes and vice versa. Results can be seen in Tab. 2. On all datasets, our model achieves near perfect disentanglement as the classifier could barely guess class labels from content codes by a random chance (same for the other direction). All the baselines fail to zero out the mutual information between the two representations. To conclude, our method is able to learn the most disentangled features without introducing adversarial constraints. For CelebA, in order to test if the content of an image is predictable from the class code we train a linear regression model to regress the position of 68 facial landmarks. It can be seen that the linear regression results in the highest error on our class representations, indicating the highest degree of disentanglement of our method. It should be noted that all methods could classify class labels from class codes and content labels from content codes very accurately (not shown). ABLATION ANALYSISWe perform a careful ablation analysis on the components on our method, a summary of this study is presented in Tab.   Figure 6: Examples of translations between anime and faces from CelebA using our method.high degree of disentanglement. We hypothesize that achieving similar degree of disentanglement by amortization requires a more sophisticated objective and a more careful hyperparameter tuning. An extended study is presented in Appendix A.4. It should be noted that latent optimization requires more iterations than optimizing an amortized encoder and leads to a slower convergence (the number of iterations increased by ×2 in our experiments). In both the amortized and semi-amortized models, we find that the KL-divergence fails to regularize the information leakage from the class representation into the content representations. A visualization of the partial posterior collapse can be found in the Appendix A.3. We finally demonstrate the importance of our second stage by assessing the performance after the first stage only. This can be done by optimizing over the latent codes of a new test image while keeping the rest of the model frozen. As can be seen, this approach suffers from low performance in all metrics. The effect of the asymmetric noise regularization can be observed from the inferior performance of training our model without regularization. A qualitative visualization of this analysis is provided in Appendix A.6. A.3 KL-DIVERGENCE POSTERIOR COLLAPSEWe provide evidence for the partial posterior collapse we experienced when regularizing the content codes with KL-divergence. Fig. 8 shows the mean and standard deviations of each of the 128 components of the content code (averaged over all samples in the dataset) in a model trained on SmallNorb. It can be seen that 126 out of 128 components of the content code collapse to match a perfect standard normal distribution, while in the remaining 2 components the standard deviation is reduced dramatically along with a substantial increase in the mean. This phenomenon implies that regularizing the distribution of the content codes with KL-divergence may require additional attention and a careful hyperparameter tuning. We find in our experiments that the asymmetric regularization introduced in our method results in better disentanglement.|",yes,"Adam, SGD",yes,,yes,Computer Vision,Computer Vision
https://github.com/cruvadom/Convolutional-RNN,cruvadom_Convolutional-RNN.tar.gz,Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data,,tensorflow,stat.ML cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cruvadom_Convolutional-RNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cruvadom_Convolutional-RNN.pdf,,,yes,Adam,not all,"learning rate, betas",no,Machine Learning,Machine Learning
https://github.com/xuxy09/texformer,xuxy09_texformer.tar.gz,3D Human Texture Estimation from a Single Image with Transformers,@staticmethod @torch.no_grad(),utils imageio PIL loss warnings sys tqdm easydict __future__ torchvision dataset_pytorch collections json transformers torch os reid_resnet matplotlib scipy functools RSC_net neural_renderer pickle argparse tensorboardX numpy config shutil math smplx cv2 random NMR lpips options layer,cs.CV cs.AI cs.GR cs.LG cs.MM cs.CV cs.AI cs.CV cs.AI cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xuxy09_texformer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xuxy09_texformer.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Scaled Dot-Product Attention  Byte Pair Encoding  Layer Normalization  Residual Connection  Adam  Dropout  Softmax  Multi-Head Attention  Label Smoothing  Dense Connections  Transformer  Low-resolution input  High-resolution input,,no,,no,,no,Computer Vision,Computer Vision
https://github.com/LayneH/self-adaptive-training,LayneH_self-adaptive-training.tar.gz,Self-Adaptive Training: Bridging the Supervised and Self-Supervised Learning,@property,utils PIL copy math datasets time random losses torch os argparse __future__ numpy models shutil torchvision,cs.LG cs.CV cs.LG stat.ML cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/LayneH_self-adaptive-training.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\LayneH_self-adaptive-training.pdf,Self-adaptive Training  Self-adaptive Training,,yes,"SGD, Adam",not all,"learning rate, momentum, weight decay, batch size, epochs, E, alpha, ",no,Machine Learning,Machine Learning
https://github.com/ruotianluo/self-critical.pytorch,ruotianluo_self-critical.pytorch.tar.gz,Controlling Length in Image Captioning,@property @staticmethod,"PIL copy pyciderevalcap csv yaml multiprocessing sys skimage lmdbdict tqdm mmap zlib __future__ base64 torchvision os, sys captioning json collections h5py transformers pytorch_lightning tempfile torch os string six scipy functools itertools hashlib codecs typing subword_nmt pickle argparse numpy eval_utils time math m2transformer pycocotools setuptools traceback random logging detectron2 models yacs pycocoevalcap io",cs.LG cs.AI cs.CV cs.CV cs.CL cs.CV cs.CV cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ruotianluo_self-critical.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ruotianluo_self-critical.pytorch.pdf,Self-critical Sequence Training  REINFORCE  REINFORCE,,no,,no,,no,Computer Vision,Machine Learning