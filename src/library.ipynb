{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def df_to_latex(df: pd.DataFrame) -> None:\n",
    "    print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlrrl}\n",
      "\\toprule\n",
      "             Class &  Count &        Category &  \\#HP &  AvgOptionsUsed &              Most Used HP \\\\\n",
      "\\midrule\n",
      "    StandardScaler &     44 &   preprocessing &    3 &            0.20 &                   default \\\\\n",
      "LogisticRegression &     40 &    linear\\_model &   15 &            1.12 &                   default \\\\\n",
      "            KMeans &     30 &         cluster &    9 &            2.30 &                n\\_clusters \\\\\n",
      "      MinMaxScaler &     25 &   preprocessing &    3 &            0.52 &    feature\\_range, default \\\\\n",
      "          Pipeline &     21 &        pipeline &    3 &            1.00 &                     steps \\\\\n",
      "      LabelEncoder &     17 &   preprocessing &    0 &            0.00 &                   default \\\\\n",
      "  LinearRegression &     13 &    linear\\_model &    5 &            0.08 &                   default \\\\\n",
      "               PCA &     12 &   decomposition &    9 &            1.00 &              n\\_components \\\\\n",
      "     OneHotEncoder &     11 &   preprocessing &    7 &            0.73 &      categorical\\_features \\\\\n",
      "      GridSearchCV &     10 & model\\_selection &   10 &            5.30 & estimator, param\\_grid, cv \\\\\n",
      "            DEFINE &    524 &          compat &    9 &            3.00 &       name, default, help \\\\\n",
      "          Variable &    381 &      tensorflow &   12 &            2.44 &             initial\\_value \\\\\n",
      "             Dense &    326 &           keras &   11 &            2.75 &                     units \\\\\n",
      "           Session &    306 &          compat &    3 &            0.54 &                   default \\\\\n",
      "             Input &    233 &           keras &    9 &            1.38 &                     shape \\\\\n",
      "           Dropout &    206 &           keras &    4 &            1.02 &                      rate \\\\\n",
      "             Model &    205 &           keras &    2 &            1.89 &           inputs, outputs \\\\\n",
      "            Conv2D &    193 &           keras &   17 &            4.77 &      filters, kernel\\_size \\\\\n",
      "             Saver &    157 &          compat &   15 &            0.62 &                   default \\\\\n",
      "       ConfigProto &    142 &          compat &   17 &            0.74 &                   default \\\\\n",
      "            Linear &   2370 &              nn &    5 &            2.17 &               in\\_features \\\\\n",
      "            Conv2d &   2295 &              nn &   11 &            4.84 &               in\\_channels \\\\\n",
      "        Sequential &   1841 &              nn &    1 &            0.92 &                     *args \\\\\n",
      "              ReLU &   1521 &              nn &    1 &            0.66 &                   inplace \\\\\n",
      "       BatchNorm2d &    925 &              nn &    7 &            1.30 &              num\\_features \\\\\n",
      "        DataLoader &    886 &           utils &   15 &            4.04 &                   dataset \\\\\n",
      "        ModuleList &    870 &              nn &    1 &            0.45 &                   default \\\\\n",
      "         Parameter &    671 &              nn &    2 &            1.18 &                      data \\\\\n",
      "           Dropout &    570 &              nn &    2 &            1.00 &                         p \\\\\n",
      "  CrossEntropyLoss &    378 &              nn &    6 &            0.39 &                   default \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_library_classes(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                classes.append(class_name)\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options and categories\n",
    "    class_options = []\n",
    "    categories = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "            parts = class_data[\"full_name\"].split(\".\")\n",
    "            if len(parts) <= 2:\n",
    "                category = parts[0]\n",
    "            else:\n",
    "                category = parts[1]\n",
    "            categories.append(category)\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class, library_name)\n",
    "            class_options.append(0)\n",
    "            categories.append(\"Unknown\")\n",
    "            continue\n",
    "\n",
    "    df_classes[\"Category\"] = categories\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if  param not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "                                \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            option_str = \", \".join(options)\n",
    "            most_used_class_option.append(option_str)\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "    \n",
    "    return df_classes\n",
    "\n",
    "\n",
    "df_sklearn_classes = get_library_classes(\"sklearn\", \"modules/sklearn_default_values.json\" , \"statistics/*\")\n",
    "df_sklearn_classes = df_sklearn_classes[:10]\n",
    "df_tf_classes = get_library_classes(\"tensorflow\", \"modules/tensorflow_default_values.json\" , \"statistics/*\")\n",
    "df_tf_classes = df_tf_classes[:10]\n",
    "df_pytorch_classes = get_library_classes(\"torch\", \"modules/torch_default_values.json\" , \"statistics/*\")\n",
    "df_pytorch_classes = df_pytorch_classes[:10]\n",
    "df_all_classes = pd.concat([df_sklearn_classes, df_tf_classes, df_pytorch_classes])\n",
    "\n",
    "df_to_latex(df=df_all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrl}\n",
      "\\toprule\n",
      "             Method &  Count &  \\#Args &  AvgArgsUsed &          Most Used Args \\\\\n",
      "\\midrule\n",
      "   train\\_test\\_split &    143 &      6 &         3.68 &               *arrays\\_0 \\\\\n",
      "   confusion\\_matrix &     68 &      5 &         2.10 &          y\\_true, y\\_pred \\\\\n",
      "           f1\\_score &     68 &      7 &         2.79 &          y\\_true, y\\_pred \\\\\n",
      " mean\\_squared\\_error &     65 &      5 &         2.51 &          y\\_true, y\\_pred \\\\\n",
      " load\\_breast\\_cancer &     43 &      2 &         0.51 &                 default \\\\\n",
      "      roc\\_auc\\_score &     41 &      7 &         2.22 &         y\\_true, y\\_score \\\\\n",
      "        check\\_array &     41 &     13 &         4.66 & array, force\\_all\\_finite \\\\\n",
      "mean\\_absolute\\_error &     39 &      4 &         2.05 &          y\\_true, y\\_pred \\\\\n",
      "          load\\_iris &     28 &      2 &         1.21 &              return\\_X\\_y \\\\\n",
      "     accuracy\\_score &     26 &      4 &         2.04 &          y\\_true, y\\_pred \\\\\n",
      "            reshape &   1467 &      3 &         2.01 &           tensor, shape \\\\\n",
      "     variable\\_scope &   1291 &     13 &         1.30 &           name\\_or\\_scope \\\\\n",
      "               cast &   1025 &      3 &         2.00 &                x, dtype \\\\\n",
      "         reduce\\_sum &    730 &      4 &         1.77 &            input\\_tensor \\\\\n",
      "             concat &    687 &      3 &         2.03 &                  values \\\\\n",
      "              shape &    673 &      3 &         1.00 &                   input \\\\\n",
      "        placeholder &    667 &      3 &         2.36 &                   dtype \\\\\n",
      "           constant &    614 &      4 &         1.75 &                   value \\\\\n",
      "        expand\\_dims &    557 &      3 &         2.00 &                   input \\\\\n",
      "          transpose &    491 &      4 &         1.82 &                       a \\\\\n",
      "                cat &   3060 &      3 &         1.82 &                 tensors \\\\\n",
      "             tensor &   1507 &      5 &         1.70 &                    data \\\\\n",
      "         from\\_numpy &   1436 &      1 &         1.00 &                 ndarray \\\\\n",
      "              zeros &   1413 &      6 &         2.11 &                   *size \\\\\n",
      "                sum &   1053 &      2 &         1.55 &                   input \\\\\n",
      "            no\\_grad &   1015 &      0 &         0.00 &                 default \\\\\n",
      "               load &    893 &      4 &         1.29 &                       f \\\\\n",
      "               save &    764 &      5 &         2.00 &                     obj \\\\\n",
      "              stack &    745 &      3 &         1.64 &                 tensors \\\\\n",
      "       is\\_available &    719 &      0 &         0.00 &                 default \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_library_methods(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].islower():\n",
    "                                method_name_parts = key.split(\"_\")[:-1]\n",
    "                                method_name = \"_\".join(method_name_parts)\n",
    "                                #for item in library_data:\n",
    "                                #    if item[\"name\"] == method_name:\n",
    "                                methods.append(method_name)\n",
    "\n",
    "    method_data = Counter(methods)\n",
    "    df_methods = pd.DataFrame.from_dict(method_data, orient=\"index\").reset_index()\n",
    "    df_methods = df_methods.rename(columns={'index':'Method', 0:'Count'})\n",
    "    df_methods = df_methods.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Compute number of args that can be set regarding the API data\n",
    "    method_options = []\n",
    "    methods = df_methods[\"Method\"].to_list()\n",
    "\n",
    "    for method in methods:\n",
    "        try:\n",
    "            method_data = next(filter(lambda x: x[\"name\"] == method, library_data))\n",
    "            method_options.append(len(method_data[\"params\"]))\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", method, library_name)\n",
    "            method_options.append(0)\n",
    "            continue\n",
    "\n",
    "    df_methods[\"#Args\"] = method_options\n",
    "\n",
    "    # Compute average number of args used per method\n",
    "    avg_method_args = []\n",
    "    most_used_method_args = []\n",
    "\n",
    "    for method in methods:\n",
    "        avg_method_args_used = []\n",
    "        method_args = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].islower():\n",
    "                                    method_name_parts = module_name.split(\"_\")[:-1]\n",
    "                                    method_name = \"_\".join(method_name_parts)\n",
    "                                    if method == method_name:\n",
    "                                        counter = 0\n",
    "                                        for arg in data.keys():\n",
    "                                            if arg == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if arg not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if arg == \"params\":\n",
    "                                                method_args.append(\"default\")\n",
    "                                            else:\n",
    "                                                method_args.append(arg)\n",
    "                                        \n",
    "                                        avg_method_args_used.append(counter)\n",
    "                                \n",
    "        avg_method_args.append(round((sum(avg_method_args_used) / len(avg_method_args_used)),2))\n",
    "        method_arg_data = Counter(method_args)\n",
    "        try:\n",
    "            most_common_num = method_arg_data.most_common(1)[0][1]\n",
    "            args = [x[0] for x in method_arg_data.most_common() if x[1] == most_common_num]\n",
    "            arg_str = \", \".join(args)\n",
    "            most_used_method_args.append(arg_str)\n",
    "        except IndexError:\n",
    "            most_used_method_args.append(\"None\")\n",
    "\n",
    "    df_methods[\"AvgArgsUsed\"] = avg_method_args\n",
    "    df_methods[\"Most Used Args\"] = most_used_method_args\n",
    "\n",
    "    return df_methods\n",
    "\n",
    "df_sklearn_methods = get_library_methods(\"sklearn\", \"modules/sklearn_default_values.json\", \"statistics/*\")\n",
    "df_sklearn_methods = df_sklearn_methods[:10]\n",
    "df_tf_methods = get_library_methods(\"tensorflow\", \"modules/tensorflow_default_values.json\", \"statistics/*\")\n",
    "df_tf_methods = df_tf_methods[:10]\n",
    "df_torch_methods = get_library_methods(\"torch\", \"modules/torch_default_values.json\", \"statistics/*\")\n",
    "df_torch_methods = df_torch_methods[:10]\n",
    "df_all_methods = pd.concat([df_sklearn_methods, df_tf_methods, df_torch_methods])\n",
    "\n",
    "df_to_latex(df=df_all_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "ML Library &  API Class Count &  Project Class Count \\\\\n",
      "\\midrule\n",
      "   sklearn &              262 &                   81 \\\\\n",
      "tensorflow &             2273 &                  193 \\\\\n",
      "     torch &              384 &                  144 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_api_and_project_classes(library_names: List, library_data_dirs: List, project_dir: str):\n",
    "    api_class_count = []\n",
    "    project_class_count = []\n",
    "\n",
    "    for library_data_dir in library_data_dirs:\n",
    "        class_counter = 0\n",
    "        with open(library_data_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "            library_data = json.load(library_file)\n",
    "\n",
    "        for module in library_data:\n",
    "            if module[\"name\"][0].isupper():\n",
    "                class_counter += 1\n",
    "\n",
    "        api_class_count.append(class_counter)\n",
    "\n",
    "\n",
    "    for library_name in library_names:\n",
    "        class_counter = 0\n",
    "        classes_visited = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for key, value in module_data.items():\n",
    "                                if key[0].isupper():\n",
    "                                    class_name = key.split(\"_\")[0]\n",
    "                                    if \"base_class_0\" in value:\n",
    "                                        continue\n",
    "                                    if class_name not in classes_visited:\n",
    "                                        class_counter += 1\n",
    "                                        classes_visited.append(class_name)\n",
    "        \n",
    "        project_class_count.append(class_counter)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"ML Library\"] = library_names\n",
    "    df[\"API Class Count\"] = api_class_count\n",
    "    df[\"Project Class Count\"] = project_class_count\n",
    "    return df     \n",
    "\n",
    "library_data_dirs = [\"modules/sklearn_default_values.json\", \"modules/tensorflow_default_values.json\",  \"modules/torch_default_values.json\"]\n",
    "library_names = [\"sklearn\", \"tensorflow\", \"torch\"]\n",
    "df_class_count = compare_api_and_project_classes(library_names, library_data_dirs, \"statistics/*\")\n",
    "df_class_count.head()\n",
    "df_to_latex(df=df_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "   Library &  Total Count &  Default Count &  Customized Count &  Not API Match Count \\\\\n",
      "\\midrule\n",
      "   sklearn &          625 &            110 &               474 &                   41 \\\\\n",
      "tensorflow &         9695 &            167 &              8293 &                 1235 \\\\\n",
      "     torch &        34653 &           2079 &             32202 &                  372 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_hyperparameter_count(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    total_count = 0\n",
    "    default_count = 0\n",
    "    customized_count = 0\n",
    "    not_match_api_count = 0\n",
    "\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for module_name, module_value in module_data.items():\n",
    "                            if module_name[0].isupper():\n",
    "                                class_name = module_name.split(\"_\")[0]\n",
    "                                if \"base_class_0\" in module_value:\n",
    "                                    continue\n",
    "                                #print(\"Module Name: \", class_name)\n",
    "                                for param, param_value in module_value.items():\n",
    "                                    if param in (\"variable\", \"params\"):\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        total_count += 1\n",
    "\n",
    "                                        api_module = next(filter(lambda x: x[\"name\"] == class_name, library_data))\n",
    "\n",
    "                                        if param in api_module[\"params\"]:\n",
    "                                            api_param_value = api_module[\"params\"].get(param)\n",
    "                                            if api_param_value == param_value[\"value\"]:\n",
    "                                                default_count +=1\n",
    "                                            else:\n",
    "                                                customized_count += 1\n",
    "                                        else:\n",
    "                                            not_match_api_count += 1\n",
    "\n",
    "    #assert total_count == default_count + customized_count + not_match_api_count                                        \n",
    "\n",
    "    data = {\n",
    "        \"Library\": [library_name],\n",
    "        \"Total Count\": [total_count],\n",
    "        \"Default Count\": [default_count],\n",
    "        \"Customized Count\": [customized_count],\n",
    "        \"Not API Match Count\": [not_match_api_count]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_sklearn = get_hyperparameter_count(\"sklearn\", \"modules/sklearn_default_values.json\", \"statistics/*\")\n",
    "df_tf = get_hyperparameter_count(\"tensorflow\", \"modules/tensorflow_default_values.json\", \"statistics/*\")\n",
    "df_torch = get_hyperparameter_count(\"torch\", \"modules/torch_default_values.json\", \"statistics/*\")\n",
    "\n",
    "df_all = pd.concat([df_sklearn, df_tf, df_torch])\n",
    "df_all.head()\n",
    "df_to_latex(df=df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrl}\n",
      "\\toprule\n",
      "                        Class &              Category &  Count &  \\#HP &  AvgOptionsUsed &       Most Used HP \\\\\n",
      "\\midrule\n",
      "           LogisticRegression &          linear\\_model &     40 &   15 &            1.12 &            default \\\\\n",
      "                       KMeans &               cluster &     30 &    9 &            2.30 &         n\\_clusters \\\\\n",
      "                     Pipeline &              pipeline &     21 &    3 &            1.00 &              steps \\\\\n",
      "             LinearRegression &          linear\\_model &     13 &    5 &            0.08 &            default \\\\\n",
      "                          PCA &         decomposition &     12 &    9 &            1.00 &       n\\_components \\\\\n",
      "                          SVC &                   svm &      7 &   15 &            2.00 &             kernel \\\\\n",
      "             NearestNeighbors &             neighbors &      7 &    8 &            1.29 &        n\\_neighbors \\\\\n",
      "       RandomForestClassifier &              ensemble &      6 &   18 &            2.33 &       n\\_estimators \\\\\n",
      "              GaussianMixture &               mixture &      5 &   14 &            3.60 &       n\\_components \\\\\n",
      "        DecisionTreeRegressor &                  tree &      5 &   11 &            0.80 &            default \\\\\n",
      "       DecisionTreeClassifier &                  tree &      4 &   12 &            1.25 &          criterion \\\\\n",
      "         KNeighborsClassifier &             neighbors &      4 &    8 &            1.50 &        n\\_neighbors \\\\\n",
      "                 SGDRegressor &          linear\\_model &      4 &   19 &            0.75 &           **kwargs \\\\\n",
      "              RANSACRegressor &          linear\\_model &      4 &   13 &            1.00 &     base\\_estimator \\\\\n",
      "    LatentDirichletAllocation &         decomposition &      3 &   16 &            1.00 &       n\\_components \\\\\n",
      "                   GaussianNB &           naive\\_bayes &      3 &    2 &            0.00 &            default \\\\\n",
      "      AgglomerativeClustering &               cluster &      3 &    8 &            2.00 &         n\\_clusters \\\\\n",
      "                    KernelPCA &         decomposition &      3 &   16 &            1.00 &       n\\_components \\\\\n",
      "                SGDClassifier &          linear\\_model &      3 &   21 &            2.00 &           **kwargs \\\\\n",
      "                       DBSCAN &               cluster &      3 &    8 &            2.00 &                eps \\\\\n",
      "                KernelDensity &             neighbors &      3 &    9 &            2.00 &             kernel \\\\\n",
      "          KNeighborsRegressor &             neighbors &      3 &    8 &            0.67 &            default \\\\\n",
      "               HuberRegressor &          linear\\_model &      2 &    6 &            1.00 &            epsilon \\\\\n",
      "                  BernoulliNB &           naive\\_bayes &      2 &    4 &            1.00 &            default \\\\\n",
      "                        Ridge &          linear\\_model &      2 &    9 &            1.00 &              alpha \\\\\n",
      "        RandomForestRegressor &              ensemble &      2 &   17 &            1.50 &       n\\_estimators \\\\\n",
      "                       KDTree &             neighbors &      2 &    4 &            1.50 &                  X \\\\\n",
      "                 TruncatedSVD &         decomposition &      2 &    7 &            2.50 &       n\\_components \\\\\n",
      "QuadraticDiscriminantAnalysis & discriminant\\_analysis &      2 &    4 &            0.50 &            default \\\\\n",
      "           IsotonicRegression &              isotonic &      2 &    4 &            1.00 &      out\\_of\\_bounds \\\\\n",
      "                      FastICA &         decomposition &      2 &    9 &            0.50 &            default \\\\\n",
      "   LinearDiscriminantAnalysis & discriminant\\_analysis &      2 &    7 &            0.50 &            default \\\\\n",
      "                        Lasso &          linear\\_model &      2 &   11 &            1.00 &              alpha \\\\\n",
      "            ColumnTransformer &               compose &      2 &    7 &            1.50 &       transformers \\\\\n",
      "              IsolationForest &              ensemble &      2 &    9 &            0.50 &            default \\\\\n",
      "         MultiOutputRegressor &           multioutput &      1 &    2 &            1.00 &          estimator \\\\\n",
      "                    LinearSVC &                   svm &      1 &   12 &            1.00 &                  C \\\\\n",
      "                   Perceptron &          linear\\_model &      1 &   16 &            1.00 &           **kwargs \\\\\n",
      "          OneVsRestClassifier &            multiclass &      1 &    3 &            1.00 &          estimator \\\\\n",
      "   TransformedTargetRegressor &               compose &      1 &    5 &            3.00 &          regressor \\\\\n",
      "               SGDOneClassSVM &          linear\\_model &      1 &   12 &            1.00 &           **kwargs \\\\\n",
      "    GradientBoostingRegressor &              ensemble &      1 &   21 &            3.00 &               loss \\\\\n",
      "               RegressorChain &           multioutput &      1 &    4 &            1.00 &     base\\_estimator \\\\\n",
      "      BayesianGaussianMixture &               mixture &      1 &   17 &            8.00 &       n\\_components \\\\\n",
      "                MultinomialNB &           naive\\_bayes &      1 &    3 &            1.00 &          fit\\_prior \\\\\n",
      "                 ComplementNB &           naive\\_bayes &      1 &    4 &            1.00 &          fit\\_prior \\\\\n",
      "                 MLPRegressor &        neural\\_network &      1 &   23 &            6.00 & hidden\\_layer\\_sizes \\\\\n",
      "                          SVR &                   svm &      1 &   11 &            1.00 &             kernel \\\\\n",
      "                    MeanShift &               cluster &      1 &    7 &            3.00 &          bandwidth \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_algorithms(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, value in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                for item in library_data:\n",
    "                                    if item[\"name\"] == class_name:\n",
    "                                        classes.append(class_name)\n",
    "\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options and category\n",
    "    categories = []\n",
    "    class_options = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "            category = class_data[\"full_name\"].split(\".\")[1]\n",
    "            categories.append(category)\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class)\n",
    "            continue\n",
    "            #raise StopIteration()\n",
    "\n",
    "    df_classes[\"Category\"] = categories\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if param not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "                                \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            #most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            #options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            #option_str = \", \".join(options)\n",
    "            most_used_class_option.append(class_option_data.most_common(1)[0][0])\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "\n",
    "    return df_classes\n",
    "\n",
    "\n",
    "\n",
    "df_sklearn_ml_algo = get_algorithms(\"sklearn\", \"modules/sklearn_ml_algorithms.json\", \"statistics/*\")\n",
    "#df_sklearn_ml_algo = df_sklearn_ml_algo[:30]\n",
    "\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo[[\"Class\", \"Category\", \"Count\", \"#HP\", \"AvgOptionsUsed\", \"Most Used HP\"]]\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "df_to_latex(df=df_sklearn_ml_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrll}\n",
      "\\toprule\n",
      "                        Class &              Category &  Count &  \\#HP &  AvgOptionsUsed &       Most Used HP & Common Value Types \\\\\n",
      "\\midrule\n",
      "           LogisticRegression &          linear\\_model &     40 &   15 &            1.12 &            default &                  - \\\\\n",
      "                       KMeans &               cluster &     30 &    9 &            2.30 &         n\\_clusters &               Call \\\\\n",
      "                     Pipeline &              pipeline &     21 &    3 &            1.00 &              steps &               List \\\\\n",
      "             LinearRegression &          linear\\_model &     13 &    5 &            0.08 &            default &                  - \\\\\n",
      "                          PCA &         decomposition &     12 &    9 &            1.00 &       n\\_components &           Constant \\\\\n",
      "                          SVC &                   svm &      7 &   15 &            2.00 &             kernel &           Constant \\\\\n",
      "             NearestNeighbors &             neighbors &      7 &    8 &            1.29 &        n\\_neighbors &               Call \\\\\n",
      "       RandomForestClassifier &              ensemble &      6 &   18 &            2.33 &       n\\_estimators &           Constant \\\\\n",
      "              GaussianMixture &               mixture &      5 &   14 &            3.60 &       n\\_components &               Call \\\\\n",
      "        DecisionTreeRegressor &                  tree &      5 &   11 &            0.80 &            default &                  - \\\\\n",
      "       DecisionTreeClassifier &                  tree &      4 &   12 &            1.25 &          criterion &           Constant \\\\\n",
      "         KNeighborsClassifier &             neighbors &      4 &    8 &            1.50 &        n\\_neighbors &           Constant \\\\\n",
      "                 SGDRegressor &          linear\\_model &      4 &   19 &            0.75 &           **kwargs &               None \\\\\n",
      "              RANSACRegressor &          linear\\_model &      4 &   13 &            1.00 &     base\\_estimator &          Attribute \\\\\n",
      "    LatentDirichletAllocation &         decomposition &      3 &   16 &            1.00 &       n\\_components &           Constant \\\\\n",
      "                   GaussianNB &           naive\\_bayes &      3 &    2 &            0.00 &            default &                  - \\\\\n",
      "      AgglomerativeClustering &               cluster &      3 &    8 &            2.00 &         n\\_clusters &              BinOp \\\\\n",
      "                    KernelPCA &         decomposition &      3 &   16 &            1.00 &       n\\_components &           Constant \\\\\n",
      "                SGDClassifier &          linear\\_model &      3 &   21 &            2.00 &           **kwargs &               None \\\\\n",
      "                       DBSCAN &               cluster &      3 &    8 &            2.00 &                eps &          Attribute \\\\\n",
      "                KernelDensity &             neighbors &      3 &    9 &            2.00 &             kernel &           Constant \\\\\n",
      "          KNeighborsRegressor &             neighbors &      3 &    8 &            0.67 &            default &                  - \\\\\n",
      "               HuberRegressor &          linear\\_model &      2 &    6 &            1.00 &            epsilon &           Constant \\\\\n",
      "                  BernoulliNB &           naive\\_bayes &      2 &    4 &            1.00 &            default &                  - \\\\\n",
      "                        Ridge &          linear\\_model &      2 &    9 &            1.00 &              alpha &               Name \\\\\n",
      "        RandomForestRegressor &              ensemble &      2 &   17 &            1.50 &       n\\_estimators &           Constant \\\\\n",
      "                       KDTree &             neighbors &      2 &    4 &            1.50 &                  X &               Call \\\\\n",
      "                 TruncatedSVD &         decomposition &      2 &    7 &            2.50 &       n\\_components &               Call \\\\\n",
      "QuadraticDiscriminantAnalysis & discriminant\\_analysis &      2 &    4 &            0.50 &            default &                  - \\\\\n",
      "           IsotonicRegression &              isotonic &      2 &    4 &            1.00 &      out\\_of\\_bounds &           Constant \\\\\n",
      "                      FastICA &         decomposition &      2 &    9 &            0.50 &            default &                  - \\\\\n",
      "   LinearDiscriminantAnalysis & discriminant\\_analysis &      2 &    7 &            0.50 &            default &                  - \\\\\n",
      "                        Lasso &          linear\\_model &      2 &   11 &            1.00 &              alpha &               Name \\\\\n",
      "            ColumnTransformer &               compose &      2 &    7 &            1.50 &       transformers &               List \\\\\n",
      "              IsolationForest &              ensemble &      2 &    9 &            0.50 &            default &                  - \\\\\n",
      "         MultiOutputRegressor &           multioutput &      1 &    2 &            1.00 &          estimator &               Call \\\\\n",
      "                    LinearSVC &                   svm &      1 &   12 &            1.00 &                  C &           Constant \\\\\n",
      "                   Perceptron &          linear\\_model &      1 &   16 &            1.00 &           **kwargs &               None \\\\\n",
      "          OneVsRestClassifier &            multiclass &      1 &    3 &            1.00 &          estimator &          Attribute \\\\\n",
      "   TransformedTargetRegressor &               compose &      1 &    5 &            3.00 &          regressor &               Call \\\\\n",
      "               SGDOneClassSVM &          linear\\_model &      1 &   12 &            1.00 &           **kwargs &               None \\\\\n",
      "    GradientBoostingRegressor &              ensemble &      1 &   21 &            3.00 &               loss &           Constant \\\\\n",
      "               RegressorChain &           multioutput &      1 &    4 &            1.00 &     base\\_estimator &               Call \\\\\n",
      "      BayesianGaussianMixture &               mixture &      1 &   17 &            8.00 &       n\\_components &     MethodArgument \\\\\n",
      "                MultinomialNB &           naive\\_bayes &      1 &    3 &            1.00 &          fit\\_prior &           Constant \\\\\n",
      "                 ComplementNB &           naive\\_bayes &      1 &    4 &            1.00 &          fit\\_prior &           Constant \\\\\n",
      "                 MLPRegressor &        neural\\_network &      1 &   23 &            6.00 & hidden\\_layer\\_sizes &           Constant \\\\\n",
      "                          SVR &                   svm &      1 &   11 &            1.00 &             kernel &           Constant \\\\\n",
      "                    MeanShift &               cluster &      1 &    7 &            3.00 &          bandwidth &     MethodArgument \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_value_types(df, library_name, project_dir) -> pd.DataFrame:\n",
    "    value_type = []\n",
    "\n",
    "    for _, row in df_sklearn_ml_algo.iterrows():\n",
    "        class_value_types = []\n",
    "\n",
    "        df_class_name = row[\"Class\"]\n",
    "        df_class_option_name = row[\"Most Used HP\"].split(\",\")[0]\n",
    "\n",
    "        for project in glob.glob(project_dir):\n",
    "                with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                    project_data = json.load(project_file)\n",
    "\n",
    "                    for file in project_data.keys():\n",
    "                        file_data = project_data[file]\n",
    "                        for library in file_data.keys():\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if library == library_name:\n",
    "                                    module_data = file_data[library]\n",
    "                                    for module_name, data in module_data.items():\n",
    "                                        if module_name[0].isupper():\n",
    "                                            name = module_name.split(\"_\")[0]\n",
    "                                            if df_class_name == name:\n",
    "                                                for key, value in data.items():\n",
    "                                                    if key == df_class_option_name:\n",
    "                                                        #print(df_class_name, df_class_option_name, key, value)\n",
    "                                                        possible_values = value[\"possible_values\"]\n",
    "                                                        if possible_values:\n",
    "                                                            for x in possible_values:\n",
    "                                                                class_value_types.append(x[1])\n",
    "                                                        else:\n",
    "                                                            class_value_types.append(value[\"type\"])\n",
    "\n",
    "        value_type_data = Counter(class_value_types)    \n",
    "        top_types = value_type_data.most_common(1)\n",
    "        try:\n",
    "            if top_types:\n",
    "                types = top_types[0][0]\n",
    "                #types = [str(x[0][0]) for x in top_types]\n",
    "                #types = \", \".join(types)\n",
    "            else:\n",
    "                types = \"-\"\n",
    "        except Exception:\n",
    "            print(row[\"Class\"], row[\"Most Used HP\"], value_type_data.most_common(3))\n",
    "            print(top_types)\n",
    "            types = \"None\"\n",
    "\n",
    "        value_type.append(types)                                                   \n",
    "    \n",
    "    df[\"Common Value Types\"] = value_type\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_value_types(df_sklearn_ml_algo, \"sklearn\", \"statistics/*\")\n",
    "df_to_latex(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrl}\n",
      "\\toprule\n",
      "              Class &             Category &  Count &  \\#HP &  AvgOptionsUsed &         Most Used HP \\\\\n",
      "\\midrule\n",
      "     StandardScaler &        preprocessing &     44 &    3 &            0.20 &              default \\\\\n",
      "       MinMaxScaler &        preprocessing &     25 &    3 &            0.52 &        feature\\_range \\\\\n",
      "       LabelEncoder &        preprocessing &     17 &    0 &            0.00 &              default \\\\\n",
      "      OneHotEncoder &        preprocessing &     11 &    7 &            0.73 & categorical\\_features \\\\\n",
      "MultiLabelBinarizer &        preprocessing &     10 &    2 &            1.00 &        sparse\\_output \\\\\n",
      "    CountVectorizer &   feature\\_extraction &      9 &   17 &            7.89 &         max\\_features \\\\\n",
      "               TSNE &             manifold &      9 &   16 &            1.78 &         n\\_components \\\\\n",
      "                RFE &    feature\\_selection &      8 &    5 &            1.00 &            estimator \\\\\n",
      "    TfidfVectorizer &   feature\\_extraction &      6 &   21 &            4.83 &               min\\_df \\\\\n",
      "      SimpleImputer &               impute &      5 &    6 &            1.00 &             strategy \\\\\n",
      "FunctionTransformer &        preprocessing &      5 &    8 &            2.00 &                 func \\\\\n",
      " PolynomialFeatures &        preprocessing &      4 &    4 &            1.00 &               degree \\\\\n",
      "       MaxAbsScaler &        preprocessing &      4 &    1 &            0.00 &              default \\\\\n",
      "   TfidfTransformer &   feature\\_extraction &      3 &    4 &            1.00 &           smooth\\_idf \\\\\n",
      "       RobustScaler &        preprocessing &      2 &    5 &            0.00 &              default \\\\\n",
      "   IterativeImputer &               impute &      1 &   14 &            2.00 &   n\\_nearest\\_features \\\\\n",
      "   KBinsDiscretizer &        preprocessing &      1 &    6 &            1.00 &             **kwargs \\\\\n",
      "         Normalizer &        preprocessing &      1 &    2 &            0.00 &              default \\\\\n",
      "         RBFSampler & kernel\\_approximation &      1 &    3 &            2.00 &                gamma \\\\\n",
      "     LabelBinarizer &        preprocessing &      1 &    3 &            0.00 &              default \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sklearn_exp_settings = get_algorithms(\"sklearn\", \"modules/sklearn_experimental_settings.json\", \"statistics/*\")\n",
    "#df_sklearn_ml_algo = df_sklearn_ml_algo[:30]\n",
    "\n",
    "df_sklearn_exp_settings = df_sklearn_exp_settings[[\"Class\", \"Category\", \"Count\", \"#HP\", \"AvgOptionsUsed\", \"Most Used HP\"]]\n",
    "df_sklearn_exp_settings = df_sklearn_exp_settings.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "#df_sklearn_exp_settings = get_value_types(df_sklearn_exp_settings, \"sklearn\", \"statistics/*\")\n",
    "\n",
    "df_to_latex(df=df_sklearn_exp_settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "baf69434822515d6eb5706280ed4fdf1abcb6899576a97bde367cb051faeb565"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
