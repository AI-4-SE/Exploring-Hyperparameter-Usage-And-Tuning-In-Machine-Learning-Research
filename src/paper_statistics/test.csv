,repo_url,tar_filename,title,decorators,imports,arxiv_categories,file location 1,file location 2,methods,paper_sections,model parameters,techniques,final values,notes,hyperparameter,Unnamed: 15,Unnamed: 16,Unnamed: 17,Unnamed: 18
1,https://github.com/google-research-datasets/dstc8-schema-guided-dialogue,google-research-datasets_dstc8-schema-guided-dialogue.tar.gz,Schema-Guided Dialogue State Tracking Task at DSTC8,,copy tensorflow absl collections json sgd_x typing os,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/google-research-datasets_dstc8-schema-guided-dialogue.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\google-research-datasets_dstc8-schema-guided-dialogue.pdf,,,no,,,,no,,,,
4,https://github.com/yhlleo/DWC-GAN,yhlleo_DWC-GAN.tar.gz,Describe What to Change: A Text-guided Unsupervised Image-to-Image Translation Approach,@property,utils PIL copy fasttext yaml sys tools torchfile torchvision collections torch os data_ios itertools codecs networks pickle argparse tensorboardX numpy time shutil solver data_loader math vocab gmm random,cs.CV cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yhlleo_DWC-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yhlleo_DWC-GAN.pdf,,,no,,,,no,,,,
12,https://github.com/felixmichels/Adversarial-Attacks-on-CapsNets,felixmichels_Adversarial-Attacks-on-CapsNets.tar.gz,On the Vulnerability of Capsule Networks to Adversarial Attacks,@abstractmethod @doublewrap @lazy_scope_property @functools.wraps(function) @lazy_property @property @lazy_scope_property(only_training=True),sys multiprocessing __main__ util tensorflow collections json os tfcaps matplotlib importlib scipy functools itertools sklearn numpy time abc attacks subprocess models inspect,cs.LG cs.CR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/felixmichels_Adversarial-Attacks-on-CapsNets.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\felixmichels_Adversarial-Attacks-on-CapsNets.pdf,,,no,,,,no,,,,
16,https://github.com/tiskw/Linear-Image2Image-Transformation,tiskw_Linear-Image2Image-Transformation.tar.gz,The Surprising Effectiveness of Linear Unsupervised Image-to-Image Translation,,glob cv2 skimage linear_image2image_translation torch os numpy docopt time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tiskw_Linear-Image2Image-Transformation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tiskw_Linear-Image2Image-Transformation.pdf,,,no,,,,no,,,,
17,https://github.com/niteshroyal/DreaML,niteshroyal_DreaML.tar.gz,Under consideration for publication in Theory and Practice of Logic Programming,,"copy subprocess, time YapPrologInterface copy, math distutils re Experiment1 core Cython Settings ScoreFinderProbabilistic collections yapWrapper os scipy sys, pickle itertools, copy TranslateToDC math, ast sklearn numpy subprocess, pickle subprocess, pickle, os ast, re math synthInterface time, math, sys Predicate TypesModesAggregationLanguage setuptools FeatureSpace logging statistics statistics, time DCLearner logging, time logging, math py_dreaml_interface",cs.AI cs.LG cs.LO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/niteshroyal_DreaML.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\niteshroyal_DreaML.pdf,,,no,,,,no,,,,
18,https://github.com/uber-research/intrinsic-dimension,uber-research_intrinsic-dimension.tar.gz,MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES,"@ops.RegisterGradient(""FastWalshHadamard"") @property @ops.RegisterGradient(""ZeroOut"") @ops.RegisterGradient(""ZeroOutFloat"")","model_builders sys fwh pdb skimage keras_ext general gym os, sys keras re tensorflow collections h5py rproj_layers data engine_topology os rl horovod gzip orderedset IPython standard_parser itertools engine_training scipy bisect sklearn argparse numpy time train math rproj_layers_util errno random keras_layers model_builders_rl cPickle colorama",cs.LG cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/uber-research_intrinsic-dimension.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\uber-research_intrinsic-dimension.pdf,,"MEASURING INTRINSIC DIMENSION VIA RANDOM SUBSPACE TRAININGThe above example had a simple enough form that we obtained d int = 10 by calculation. But in general we desire a method to measure or approximate d int for more complicated problems, including problems with data-dependent objective functions, e.g. neural network training. Random subspace optimization provides such a method. Standard optimization, which we will refer to hereafter as the direct method of training, entails evaluating the gradient of a loss with respect to θ (D) and taking steps directly in the space of θ (D) . To train in a random subspace, we instead define θ (D) in the following way:θ (D) = θ (D) 0 + P θ (d)(2)where P is a randomly generated D × d projection matrix 1 and θ (d) is a parameter vector in a generally smaller space R d . θ (D) 0and P are randomly generated and frozen (not trained), so the system has only d degrees of freedom. We initialize θ (d) to a vector of all zeros, so initially θ (D) = θ (D) 0 . This convention serves an important purpose for neural network training: it allows the network to benefit from beginning in a region of parameter space designed by any number of good initialization schemes (Glorot & Bengio, 2010; to be well-conditioned, such that gradient descent via commonly used optimizers will tend to work well. 2 Training proceeds by computing gradients with respect to θ (d) and taking steps in that space. Columns of P are normalized to unit length, so steps of unit length in θ (d) chart out unit length motions of θ (D) . Columns of P may also be orthogonalized if desired, but in our experiments we relied simply on the approximate orthogonality of high dimensional random vectors. By this construction P forms an approximately orthonormal basis for a randomly oriented d dimensional subspace of R D , with the origin of the new coordinate system at θ (D) 0 . Fig. 1 (left and middle) shows an illustration of the related vectors.Consider a few properties of this training approach. If d = D and P is a large identity matrix, we recover exactly the direct optimization problem. If d = D but P is instead a random orthonormal basis for all of R D (just a random rotation matrix), we recover a rotated version of the direct problem. Note that for some ""rotation-invariant"" optimizers, such as SGD and SGD with momentum, rotating the basis will not change the steps taken nor the solution found, but for optimizers with axis-aligned assumptions, such as RMSProp (Tieleman & Hinton, 2012) and Adam (Kingma & Ba, 2014), the path taken through θ (D) space by an optimizer will depend on the rotation chosen. Finally, in the general case where d < D and solutions exist in D, solutions will almost surely (with probability 1) not be found if d is less than the codimension of the solution. On the other hand, when d ≥ D −s, if the solution set is a hyperplane, the solution will almost surely intersect the subspace, but for solution sets of arbitrary topology, intersection is not guaranteed. Nonetheless, by iteratively increasing d, re-running optimization, and checking for solutions, we obtain one estimate of d int . We try this sweep of d for our toy problem laid out in the beginning of this section, measuring (by convention as described in the next section) the positive performance (higher is better) instead of loss. 3 As expected, the solutions are first found at d = 10 (see Fig. 1, right), confirming our intuition that for this problem, d int = 10. MNISTWe begin by analyzing a fully connected (FC) classifier trained on MNIST. We choose a network with layer sizes 784-200-200-10, i.e. a network with two hidden layers of width 200; this results in a total number of parameters D = 199, 210. A series of experiments with gradually increasing subspace dimension d produce monotonically increasing performances, as shown in Fig. 2 (left). By checking the subspace dimension at which performance crosses the 90% mark, we measure this network's intrinsic dimension d int90 at about 750. Some networks are very compressible. A salient initial conclusion is that 750 is quite low. At that subspace dimension, only 750 degrees of freedom (0.4%) are being used and 198,460 (99.6%) unused to obtain 90% of the performance of the direct baseline model. A compelling corollary of this result is a simple, new way of creating and training compressed networks, particularly networks for applications in which the absolute best performance is not critical. To store this network, one need only store a tuple of three items: (i) the random seed to generate the frozen θ  random seed to generate P and (iii) the 750 floating point numbers in θ (d) * . It leads to compression (assuming 32-bit floats) by a factor of 260× from 793kB to only 3.2kB, or 0.4% of the full parameter size. Such compression could be very useful for scenarios where storage or bandwidth are limited, e.g. including neural networks in downloaded mobile apps or on web pages. This compression approach differs from other neural network compression methods in the following aspects. (i) While it has previously been appreciated that large networks waste parameters (Dauphin & Bengio, 2013) and weights contain redundancy (Denil et al., 2013) that can be exploited for posthoc compression (Wen et al., 2016), this paper's method constitutes a much simpler approach to compression, where training happens once, end-to-end, and where any parameterized model is an allowable base model. (ii) Unlike layerwise compression models (Denil et al., 2013;Wen et al., 2016), we operate in the entire parameter space, which could work better or worse, depending on the network. (iii) Compared to methods like that of Louizos et al. (2017), who take a Bayesian perspective and consider redundancy on the level of groups of parameters (input weights to a single neuron) by using group-sparsity-inducing hierarchical priors on the weights, our approach is simpler but not likely to lead to compression as high as the levels they attain. (iv) Our approach only reduces the number of degrees of freedom, not the number of bits required to store each degree of freedom, e.g. as could be accomplished by quantizing weights . Both approaches could be combined. (v) There is a beautiful array of papers on compressing networks such that they also achieve computational savings during the forward pass (Wen et al., 2016;Yang et al., 2015); subspace training does not speed up execution time during inference. (vi) Finally, note the relationships between weight pruning, weight tying, and subspace training: weight pruning is equivalent to finding, post-hoc, a subspace that is orthogonal to certain axes of the full parameter space and that intersects those axes at the origin. Weight tying, e.g. by random hashing of weights into buckets (Chen et al., 2015), is equivalent to subspace training where the subspace is restricted to lie along the equidistant ""diagonals"" between any axes that are tied together.Robustness of intrinsic dimension. Next, we investigate how intrinsic dimension varies across FC networks with a varying number of layers and varying layer width. 4 We perform a grid sweep of networks with number of hidden layers L chosen from {1, 2, 3, 4, 5} and width W chosen from {50, 100, 200, 400}. Fig. S6 in the Supplementary Information shows performance vs. subspace dimension plots in the style of Fig. 2 for all 20 networks, and Fig. 3 shows each network's d int90 plotted against its native dimension D. As one can see, D changes by a factor of 24.1 between the smallest and largest networks, but d int90 changes over this range by a factor of only 1.33, with much of this possibly due to noise.Thus it turns out that the intrinsic dimension changes little even as models grown in width or depth! The striking conclusion is that every extra parameter added to the network -every extra dimension added to D -just ends up adding one dimension to the redundancy of the solution, s. Often the most accurate directly trained models for a problem have far more parameters than needed (Zhang et al., 2017); this may be because they are just easier to train, and our observation suggests a reason why: with larger models, solutions have greater redundancy and in a sense ""cover"" more of the space. 5 To our knowledge, this is the first time this phenomenon has been directly measured. We should also be careful not to claim that all FC nets on MNIST will have an intrinsic dimension of around 750; instead, we should just consider that we have found for this architecture/dataset combination a wide plateau of hyperparamter space over which intrinsic dimension is approximately constant.Are random subspaces really more parameter-efficient for FC nets? One might wonder to what extent claiming 750 parameters is meaningful given that performance achieved (90%) is far worse than a state of the art network trained on MNIST. With such a low bar for performance, could a directly trained network with a comparable number of trainable parameters be found that achieves the same performance? We generated 1000 small networks (depth randomly chosen from {1, 2, 3, 4, 5}, layer width randomly from {2, 3, 5, 8, 10, 15, 20, 25}, seed set randomly) in an attempt to find high-performing, small FC networks, but as Fig. 4 (left) shows, a gap still exists between the subspace dimension and the smallest direct FC network giving the same performance at most levels of performance.Measuring d int90 on a convolutional network. Next we measure d int90 of a convolutional network, LeNet (D=44,426). Fig. 2 (right) shows validation accuracy vs. subspace dimension d, and we find d int90 = 290, or a compression rate of about 150× for this network. As with the FC case above, we also do a sweep of random networks, but notice that the performance gap of convnets between direct and subspace training methods becomes closer for fixed budgets, i.e., the number of trainable parameters. Further, the performance of direct training varies significantly, depending on the extrinsic design of convet architectures. We interpret these results in terms of the Minimum Description Length below. CONCLUSIONS AND FUTURE DIRECTIONSIn this paper, we have defined the intrinsic dimension of objective landscapes and shown a simple method -random subspace training -of approximating it for neural network modeling problems. We use this approach to compare problem difficulty within and across domains. We find in some cases the intrinsic dimension is much lower than the direct parameter dimension, and hence enable network compression, and in other cases the intrinsic dimension is similar to that of the best tuned models, and suggesting those models are better suited to the problem.Further work could also identify better ways of creating subspaces for reparameterization: here we chose random linear subspaces, but one might carefully construct other linear or non-linear subspaces to be even more likely to contain solutions. Finally, as the field departs from single stackof-layers image classification models toward larger and more heterogeneous networks Kaiser et al., 2017)  In the main paper, we attempted to find d int90 across 20 FC networks with various depths and widths. A grid sweep of number of hidden layers from {1,2,3,4,5} and width of each hidden layer from {50,100,200,400} is performed, and all 20 plots are shown in Fig. S6. For each d we take 3 runs and plot the mean and variance with blue dots and blue error bars. d int90 is indicated in plots (darkened blue dots) by the dimension at which the median of the 3 runs passes 90% performance threshold. The variance of d int90 is estimated using 50 bootstrap samples. Note that the variance of both accuracy and measured d int90 for a given hyper-parameter setting are generally small, and the mean of performance monotonically increases (very similar to the single-run result) as d increases.This illustrates that the difference between lucky vs. unlucky random projections have little impact on the quality of solutions, while the subspace dimensionality has a great impact. We hypothesize that the variance due to different P matrices will be smaller than the variance due to different random initial parameter vectors θ (D) 0because there are dD i.i.d. samples used to create P (at least in the dense case) but only D samples used to create θ (D) 0 , and aspects of the network depending on smaller numbers of random samples will exhibit greater variance. Hence, in some other experiments we rely on single runs to estimate the intrinsic dimension, though slightly more accurate estimates could be obtained via multiple runs.In similar manner to the above, in Fig. S7 we show the relationship between d int90 and D across 20 networks but using a per-model, directly trained baseline. Most baselines are slightly below 100% accuracy. This is in contrast to Fig. 3, which used a simpler global baseline of 100% across all models. Results are qualitatively similar but with slightly lower intrinsic dimension due to slightly lower thresholds. FigureFigure S6: A sweep of FC networks on MNIST. Each column contains networks of the same depth, and each row those of the same number of hidden nodes in each of its layers. Mean and variance at each d is shown by blue dots and blue bars. d int90 is found by dark blue dots, and the variance of it is indicated by red bars spanning in the d axis.",no,,,,no,,,,
19,https://github.com/ContinualAI/avalanche,ContinualAI_avalanche.tar.gz,Avalanche: an End-to-End Library for Continual Learning,@classmethod @abstractmethod @torch.no_grad() @active.setter @dataclass @property @staticmethod @buffer.setter @runtime_checkable,gc PIL copy csv warnings sys weakref glob check_vision_benchmark tqdm psutil __future__ pprint torchvision gym threading pathlib EndlessCLSimDataset re datetime collections json unittest tests enum dataclasses fnmatch torch os AvalancheDataset matplotlib functools itertools quadprog codecs bisect urllib pytorchcv typing pickle sklearn argparse GPUtil numpy time abc shutil math tarfile avalanche pycocotools typing_extensions traceback setuptools random logging operator wandb gdown zipfile,cs.LG cs.AI cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ContinualAI_avalanche.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ContinualAI_avalanche.pdf,,"Related WorksReproducibility is one of the main principles upon which Avalanche is based. Experiments in the continual learning field are often challenging to reproduce, due to the different implementations of protocols, benchmarks and strategies by different authors. This issue of insufficient reproducibility is not limited to continual learning. The whole artificial intelligence community is affected; a number of authors have recently discussed some possible solutions to the problem [16,42,44].The advent of machine (and deep) learning libraries, mainly TensorFlow [1] and PyTorch [39] has partially mitigated the reproducibility problem. Using these libraries assures a standard implementation of many machine learning building blocks, reducing ambiguities due to bespoke and different implementations of basic concepts.In recent times, the continual learning community has put a lot of effort into addressing these problems, by providing code and libraries aimed to increase the reproducibility of continual learning experiments [9,20,36,50,53,54]. On the one hand, these first attempts lack the generality and the consistency of Avalanche, especially regarding the creation of different and complex benchmarks, and the continual support of a large community. On the other hand, they demonstrate, however, the growing interest of the entire community towards these issues.Another area other than continual learning which has recently seen a proliferation of libraries and tools similar in spirit to Avalanche is reinforcement learning (RL). One of the most popular such benchmark RL libraries is OpenAI Gym [4], within which a multitude of different RL environments is available. A similar library is ViZDoom [56], in which an agent plays the famous computer game Doom. Other relevant projects in the field of reinforcement learning are Dopamine [5], which focuses on simplicity and easy prototyping, and project Malmo [24], which is based on the famous Minecraft game.Many of these libraries, however, only focus on the agent's interaction with the environment (which, in the continual learning domain, can be translated into the definition of benchmarks and scenarios), providing none or just a few base strategies or baselines. In the reinforcement learning field, this problem is addressed by other libraries that include standard implementations of baselines algorithms, such as OpenAI baselines [10] and stable baselines [17].Another prominent example of a collection of baseline training strategies and pretrained models is the natural language processing transformers library by Hugging Face [55]. Many basic concepts upon which Avalanche is based (e.g. plugins, loggers, benchmarks) can also be found in more general machine learning libraries such as PyTorch Lighting [12] and fastai [19]. Indeed, Avalanche is based on the same comprehensiveness and consistency principle, hence not only benchmarks but also strategies and metrics are included. This promotes consistency across the different parts of the library and simplifies the interaction between different modules. Moreover, Avalanche could be integrated with the mentioned deep learning libraries, thanks to the similarity regarding design principles and code structure.Another important problem in research is the bookkeeping of experiments. Having a tool that keeps track of any single run (hyper-parameters, model used, algorithm used, variants, inputs to the model, etc.) is crucial for reproducibility, especially when strategies such as grid search are applied. As discussed in Sec. 4.4, Avalanche already implements a fine-grained and punctual logging, which allows to visualize and save the results of different experiments. Moreover, Avalanche could be easily integrated with standalone libraries specifically developed for experiments bookkeeping and visualization, such as Sacred [27] or Weights and Biases (wandb) [3].",no,,,library is presented,no,,,,
20,https://github.com/nearai/program_synthesis,nearai_program_synthesis.tar.gz,NAPS: Natural Program Synthesis Dataset,"@pytest.fixture @parameterized.expand([ @watchable(""statement"") @cached_property @pytest.mark.parametrize('code', [ @ray.remote @property @staticmethod @watchable(""foreach_block"") @watchable(""if_block"") @watchable(""block"") @watchable('func_block') @contextmanager @watchable(""while_block"") @pytest.mark.parametrize('tokens,linearized', [ @classmethod @unittest.skip('TransposedPackedSequence is missing') @watchable(""ternary_expression"") @watchable(""expression"") @six.add_metaclass(MetaNode)","parameterized copy slicing_joining program_synthesis pandas contextlib warnings sys multiprocessing signal types glob tqdm glob, re, os __future__ pprint cached_property gym re tensorflow unittest collections json torchfold enum data tokenize torch os bleu pytest ray tempfile prompt_toolkit string six struct gzip functools IPython itertools codecs ply dataset pickle argparse pyparsing numbers numpy pylru time shutil math unicodedata sortedcontainers setuptools traceback mock errno random operator subprocess cPickle inspect Levenshtein",cs.LG cs.AI cs.PL stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nearai_program_synthesis.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nearai_program_synthesis.pdf,,,no,,,dataset is presented,no,,,,
24,https://github.com/Papirapi/Few_shot-learning-for-Object-Detection,Papirapi_Few_shot-learning-for-Object-Detection.tar.gz,Few-shot Object Detection via Feature Reweighting,"@trainer.events.start @pytest.mark.parametrize(""node, shape"", [ @pytest.fixture(params=[False, True]) @trainer.events.end_epoch @pytest.mark.skip() @param clear_devices Remove the device directives from the graph for better portability. @pytest.mark.parametrize(""param_space"", [ @test_utility.skipmultigpu @pytest.mark.parametrize(""node, size"", [ @pytest.mark.parametrize(""node, size, raise_error"", [ @pytest.mark.parametrize(""generator_type"", [ @pytest.mark.parametrize(""node, seed"", [ @param output_names Names of the relevant graph outputs. @trainer.events.start_epoch @pytest.mark.parametrize(""a, b"", [ @pytest.mark.skip @comp_set(s) @trainer.events.updated @pytest.mark.skipif(np.lib.NumpyVersion(np.__version__) < '1.13.0', reason='na') @trainer.events.backward @property @pytest.mark.parametrize(""a"", [ @pytest.mark.parametrize(""data_shape"", [ @skiph5py @pytest.mark.parametrize(""a, mode"", [ @return The frozen graph definition. @pytest.mark.parametrize(""node, index, error"", [ @pytest.mark.skip('FIXME') @pytest.mark.parametrize(""a, axis"", [ @pytest.mark.parametrize(""node, x"", [ @trainer.events.forward @pytest.mark.parametrize(""node, error"", [ @comp_set(np.array([[1, 1], [1, 1], [1, 1]])) @classmethod @pytest.mark.parametrize(""action_size, state_size, data_size"", [ @param session The TensorFlow session to be frozen. @pytest.mark.parametrize(""node, x, raise_error"", [ @comp_set(ss) @pytest.mark.parametrize(""node, rois"", [ @param keep_var_names A list of variable names that should not be frozen, @pytest.mark.parametrize(""node, axis"", [ @pytest.mark.parametrize(""node"", [ @pytest.mark.parametrize(""node, x, delta"", [ @test_utility.skipgpu",requests PIL copy warnings multiprocessing sys glob tqdm yolo_v2 onnx __future__ distutils keras tensorflow re Cython json h5py yolov2_train tempfile os pytest matplotlib darkflow itertools test_utility xml dataprepa imp urllib pickle sklearn numpy time postprocessing math setuptools colorsys cv2 random renom subprocess graphviz,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Papirapi_Few_shot-learning-for-Object-Detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Papirapi_Few_shot-learning-for-Object-Detection.pdf,,,no,,,,no,,,,
31,https://github.com/jessychen1016/DPCN,jessychen1016_DPCN.tar.gz,Deep Phase Correlation for End-to-End Heterogeneous Sensor Measurements Matching,,utils log_polar PIL copy unet pandas sys validate __future__ torchvision tensorflow collections imreg_dft doctest data torch os matplotlib dft_test scipy functools itertools tflearn kornia phase_correlation argparse tensorboardX numpy time shutil math cv2 random ndimage pyfftw graphviz,cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jessychen1016_DPCN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jessychen1016_DPCN.pdf,,,no,,,,no,,,,
36,https://github.com/Peilun-Li/SG-GAN,Peilun-Li_SG-GAN.tar.gz,Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption,,utils copy sys multiprocessing glob __future__ pprint tensorflow model collections os module scipy argparse numpy time shutil math ops random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Peilun-Li_SG-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Peilun-Li_SG-GAN.pdf,,"Ablation studiesEffectiveness of soft gradient-sensitive objective. To demonstrate the effectiveness of soft gradient-sensitive loss L grad , we train a variant of SG-GAN without applying L grad and compare it with SG-GAN-25K. Figure 5 shows an example by inspecting details through a 4X zoom. Compared with SG-GAN-25K, the variant without L grad has coarse semantic boundaries and rough textures, which demonstrates soft gradient-sensitive loss can help generate adapted images with clearer semantic boundaries and smoother textures.Effectiveness of semantic-aware discriminator. We use a variant of SG-GAN without applying semantic-aware discriminator (SD) and compare it with SG-GAN-25K to study the effectiveness of SD. As shown in Figure 6, comparing (g) and (h), the variant without SD lacks for details, e.g., the color of traffic light, and generates coarser textures, e.g., the sky. The difference maps, i.e., (b), (c), (d) in Figure 6, further reveal that semantic-aware discriminator leads to personalized texture rendering for each distinct region with specific semantic meaning.The effect of virtual training image size. Figure 4 compares variants of SG-GAN that use distinct numbers of virtual-world images for training. Generally, SG-GAN-25K generates clearer details than SG-GAN-2K for some images. Further A/B tests between them in Table 1 show SG-GAN-25K is slightly better than SG-GAN-2K because of using more training data. Both qualitative and quantita-tive comparisons indicate more data could help, however, the improved performance may be only notable if dataset difference is in orders of magnitude.Discussion. While SG-GAN generates realistic results for almost all tested images, in very rare case the adapted image is unsatisfactory, as shown in Figure 7. Our model learns the existence of sunlight, however it is unaware of the image is taken in a tunnel and thus sunlight would be abnormal. We attribute this rare unsatisfactory case to the lack of diversity of real-world dataset compared with virtual-world dataset, thus such case could be seen as an outlier.More real-world images could help alleviate such unsatisfactory case, but SG-GAN is restricted by the limited number of fine-grained real-world annotations for training, e.g., Cityscapes dataset only contains a fine-grained training set of 2975 images. However, we foresee a possibility to solve the data insufficient issue by using coarse annotations labeled by human or semantic segmentation models. In our implementation of semantic-aware discriminator, semantic masks are actually clustered to avoid sparse classes, e.g., semantic classes ""building"", ""wall"", ""fence"", ""guard rail"", ""bridge"" and ""tunnel"" are clustered into a single mask indicating ""construction"". Considering such cluster, annotation granularity may not be a vital factor for our model. Thus investigating the trade-off between annotation granularity and dataset size would be a possible next step.",no,,,,no,,,,
39,https://github.com/fnzhan/RABIT,fnzhan_RABIT.tar.gz,Bi-level Feature Alignment for Versatile Image Translation and Manipulation,@functools.wraps(old_replicate) @contextlib.contextmanager @property @staticmethod,"PIL copy contextlib warnings sys skimage torchvision util threading sys, os re trainers jactorch collections unittest queue data torch os matplotlib importlib apex functools scipy pickle sklearn argparse numpy topk_ranking time math sync_batchnorm pykeops sinkhorn_solver random cv2 operator options sinkhorn models inspect",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fnzhan_RABIT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fnzhan_RABIT.pdf,,,no,,,,no,,,,
40,https://github.com/RLforlife/VER,RLforlife_VER.tar.gz,Revisiting Prioritized Experience Replay: A Value Perspective,,"os, subprocess, sys copy warnings cloudpickle sys tqdm zlib base64 psutil user_config gym mpi_tools tensorflow serialization_utils collections json torch os string logx textwrap joblib numpy time shutil cv2 subprocess mpi4py",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/RLforlife_VER.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\RLforlife_VER.pdf,Prioritized Experience Replay  Experience Replay,,no,,,,no,,,,
50,https://github.com/andreasbock/enkf_landmarks,andreasbock_enkf_landmarks.tar.gz,Learning landmark geodesics using Kalman ensembles,,utils lddmm csv sys glob pathlib re datetime collections src torch os matplotlib scipy pickle argparse numpy time math pykeops logging,stat.ML cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/andreasbock_enkf_landmarks.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\andreasbock_enkf_landmarks.pdf,,,no,,,,no,,,,
54,https://github.com/weberdc/socmed_sna,weberdc_socmed_sna.tar.gz,"#ArsonEmergency and Australia's ""Black Summer"": Polarisation and Misinformation on Social Media",,copy csv sys __future__ community basic_tweet_corpus_stats re datetime json collections os matplotlib gzip scipy itertools sklearn argparse numpy heapq time math ntpath networkx operator statistics plot_ranked_items,cs.SI cs.SI cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/weberdc_socmed_sna.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\weberdc_socmed_sna.pdf,,,no,,,,no,,,,
60,https://github.com/DanielMagro97/LEXACTUM,DanielMagro97_LEXACTUM.tar.gz,A Comparative Study of Convolutional Neural Networks for the Detection of Strong Gravitational Lensing,,timeit utils re pandas tensorflow sys neural_networks imgaug typing astropy sklearn argparse os numpy matplotlib,astro-ph.IM eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DanielMagro97_LEXACTUM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DanielMagro97_LEXACTUM.pdf,,,no,,,,no,,,,
61,https://github.com/francesco-sovrano/from-philosophy-to-interfaces-an-explanatory-method-and-a-tool-inspired-by-achinstein-s-theory-of-e,francesco-sovrano_from-philosophy-to-interfaces-an-explanatory-method-and-a-tool-inspired-by-achinstein-s-theory-of-e.tar.gz,From Philosophy to Interfaces: an Explanatory Method and a Tool Inspired by Achinstein's Theory of Explanation,"|@get(""/resources/static/<filepath:re:.*\.js>"") @hook('after_request') @get('/overview') @get(""/resources/static/<filepath:re:.*\.json>"") @get('/answer') @route('/<:re:.*>', method='OPTIONS') @get('/explainable_classification') @get(""/resources/static/<filepath:re:.*\.(jpg||png||gif||ico||svg)>"") @get('/sample') @get(""/"") @get(""/favicon.ico"") @get(""/resources/static/<filepath:re:.*\.css>"") @staticmethod @get(""/resources/static/<filepath:re:.*\.(eot||otf||svg||ttf||woff||woff2?)>"") @get(""/documents/<filepath:re:.*\.pdf>"") @get(""/<filepath:re:.*\.html>"") @get('/annotation') @get('/classification')|","bottle pygraphviz concepts pandas server_interface csv warnings sys multiprocessing types bs4 tensorflow_hub aix360 gensim pathlib keras tensorflow re json collections wikipedia transformers pydotplus nltk torch os matplotlib html scipy itertools spacy seaborn codecs bisect distribution_fit_lib pickle more_itertools sklearn numpy time tika tensorflow_text unicodedata math explainable_loan_risk_classifier sched, time networkx pywsd random misc models Levenshtein",cs.AI cs.HC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/francesco-sovrano_from-philosophy-to-interfaces-an-explanatory-method-and-a-tool-inspired-by-achinstein-s-theory-of-e.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\francesco-sovrano_from-philosophy-to-interfaces-an-explanatory-method-and-a-tool-inspired-by-achinstein-s-theory-of-e.pdf,,,no,,,,no,,,,
68,https://github.com/Drenata/EBM-buffer-PCD-Pytorch,Drenata_EBM-buffer-PCD-Pytorch.tar.gz,Implicit Generation and Modeling with Energy-Based Models,,math torch sample_replay_buffer tensorboardX torchvision net,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Drenata_EBM-buffer-PCD-Pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Drenata_EBM-buffer-PCD-Pytorch.pdf,Convolution  Generative Adversarial Network,"CIFAR-10 UnconditionalPixelCNN [Van Oord et al., 2016] 4.60 65.93 PixelIQN [Ostrovski et al., 2018] 5.29 49.46 EBM (single) 6.02 40.58 DCGAN  6.40 37.11 WGAN + GP [Gulrajani et al., 2017] 6.50 36.4 EBM (10 historical ensemble) 6.78 38.2 SNGAN [Miyato et al., 2018] 8   We quantitatively evaluate image quality of EBMs with Inception score  and FID score [Heusel et al., 2017] in Table 4. Overall we obtain significantly better scores than likelihood models PixelCNN and PixelIQN, but worse than SNGAN [Miyato et al., 2018]. We found that in the unconditional case, mode exploration with Langevin took a very long time, so we also experimented in EBM (10 historical ensemble) with sampling joint from the last 10 snapshots of the model. At training time, extensive exploration is ensured with the replay buffer (Figure 3d). Our models have similar number of parameters to SNGAN, but we believe that significantly more parameters may be necessary to generate high fidelity images with mode coverage. On ImageNet128x128, due to computational constraints, we train a smaller network than SNGAN and do not train to convergence. ModelLower Bound Upper Bound EBM + PCD 380.9 482 GAN 50  618.4 636.1 VAE 50  985.0 997.1 NICE [Dinh et al., 2014] 1980 To train EBM models on the continual learning scenario of Split MNIST, we train an EBM following Algorithm 1 in the main body of the paper. Initially, negative sampling is done with labels of the digits 0 and 1. Afterwards, negative sampling is done with labels of the digits 2 and 3 and so forth. Simultaneously, we train EBMs on ground truth image label annotations. We maintain a replay buffer of negative samples to enable effective training of the EBM.",no,,,,no,,,,
69,https://github.com/dnap512/SelfReg,dnap512_SelfReg.tar.gz,Self-Challenging Improves Cross-Domain Generalization,@staticmethod,pathlib PIL copy collections domainbed torch os numpy matplotlib torchvision,cs.CV cs.LG cs.CV cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dnap512_SelfReg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dnap512_SelfReg.pdf,Mixup,,no,,,,no,,,,
70,https://github.com/soply/mpgraph,soply_mpgraph.tar.gz,ADAPTIVE MULTI-PENALTY REGULARIZATION BASED ON A GENERALIZED LASSO PATH,@staticmethod,timeit which getopt scipy random_matrices tabulate sys unittest testcases_graph_operations pdb sklearn the os numpy random_vectors mpgraph,stat.ML cs.LG math.NA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/soply_mpgraph.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\soply_mpgraph.pdf,,,no,,,,no,,,,
74,https://github.com/matthew-ding/primes-project-2020,matthew-ding_primes-project-2020.tar.gz,Relay Protocol for Approximate Byzantine Consensus,,sympy copy Dataset_Generation scipy pandas Other sys bisect networkx random broadcastRelay sklearn trivialRelay os Graph_Generation Broadcast_Relay numpy matplotlib,cs.DC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/matthew-ding_primes-project-2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\matthew-ding_primes-project-2020.pdf,,,no,,,,no,,,,
77,https://github.com/fjxmlzn/BSN,fjxmlzn_BSN.tar.gz,Why Spectral Normalization Stabilizes GANs: Analysis and Improvements,@property @staticmethod,imageio copy csv warnings multiprocessing sys tqdm psutil sn gan_task latent tensorflow gpu_task_scheduler datetime resource load_data network os functools dataset op numpy gan config tarfile math metric cv2 cPickle zipfile,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fjxmlzn_BSN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fjxmlzn_BSN.pdf,Xavier Initialization  Spectral Normalization,,no,,,,no,,,,
82,https://github.com/alacrity2001/PTSA-public-,alacrity2001_PTSA-public-.tar.gz,Precision and Recall for Time Series,@njit @abc.abstractmethod @six.add_metaclass(abc.ABCMeta) @property,pandas warnings multiprocessing sys tqdm io __future__ ctypes keras arch collections statsmodels os hurst plotly matplotlib igraph six matrix_profile scipy functools itertools numba builtins imp sklearn numbers joblib numpy time abc math ptsa version setuptools random pmdarima dis statistics operator cgi inspect tsfresh,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alacrity2001_PTSA-public-.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alacrity2001_PTSA-public-.pdf,,,no,,,,no,,,,
87,https://github.com/hanchenye/scalehls,hanchenye_scalehls.tar.gz,ScaleHLS: Scalable High-Level Synthesis through MLIR,,scalehls mlir argparse torch subprocess numpy shutil io,cs.PL cs.AR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hanchenye_scalehls.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hanchenye_scalehls.pdf,,,no,,,,no,,,,
88,https://github.com/krikamol/DualIV-NeurIPS2020,krikamol_DualIV-NeurIPS2020.tar.gz,Dual Instrumental Variable Regression,@property @staticmethod,copy pandas learning deepiv warnings sys yaml types methods econml pdb __future__ base64 torchvision threading pathlib keras tensorflow datetime json collections unittest h5py torch statsmodels os model_selection matplotlib optimizers rpy2 scipy baselines itertools scenarios urllib pickle sklearn argparse data_generator numpy time getpass cryptography math game_objectives setuptools random theano mpl_toolkits models,stat.ML cs.LG econ.EM,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/krikamol_DualIV-NeurIPS2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\krikamol_DualIV-NeurIPS2020.pdf,,,no,,,,no,,,,
91,https://github.com/dingliu0305/Hybrid-Tensor-Network,dingliu0305_Hybrid-Tensor-Network.tar.gz,Quantum-Classical Machine learning by Hybrid Tensor Networks,@catch_except,myModule scipy itertools TN_FC_MNIST_pytorch_GPU traceback torch os numpy matplotlib time torchvision,cs.LG quant-ph stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dingliu0305_Hybrid-Tensor-Network.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dingliu0305_Hybrid-Tensor-Network.pdf,,,no,,,,no,,,,
103,https://github.com/paninski-lab/deepgraphpose,paninski-lab_deepgraphpose.tar.gz,Deep Graph Pose: a semi-supervised deep graphical model for improved animal pose tracking,"@click.option('-a', '--algo', 'algo', @click.argument('experimenter') @click.argument('video',nargs=-1) @click.argument('videos', nargs=-1, type=click.Path(exists=True, dir_okay=False)) @click.option('-extract', '--extraction_algo', 'extractionalgorithm', @click.option('-mad', '--ma_degree', 'MAdegree', @click.group(invoke_without_command=True) @click.option('-s', '--show', 'showfigures', @click.option('-outlier', '--outlier_algo', 'outlieralgorithm', @main.command(context_settings=CONTEXT_SETTINGS) @click.option('-compare','--comparisonbodyparts','comparisonbodyparts', @click.option('-num', '--num_shuffles', 'shuffle', @click.option('-d', '--delete', 'delete', @click.option('-p', '--p_bound', 'p_bound', @click.option('--copy_videos/--dont_copy_videos', @click.option('-v', '--verbose', is_flag=True, help='Verbose printing') @click.option('-v', '--video_type', 'videotype', @click.option('-num', '--num_shuffles', 'num_shuffles', @click.option('-e', '--epsilon', 'epsilon', @click.option('-d', '--wd', 'working_directory', @click.argument('config') @click.argument('video', nargs=-1) @click.pass_context @click.option('-a', '--alpha', 'alpha', @click.option('-vtype', '--video_type', 'videotype', @click.option('-ard', '--ar_degree', 'ARdegree', @click.argument('project') @contextlib.contextmanager @click.option('-c','--save','save_as_csv', @click.argument('mode') @click.option('--crop', @slim.add_arg_scope @click.option('-p','--plot','plotting', @click.option('-s', '--save_frames', 'save_frames', @click.argument('video')","PIL copy pandas yaml warnings sys multiprocessing contextlib glob math, os subprocess, os skimage tqdm easydict __future__ platform pprint threading pathlib zipfile, urllib tensorflow re argparse, glob, os click webbrowser,subprocess os,sys datetime collections logging, os os, deeplabcut h5py imgaug os, pickle enum ruamel src statsmodels os deepgraphpose matplotlib os,  subprocess, deeplabcut importlib deeplabcut scipy functools itertools os,sys,pydoc,platform pydoc urllib pickle os, pickle, yaml sklearn argparse preprocess numpy time shutil math tarfile setuptools cv2 logging random moviepy subprocess mpl_toolkits wx tensorpack PoseDataLoader os,sys,pydoc io",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/paninski-lab_deepgraphpose.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\paninski-lab_deepgraphpose.pdf,,"ResultsWe applied DGP and DLC 2 to a variety of datasets, including behavioral videos from three different species, in a variety of poses and environments (see Table 1 for a summary). The new model (DGP) consistently outperformed the baseline (DLC). In each example video analyzed here, DLC outputs occasional ""glitch"" frames where tracking of at least one target was lost (e.g., around frame index 100 in the lower right panel); these glitches were much less prevalent in the DGP output. We experimented with running Kalman smoothers and total variation denoisers to post-process the DLC output, but were unable to find any parameter settings that could reliably remove these glitches without oversmoothing the data (results not shown). The frequency of these ""glitches"" can be reduced by increasing the training set through labeling more data -but this is precisely the user effort we aim to minimize here. See the full videos summarizing the performance of the two methods. An example screenshot for the mouse-wheel dataset [30] is shown in Figure 2. The comparison between DLC and DGP on all other datasets can be found in Figures S3-S6 in the appendix. More information regarding experimental setup can be found in Section S4 in the appendix.We also examined the ""confidence maps"" generated by visualizing the output of the neural network n as an image; large values of the confidence map indicated the regions where the network ""believed"" the target was located with high confidence. Comparing the confidence maps output by DLC versus DGP, we see that the latter tended to be more unimodal (see Figure 2, small panels in the middle column). Nonetheless, DGP did occasionally output multi-modal confidence maps (e.g., in frames where the target was occluded), since the ELBO objective function used to train DGP encouraged unimodality but did not impose unimodality as a hard constraint.To better understand the source of the performance gains exhibited by DGP, we also experimented with a model in which the spatial and temporal potentials were turned off (i.e., w s = w t = 0). The resulting graphical model can be factorized over targets j and frames t. We call the resulting model DGP-semi, since the resulting ELBO objective function combines a usual supervised loss (as in DLC) with an unsupervised term that encourages the output of the image potential n to match its Gaussian approximation for each (t, j) pair (i.e., the resulting loss can be considered a semi-supervised hybrid model). Comparing DLC, DGP-semi, and DGP provides a qualitative sense of the relative benefits of the semi-supervised loss and the spatial and temporal cliques (see videos).To develop more quantitative comparisons, we manually labeled 1000 frames in the mouse-wheel dataset 3 . We randomly assigned 55 labeled frames to the training set and used the remaining 945 frames as the test set. Next we randomly subsampled 10%-90% of this training set and retrained the models to quantify the relation between the test errors and the number of labeled frames. Figure 3 shows the test errors averaged over five random subsamples. We see that DGP-semi and DGP outperformed DLC uniformly over the training set fractions (i.e., the number of labeled frames used to train the model) with a significant amount of improvement. DGP further decreased the errors with the extra spatial and temporal constraints. Similar results were obtained using an ✏-insensitive loss that ignored errors below a threshold ✏ (on the order of 5-10 pixels here) below which the ""true"" marker location becomes somewhat subjective (results not shown here).From both qualitative and quantitative analyses, we can tell that although DGP-semi does not enforce any spatial constraints or temporal smoothness, the extra regularization from the unsupervised term in the ELBO encourages the model output to be more unimodal, leading to significantly improved predictions compared to DLC. With the additional temporal and spatial constraints, DGP can further improve the performance.",no,,,,no,,,,
105,https://github.com/lcs2-iiitd/aquavs,lcs2-iiitd_aquavs.tar.gz,Assessing the Quality of the Datasets by Identifying Mislabeled Samples,@property,math scipy tensorflow collections random pickle sklearn torch numpy matplotlib torchvision,cs.LG cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lcs2-iiitd_aquavs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lcs2-iiitd_aquavs.pdf,AutoEncoder,,no,,,,no,,,,
115,https://github.com/johny-c/noge,johny-c_noge.tar.gz,Neural Online Graph Exploration,@eval_ing.capture @loop_ing.capture @ex2.automain @ex2.config @dataclass @ex.automain @property @ex.config @sum_writer.capture @ex.main,copy tabulate pandas mlflow tqdm xlog pprint gym pathlib datetime collections enum tempfile dataclasses torch os matplotlib torch_geometric itertools typing pickle sklearn numpy time osmnx math networkx noge logging sacred,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/johny-c_noge.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\johny-c_noge.pdf,,,no,,yes,"test set ratio, steps, feature range, node history, target normalization, training steps, evaluation episodes, learning rate, mini batch size",no,,,,
125,https://github.com/UKPLab/naacl2019-does-my-rebuttal-matter,UKPLab_naacl2019-does-my-rebuttal-matter.tar.gz,Does My Rebuttal Matter? Insights from a Major NLP Conference,@staticmethod,"pandas sys pythonrouge resources gensim re collections corenlp_preprocess nltk statsmodels sys,numpy os matplotlib string scipy seaborn sklearn reader argparse numpy RebuttalAnalysis math dateutil operator random os, corenlp os, glob, json, re",cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UKPLab_naacl2019-does-my-rebuttal-matter.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UKPLab_naacl2019-does-my-rebuttal-matter.pdf,,,no,,,,no,,,,
127,https://github.com/IntelLabs/distiller,IntelLabs_distiller.tar.gz,Neural Network Distiller: A Python Package For DNN Compression Research,"@rank.setter @pytest.fixture(params=[torch.qint8, torch.quint8], ids=['torch.qint8', 'torch.quint8']) @pytest.fixture(name='train_with_fp_copy', params=[False, True], ids=['fp_copy_off', 'fp_copy_on']) @pytest.mark.filterwarnings('ignore:new_zeros is a legacy constructor and is not supported in the JIT') @output_clipping.setter @weight_clipping.setter @pytest.mark.filterwarnings('ignore:Converting a tensor to a Python boolean might cause the trace to be incorrect') @pytest.fixture(name='optimizer') @pytest.mark.parametrize('what', SUMMARY_CHOICES) @pytest.mark.parametrize( @pytest.fixture(params=[True, False], ids=['denorm_name', 'norm_name']) @pytest.fixture(params=[True, False]) @pytest.mark.filterwarnings('ignore:Detected call of') @torch.no_grad() @pytest.fixture(params=[0.1, None], ids=['ema', 'cma']) @pytest.fixture(name='model') @property @staticmethod @pytest.fixture(params=[False, True], ids=['reduce_off', 'reduce_on']) @pytest.mark.parametrize(""check_loss_components"", [False, True]) @pytest.mark.parametrize('add_softmax', [True, False]) @pytest.fixture(params=[False, True], ids=['perch_off', 'perch_on']) @pytest.mark.filterwarnings('ignore:Iterating over a tensor might cause the trace to be incorrect') @pytest.fixture(params=[False, True], ids=['per_tensor', 'per_channel']) @interact(window_size=(0,50,5), top1=True, macs=True, params=False, reward=True, zoom=(0,df_len,1)) @contextmanager @pytest.fixture(params=[True, False], ids=['affine_on', 'affine_off']) @pytest.fixture(name='bidirectional', params=[False, True], ids=['bidirectional_off', 'bidirectional_on']) @classmethod @pytest.fixture(name='parallel', params=[False, True], ids=['parallel_off', 'parallel_on']) @pytest.mark.parametrize('display_param_nodes', [True, False]) @pytest.fixture(params=[False, True], ids=['no_bias', 'with_bias']) @pytest.fixture(params=[quantization.LinearQuantMode.SYMMETRIC, @pytest.mark.filterwarnings('ignore:Converting a tensor to a Python index might cause the trace to be incorrect') @contextlib.contextmanager @pytest.fixture(params=[False, True], ids=['bias_off', 'bias_on']) @pytest.mark.parametrize('dataset, arch', [('imagenet', 'vgg19'), @pytest.mark.parametrize('arch', @pytest.fixture() @pytest.fixture(params=[False, True], ids=['parallel_off', 'parallel_on']) @pytest.fixture(params=[False, True], ids=['sequential', 'parallel']) @pytest.mark.parametrize('device', [None, 'cpu', 'cuda:0'])","copy csv git warnings multiprocessing types rewards torchvision tensorflow re torch pytest convert seq2seq codecs pickle numbers numpy heapq ipywidgets traceback random spinup transforms inspect utils yaml torchnet parser __future__ pydot gym examples load model xlsxwriter os functools dataset time shutil pycocotools operator subprocess rl_coach os, fnmatch, sys pandas rl_libs coco_eval sys tqdm datetime coco_utils distiller enum tempfile pkg_resources matplotlib common scipy itertools IPython lsb_release typing sklearn neumf setuptools errno PIL tabulate contextlib glob platform ast environment collections json pretrainedmodels data concurrent ptq_lapq bisect builtins argparse math amc_args logging",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IntelLabs_distiller.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IntelLabs_distiller.pdf,,,no,,,,no,,,,
128,https://github.com/greydanus/mnist1d,greydanus_mnist1d.tar.gz,Scaling down Deep Learning,,"requests time, copy scipy random pickle torch os numpy matplotlib",cs.LG cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/greydanus_mnist1d.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\greydanus_mnist1d.pdf,,,no,,yes,"train-test split, template length, padding points, max translation, noise scale, shear scale, shuffle sequence, sequence length, seed",no,,,,
132,https://github.com/hjSim/KOALAnet,hjSim_KOALAnet.tar.gz,KOALAnet: Blind Super-Resolution using Kernel-Oriented Adaptive Local Adjustment,@property,utils math PIL tensorflow datetime ops glob random skimage argparse koalanet os __future__ numpy time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hjSim_KOALAnet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hjSim_KOALAnet.pdf,,,no,,yes,"mini batch size, learning rate, decay, iterations",no,,,,
137,https://github.com/zjysteven/bitslice_sparsity,zjysteven_bitslice_sparsity.tar.gz,Exploring Bit-Slice Sparsity in Deep Neural Networks for Efficient ReRAM-Based Deployment,@wraps(func) @six.add_metaclass(FixMeta) @contextmanager @pytest.fixture @staticmethod @pytest.mark.parametrize(,copy contextlib yaml sys __future__ torchvision util_func collections torch os pytest six functools nics_fix_pt argparse numpy time shutil math vgg setuptools ResNet inspect,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zjysteven_bitslice_sparsity.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zjysteven_bitslice_sparsity.pdf,,,no,,,,no,,,,
139,https://github.com/syitong/randfourier,syitong_randfourier.tar.gz,But How Does It Work in Theory? Linear SVM with Random Features,@property,"itertools tensorflow csv sys dataplot datagen sklearn log rff joblib numpy datagen, dataplot matplotlib time",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/syitong_randfourier.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\syitong_randfourier.pdf,,,no,,,,no,,,,
144,https://github.com/IBM/UQ360,IBM_UQ360.tar.gz,Uncertainty Characteristics Curves: A Systematic Assessment of Prediction Intervals,"@abc.abstractmethod @property @staticmethod @unittest.skip(""too long"") @classmethod @ staticmethod",requests copy pandas csv warnings sys tqdm tensorflow_hub atexit re tensorflow uuid unittest collections json tests botorch autograd tempfile torch os uq360 gpytorch matplotlib ot scipy builtins pickle typing sklearn umap GPy numpy time shutil abc math setuptools traceback logging subprocess inspect zipfile,cs.LG cs.AI stat.ML cs.AI stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IBM_UQ360.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IBM_UQ360.pdf,,"|Synthetic DataThe data used to produce the results in Section 4.1 are generated using a function x sin x. The training data is created by sampling this function and adding a gaussian noise with randomly varying variance, as in URL [2021]. We set the range to be x ∈ [0, 20]. A total of 4000 training samples were used to create the GBR models, and 1000 equidistant, non-noisy samples sweeping the entire range of x were used for evaluation. Additional 1000 held-out set samples were generated for tuning the operating points. The synthetic data and the GBR model predictions are shown in Figure 12. The ground truth in the plot is the function x sin x. A subset of the noisy training samples, the GBR target predictions, and the GBR prediction intervals are shown. The prediction intervals are tuned to contain, on average, 95% of the ground truth.The resulting UCCs are shown in the Figure 5 in Section 4.1 of the main paper. Here, summary metrics for the same experiment are shown in Table 3. These include the AUUCC, the Cost, the Optimum Cost, and the Mean Absolute Error (MAE). While the Cost is calculated at the operating point (OP) determined on the held-out dataset, the Minimum Cost is calculated on the test data themselves, thus representing a minimum achievable cost. The MAE is defined in the Remark in Section 2. All values in Table 3 are consistent with the overall model ranking apparent in the Figure  5. The rather large gap between the actual and minimum cost can be attributed to the fact that the held-out data set contains noise, while the test set does not. This gap disappears when using a held-out set without noise.",no,,yes,"number estimators, max tree depth, subsample fraction, random seed, learning rate, minimum samples per leaf",no,,,,
146,https://github.com/ctuning/ck,ctuning_ck.tar.gz,"The Collective Knowledge project: making ML models more portable and reproducible with open APIs, reusable best practices and MLOps","@click.option('-a', '--api_key', 'api_key', required=False, default='') @click.option('--skip_graph_init', 'skip_graph_init', is_flag=True, default=False) @click.option('--private', is_flag=True) @click.option('-v', '--version', 'version', required=False, default='') @click.option('-h', '--host', 'host', required=False) @click.option('--license', 'license', required=False) @click.option('-wca', '--workflow_cmd_after', 'workflow_cmd_after', required=False, default='') @click.option('--force', 'force', required=False, is_flag=True) @click.option('-di', '--device_id', 'device_id', required=False, default='') @click.option('-pvf', '--python_version_from', required=False, default='') @click.argument('uid', required=True) @click.option('-wid', '--workflow_input_dir', 'workflow_input_dir', required=False, default='') # Input directory (will be cleaned) @click.option('-wod', '--workflow_output_dir', 'workflow_output_dir', required=False, default='') # Output directory (will be cleaned) @click.option('-c', '--cmd', 'cmd', required=False, default='') @click.option('-u', '--username', 'username', required=False) @click.option('--author', 'author', required=False) @click.option('-gc', '--graph_convertor', 'graph_convertor', required=False, default='') @click.option('-p', '--port', 'port', required=False) @click.option('-m', '--mute', 'display', is_flag=True, default=True) @click.option('-t', '--tags', 'tags', required=False, default='') @click.option('--copyright', 'copyright', required=False) @click.option('-sp', '--server_pass', 'server_pass', required=False) @click.option('-v', '--version', 'version', required=False) @click.option('-s', '--add_extra_scripts', 'add_extra_scripts', required=False, default='') @click.option('-rf', '--result_file', 'result_file', required=False, default='') @click.option('-r', '--resume', 'resume', is_flag=True, default=False) @click.option('-to', '--target_os', 'target_os', required=False, default='') @click.argument('cid') @click.option('--permanent', is_flag=True) @click.option('-wcb', '--workflow_cmd_before', 'workflow_cmd_before', required=False, default='') @click.option('-wi', '--workflow_input', 'workflow_input', required=False, default='') # Input source (stream, webcam, etc) @click.option('-e', '--add_extra_meta_from_file', 'add_extra_meta_from_file', required=False, default='') @click.option('-g', '--graphs', 'graphs', required=False, default='') @click.option('-ss', '--skip_stop', 'skip_stop', is_flag=True, default=False) @click.option('-p', '--point', 'point', required=False, default='') @click.option('-s', '--server_url', 'server_url', required=False) @click.option('-dp', '--desc_prepare', 'desc_prepare', required=False, default='') @staticmethod @click.option('-dg', '--desc_graph', 'desc_graph', required=False, default='') @click.option('-ss', '--server_skip_validation', 'server_skip_validation', required=False) @click.option('-w', '--workspaces', 'workspaces', required=False) @click.option('-u', '--username', 'username', required=False, default='') @click.option('-h', '--hostname', 'hostname', required=False, default='') @click.option('-wce', '--workflow_cmd_extra', 'workflow_cmd_extra', required=False, default='') @click.argument('uid') @click.option('-ho', '--host_os', 'host_os', required=False, default='') @click.option('-d', '--desc_prereq', 'desc_prereq', required=False, default='') @click.group() @click.option('-t', '--tags', 'tags', required=False) @click.option('-j', '--json', 'json_string', required=False, default='') @click.option('-w', '--workflow', 'workflow', required=False, default='') @click.option('-su', '--server_user', 'server_user', required=False) @contextmanager @cli.command() @click.option('--author_id', 'author_id', required=False) @click.option('-a', '--all', 'all', required=False, is_flag=True) @click.option('-dr', '--desc_run', 'desc_run', required=False, default='') @click.option('-f', '--force', 'force', required=False, is_flag=True) @click.option('-n', '--name', 'name', required=False) @click.option('-d', '--desc_file', 'desc_file', required=False) @click.option('-t', '--tunnel', 'tunnel', required=False) @click.option('-f', '--filename', 'filename', required=False, default='') @click.argument('uid', required=False) @click.option('--quiet', 'quiet', required=False, is_flag=True) @click.option('-pv', '--python_version', required=False, default='') @click.option('-wr', '--workflow_repo_url', 'workflow_repo_url', required=False, default='') @click.option('-pvt', '--python_version_to', required=False, default='') @click.option('-a', '--api_key', 'api_key', required=False) @click.option('-wc', '--workflow_cmd', 'workflow_cmd', required=False, default='') @click.option('-pl', '--python_localenv', 'python_localenv', is_flag=True, default=True) @click.option('-pp', '--python_path', required=False, default='') @click.option('-et', '--extra_tags', 'extra_tags', required=False, default='') @click.option('--source', 'source', required=False) @click.option('-n', '--name', 'name', required=False, default='') @click.option('--update_meta_and_stop', 'update_meta_and_stop', is_flag=True, default=False)",copy csv filecmp re tensorflow urllib2 StringIO importlib tkinter pickle numpy traceback random inspect requests yaml prof_parser psutil __future__ SocketServer pprint distutils pathlib click unittest RPi fnmatch os BaseHTTPServer hashlib imp ssl time shutil getpass operator subprocess mpl_toolkits cgi gc pandas sys webbrowser datetime tempfile site matplotlib struct scipy itertools cbench shlex sklearn urlparse elasticsearch setuptools errno connectme zipfile io PIL contextlib ck_032630d041b4fd8a Tkinter glob cdatabase base64 platform telnetlib locale stat uuid ck json collections urllib http lxml pyperclip math socketserver,cs.LG cs.SE stat.ML cs.LG cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ctuning_ck.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ctuning_ck.pdf,,,no,,,,no,,,,
152,https://github.com/micahjsmith/ballet-cscw-2021,micahjsmith_ballet-cscw-2021.tar.gz,Enabling collaborative data science development with the Ballet framework,"@click.option('--add-specific-repos/--no-add-specific-repos', 'add_specific', @click.option('--pdb', is_flag=True) @click.group() @click.option('--count-contributors/--no-count-contributors', @postmortem @click.option('--get-interesting/--no-get-interesting', @fy.silent @cli.command() @click.option('--update-stars-etc/--no-update-stars-etc', @click.option('--filter-eligible/--no-filter-eligible', @fy.memoize @fy.post_processing(lambda s: s.replace(',', '')) @fy.post_processing(int) @click.pass_context",requests pandas contextlib urllib3 sys pdb bs4 tqdm pathlib sqlalchemy re click model json collections load_data os functools settings typing sklearn argparse stacklog joblib numpy random funcy models predict github,cs.LG cs.HC cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/micahjsmith_ballet-cscw-2021.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\micahjsmith_ballet-cscw-2021.pdf,,"Testing and end-to-end MLAs part of our framework, we discuss the use of testing in continuous integration to validate contributions to data science pipelines. Other research has also explored the use of continuous integration in machine learning. Renggli et al. [67] investigate practical and statistical considerations arising from testing conditions on overall model accuracy in a continuous integration setting. Specific models and algorithms can be tested [36] and input data can be validated directly [11,42]. Testing can also be tied to reproducibility in ML research [69]. We build on this work by designing and implementing the first system and algorithms to conduct ML testing at the level of individual feature definitions.Feature engineering is just one step in the data science process. Other research has looked at this process from a distance, considering the end-to-end process of delivering a predictive model from some initial specification. DAWNBench [20] provides a standardized benchmarking environment for end-to-end deep learning models, while automated machine learning (AutoML) systems like AutoBazaar, AutoGluon, and commercial offerings from cloud vendors [28,77] can automatically create predictive models for a variety of ML tasks. A survey of techniques used in AutoML like hyperparameter tuning, model selection, and neural architecture search, can be found in [97]. On the other hand, researchers and practitioners are increasingly realizing that AutoML does not solve all problems and that human factors such as design, monitoring, and configuration are still required [14,88,91,94]. In our experiments, we use an AutoML system as one way of evaluating the performance of different feature sets without otherwise incorporating these powerful techniques as part of our framework. ChallengesWhen we set out to apply the pull request model to data science projects, we found the model was not a natural fit, and we discovered key challenges to address, which we describe here. We are embedded in data science work and build on our own experience developing and researching feature engineering pipelines and other data science steps from a machine learning perspective. We also revealed and investigated these challenges in preliminary user studies with prototypes of our framework (Section 6).We synthesize these challenges in the context of the literature on collaborative data work and machine learning workflows. Previous work, not in the context of open-source development, has identified challenges in communication, coordination, observability, and algorithmic aspects (Section 2.2). Then, a classic observation is that the number of possible direct communication channels in a collaborative software project scales quadratically with the number of developers [12]. So at small scales, data science teams may use phone calls or video chats, with 74% communicating synchronously and in person [18]. At larger scales, like those made possible by the open-source development process, communication can take place more effectively through coordination around a shared work product, or through discussion threads and chat rooms.Ultimately, we list four challenges for a collaborative framework to address. This is not an exhaustive list of challenges, but they are the ones we focus on in this work, though we review and discuss additional challenges in Sections 2 and 8. C1 Task management. Working alone, data science developers often write end-to-end scripts that prepare the data, extract features, build and train model models, and tune hyperparameters [60,70,81]. How can this large task be broken down so that all collaborators can coordinate with each other and contribute without duplicating work? C2 Tool mismatch. Data science developers are accustomed to working in computational notebooks and have varying expertise with version control tools like git [15,49,70,81]. How can these workflows be adapted to use a shared codebase and build a single product? C3 Evaluating contributions. Prospective collaborators may submit code to a shared codebase. Some code may introduce errors or decrease the performance of the ML model [45,47,67,78]. How can their contributions be evaluated? C4 Maintaining infrastructure. Data science requires carefully managing data and computation [75,78]. Will it be necessary to establish shared data stores and computing infrastructure? Would this be expensive and require significant technical and DevOps expertise? Is this appropriate for the open-source setting?",no,,,,no,,,,
155,https://github.com/ainagari/scalar_adjs,ainagari_scalar_adjs.tar.gz,"BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations",,"copy pymagnitude pandas warnings sys pdb os, sys extract_representations collections transformers nltk torch os string scipy gzip itertools predict spacy pickle sklearn argparse numpy math read_scalar_datasets operator random eval",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ainagari_scalar_adjs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ainagari_scalar_adjs.pdf,Dense Connections  Layer Normalization  WordPiece  Multi-Head Attention  Dropout  Linear Warmup With Linear Decay  Attention Dropout  Weight Decay  Scaled Dot-Product Attention  Adam  Softmax  Residual Connection  Gaussian Error Linear Units  BERT,,no,,,,no,,,,
166,https://github.com/AntreasAntoniou/DAGAN,AntreasAntoniou_DAGAN.tar.gz,Sampling Generative Networks,,utils scipy tensorflow dagan_architectures experiment_builder csv data tqdm dagan_networks_wgan argparse os generation_builder numpy,cs.NE cs.LG stat.ML stat.ML cs.CV cs.LG cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AntreasAntoniou_DAGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AntreasAntoniou_DAGAN.pdf,,,no,,,,no,,,,
167,https://github.com/dawenl/expo-mf,dawenl_expo-mf.tar.gz,Modeling User Exposure in Recommendation,,math scipy bottleneck sys sklearn os joblib numpy rec_eval time,stat.ML cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dawenl_expo-mf.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dawenl_expo-mf.pdf,,,no,,no,,no,,,,
170,https://github.com/bhpachulski/ICPC-RENE-Paper,bhpachulski_ICPC-RENE-Paper.tar.gz,What is the Vocabulary of Flaky Tests? An Extended Replication,,re arff pandas csv seaborn sys json pickle sklearn numpy matplotlib time io,cs.SE cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bhpachulski_ICPC-RENE-Paper.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bhpachulski_ICPC-RENE-Paper.pdf,,,no,,,,no,,,,
173,https://github.com/NickLucche/pyramid-cnn-leaves-segmentation,NickLucche_pyramid-cnn-leaves-segmentation.tar.gz,A Pyramid CNN for Dense-Leaves Segmentation,@njit @jit(nopython=True),utils torchvision copy numba sys glob cv2 random tqdm torch argparse os msu_leaves_dataset numpy matplotlib pyramid_network,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/NickLucche_pyramid-cnn-leaves-segmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\NickLucche_pyramid-cnn-leaves-segmentation.pdf,,,no,,,,no,,,,
174,https://github.com/yiyuan1991/3D-Face-GCNs,yiyuan1991_3D-Face-GCNs.tar.gz,Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images Using Graph Convolutional Networks,,utils imageio PIL glob rasterize_triangles lib pdb tqdm model_normal __future__ tensorflow model_resnet collections h5py torch os matplotlib scipy camera_utils sklearn argparse numpy dlib heapq time shutil base_model math face_segment cv2 logging random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yiyuan1991_3D-Face-GCNs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yiyuan1991_3D-Face-GCNs.pdf,Graph Convolutional Networks,"Ablation StudyFig. 7 demonstrates the ablation study of our approach, where our full model presents a more detailed and realistic texture than its variants. Tab. 3DMM model, which is able to produce a basic shape and texture in general, however, the details such as lentigo or eyes are not faithfully predicted.Starting by L pix and L id , the networks are no longer restricted to a 3DMM model, and predicts higher fidelity face skin and eyes. With the help of adversarial training, the results become less blurry and more realistic. Finally, we construct our full model by adding the L vert , the predictions contain more details, and looked very similar to the input image.We also replace GCNs with fully-connected layers or convolutional layers on unwrapped UV space, and the performance are no better than GCNs, as shown in Fig. 7 and Tab. 2. We reach to the similar conclusion as [39], that using FCs or CNNs on UV spaces leads to a large number of parameters in the network and does not utilize the spatial information of the 3D facial structure. To improve the results, we train and test our proposed networks on a higher resolution image dataset, CelebA-HQ [19], as well. Higher resolution helps to reduce the checkerboard-like artifacts and produce better results. A comparison is shown in Fig. 8.",no,,,,no,,,,
175,https://github.com/Juintin/Bridge-FAST-and-PGD-AT,Juintin_Bridge-FAST-and-PGD-AT.tar.gz,Bridging the Performance Gap between FGSM and PGD Adversarial Training,,utils PIL copy math sys preact_resnet wide_resnet argparse os torch __future__ numpy matplotlib time torchvision,cs.CR cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Juintin_Bridge-FAST-and-PGD-AT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Juintin_Bridge-FAST-and-PGD-AT.pdf,,,no,,,,no,,,,
181,https://github.com/DrVonMartinez/NPI,DrVonMartinez_NPI.tar.gz,Quantum optimal control of photoelectron spectra and angular distributions,,functions scipy card table_helper random torch numpy matplotlib,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DrVonMartinez_NPI.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DrVonMartinez_NPI.pdf,,,no,,,,no,,,,
182,https://github.com/GoodAI/torchsim,GoodAI_torchsim.tar.gz,ToyArchitecture: Unsupervised Learning of Interpretable Models of the World,"@pytest.mark.parametrize('start_training', [True, False]) @pytest.mark.parametrize('sp_data_input_shape, flock_size, context_size, context_input_shape, is_valid', [ @pytest.mark.parametrize('inputs', [[1, 2, 3, 1], [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 0]]]) @cluster_boost_threshold.setter @dir_y.setter @parent_rf_stride.setter @pytest.mark.parametrize('dataset_size', [SeDatasetSize.SIZE_24, SeDatasetSize.SIZE_32, SeDatasetSize.SIZE_64]) @pytest.mark.parametrize('tensor_shape, shape, dim, expected_shape', [ @boost.setter @trim_output_size.setter @pytest.mark.parametrize(""cluster_boosting_durations, variance_batch, cluster_batch, expected_boosting_targets"", @n_frequent_seqs.setter @vertical_segments.setter @pytest.mark.parametrize('seqs, likelihoods, expected_output_projection', [ @pytest.mark.parametrize('generator_sequences', [None, [[1, 2, 3], [5, 2]]]) @p_float.setter @pytest.mark.parametrize(""scale, dims, minimal_size, expected_size"", [ @flatten_output_grid_dimensions.setter @update_period.setter @pytest.mark.parametrize( @pytest.mark.parametrize('input_value', [ @pytest.mark.parametrize(""param, exception"", [ @pytest.mark.skip(""Run this to profile the kernel."") @cmin.setter @p_enum.setter @pytest.mark.parametrize('plot_input', @size_coef.setter @decorate_class @sx.setter @show_projections.setter @pytest.mark.parametrize(""desired_max_new_seqs"", @horizontal_segments.setter @pytest.mark.slow @pytest.mark.parametrize('creator', [MeasuringCreator(), AllocatingCreator('cpu')]) @pytest.mark.parametrize('flock_size', [1, 3]) @own_rewards_weight.setter @exploration_attempts_prior.setter @enable_learning.setter @sy.setter @randomize_intervals.setter @get_index_from_input.setter @measure_time(iterations=200, function_repetitions=1000) @pytest.mark.parametrize(""initializable, state, strategy, exp_state, exp_description"", [ @parent_rf_dims.setter @pytest.mark.parametrize('flock_size', [1, 9]) @context_prior.setter @pytest.mark.parametrize('generator', [[0, 1, 2], 0, 1]) @pytest.mark.parametrize(""input_shape, dim, desired_size, expected_output_shape"", @follow_goals.setter @compute_reconstruction.setter @num_inputs.setter @pytest.mark.parametrize('move_strategy, vector, point, expected', [ @pytest.mark.parametrize(""t_input, p_min, p_max, expected_result"", [ @pytest.mark.parametrize('use_default_stream', [True, pytest.param(False, marks=pytest.mark.skip( @forgetting_limit.setter @pytest.mark.parametrize(""expected_sequence, seq_params"", [(torch.tensor([0, 1, 2, 3]), DatasetSequenceMNISTNodeParams([[0, 1, 2, 3]]))]) @pytest.mark.parametrize('tensor_shape, index_shape, dim, expected_shape', [ @pytest.mark.skip(""This test would need the space to end the curriculum."") @base_shape.setter @pytest.mark.parametrize('SP_type', [SPFlock, ConvSPFlock]) @pytest.mark.parametrize('steps', [1, 7]) @pytest.mark.parametrize(""name, expected_name"", [ @pytest.mark.parametrize('grids, expected_result, expected_shape', [ @pytest.mark.parametrize(""t_input_size, t_shape, items_per_row, result"", [ @pytest.mark.parametrize(""flock_indices"", [([0, 1]), ([0, 1, 2]), ([0, 2]), None]) @pytest.mark.parametrize('params, should_pass', [ @pytest.mark.parametrize('mode, vector, expected_indexes', [ @canvas_shape.setter @ball_radius.setter @pytest.mark.parametrize('stream', [torch.cuda.current_stream()]) @pytest.mark.skip(""Used for measuring performance of the node."") @pytest.mark.parametrize(""source, indices, dim, raises_exception"", [ @is_hide_labels.setter @sequence_significance_source.setter @seq_length.setter @pytest.mark.parametrize('sampling_method', @pytest.mark.parametrize('tensor_shape, interpreted_dim, interpretation, expected_interpret_shape', [ @pytest.mark.parametrize('topology_class', [MnistSpTopology])#, Task0TaSeTopology, L3ConvTopology, L3SpConvTopology, GradualLearningBasicTopology]) @base_multiplier.setter @flock_size.setter @use_thresholding.setter @channel_first.setter @pytest.mark.skip(""Run this to profile the function gather_from_dim."") @pytest.mark.xfail @lower_bound.setter @pytest.mark.parametrize(""device"", [""cpu"", ""cuda""]) @custom_transition_probs.setter @random_position_direction_switch_after.setter @p_bool.setter @ball_shapes.setter @pytest.mark.flaky(reruns=5) @class_filter.setter @id.setter @pytest.mark.parametrize('input, mask, output_shape, dimension, expected_result', [ @pytest.mark.parametrize('shape, pattern, is_valid', [ @pytest.mark.parametrize('device', ['cuda', 'cpu']) @p_int.setter @buffer_size.setter @fixed_region_size.setter @do_delay_input.setter @pytest.mark.parametrize(""mask, total_data_written, data_since_last_sample, expected_learn_tensor"", @kernel_shape.setter @save_gpu_memory.setter @pytest.mark.parametrize('device', ['cpu', 'cuda']) @pytest.mark.parametrize('topology_class', discover_main_topology_classes(skip_topologies)) @pytest.mark.parametrize('sp_data_input_shape, flock_size, context_size, context_input_shape, exception_message', [ @n_cluster_centers.setter @group_id.setter @max.setter @pytest.mark.parametrize('s', [ @pytest.mark.parametrize(""action_in, override, override_action"", [(torch.zeros(descriptor.ACTION_COUNT), False, @skip_execution.setter @max_boost_time.setter @active_dataset.setter @pytest.mark.parametrize('topology_class', get_task_0_topologies()) @pytest.mark.parametrize('child_dims, parent_dims, stride, expected_result', [ @pytest.mark.parametrize('buffer, expected_result', [ @pytest.mark.parametrize('classifier_class', [SvmClassifier, NNClassifier]) @pytest.mark.parametrize('run', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) @trim_output.setter @items_per_row.setter @p_int_list.setter @sampling_method.setter @is_learning_enabled.setter @pytest.mark.parametrize('symbols, text, expected', [ @use_fixed_region_size.setter @p_str.setter @pytest.mark.skip(reason=""Specialized test"") @shape.setter @one_hot_labels.setter @upper_bound.setter @noise_amplitude.setter @pytest.mark.parametrize('item, sequences, expected_result', [ @min.setter @pytest.mark.parametrize(""value, should_pass"", [ @weights_on_input.setter @generate_new_every_n.setter @lru_cache(maxsize=None) @pytest.mark.parametrize('p, q, expected_result', [ @pytest.mark.parametrize('indexes, number_of_elements, dtype, expected_output', [ @pytest.mark.parametrize('sampling_method', [e for e in SamplingMethod]) @pytest.mark.parametrize('input_shape, flock_size, expected_flock_shape', [ @pytest.mark.parametrize('sequence_type', ['ordered', 'random', 'sequence_generator']) @output_projection_persistance.setter @batch_size.setter @pytest.mark.parametrize('n_providers', [1, 4]) @do_delay_coefficients.setter @input_weights.setter @current_size_coef.setter @pytest.mark.parametrize(""flock_size, batch_size, overlap"", [(3, 20, 3), (5, 10, 2)]) @pytest.yield_fixture(autouse=True) @pytest.mark.parametrize('source, projections, expected_result', [ @pytest.mark.parametrize(""t_input, minimum, maximum, expected_result"", [ @pytest.mark.parametrize(""expected_input_shape, dim, desired_size, output_shape"", @pytest.mark.skip(reason=""This test is here because it failed on the non-default stream. "" @name.setter @smoothing.setter @switch_next_shape_after.setter @pytest.mark.parametrize('device', ['cpu', pytest.param('cuda', marks=pytest.mark.slow)]) @pytest.mark.parametrize('should_end_1,should_end_2', [ @images_path.setter @pytest.mark.parametrize(""device"", ['cpu', 'cuda']) @learning_period.setter @distribution.setter @pytest.mark.parametrize('shape, view, expected_result', [ @abc.abstractmethod @pytest.mark.parametrize('data, n_dims, expected_result', [ @point_pos.setter @attractor_distance.setter @overload @exploration_probability.setter @pytest.mark.parametrize(""new_batch_size, new_seq_length"", @pytest.mark.parametrize('seq_lookahead', [1, 2]) @max_encountered_seqs.setter @compute_best_matching_context.setter @generate_torch_delegators @pytest.mark.parametrize('clamped_data, min, max, expected_result', [ @dataclass @pytest.mark.flaky(reruns=100) @pytest.mark.parametrize('device', ['cpu']) @pytest.mark.parametrize('enable_learning', [True, False]) @property @pytest.mark.parametrize(""sequences, trans_probs, iters"", [([[0], [1], [2], [4]], [[0.0, 1., 0.0, 0.0], @pytest.mark.parametrize('topology_class', discover_main_topology_classes()) @pytest.mark.parametrize('examples_per_class', [None, 1, 11]) @n_hidden_layers.setter @p_optional_list_int.setter @pytest.mark.parametrize(""tensor_dims, shape, is_rgb, result_dims"", [ @pytest.mark.flaky(reruns=3) @abstractmethod @pytest.mark.parametrize('shape, expand, expected_result', [ @random_order.setter @amplitude.setter @fading_factor.setter @pytest.mark.parametrize(""flock_size, batch_size, n_cluster_centers, overlap"", [(2, 5, 3, 2), (100, 60, 17, 4), @tensor.setter @mode.setter @pytest.mark.skip(reason=""Ignoring non-default stream tests for now."") @pytest.mark.parametrize('create_func', [ @properties.setter @pytest.mark.parametrize('input_a, input_b, expected_result', [ @pytest.mark.parametrize('random_order', [True, False]) @pytest.mark.parametrize('topology_class, n_steps', [(MnistSpTopology, 1000)]) @pytest.mark.parametrize('device', ['cpu', 'cuda'],) @classmethod @measure_time(iterations=10) @measure_time(iterations=100, function_repetitions=100) @pytest.mark.parametrize('list1, list2, eps, expected_result', [ @pytest.mark.skip(""Test obseleted"") @dataset_config.setter @seqs.setter @pytest.mark.parametrize('flock_class', [SPFlock, ConvSPFlock]) @pytest.mark.parametrize('input_shape, flock_size', [ @p_tuple_int_int.setter @location_filter_ratio.setter @dataset_size.setter @pytest.mark.parametrize('labels', [[0, 2, 0, 1], [[1, 0, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]]) @use_fixed_region.setter @colormap.setter @hidden_size.setter @pytest.mark.parametrize('learning_coefficients', [ @lru_cache(maxsize=10) @pytest.mark.parametrize('pattern_item, expected_str', [ @is_rgb.setter @decorate @sum_dim.setter @pytest.mark.parametrize(""flock_size, batch_size, n_subbatches"", @active_input_index.setter @p_tuple_float_float.setter @buffer.setter @pytest.mark.parametrize('packet, expected_object', [ @pytest.mark.parametrize('comparison_method', ([same_lists, same])) @cmax.setter @pytest.mark.parametrize(""state, enabled, result, should_pass"", [ @pytest.mark.skip(reason=""Ignoring tests on non-default streams for now."") @pytest.mark.parametrize('class_filter', [None, [1, 9, 4], [-2], [11, 2]]) @pytest.mark.parametrize('input_shape, dim', [ @pytest.mark.parametrize('seed', [None, 10]) @pytest.mark.parametrize('shape, pattern, groups', [ @pytest.mark.parametrize('topology_class', discover_local_topology_classes()) @threshold_value.setter @step_delay.setter @compute_backward_pass.setter @pytest.mark.flaky(reruns=2) @staticmethod @pytest.mark.skip(""Run this to profile different boxes and configs"") @patch('torchsim.core.nodes.flock_networks.neural_network_flock.NeuralNetworkFlock.make_data_loaders', new=make_data_loaders) @input_dims.setter @outer_shift_coef.setter @seq_lookahead.setter @draw_target.setter @learning_rate.setter @pytest.mark.parametrize('seeds', [(None, None), (1, 1), (1, 104)]) @switch_train_resets_train_pos.setter @pytest.mark.parametrize(""size, func, iters, name"", [(10, (lambda x: torch.sum(x)), 10000, ""Sum""), @contextmanager @transition_probs.setter @significance_root_power.setter @seed.setter @max_new_seqs.setter @wraps(f) @pytest.mark.skip(reason=""benchmarking purposes only"") @pytest.mark.parametrize(""value, size, should_pass"", [ @inner_shift_coef.setter @pytest.mark.parametrize('topology_class', discover_local_topology_classes(skip_topologies)) @dir_x.setter @p_optional_int.setter @pytest.mark.parametrize('data, seqs, likelihoods, n_top_sequences, expected_output', [ @incoming_context_size.setter @pytest.mark.parametrize('seqs, expected_output_projection', [ @pytest.mark.skip('Just used for manual checks of UI') @pytest.mark.parametrize('shape, expected_result', [ @pytest.mark.parametrize(""input,expected"", [ @pytest.mark.parametrize('squeeze_channel, tensor_data', [(True, [1, 0, 0.2126, 0.7152, 0.0722]), @pytest.mark.parametrize('input, first_value, expected_output', [ @pytest.fixture() @pytest.mark.parametrize(""value, shape, should_pass"", [ @examples_per_class.setter @pytest.mark.parametrize('seq_length, seq_lookahead, expected_result', [ @pytest.mark.parametrize('test_item', [",gc PIL copy contextlib warnings sys prettytable glob torchsim tqdm base64 platform ast torchvision pprint threading pathlib pygame re datetime unittest collections json tests enum dataclasses tempfile torch os pytest pkgutil ruamel matplotlib string six importlib dacite functools itertools genericpath gzip pymunk typing pickle sklearn argparse numpy eval_utils time abc shutil math traceback colour tornado random logging statistics operator zmq inspect websocket io,cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/GoodAI_torchsim.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\GoodAI_torchsim.pdf,,,no,,,,no,,,,
183,https://github.com/atlarge-research/AIP,atlarge-research_AIP.tar.gz,"A Survey and Annotated Bibliography of Workflow Scheduling in Computing Infrastructures: Community, Keyword, and Article Reviews -Extended Technical Report",@renderer_classes([JSONRenderer]) @api_view(['GET']) @cache_page(60 * 60 * 24 * 30),requests rake_nltk pandas csv parse_mag publications sys multiprocessing xxhash glob tools bs4 tqdm parser venue_mapper rest_framework ast psycopg2 util pathlib parse_semantic_scholar threading re asyncio automatic_checks datetime json collections pyppeteer renew_data_locally author_paper_pairs data_mining parse_dblp parse_aminer nltk os citations textblob sqlite3 matplotlib leidenalg igraph neo4j scipy gzip paper_word_pairs cites sklearn authors lxml orjson dask_jobqueue numpy joblib time shutil dask PyQt5 venue_mappings networkx database_manager logging statistics django_filters django raw_db_access,cs.DC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/atlarge-research_AIP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\atlarge-research_AIP.pdf,,,no,,,,no,,,,
185,https://github.com/ShenYujun/HiGAN,ShenYujun_HiGAN.tar.gz,Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis,@tf.custom_gradient @staticmethod,"utils requests PIL gzip, pickle copy dnnlib pwd warnings sys types glob legacy tqdm projector base64 __future__ ctypes pprint platform distutils torchvision threading pathlib tensorflow re zipfile uuid datetime collections json tfutil h5py enum tempfile fnmatch torch os html six importlib gzip scipy hashlib bisect imp dataset lmdb training urllib pretrained_networks pickle networks typing argparse sklearn predictors numpy time cryptography config shutil train tarfile math tensorboard traceback bz2 cv2 logging misc moviepy metrics models inspect io",cs.CV cs.GR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ShenYujun_HiGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ShenYujun_HiGAN.pdf,Softmax  Rectified Linear Units  Conditional Batch Normalization  Residual Block  Two Time-scale Update Rule  GAN Hinge Loss  Residual Connection  Non-Local Operation  Non-Local Block  Truncation Trick  Linear Layer  Dot-Product Attention  Projection Discriminator  Spectral Normalization  Off-Diagonal Orthogonal Regularization  Adam  Batch Normalization  Early Stopping  1x1 Convolution  SAGAN Self-Attention Module  Self-Attention GAN  BigGAN  Adaptive Instance Normalization  R1 Regularization  Leaky ReLU  Dense Connections  Feedforward Network  StyleGAN  Convolution  Generative Adversarial Network,,no,,,,no,,,,
190,https://github.com/china-ai-law-challenge/CAIL2019,china-ai-law-challenge_CAIL2019.tar.gz,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,@property @staticmethod @profile,"my sys tqdm memory_profiler pprint tensorflow re json collections basic prepare nltk os os, re string gzip functools itertools jieba pickle sklearn argparse numpy time shutil math random operator",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/china-ai-law-challenge_CAIL2019.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\china-ai-law-challenge_CAIL2019.pdf,,,no,,,,no,,,,
192,https://github.com/amirhk/mace,amirhk_mace.tar.gz,Algorithmic Recourse: from Counterfactual Explanations to Interventions,@utils.Memoize,"generateARExplanations utils copy pandas warnings pysmt loadData sys glob normalizedDistance tqdm __future__ fair_utils_data pprint debug loadModel os,sys datetime collections loadCausalConstraints _data_main traitlets os recourse matplotlib scipy IPython seaborn urllib pickle sklearn argparse modelConversion numpy time shutil generateSATExplanations generateMOExplanations treeUtils random generateFTExplanations inspect graphviz",cs.LG cs.AI stat.ML cs.LG cs.AI cs.LO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amirhk_mace.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amirhk_mace.pdf,,,no,,,,no,,,,
193,https://github.com/mit-gfx/ContinuousParetoMTL,mit-gfx_ContinuousParetoMTL.tar.gz,Efficient Continuous Pareto Exploration in Multi-Task Learning,@contextmanager @staticmethod @torch.no_grad() @torch.enable_grad(),PIL pareto contextlib sys termcolor torchvision pathlib torch cvxpy matplotlib common gzip scipy functools itertools codecs urllib typing numpy setuptools random mpl_toolkits,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mit-gfx_ContinuousParetoMTL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mit-gfx_ContinuousParetoMTL.pdf,,"PreliminariesIn this work, we consider an unconstrained multi-objective optimization problem described by f (x) : R n → R m where each f i (x) : R n → R, i = 1, 2, • • • , m represents the objective function of the i-th task to be minimized. For any x, y ∈ R n , x dominates y if and only if f (x) ≤ f (y) and f (x) = f (y). A point x is said to be Pareto optimal if x is not dominated by any points in R n . Similarly, x is locally Pareto optimal if x is not dominated by any points in a neighborhood of x. The Pareto set of this problem consists of all Pareto optimal points, and the Pareto front is the image of the Pareto set. In the context of deep MTL, x represents the parameters of a neural network instance and each f i (x) represents one learning objective, e.g., a certain classification loss.Similar to single-objective optimization, solving for local Pareto optimality is better established than global Pareto optimality. A standard way is to run gradient-based methods to solve for local Pareto optimality then prune the results.  describes the following necessary condition: Definition 3.1 . Assuming each . All Pareto optimal points are Pareto stationary.f i (x) is continuously differentiable, a point x is called Pareto stationary if there exists α ∈ R m such that α i ≥ 0, m i=1 α i = 1, and m i=1 α i ∇f i (x) = 0. Proposition 3.1Once a Pareto optimal solution x * is found, previous papers Martín & Schütze, 2018;Schulz et al., 2018) have proven a strong result revealing the first-order approximation of the local, continuous Pareto set: Proposition 3.2 . Assuming that f (x) is smooth and x * is Pareto optimal, consider any smooth curve x(t) : (− , ) → R n in the Pareto set and passing x * at t = 0, i.e., x(0) = x * , then ∃β ∈ R m such that:H(x * )x (0) = ∇f (x * ) β (1)where H(x * ) is defined asH(x * ) = m i=1 α i ∇ 2 f i (x * ) (2)and α i is given by Definition 3.1.In other words, in the Pareto set, for any smooth curve passing x * , H(x * ) transforms its tangent at x * to a vector in the space spanned by {∇f i (x * )}. By gradually changing the curve, its tangent sweeps the tangent plane of the Pareto set at x * . Essentially, the theorem states that H(x * ) connects the whole tangent plane to the column space of ∇f (x * ) . Note that, however, this theorem is not directly applicable to MTL because of its requirement of full Hessians.",no,,,,no,,,,
194,https://github.com/gradslam/gradslam,gradslam_gradslam.tar.gz,∇SLAM: Automagically differentiable SLAM,"@pytest.mark.parametrize(""dims"", ((4, 3), (3, 4))) @pytest.mark.parametrize(""channels_first"", (False, True)) @pytest.mark.skipif(not Path(TUM_ROOT).exists(), reason=TUM_NOT_FOUND) @pytest.mark.skipif(not torch.cuda.is_available(), reason=""cuda not available"") @pytest.mark.parametrize(""lastdim"", (3, 4)) @pytest.mark.parametrize(""lastdim"", (2, 3)) @normals_list.setter @features_list.setter @pytest.mark.skipif(not torch.cuda.is_available(), reason=CUDA_NOT_AVAILABLE) @property @staticmethod @pytest.mark.skipif( @poses.setter @classmethod @pytest.mark.skipif(not Path(ICL_ROOT).exists(), reason=ICL_NOT_FOUND) @normals_padded.setter @features_padded.setter @depth_image.setter @pytest.mark.parametrize(""device"", (""cpu"", ""cuda:0"")) @pytest.mark.skipif(not Path(SCANNET_ROOT).exists(), reason=SCANNET_NOT_FOUND)",imageio copy yaml warnings sys glob base64 ast chamferdist pathlib open3d collections unittest tests tempfile torch os pytest plotly sphinx runpy importlib kornia typing argparse gradslam natsort numpy abc math setuptools cv2 logging io,cs.RO cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gradslam_gradslam.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gradslam_gradslam.pdf,,,no,,,,no,,,,
195,https://github.com/gianlucatruda/warfit-learn,gianlucatruda_warfit-learn.tar.gz,Warfarin dose estimation on multiple datasets with automated hyperparameter optimisation and a novel software framework,@property,tabulate pandas warnings setuptools multiprocessing typing sklearn os joblib numpy,q-bio.QM cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gianlucatruda_warfit-learn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gianlucatruda_warfit-learn.pdf,Support Vector Machine  Linear Regression,"IntroductionMany individuals suffer from blood clots that lead to arterial and venous thromboembolism. The standard method for treating these conditions is the use of anticoagulant drugs such as vitamin K antagonists, the most widely used of which is warfarin. Whilst effective, the drug has a narrow therapeutic range and severe side-effects at extreme concentrations. This makes the precise dosing of warfarin an important concern for clinicians. Unfortunately, warfarin metabolism differs across individuals based on age, weight, genetics, diet, drug interactions, and various pre-existing conditions [1,2]. To standardise the process of anticoagulant monitoring, the World Health Organisation adopted the international dose was square rooted to compensate for the skewed distribution of the variable. The researchers made use of a stacked generalisation (stacking) framework to implement heterogeneous ensembles. Results were obtained through 100 rounds of 80/20 re-sampling. Their findings suggest that stacked generalisation ensembles are significantly more accurate than the existing isolated models on the IWPC dataset. Crucially, however, these high-performing stacked ensembles were given extra input parameters that were not accessible to the other algorithms. In addition to the 11 common parameters used in the IWPC and Liu et al. models [17,5], their stacked ensembles included indicators for diabetes mellitus, heart failure, valve replacement, use of statins, and additional VKORC1 genotypes. Despite acknowledging the use of additional parameters in their methodology, the authors still claim to have performed a direct comparison. Although they appear to have successfully utilised these additional parameters, the inconsistency of the methodology renders their findings inconclusive. Considering that hyperparameter tuning was performed on the entire dataset, it is hard to differentiate their reported accuracy improvements from possible overfitting effects. With our open-sourced Warfit-learn framework, we hope to reduce the possibility of such inconsistencies in future work. Table 3 :3Learning algorithms replicating the work of the IWPC[16], Liu et al.[5], and Ma et al.[12] that were compared in this study.Name Implementation Key hyperparametersLR sklearn.linear_model.LinearRegression normalize=False, fit_intercept=TrueSVR sklearn.svm.LinearSVR epsilon=0.0, tol=0.0001, C=1.0, loss='epsilon_insensitive'SV sklearn.svm.SVR kernel='linear', cache_size=1000RR sklearn.linear_model.Ridge alpha=1.0BRT sklearn.ensemble.GradientBoostingRegressor loss='least squares', learning_rate=0.1, n_estimators=100GBT sklearn.ensemble.GradientBoostingRegressor learning_rate=0.1, loss='lad', max_depth=4NN sklearn.neural_network.MLPRegressor hidden_layer_sizes=(100, ), activation='logistic', solver='lbfgs'Stacked_SV mlxtend.regressor.StackingCVRegressor regressors={GBT, SV, NN}, meta_regressor=SV, cv=5Stacked_RR mlxtend.regressor.StackingCVRegressor regressors={GBT, RR, NN}, meta_regressor=RR, cv=5",no,,yes,differen algorithms with different parameters,no,,,,
204,https://github.com/cdluminate/advorder,cdluminate_advorder.tar.gz,Practical Relative Order Attack in Deep Ranking,@staticmethod,"requests PIL pandas yaml sys multiprocessing glob lib tqdm termcolor argparse, collections reorder base64 srckernel_cc ctypes sys, os, yaml torchvision pylab os, sys, re json rich collections sys, os, yaml, re torch os matplotlib sys, os, yaml, re, json scipy apex functools gzip hashlib seaborn pickle typing argparse joblib numpy srckernel_rs time math datasets traceback sys, os, yaml, re, json, csv random statistics srckernel_py io",cs.LG cs.CV cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cdluminate_advorder.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cdluminate_advorder.pdf,,"ExperimentsTo evaluate the white-box and black-box OA, we conduct experiments on the Fashion-MNIST [54] and the Stanford-Online-Products (SOP) [37] datasets which comprise images of retail commodity. Firstly, we train a CNN with 2convolution-1-fully-connected network on Fashion-MNIST, and a ResNet-18 [21] without the last fully-connected layer on SOP following [60] that focuses on the absolute rank attack. Then we perform OA with the corresponding test set as the candidate database D. Additionally, we also qualitatively evaluate black-box OA on ""JD SnapShop"" [23] to further illustrate its efficacy. In our experiments, the value of rank function R X (•) starts from 0, i.e., the k-th ranked candidate has the rank value of k − 1.Selection of C and p. As discussed, we assume that the attacker is inclined to select the candidate set C from the top-N ranked candidates given the visible range limit. For simplicity, we only investigate the (k, N )-OA, i.e., OA with the top-k-within-top-N (k ≤ N ) candidates selected as as C. It is representative because an OA problem with some candidates randomly selected from the top-k results as C is a sub-problem of (k, N )-OA. Namely, our attack will be effective for any selection of C as long as the (k, N )-OA is effective. For white-box OA, we conduct experiments with N =∞, and k∈{5, 10, 25}. For black-box attack, we conduct experiments with N ={∞, 50, k}, and k={5, 10, 25}. A random permutation vector p is specified for each query.Evaluation Metric. Since τ S is equivalent to τ when N = ∞, we use τ S as the performance metric for both whitebox and black-box OA. Specifically, in each experiment, we conduct T = 10 4 times of OA attack. In each attack, we randomly draw a sample from D as the query q. In the end, we report the average τ S over the T trials. Also, when N = ∞, we additionally calculate the mean rank of the candidate set C (demoted as ""mR"", which equals } following [28] for both white-box and black-box attacks. The query budget Q is set to 1.0×10 3 . For white-box OA, the PGD step size η is set to 1 255 , the PGD step number to 24. The balancing parameter ξ is set as 10 1 and 10 3 for Fashion-MNIST and SOP respectively. The learning rates of black-box optimizer NES [22] and SPSA [50] are both set to 2/255. See supplementary for more details of the black-box optimizers.[ k i R X (c i )]/k),Search Space Dimension Reduction. As a widely adopted trick, dimension reduction of the adversarial perturbation search space has been reported effective in [14,9,44,29]. Likewise, we empirically reduce the space to (3×32×32) for black-box OA on Stanford-Online-Products dataset and ""JD SnapShop"". In fact, a significant performance drop in τ S can be observed without this trick.",no,,,,no,,,,
206,https://github.com/ilyakava/py3fst,ilyakava_py3fst.tar.gz,CORTICAL FEATURES FOR DEFENSE AGAINST ADVERSARIAL AUDIO ATTACKS,,copy csv googleapiclient sys multiprocessing art glob rgb_pixelNN augment_audio pdb lib tqdm pyrubberband windows __future__ tf util hdf5storage soundfile tensorflow pydub datetime json collections h5py enum os plotly matplotlib scipy sounddevice itertools functools pyroomacoustics networks sklearn argparse numpy time shutil nsltools math librosa random logging cv2 st_2d io,cs.SD cs.LG eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ilyakava_py3fst.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ilyakava_py3fst.pdf,,,no,,,,no,,,,
208,https://github.com/CDL-Quantum/Hackathon2021,CDL-Quantum_Hackathon2021.tar.gz,Implementing a distance-based classifier with a quantum interference circuit,@abstractmethod @property @staticmethod,requests utils PIL copy pandas solovay_kitaev qiskit sys warnings pennylane QVC qcs_api_client torchvision cirq re datetime json collections unittest strawberryfields src torch os pyquil matplotlib pyquil_circuits scipy functools itertools seaborn typing sklearn numpy time abc config dimod neal math toolz setuptools simple_dmrg random subprocess dwave models qutip,quant-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CDL-Quantum_Hackathon2021.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CDL-Quantum_Hackathon2021.pdf,,,no,,,,no,,,,
209,https://github.com/dgromann/OntologyAlignmentWithEmbeddings,dgromann_OntologyAlignmentWithEmbeddings.tar.gz,Comparing Pretrained Multilingual Word Embeddings on an Ontology Alignment Task,,pandas normalizer polyglotEmbeddings __future__ distutils gensim re collections nltk scipy word2vecEmbeddings decimal pickle sklearn numpy time math semSim fastTextEmbeddings operator polyglot,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dgromann_OntologyAlignmentWithEmbeddings.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dgromann_OntologyAlignmentWithEmbeddings.pdf,,,no,,,,no,,,,
210,https://github.com/DESI-UR/KITCAT,DESI-UR_KITCAT.tar.gz,A Computationally Efficient Approach for Calculating Galaxy Two-Point Correlations,,sys glob lib __future__ distutils re astropy os matplotlib configparser scipy KITCAT pickle sklearn argparse numpy py pip setuptools,astro-ph.CO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DESI-UR_KITCAT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DESI-UR_KITCAT.pdf,,,no,,,,no,,,,
213,https://github.com/zhikunch/Semi-Federated-Learning,zhikunch_Semi-Federated-Learning.tar.gz,Semi-Federated Learning,,utils copy random pickle tqdm torch sklearn argparse tensorboardX numpy models matplotlib time torchvision,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zhikunch_Semi-Federated-Learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zhikunch_Semi-Federated-Learning.pdf,,,no,,yes,"batch soze. learning rate, number of clusters, number of clients, communication rounds",no,,,,
214,https://github.com/eurunuela/pfm_vs_ta,eurunuela_pfm_vs_ta.tar.gz,Hemodynamic Deconvolution Demystified: Sparsity-Driven Regularization at Work,,"Scripts scipy itertools csv sys, argparse, os, socket, getpass, datetime nibabel random pywt sklearn subprocess numpy time shutil",q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eurunuela_pfm_vs_ta.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eurunuela_pfm_vs_ta.pdf,,,no,,,,no,,,,
216,https://github.com/yuanl12/tda_updating_persistence,yuanl12_tda_updating_persistence.tar.gz,ACCELERATING ITERATED PERSISTENT HOMOLOGY COMPUTATIONS WITH WARM STARTS,,scipy keras pandas gudhi csv comp_function topologylayer dionysus ripser tqdm numpy os torch freudenthal bats matplotlib time plyfile,math.AT cs.CG cs.LG math.AT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yuanl12_tda_updating_persistence.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yuanl12_tda_updating_persistence.pdf,,,no,,,,no,,,,
222,https://github.com/jordipons/musicnn-training,jordipons_musicnn-training.tar.gz,MUSICNN: PRE-TRAINED CONVOLUTIONAL NEURAL,,"pandas config_file, shared csv warnings sys _pickle tqdm models_midend models_frontend pathlib tensorflow pescador datetime json os config_file models_baselines pickle sklearn argparse joblib numpy time models_backend librosa random subprocess config_file, shared, train models",cs.SD cs.CL eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jordipons_musicnn-training.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jordipons_musicnn-training.pdf,,"These models are trained with two different datasets: the MagnaTagATune dataset (the MTT of 19k training songs [3]) 1 and the Million Song Dataset (the MSD of 200k training songs [1]) 2 .Which pre-trained models are available? Although the main focus of the library is to release pre-trained musically motivated convolutional neural networks, we also provide several vgg-like models 3 (as baselines for comparison). A high-level depiction of the musicnn architecture 4 is depicted in the following figure: What's the musicnn library for? Out-of-the-box music audio tagging. From within python, one can estimate the top-10 tags by simply running: from musicnn.tagger import top_tags top_tags('music_file.mp3', model='MTT_musicnn', topN=10)From the command-line, one can also print the top-N tags on the screen (top) or save them to a file (bottom): python -m musicnn.tagger music.au --model 'MTT_musicnn' --topN 10 --print python -m musicnn.tagger audio.wav -m 'MTT_vgg' --topN 5 --save out.tags What's the musicnn library for? Music feature extraction. Out of the extractor, see the example below, one gets the output of the model (the taggram and its associated tags) and all the intermediate representations of it (we refer to those as features). The features are packed in a dictionary and, for the musicnn models, you can extract timbral, temporal, cnn1, cnn2, cnn3, mean_pool, max_pool, and penultimate features 4 . For the vgg models, you can extract pool1, pool2, pool3, pool4, and pool5 features 3 . from musicnn.extractor import extractor output = extractor(file, model='MTT_musicnn', extract_features=True) taggram, tags, features = output What's the musicnn library for? Transfer learning. Our pre-trained deep learning models can be finetuned, together with an output neural-network that acts as a classifier, to perform any other music task. To assess the utility of our embeddings, we build SVM classifiers on top of several pre-trained models that act as music feature extractors. The tools used to run this simple transfer learning experiment are accessible online: https://github.com/jordipons/sklearn-audio-transfer-learningWe report accuracy results on the test set of the GTZAN (fault-filtered) dataset, and our processing pipeline consists of ""feature extraction"" + 128 PCA + SVM. The feature extraction can be based on VGGish audioset features (77.58% accuracy), OpenL3 audioset features (74.65% accuracy), MTT_musicnn features (71.37% accuracy), MTT_vgg features (72.75% accuracy), or MSD_musicnn features (77.24% accuracy). Note that our MSD pre-trained models outperform the MTT ones. Besides, the MSD_musicnn achieves similar results than the VGGish audioset features (that are trained with a much larger dataset: 2M audios).How did you train musicnn models? The code we employed to train the models above is also accessible: But the musicnn-training framework also allows to implement other models. For example, a similar architecture than musicnn but with an attention-based output layer (instead of the temporal pooling layer) can achieve 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset -and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset. You can find further details about this new architecture online.Figure 1 :1Figure 1: The musicnn architecture: a musically motivated convolutional neural network [4, 5].",no,,,,no,,,,
231,https://github.com/p-lambda/unlabeled_outputs,p-lambda_unlabeled_outputs.tar.gz,Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization,"@register_optimizer('adagrad') @register_optimizer('sgd') @register_model_architecture(""transformer"", ""transformer"") @register_optimizer('adadelta') @register_model_architecture('dummy_model', 'dummy_model') @register_model('dummy_model') @register_lr_scheduler('reduce_lr_on_plateau') @lru_cache() @register_bpe('byte_bpe') @register_model_architecture('hf_gpt2', 'hf_gpt2') @register_model_architecture(""wav2vec"", ""wav2vec"") @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('roberta', 'roberta_base') @register_model('xlmr') @register_model_architecture('roberta', 'roberta') @register_optimizer('adamax') @register_criterion('masked_lm') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_criterion('reinforce_criterion') @register_bpe('hf_byte_bpe') @register_model('camembert') @register_model_architecture('masked_lm', 'masked_lm') @register_task('sentence_ranking') @metrics.aggregate('train') @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_fr_big"") @register_model_architecture('transformer_lm', 'transformer_lm') @include_dirs.setter @register_tokenizer('space') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_task('multilingual_masked_lm') @torch.jit.unused @register_model('model_parallel_transformer_lm') @register_optimizer('lamb') @torch.jit.export @register_task('denoising') @register_model('transformer_lm') @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_model_architecture(""transformer_align"", ""transformer_align"") @register_bpe('bert') @register_model_architecture('masked_lm', 'bert_large') @register_model(""iterative_nonautoregressive_transformer"") @register_task(""language_modeling"") @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @metrics.aggregate(""train"") @register_criterion('binary_cross_entropy') @register_model_architecture(""transformer_align"", ""transformer_wmt_en_de_big_align"") @register_task('multilingual_denoising') @register_model_architecture('double_transformer', 'double_transformer') @register_model('lstm_lm') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron_big') @register_tokenizer('moses') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_criterion('sentence_prediction') @register_model_architecture('cls_transformer', 'cls_transformer') @register_bpe('subword_nmt') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_model_architecture('transformer_lm', 'transformer_lm_big') @register_task('audio_pretraining') @metrics.aggregate(""valid"") @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @register_model('model_parallel_transformer') @register_criterion('label_smoothed_cross_entropy_with_alignment') @s3_request @register_model_architecture('bart', 'bart_base') @ensemble_encoder @register_task('cross_lingual_lm') @register_model('lightconv') @register_model_architecture(""levenshtein_transformer"", ""levenshtein_transformer"") @register_bpe('fastbpe') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big_t2t"") @register_bpe('gpt2') @register_task('multilingual_translation') @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"") @register_model(""cls_transformer"") @register_model_architecture('roberta', 'roberta_large') @register_model('fconv_lm') @register_model_architecture('hf_gpt2', 'hf_gpt2_large') @register_model_architecture('lstm_lm', 'lstm_lm') @register_bpe('characters') @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer"") @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_de_big"") @register_model_architecture(""transformer"", ""transformer_iwslt_de_en"") @register_model(""insertion_transformer"") @register_model_architecture('bart', 'bart_large') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model('hf_gpt2') @register_model(""transformer_align"") @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer_wmt_en_de"") @register_model_architecture('custom_transformer', 'custom_transformer') @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture(""insertion_transformer"", ""insertion_transformer"") @register_criterion('sentence_ranking') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_optimizer('nag') @property @register_model('roberta') @register_criterion('adaptive_loss') @register_model(""double_transformer"") @ensemble_decoder @register_model_architecture('roberta', 'xlm') @register_task('translation_from_pretrained_bart') @abstractmethod @register_criterion('cross_entropy') @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU') @register_bpe('bytes') @register_model_architecture('bart', 'mbart_base') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @register_task('dummy_masked_lm') @register_model_architecture(""nacrf_transformer"", ""nacrf_transformer"") @wraps(func) @register_criterion('legacy_masked_lm_loss') @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @register_model('masked_lm') @register_bpe('sentencepiece') @register_model_architecture(""transformer"", ""transformer_wmt_en_de"") @contextlib.contextmanager @register_model_architecture('bart', 'mbart_large') @register_model_architecture('fconv', 'fconv') @register_task('legacy_masked_lm') @register_task('sentence_prediction') @register_task('translation_lev') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_model_architecture('lightconv', 'lightconv') @register_model('lightconv_lm') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_model('lstm') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @torch.jit.script @register_task('masked_lm') @register_model_architecture('hf_gpt2', 'hf_gpt2_xl') @with_incremental_state @register_criterion('vocab_parallel_cross_entropy') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_optimizer('adam') @register_lr_scheduler('triangular') @register_tokenizer('nltk') @register_criterion('denoise_criterion') @register_model(""nacrf_transformer"") @register_lr_scheduler('tri_stage') @register_model(""custom_transformer"") @lru_cache(maxsize=8) @register_model(""nonautoregressive_transformer"") @unittest.skipIf( @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @register_model(""wav2vec"") @staticmethod @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_model(""transformer"") @register_model_architecture('bart', 'mbart_base_wmt20') @register_model(""levenshtein_transformer"") @register_model('bart') @register_model('fconv') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big"") @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @contextmanager @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_criterion(""nat_loss"") @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_task('dummy_lm') @register_criterion('label_smoothed_cross_entropy') @register_model(""cmlm_transformer"") @register_task('translation') @register_model_architecture('hf_gpt2', 'hf_gpt2_medium') @register_model_architecture('fconv', 'fconv_iwslt_de_en')","copy unet warnings multiprocessing types collections, os ctypes torchvision re h5py torch importlib spacy fairseq_cli pickle boto3 numbers tensorboardX numpy heapq sentencepiece tarfile clang traceback random fonts inspect requests csv, os, sys, math, time pdb __future__ pathlib examples stitch unittest botocore tests transformers fnmatch nltk os socket gzip functools hashlib evaluate_full time shutil abc operator subprocess pandas sys regex tqdm tokenizers atexit cython c_tokenizer_mod enum tempfile lightconv_cuda struct fastBPE itertools pytorch_transformers typing shlex subword_nmt urlparse setuptools sacrebleu err_utils io zipfile PIL fvcore scripts contextlib sacremoses fairseq sys, os, shutil, re, argparse, json fileinput soundfile uuid collections json pyarrow apex bisect urllib format_pairs palaas argparse math evaluate_synthetic logging dynamicconv_cuda",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/p-lambda_unlabeled_outputs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\p-lambda_unlabeled_outputs.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Label Smoothing  Multi-Head Attention  Adam  Rectified Linear Units  Dropout  Byte Pair Encoding  Dense Connections  Layer Normalization  Softmax  Scaled Dot-Product Attention  Transformer,"|Effect of pre-training objectivesNaively, better pre-trained denoisers should improve the gains with composed fine-tuning. For example, the SPOC denoiser is not as effective as the SANSTYPE denoiser, which correlates with less gains with composed fine-tuning. Interestingly, the relationship between pre-trained denoiser quality and downstream performance is not straightforward. Table 5 shows results when we train a SANSTYPE denoiser using random word deletions (no perturbations with any domain knowledge of code). The baseline with test-time denoiser worsens from 58.4% to 46.2% (correct rate), suggesting that the denoiser is worse when trained with the random deletion objective. However, the downstream standard and composed fine-tuning performance improve substantially for both in-and out-of-distribution examples. Thus the new denoiser (using more canonical perturbations) is worse at correcting baseline outputs but improves fine-tuning results. The effect of pre-training on the downstream performance after adaptation should be a topic of further investigation. Table 5: SANSTYPE results on standard and composed fine-tuning when changing the pre-training denoising autoencoder objective from correcting code-based perturbations to correcting random, unstructured word deletions. The denoising quality of the random deletion denoiser is worse when applied to a baseline model, but it leads to better downstream performance both in-distribution and OOD.  E. SPOCDenoising objective. We use 284477 unlabeled code examples from codeforce.com to generate 1137908 pairs of noisy code to valid code. For each unlabeled program, we generate 1 unnoised example and 3 noised examples, where each noised example has one line with an error. We follow (Yasunaga & Liang, 2020) to generate error lines by random semantic and syntactic changes, including insertion, deletion, and replacement of keywords, variables, and syntactical symbols.Pre-trained model. We use random syntactic and semantic corruptions of additional ∼280k unlabeled code examples from codeforces.com as in Yasunaga & Liang (2020). Previous program repair works (Yasunaga & Liang, 2020) utilize compiler error messages to guide the repair model. We only use code as input, and thus the task is relatively difficult. We define p Π in two parts. First, we train a binary classifier g γ : Y → {0,1} which detects if a program has an error (error is label 1), trained using the denoising dataset. For an output y , if g γ (y ) = 0 then we define p Π (y || y ) = δ(y ) to be a delta distribution on y . Otherwise, if g γ (y ) = 1, then p Π (y || y ) = p ν (y || y ), where p ν is a Transformer. The Transformer p ν is first pretrained using a linewise code repair dataset generated from unlabeled examples, then trained on full-program repair where the input program has one random corrupted line with probability 0.75. Thus, taking the parameters of Π to be (γ,ν), we have Π(y ) = y if g γ (y ) = 0 and Π(y ) = argmax y p ν (y || y ) otherwise.Data processing. Since the SPOC dataset contains a small fraction of programs which have a large number of tokens, we filter out the longest examples from the training data. After filtering, we retain over 95% of the original training set. Similarly to SANSTYPE, special symbole ($ and ∼) are added to delineate lines and tabs in the pseudocode and code, and we preprocess the code using a byte-pair encoding using SentencePiece (Kudo & Richardson, 2018), with joined vocabulary size 10000.Data filtering. We train and test on a truncated version of the SPOC dataset (Kulal et al., 2019). We filter out an example during preprocessing if, after adding special tokens for tab and code lines, the number of characters exceeds 1000. This Training details. For SPOC experiments, we use a Transformer architecture for all models with 5 layers, 8 attention heads, embedding dimension 256, and FFN embedding dimension 1024. We use this architecture for both the denoiser and the models. We use dropout probability 0.4, attention dropout probability 0.2, ReLU dropout probability 0.2, weight decay 0.0001, taken as reasonable defaults from Guzmán et al. (2019). We use a decaying label smoothing schedule with smoothing parameter starting with 0.2 for 150 epochs, then 0.1 and 0.05 for 25 epochs each. We found that reducing the label smoothing parameter near the end of training improves generalization for all models. The composed predictor initializes from the pretrained predictor and are trained for 20 epochs, taking the best model according to (label-smoothed) cross entropy loss of the composition Π•f θ on the validation set. For backtranslation models, we first train a code-to-pseudocode model using the labeled data and use this model to produce synthetic pseudocode examples for unlabeled code. Then, we train a pseudocode-to-code model using the labeled examples and synthetically generated examples. Finally, we use this model as initialization to finetune on the labeled data only.Our models were trained on 1 Titan Xp or Titan RTX GPU. All SPOC experiments took around 10 hours for the pre-training and fine-tuning steps.38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).|",no,,,,no,,,,
238,https://github.com/fostiropoulos/dvq,fostiropoulos_dvq.tar.gz,Depthwise Discrete Representation Learning,,numpy tqdm tensorflow PIL,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fostiropoulos_dvq.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fostiropoulos_dvq.pdf,,,no,,,,no,,,,
244,https://github.com/aniketde/ZeroShotDG,aniketde_ZeroShotDG.tar.gz,Zero Shot Domain Generalization,,math sys random lib statistics sklearn torch numpy models matplotlib torchvision,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/aniketde_ZeroShotDG.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\aniketde_ZeroShotDG.pdf,,,no,,,,no,,,,
269,https://github.com/hjkuijf/MixMicrobleedNet,hjkuijf_MixMicrobleedNet.tar.gz,MixMicrobleedNet: segmentation of cerebral microbleeds using nnU-Net,@abstractmethod @staticmethod,requests PIL copy pandas multiprocessing sys nibabel skimage tqdm __future__ ast medpy pathlib re SimpleITK datetime collections json tifffile h5py unittest nnunet torch os pkgutil hiddenlayer matplotlib cremi importlib scipy functools itertools hashlib _warnings typing pickle sklearn argparse dicom2nifti http numpy batchgenerators time shutil abc math evalutils setuptools subprocess unittest2 meddec inspect zipfile,eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hjkuijf_MixMicrobleedNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hjkuijf_MixMicrobleedNet.pdf,,,no,,,,no,,,,
278,https://github.com/flowersteam/teachDeepRL,flowersteam_teachDeepRL.tar.gz,Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments,@classmethod @property,"os, subprocess, sys copy imageio pandas sys cloudpickle tqdm zlib psutil base64 gym tensorflow collections json treelib os matplotlib string scipy itertools seaborn pickle textwrap argparse sklearn teachDRL joblib numpy time shutil setuptools spinup subprocess",cs.LG cs.RO stat.ML cs.LG cs.AI cs.CV cs.NE cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/flowersteam_teachDeepRL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\flowersteam_teachDeepRL.pdf,Absolute Learning Progress and Gaussian Mixture Models for Automatic Curriculum Learning,,no,,yes,,no,,,,
284,https://github.com/renweiya/RFQ-RFAC-Represented-Value-Function-Approach-for-Large-Scale-Multi-Agent-Reinforcement-Learning,renweiya_RFQ-RFAC-Represented-Value-Function-Approach-for-Large-Scale-Multi-Agent-Reinforcement-Learning.tar.gz,Represented Value Function Approach for Large Scale Multi Agent Reinforcement Learning,@property,utils examples math keras tensorflow collections queue magent random sklearn argparse os numpy time pylab,cs.LG cs.CV cs.MA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/renweiya_RFQ-RFAC-Represented-Value-Function-Approach-for-Large-Scale-Multi-Agent-Reinforcement-Learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\renweiya_RFQ-RFAC-Represented-Value-Function-Approach-for-Large-Scale-Multi-Agent-Reinforcement-Learning.pdf,,"C. Results and DiscussionFollowing all parameter settings in [1], we train all six models in the default battle scenario. For testing, we use them for comparative games in two scenarios, which are the default battle scenario and the wild war scenario. The unique difference between them is that agents start with two divided camps in the default battle scenario and agents start with a random position in the wild war scenario, as shown in Fig. 1. Training in the default battle scenario and testing in both the default battle scenario and the wild war scenario can test the generalization ability of models. Some game silhouettes are shown in Fig. 2.  In training phase, each algorithm is trained three times independently. After training, 18 models are created as 18 independent players (IL_A, IL_B, IL_C, MFQ_A, MFQ _B, MFQ _C, RFQ_A, RFQ_B, RFQ_C, AC_A, AC_B, AC_C, MFAC_A, MFAC_B, MFAC_C, RFAC_A, RFAC_B, RFAC_C). In testing phase, skill levels of all players are estimated by calculating Elo scores (adapted from chess [3]) based on outcomes of battle or wild war. For each testing game, we randomly select two players among the 18 players to play.For battle scenario, we played about 136,000 games in total. The result demonstrate that actor-critic MARL algorithms outperform Q-learning MARL algorithms, as result shown in Tab. 1. The proposed RFAC algorithm performs well while the proposed RFQ algorithm has a general performance in battle scenario.As shown in Tab. 2, the proposed RFQ algorithm and MFAC nearly went to deuce, however, the actor-critic MARL algorithms swept RFQ. As shown in Tab. 3, the proposed RFAC algorithm has a good performance when fight with the other algorithms. The win-table among all the 18 players of the two scenarios is shown in Fig. 3, we can find that the actor-critic algorithms always have a good performance. Furthermore, the proposed algorithms have better performance in the wild war scenario.",no,,,,no,,,,
289,https://github.com/YiZeng623/FenceBox,YiZeng623_FenceBox.tar.gz,FENCEBOX: A Platform for Defeating Adversarial Examples with Data Augmentation Techniques,@functools.wraps(func),utils math scipy PIL tensorflow functools imagenet_labels defense albumentations cv2 random skimage os numpy matplotlib io,cs.LG cs.CR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YiZeng623_FenceBox.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YiZeng623_FenceBox.pdf,,,no,,,,no,,,,
296,https://github.com/BY571/DQN-Atari-Agents,BY571_DQN-Atari-Agents.tar.gz,Dueling Network Architectures for Deep Reinforcement Learning,@staticmethod,math multiprocessing cloudpickle collections random pickle cv2 Agents torch os Wrapper argparse numpy time gym,cs.LG cs.LG stat.ML cs.LG cs.AI stat.ML cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BY571_DQN-Atari-Agents.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BY571_DQN-Atari-Agents.pdf,Double Q-learning  Dense Connections  Convolution  Dueling Network  Double Q-learning  Softmax  Entropy Regularization  A3C  Dueling Network  Q-Learning  NoisyNet-A3C  NoisyNet-Dueling  NoisyNet-DQN  Noisy Linear Layer  Dense Connections  Convolution  Deep Q-Network  Adam  Double Q-learning  Prioritized Experience Replay  Noisy Linear Layer  Dueling Network  N-step Returns  Rainbow DQN  Q-Learning  Dense Connections  Convolution  Deep Q-Network,,no,,,,no,,,,
299,https://github.com/genforce/sefa,genforce_sefa.tar.gz,Closed-Form Factorization of Latent Semantics in GANs,"@st.cache(allow_output_mutation=True, show_spinner=False) @staticmethod",utils SessionState cv2 tqdm torch os argparse subprocess base64 numpy models streamlit,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/genforce_sefa.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\genforce_sefa.pdf,,"Comparison with Supervised ApproachWe compare our closed-form algorithm with the stateof-the-art supervised method, InterFaceGAN [24]. We conduct experiments on face synthesis models due to the well definition of facial attributes. In particular, we make comparison between SeFa and InterFaceGAN on both the conventional generator (i.e., PGGAN [16]) and the stylebased generator (i.e., StyleGAN [17]). Qualitative Results. Fig. 5 visualizes some manipulation results by using the identified semantics. We can tell that SeFa achieves similar performance as InterFaceGAN from the perspective of editing pose, gender, eyeglasses, and expression (smile), suggesting its effectiveness. More importantly, InterFaceGAN requires sampling numerous data and pre-training attribute predictors. By contrast, SeFa is completely independent of data sampling and model training, which is more efficient and generalizable.Re-scoring Analysis. For quantitative analysis, we train an attribute predictor on CelebA dataset [19] with ResNet-50 structure [11], following [24]. With this predictor, we are able to perform re-scoring analysis to quantitatively evaluate whether the identified directions can properly represent the corresponding attributes. In particular, we randomly sample 2K images and manipulate them along a certain discovered direction. We then use the prepared predictor to check how the semantic score varies in such manipulation process. Tab. 2 shows the results where we have three observations. (i) SeFa can adequately control some attribute, such as pose and gender, similar to Inter-FaceGAN. (ii) When altering one semantic, InterFaceGAN shows stronger robustness to other attributes, benefiting from its supervised training manner. For example, the age and eyeglasses corresponding to the same latent direction identified by SeFa. That is because the training data is somewhat biased (i.e., older people are more likely to wear eyeglasses), as pointed out by [24]. By contrast, involving labels as the supervision can help learn a more accurate direction to some extent. (iii) SeFa fails to discover the direction corresponding to eyeglasses. The reason is that the presence of eyeglasses is not a large variation and hence does not meet the optimization objective in Eq. ( 4).Diversity Comparison. Supervised approach highly depends on the available attribute predictors. By contrast, our method is more general and can find more diverse semantics in the latent space. As shown in Fig. 6 (a), we successfully identify the directions corresponding to hair color, hair style, and brightness. This surpasses InterFaceGAN since predictors for these attributes are not easy to acquire in practice. Also, supervised methods are usually limited by the training objective. For example, InterFaceGAN is proposed to handle binary attributes [24]. In comparison, our method can identify more complex attributes, like the different hair styles shown in Fig. 6 (b).",no,,,,no,,,,
307,https://github.com/illidanlab/cdrl,illidanlab_cdrl.tar.gz,Collaborative Deep Reinforcement Learning,,envs threading scipy tensorflow a3c go_vncdriver model sys collections cv2 logging argparse os __future__ numpy time shutil six,cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/illidanlab_cdrl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\illidanlab_cdrl.pdf,Knowledge Distillation,"Deep knowledge distillationAs we introduced before, knowledge distillation [15] is trying to train a student network that can behave similarly to the teacher network by utilizing the logits from the teacher as supervision. However, transferring the knowledge among heterogeneous tasks faces several di culties. First, the action spaces of di erent tasks may have di erent dimensions. Second, even if the dimensionality of action space is same among tasks, the action probability distributions for di erent tasks could vary a lot, as we illustrated in Figure 5 (a) and (b). us, the action pa erns represented by the logits of di erent policy networks are usually di erent from task to task. If we directly force a student network to mimic the action pa ern of a teacher network for a di erent task, it could be trained in a wrong direction, and nally ends up with worse performance than isolated training. In fact, this suspect has been empirically veri ed in our experiments.Based on the above observation, we propose deep knowledge distillation to transfer knowledge between heterogeneous tasks. As illustrated in Figure 2 (a), the approach for deep knowledge distillation is straightforward. We use a deep alignment network to map the logits of the teacher network from a heterogeneous source task e α (environment ε α ), then the logits is used as our supervision to update the student network of target task e β (environment ε β ).is procedure is performed by minimizing following objective function over student policy network parameters θ β p :L K L (D, θ β p , τ ) = t l K L (F θ ω (z α t ), z β t , τ ),(3)wherel K L (F θ ω (z α t ), z β t , τ ) = so max(F θ ω (z α t )/τ ) ln so max(F θ ω (z α t )/τ ) so max(z β t ).Here θ ω denotes the parameters of the deep alignment network, which transfers the logits z α t from the teacher policy network for knowledge distillation by function F θ ω (z α t ) at step t. As we show in Figure 2 (b), θ β p is the student policy network parameters (including parameters of CNN, LSTM and policy layer) for task e β , while θ β p denotes student network parameters of CNN, LSTM and distillation layer. It is clear that the distillation logits z β t from the student network does not determine the action probability distribution directly, which is established by the policy logits z β t , as illustrated in Figure 2 (b). We add another fully connected distillation layer to deal with the mismatch of action space dimensionality and the contradiction of the transferred knowledge from source domain and the learned knowledge from target domain. e input to both of the teacher and the student network is the state of environment ε β of target task e β . It means that we want to transfer the expertise from an expert of task e α towards the current state. Symbol D is a set of logits from the teacher network in one batch and τ is the temperature same as described in Eq (1). In a trivial case that the teacher network and the student network are trained for same task (e α equals e β ), then the deep alignment network F θ ω would reduce to an identity mapping, and the problem is also reduced to a single task policy distillation, which has been proved to be e ective in [26]. Before we can apply the deep knowledge distillation, we need to rst train a good deep alignment network. In this work, we provide two types of training protocols for di erent situations: O line training: is protocol rst trains two teacher networks in both environment ε α and ε β . en we use the logits of both two teacher networks to train a deep alignment network F θ ω . A er acquiring a pre-trained F θ ω , we train a student network of task e β from scratch, in the meanwhile the teacher network of task e α and F θ ω are used for deep knowledge distillation.Online training: Suppose we only have a teacher network of task e α , and we want to use the knowledge from task e α to train the student network for task e β to get higher performance from scratch.e pipeline of this method is that, we rstly train the student network by interacting with the environment ε β for a certain amount of steps T 1 , and then start to train the alignment network F θ ω , using the logits from the teacher network and the student network. A erwards, at step T 2 , we start performing deep knowledge distillation. Obviously T 2 is larger than T 1 , and the value of them are task-speci c, which is decided empirically in this work.e o ine training could be useful if we have already had a reasonably good model for task e β , while we want to further improve the performance using the knowledge from task e α . e online training method is used when we need to learn the student network from scratch. Both types of training protocol can be extended to multiple heterogeneous tasks. Certi cated Homogeneous transferIn this subsection, we verify the e ectiveness of knowledge distillation as a type of interaction in collaborative deep reinforcement learning for homogeneous tasks.is is also to verify the e ectiveness of the simplest case for deep knowledge distillation. Although the e ectiveness of policy distillation in deep reinforcement learning has been veri ed in [26] based on DQN, there is no prior studies on asynchronous online distillation. erefore, our rst experiment is to demonstrate that the knowledge distilled from a certi cated task can be used to train a decent student network for a homogeneous task. Otherwise, the even more challenging task of transferring among heterogeneous sources may not work. We note that in this case, the Assumption 1 is fully satis ed givenk 1 = k 2 ,where k 1 and k 2 are the knowledge needed to master task e 1 and e 2 , respectively. In this experiment, we conduct experiments in a gym environment named P . It is a classic Atari game that an agent controls a paddle to bounce a ball pass another player agent.e maximum reward that each episode can reach is 21.First, we train a teacher network that learns from its own environment by asynchronously performing GAE updates. We then train a student network using only online knowledge distillation from the teacher network. For fair comparisons, we use 8 agents for all environments in the experiments. Speci cally, both the student and the teacher are training in P with 8 agents. e 8 agents of the teacher network are trained using the A3C algorithm (equivalent to CDRL with GAE updates in one task). e 8 agents of student network are trained using normal policy distillation, which uses the logits generated from the teacher network as supervision to train the policy network directly. From the results in Figure 3 (a) we see that the student network can achieve a very competitive performance that is is almost same as the state-of-arts, using online knowledge distillation from a homogeneous task. It also suggests that the teacher doesn't necessarily need to be an expert, before it can guide the training of a student in the homogeneous case. Before 2 million steps, the teacher itself is still learning from the environment, while the knowledge distilled from teacher can already be used to train a reasonable student network. Moreover, we see that the hybrid of two types of interactions in CDRL has a positive e ect on the training, instead of causing performance deterioration.In the second experiment, the student network is learning from both the online knowledge distillation and the GAE updates from the environment. We nd that the convergence is much faster than the state-of-art, as shown in Figure 3 (b). In this experiment, the knowledge is distilled from the teacher to student in the rst one million steps and the distillation is stopped a er that. We note that in homogeneous CDRL, knowledge distillation is used directly with policy logits other than distillation logits.e knowledge transfer se ing in this experiment is not a practical one because we already have a well-trained model of P , but it shows that when knowledge is correctly transferred, the combination of online knowledge distillation and the GAE updates is an e ective training procedure. Collaborative Deep Reinforcement LearningIn previous experiments, we assume that there is a well-trained P expert, and we transfer knowledge from the P expert to the B student via deep knowledge distillation. A more challenging se ings that both of B and P are trained from scratch. In this experiment, we we show that the CDRL framework can still be e ective in this se ing. In this experiment, we train a B network and a P network from scratch using the proposed cA3C algorithm. e P agents are trained with GAE interactions only, and the target B receive supervision from both GAE interactions and distilled knowledge from P via a deep alignment network. We start to train the deep alignment network a er 3 million steps, and perform deep knowledge distillation a er 4 million steps, where the P agents are still updating from the environment. We note that in this se ing, the teacher network is constantly being updated, as knowledge is distilled from the teacher until 15 million steps. Results in Figure 6 (d) show that the proposed cA3C is able to converge to a higher performance than the current state-of-art. e reward of last one hundred episodes  of A3C is 61.48 ± 1.48, while cA3C achieves 68.35 ± 1.32, with a signi cant reward improvement of 11.2%. (a) and (b) that the action distributions of P and B are very di erent. To resolve this, we distill the knowledge through an extra distillation layer as illustrated in Figure 2 (b). As such, the knowledge distilled from the certi cated heterogeneous task can be successfully transferred to the student network with improved performance a er the learning is complete. However, this leads to a much slower convergence than the baseline as shown in Figure 4 (b), because that it takes time to learn a good distillation layer to align the knowledge distilled from P to the current learning task. An interesting question is that, is it possible to have both improved performance and faster convergence? Deep knowledge distillation -O line training. To handle the heterogeneity between P and B , we rst verify the effectiveness of deep knowledge distillation with an o ine training procedure. e o ine training is split into two stages. In the rst stage, we train a deep alignment network with four fully connected layers using the Relu activation function. e training data are logits generated from an expert P network and B network. e rewards of the networks at convergence are 20 and 60 respectively. In stage 2, with the P teacher network and trained deep alignment network, we train a B student network from scratch. e student network is trained with both GAE interactions with its environment, and the distillation interactions from the teacher network and the deep alignment network. e results in Figure 6 (a) show that deep knowledge distillation can transfer knowledge from P to B both e ciently and e ectively. Deep knowledge distillation -Online training. A more practical se ing of CDRL is the online training, where we simultaneously train deep alignment network and conduct the online deep knowledge distillation. We use two online training strategies: 1) e training of deep alignment network starts a er 4 million steps, when the student B network can perform reasonably well, and the knowledge distillation starts a er 6 million steps. 2) e training of deep alignment network starts a er 0.1 million steps, and the knowledge distillation starts a er 1 million steps. Results are shown in Figure 6 (b) and (c) respectively.",no,,,,no,,,,
308,https://github.com/shengliu66/ELR,shengliu66_ELR.tar.gz,Early-Learning Regularization Prevents Memorization of Noisy Labels,@classmethod @property @abstractmethod,utils requests PIL mlflow warnings sys tqdm __future__ torchvision pathlib model datetime json collections torch os socket trainer importlib functools itertools typing sklearn argparse logger numpy abc data_loader math parse_config random logging operator base,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shengliu66_ELR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shengliu66_ELR.pdf,,,no,,no,,no,,,,
310,https://github.com/Zhenyu-LIAO/RMT4ELM,Zhenyu-LIAO_RMT4ELM.tar.gz,A Random Matrix Approach to Neural Networks,,math scipy sklearn numpy matplotlib time,math.PR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Zhenyu-LIAO_RMT4ELM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Zhenyu-LIAO_RMT4ELM.pdf,,,no,,,,no,,,,
313,https://github.com/OscarcarLi/PrototypeDL,OscarcarLi_PrototypeDL.tar.gz,Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions,,scipy tensorflow autoencoder_helpers data_preprocessing os __future__ numpy matplotlib time,cs.AI cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/OscarcarLi_PrototypeDL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\OscarcarLi_PrototypeDL.pdf,Interpretability  AutoEncoder,,no,,yes,,no,,,,
316,https://github.com/clips/gsoc2019_bias,clips_gsoc2019_bias.tar.gz,Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior,@property @abstractmethod,pandas csv sys bs4 threading keras tensorflow asyncio datetime src queue torch os mongoengine google matplotlib string scipy torchtext seaborn pickle typing sklearn gatherers joblib numpy time abc random logging aiohttp,cs.SI cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/clips_gsoc2019_bias.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\clips_gsoc2019_bias.pdf,,,no,,,,no,,,,
319,https://github.com/uber/causalml,uber_causalml.tar.gz,CausalML: Python Package for Causal Machine Learning,"@ignore_warnings(category=ConvergenceWarning) @abstractmethod @pytest.fixture(scope='module') @pytest.fixture @pytest.mark.parametrize('synthetic_data_func', [simulate_nuisance_and_easy_treatment, @property @staticmethod @abstractclassmethod @ignore_warnings(category=FutureWarning)",copy packaging pandas pstats warnings multiprocessing sys causalml tqdm __future__ lightgbm tensorflow Cython collections tests pydotplus cProfile statsmodels torch os pytest shap pygam matplotlib sphinx_rtd_theme importlib scipy seaborn sklearn argparse joblib xgboost numpy abc setuptools logging pyro,cs.CY cs.LG stat.CO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/uber_causalml.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\uber_causalml.pdf,Causal Inference,,no,,,,no,,,,
320,https://github.com/SeongjuLee/DCENet-PyTorch,SeongjuLee_DCENet-PyTorch.tar.gz,Exploring Dynamic Context for Multi-path Trajectory Prediction,@torch.no_grad(),utils math scipy loss sys json loader glob neptune argparse torch os numpy models matplotlib time,cs.CV cs.MA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/SeongjuLee_DCENet-PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\SeongjuLee_DCENet-PyTorch.pdf,Tanh Activation  Sigmoid Activation  Long Short-Term Memory,,no,,,,no,,,,
329,https://github.com/jjanicechen/GraphletCountEstimationCNN,jjanicechen_GraphletCountEstimationCNN.tar.gz,Graphlet Count Estimation via Convolutional Neural Networks,,utils keras tensorflow datetime sys networkx h5py sklearn argparse os NNModel numpy time,cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jjanicechen_GraphletCountEstimationCNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jjanicechen_GraphletCountEstimationCNN.pdf,,"|1 Introduction. Graphlets are defined as k-node connected induced subgraph patterns. For an undirected graph, 3-node graphlets include close triangle ( ) and open triangle ( ). When k = 4, there are six different types of graphlets, e.g., tailed-triangle ( ) and clique ( ) are two possible 4-node graphlets. The number of each graphlet, called graphlet count, is a signature which characterizes the local network structure of a given graph. Graphlet count plays a prominent role in network analysis of many fields, most notably bioinformatics [4] and social science [3].However, enumerating exact graphlet count is inherently difficult and computational expensive because the number of graphlets grows exponentially large as the graph size and/or graphlet size k grow [3]. To deal with this difficulty, many sampling methods were proposed for estimating graphlet count with bounded error [2,3,5]. Nevertheless, these methods require large number of samples to be statistically reliable, which is still computationally demanding. Moreover, they have to repeat laborious counting procedure even if a new graph is similar or exactly the same as previous studied graphs.Intuitively, learning from historic graphs can make estimation more accurate and avoid many repetitive counting to reduce computational cost. Based on this idea, we propose a convolutional neural network (CNN) framework and two preprocessing techniques to estimate graphlet count. 3 Extensive experiments on two types of random graphs and real world biochemistry graphs show that our framework can offer substantial speedup on estimating graphlet count of new graphs with high accuracy.2 Method. Given a set of undirected graphs and a particular type of k-node graphlet, our objective is to develop a CNN which will be trained using part of dataset with known graphlet counts. After training, the CNN can quickly and accurately predict graphlet counts of other unseen graph samples in the set. Our framework takes the graph adjacency matrix as input and outputs the graphlet count of the input graph. Let us define some notations for our CNN. Let O (l) ∈ R N (l) ×N (l) ×C (l) be the output tensor at layer l, where l = 0, 1, 2, 3, N (l) denotes the width (and height) along each channel and C (l) denotes the channel size. Let O (l) i, j,t be the (i, j) th element along the t th channel. We assign O (0) as the graph adjacency matrix. Mathematically, our CNN structure can be described as follows:O (l) i, j,t = ReLU(W (l) t • O (l−1) [i : i + H (l) − 1, j : j + H (l) − 1, : ] + b (l) t ), l = 1, 2, (1) O (3) = ReLU(Flatten(O (2) ) T W (3) + b (3) ).(2) * Both authors contributed equally to this work 3 Our code is accessible at https://github.com/jjanicechen/GraphletCountEstimationCNN.git arXiv:1810.03078v1 [cs.LG] 7 Oct 2018Equation (1) corresponds to two convolution layers. Each layer applies C (l+1) filters over the input feature map O (l−1) , and the t th filter is parameterized by a trainable 3D weight tensorW (l) t ∈ R H (l) ×H (l) ×C (l), where H (l) denotes the width (and height) of the filter. [a : b, c : d, :] is a slicing function which extracts subset of elements indexing from a to b in width, c to d in height and all in channel to form a new tensor. • is the sum of element wise product of two tensors. After adding bias term b (l) t , we apply ReLU (max(0, x)) as the activation function to obtain the output feature map O (l) . Equation ( 2) is associated with the fully connected layer. It flattens the output O (2) into a column vector, applies W (3) , b (3) and ReLU to obtain the estimated graphlet count. Finally, our CNN is trained with back propagation and mean squared error as the loss function.The above CNN structure inherits the learning power for local structural information of graphs. However, we still need to address the following challenges: (1) The input adjacency matrix is not consistent because graphs in the training set may have different sizes. (2) In practice, real world network dataset may not contain sufficient amount of graph samples for training, which will cause overfitting problem. To address these challenges, we introduce two preprocessing techniques: Adjacency Matrix Zero Padding. To preserve edge connectivity information of all training graphs, we consider the largest graph in the training set, and use its dimension (say N) as the dimension of the input adjacency matrix (N × N). For other graphs in the training set, we take each adjacency matrix and pad it with zero till we have an input matrix of dimension N × N. This solves the varying input size problem. Swapping Augmentation. To acquire sufficient data for training, we take advantage of the graph isomorphism property, where a graph can be expressed by different input adjacency matrices having the same underlying network structure. Our approach is to randomly pick indices i and j, then swap the i th row with j th row and i th column with j th column of the adjacency matrix. We can repeat the swapping operation for each graph m times to create m more training data. Analogous to flipping or rotation of images, we improve CNN's generalization ability and thus improve the accuracy of our model.3 Data and Metric. Here, we introduce our testing datasets, benchmarking works, and evaluation metrics.Random Graph. We synthesize datasets with two random graph models: random geometric graph (RGG) and Erdos-Renyi (ER) graph. A RGG is constructed by placing nodes uniformly at random in a unit cube and connecting two nodes by an edge if and only if their distance is within a given radius r. In a ER graph, the edge between every two nodes exists with probability p. In each synthetic dataset, we have 3000 training graphs, 300 validation graphs, and 300 testing graphs.Empirical Network. We test on three real world biochemistry datasets: MUTAG [6], NCI1 and NCI109 [7]. MUTAG dataset contains 188 mutagenic compound graphs. NCI1 and NCI109 each has 4110 and 4127 chemical compound graphs tested on lung and ovarian cancer cells respectively. For MUTAG, we use swapping augmentation to increase the number of training samples. We also apply adjacency matrix zero padding to make all graphs in each dataset have the same size.Benchmark. We compare CNN with three existing frameworks: GRAFT [5], CC2 [2], GUISE [1], which are based on edge sampling, color coding, and Markov Chain Monte Carlo method respectively. Relative Error. Let c i be the ground truth graphlet count of sample graph i, c i be its estimated count, and there are S samples in the dataset. We compute the mean absolute error of the estimations, mae = Σ S i=1 ||c i − c i ||/S, and mean of ground truth counts, µ = Σ S i=1 c i /S. We take relative error as e = mae/µ . Speed. To ensure a fair comparison, we do not choose running time as the performance metric since it highly depends on hardware and implementation (e.g. running on GPU/CPU). Instead, we measure the number of arithmetic operations they use. For CNN model, we compute the number of floating-point operations (FLOPs). For benchmarking works, we calculate the number of comparison operations in the algorithms.4 Result. We test our framework on random graph datasets. For approximating 4clique counts, our CNN model achieves less than 8% relative error on 50-node RGGs with radius 0.45 and less than 5% relative error on 50-node ER graphs with edge existing probability 0.5. We also train our CNN models for estimating 4-path ( ), 3-star ( ), 5-path ( ) on the empirical biochemistry datasets. The relative errors on all three datasets are less than 20% of the ground truth counts. For estimating 4-path on MUTAG dataset, our model performs especially well making only 6% relative error. To compare the speed of our CNN with existing methods, the number of arithmetic operations used are calculated. For a fair comparison, we tune the number of iterations for all benchmarking sampling methods, so that they obtain as close relative errors to that of CNN as possible. Figure 1 (a, c) shows that the numbers of arithmetic operations used by GRAFT, CC2, or GUISE are significantly more than that used by CNN. This result demonstrates that our CNN based graphlet count estimation approach offers remarkable speedup on predicting graphlet counts while still maintaining high accuracy.Fig. 1 .1Fig. 1. Comparison of the number of arithmetic operations used for estimating 4-clique counts, tailed-triangle counts on 50-node ER graphs with edge existing probability 0.5. (a, c) The number of operations used by each framework. (b, d) The relative error each framework makes.|",no,,,,no,,,,
334,https://github.com/NVlabs/GA3C,NVlabs_GA3C.tar.gz,REINFORCEMENT LEARNING THROUGH ASYN-CHRONOUS ADVANTAGE ACTOR-CRITIC ON A GPU,@staticmethod,ThreadDynamicAdjustment Server NetworkVP warnings sys multiprocessing GameManager ProcessStats gym threading Config re tensorflow datetime ThreadPredictor queue os scipy ProcessAgent numpy Queue time Environment Experience ThreadTrainer,cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/NVlabs_GA3C.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\NVlabs_GA3C.pdf,Entropy Regularization  Convolution  Dense Connections  Softmax  A3C,,no,,,,no,,,,
337,https://github.com/scope-lab-vu/deep-nn-car,scope-lab-vu_deep-nn-car.tar.gz,A Component-Based Simplex Architecture for High-Assurance Cyber-Physical Systems,@j.event @staticmethod,pigpio imageprocess1 csv sys RLCar signal glob psutil base64 __future__ ctypes ast Client plotTool threading pathlib HelperFunctions keras tensorflow PluggableFeatures atexit model datetime json collections queue RPi IO pydrive os socket matplotlib Peripherals Pin_Controller imageprocess itertools CPU pyformulas pickle sklearn preprocess pyglet numpy pynput time shutil math CommunicationProtocol SafetyManager cv2 logging random operator Webcam zmq video2image DaveIIModel SafetyManagerAutonomousClient,cs.SY cs.OH cs.DC cs.PF cs.AI cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/scope-lab-vu_deep-nn-car.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\scope-lab-vu_deep-nn-car.pdf,,,no,,,,no,,,,
340,https://github.com/larksq/lane_detector_for_KITTI,larksq_lane_detector_for_KITTI.tar.gz,A Perception Centred Self-driving System without HD Maps,@property @staticmethod,"math scipy pandas os, shutil cv2 random sklearn numpy time cubic",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/larksq_lane_detector_for_KITTI.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\larksq_lane_detector_for_KITTI.pdf,Greedy Policy Search,,no,,,,no,,,,
348,https://github.com/Player514/BachelorThesis,Player514_BachelorThesis.tar.gz,SinGAN: Learning a Generative Model from a Single Natural Image,,PIL imageio sys skimage tqdm __future__ torchvision pathlib torch os matplotlib scipy inception pickle sklearn argparse SinGAN numpy config math cv2 random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Player514_BachelorThesis.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Player514_BachelorThesis.pdf,Convolution  Generative Adversarial Network,,no,,,,no,,,,
352,https://github.com/KaosEngineer/structured-uncertainty,KaosEngineer_structured-uncertainty.tar.gz,Scaling Ensemble Distribution Distillation to Many Classes with Proxy Targets,"@register_optimizer('adagrad') @register_model_architecture(""asr_vggtransformer_encoder"", ""vggtransformer_enc_1"") @register_optimizer('sgd') @register_criterion('dirichlet_mediator_distillation') @register_optimizer('adadelta') @register_model_architecture('dirichlet_transformer', 'dirichlet_transformer_wmt_en_de_big') @register_lr_scheduler('reduce_lr_on_plateau') @lru_cache() @register_model_architecture('transformer', 'transformer_iwslt_de_en') @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_2"") @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('roberta', 'roberta_base') @register_model(""asr_w2l_conv_glu_encoder"") @register_model('xlmr') @register_model_architecture('roberta', 'roberta') @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_de_big') @register_optimizer('adamax') @register_criterion('masked_lm') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_criterion(""cross_entropy_acc"") @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_model('camembert') @register_model_architecture('masked_lm', 'masked_lm') @register_task('sentence_ranking') @register_model(""asr_vggtransformer_encoder"") @register_model_architecture('transformer_lm', 'transformer_lm') @include_dirs.setter @register_tokenizer('space') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_task('multilingual_masked_lm') @register_optimizer('lamb') @register_model_architecture('transformer', 'transformer_wmt_en_de') @register_task('denoising') @register_model('transformer_lm') @register_task('translation_moe') @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_bpe('bert') @register_model(""asr_vggtransformer"") @register_model_architecture('masked_lm', 'bert_large') @register_model(""iterative_nonautoregressive_transformer"") @register_task(""language_modeling"") @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @register_criterion('binary_cross_entropy') @register_model_architecture('transformer', 'transformer_wmt_en_de_big_t2t') @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_1"") @register_tokenizer('moses') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_criterion('sentence_prediction') @register_bpe('subword_nmt') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_model_architecture('transformer_lm', 'transformer_lm_big') @register_task('audio_pretraining') @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @register_task('commonsense_qa') @register_criterion('reverse_kl_mean_distillation') @register_criterion('rkl_dirichlet_mediator_distillation') @register_model_architecture('dirichlet_vggtransformer', 'dirichlet_vggtransformer_2') @register_criterion('label_smoothed_cross_entropy_with_alignment') @s3_request @ensemble_encoder @register_task('cross_lingual_lm') @register_model('lightconv') @register_model_architecture(""levenshtein_transformer"", ""levenshtein_transformer"") @register_bpe('fastbpe') @register_model('transformer') @register_bpe('gpt2') @register_model_architecture('transformer', 'transformer') @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @register_task('multilingual_translation') @register_model_architecture('transformer_align', 'transformer_wmt_en_de_big_align') @unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"") @lru_cache(maxsize=1) @register_task('winogrande') @register_model_architecture('roberta', 'roberta_large') @register_model('fconv_lm') @register_criterion(""asg_loss"") @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer"") @register_criterion(""ctc_loss"") @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_base"") @register_criterion('mean_reverse_kl_distillation') @register_model(""insertion_transformer"") @register_model_architecture('bart', 'bart_large') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model('wav2vec') @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer_wmt_en_de"") @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture(""insertion_transformer"", ""insertion_transformer"") @register_model_architecture('transformer', 'transformer_wmt_en_de_big') @register_task(""speech_recognition"") @register_criterion('sentence_ranking') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_optimizer('nag') @property @register_model('roberta') @register_criterion('adaptive_loss') @register_model_architecture('wav2vec', 'wav2vec') @ensemble_decoder @register_model_architecture('roberta', 'xlm') @register_criterion('cross_entropy') @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU') @register_model('dirichlet_vggtransformer') @register_model_architecture('transformer_align', 'transformer_align') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_criterion('forward_kl_mean_distillation') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @register_model_architecture(""nacrf_transformer"", ""nacrf_transformer"") @wraps(func) @register_criterion('legacy_masked_lm_loss') @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @register_model('masked_lm') @register_task('wsc') @register_bpe('sentencepiece') @contextlib.contextmanager @register_task('legacy_masked_lm') @register_model_architecture('fconv', 'fconv') @register_task('sentence_prediction') @register_task('translation_lev') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_model_architecture('lightconv', 'lightconv') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_model('lightconv_lm') @register_model('lstm') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @torch.jit.script @register_task('masked_lm') @register_model_architecture(""asr_w2l_conv_glu_encoder"", ""w2l_conv_glu_enc"") @register_criterion('wsc') @register_lr_scheduler('triangular') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_optimizer('adam') @register_tokenizer('nltk') @register_criterion('mean_forward_kl_distillation') @register_model(""nacrf_transformer"") @register_lr_scheduler('tri_stage') @lru_cache(maxsize=8) @register_model(""nonautoregressive_transformer"") @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @staticmethod @register_criterion('sequence_distribution_distillation') @register_criterion('winogrande') @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_model(""levenshtein_transformer"") @register_model('bart') @register_model('fconv') @register_model('dirichlet_transformer') @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @contextmanager @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_fr_big') @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_criterion(""nat_loss"") @register_task('asr_distillation') @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_model('transformer_align') @register_criterion('label_smoothed_cross_entropy') @register_model(""cmlm_transformer"") @register_task('translation') @register_task('distillation') @register_model_architecture('fconv', 'fconv_iwslt_de_en')",copy warnings multiprocessing types ctypes en_core_web_lg re h5py torch importlib spacy pickle boto3 numbers tensorboardX numpy sentencepiece tarfile traceback random inspect requests rerank_score_lm generate pdb validate __future__ nirvana_dl examples pathlib unittest botocore tests fnmatch nltk rerank_utils os socket interactive gzip functools xml hashlib time shutil operator subprocess sys regex tqdm cython enum tempfile lightconv_cuda matplotlib eval_lm struct scipy fastBPE itertools pytorch_transformers seaborn typing subword_nmt sklearn urlparse preprocess joblib rerank train setuptools sacrebleu io fvcore scripts contextlib sacremoses glob fairseq rerank_generate fileinput soundfile uuid wav2letter json collections pyarrow concurrent rerank_score_bw apex optparse bisect builtins urllib palaas argparse math torchaudio logging dynamicconv_cuda,cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/KaosEngineer_structured-uncertainty.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\KaosEngineer_structured-uncertainty.pdf,,,no,,,,no,,,,
355,https://github.com/lizhemin15/self-distillation,lizhemin15_self-distillation.tar.gz,DISTILLATION ≈ EARLY STOPPING? HARVESTING DARK KNOWLEDGE UTILIZING ANISOTROPIC INFORMATION RETRIEVAL FOR OVERPARAMETERIZED NEURAL NETWORK,@staticmethod,PIL new frequency sys tqdm __future__ torchvision cross_entropy model torch os matplotlib six hashlib pickle argparse numpy cosine_optim tarfile errno cPickle,cs.LG stat.ML stat.ML cs.CV cs.IT cs.LG math.IT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lizhemin15_self-distillation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lizhemin15_self-distillation.pdf,Early Stopping,"No Early Stopping, No Dark KnowledgeAs mentioned in the introduction, an overparameterized teacher network is able to extract dark knowledge from the on-hot hard labels because of early stopping. In this section we present an experiment to verify this effect of early stopping, where we use a big model as a teacher to teach a smaller model as the student.In this experiment, we train a WRN-28 [50] on CIFAR100 [23] as the teacher model, and a 5-layers CNN as the student.The experimental details are in the supplementary material. The teacher model was trained by 40, 80, 120, 160 and 200 epochs respectively. The results of knowledge distillation by the student model is shown in Fig. 1.As we can see, the teacher model does not suffer from overfitting during the training, while it does not always educate a good student model either. As suggested by [45], a more tolerant teacher educates better students. The teacher model trained with 80 epochs produces the best result which indicates that early stopping of the bigger model can extract more informative information for distillation.",no,,,,no,,,,
358,https://github.com/thu-ml/zhusuan,thu-ml_zhusuan.tar.gz,ZhuSuan: A Library for Bayesian Deep Learning *,"@zs.reuse_variables(scope=""proposal"") @zs.meta_bayesian_net(scope=""pmf"", reuse_variables=True) @zs.reuse_variables(scope=""q_net"") @zs.reuse_variables(scope=""qz_xy"") @meta_bayesian_net(scope='scp', reuse_variables=True) @zs.reuse_variables(""classifier"") @add_name_scope @log_joint.setter @zs.reuse_variables(scope=""qy_x"") @wraps(self.mthd, assigned=('__name__', '__module__')) @zs.reuse_variables(scope=""disc"") @property @zs.reuse_variables(scope=""gen"") @staticmethod @zs.meta_bayesian_net(scope=""bnn"", reuse_variables=True) @reuse_variables(""test"") @zs.reuse_variables(scope=""variational"") @zs.meta_bayesian_net(scope=""sbn"", reuse_variables=True) @zs.meta_bayesian_net() @contextmanager @zs.meta_bayesian_net(scope='lntm') @wraps(f) @meta_bayesian_net(scope='scp', reuse_variables=False) @classmethod @add_arg_scope @meta_bayesian_net(reuse_variables=True) @zs.meta_bayesian_net(scope=""model"") @zs.meta_bayesian_net(scope='model', reuse_variables=True) @meta_bayesian_net() @zs.meta_bayesian_net(scope=""gen"", reuse_variables=True) @zs.meta_bayesian_net(scope=""model"", reuse_variables=True)",copy contextlib warnings sys skimage __future__ docutils examples re tensorflow collections tests os sphinx matplotlib sphinx_rtd_theme six progressbar scipy gzip itertools zhusuan functools codecs argparse numpy time tarfile math setuptools mock zipfile,stat.ML cs.AI cs.LG cs.NE stat.CO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thu-ml_zhusuan.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thu-ml_zhusuan.pdf,,,no,,,,no,,,,
360,https://github.com/derekgreene/topic-ensemble,derekgreene_topic-ensemble.tar.gz,Stability of Topic Modeling via Matrix Factorization,@staticmethod,"os, sys scipy csv os, os prettytable optparse math, string os, sys, random, operator logging os, sys, random sklearn nltk codecs, os, os random unsupervised numpy text",cs.IR cs.CL cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/derekgreene_topic-ensemble.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\derekgreene_topic-ensemble.pdf,,,no,,,,no,,,,
361,https://github.com/llmpass/medianDenoise,llmpass_medianDenoise.tar.gz,Convolutional Neural Network with Median Layers for Denoising Salt-and-Pepper Contaminations,,math keras tensorflow model sys cv2 random skimage os numpy,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/llmpass_medianDenoise.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\llmpass_medianDenoise.pdf,,"A. Training and testing setupFor fair comparisons, we train all models with the same data set described in [12] that contains 91 different images, which is also employed in other works [4]. Since our network is a fully convolutional network, the input size can be arbitrary. We first resize these 91 images to 200 × 200 and then we generate 70 × 70 patches from them as clean images. We degrade each patch by the s&p noise with levels from 10% to 90% with a step equals to 10% as a sequence of noisy images. The    models are trained to learn a series of weights in layers that can transfer the input noisy image to the clean image.To quantitatively compare the performance of different methods, we perform denoising on 3 sets of the images. The first set of image consist of some classical images in the image processing field (also used in [4]); the second set is BSD300 [13] (https://www2.eecs.berkeley.edu/ Research/Projects/CS/vision/grouping/segbench/BSDS300/ html/dataset/images.html); the third set is Kodak Image Dataset (http://r0k.us/graphics/kodak/), which has been widely used as the evaluation set [1], [2], [5].The metric considered in the comparison is Peak Signal to Noise Ratio (PSNR). It is defined byP SN R = 10 log 10 ( 255 2 M SE ),(2)where M SE is the mean-squared error between two M × N 8-bit images I 1 and I 2 , defined byM SE = M,N [I 1 (m, n) − I 2 (m, n)] 2 M × N .(3)",no,,,,no,,,,
362,https://github.com/s-mohammad-hashemi/repo,s-mohammad-hashemi_repo.tar.gz,Enhancing Robustness Against Adversarial Examples in Network Intrusion Detection Systems,@tf.function,tensorflow pandas sys os numpy time,cs.CR cs.LG cs.NI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/s-mohammad-hashemi_repo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\s-mohammad-hashemi_repo.pdf,,,no,,,,no,,,,
366,https://github.com/qvdang-dev/Facial-Recognition,qvdang-dev_Facial-Recognition.tar.gz,Max-Margin Object Detection,,utilities PIL tensorflow cv2 face_detection_utilities pickle sklearn argparse os imutils numpy dlib matplotlib,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/qvdang-dev_Facial-Recognition.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\qvdang-dev_Facial-Recognition.pdf,,"FDDBFinally, we evaluate our method on the Face Detection Data Set and Benchmark (FDDB) challenge. This challenging dataset contains images of human faces in multiple poses captured in indoor and outdoor settings. To test MMOD, we used it to learn a basic HOG sliding window classifier. Therefore the feature extractor (φ) takes in a window and outputs a HOG vector describing the entire window as was done in Dalal and Triggs's seminal paper [3]. To illustrate the learned model, the HOG filter resulting from the first FDDB fold is visualized in Figure 4. During learning, the parameters were set as follows: C = 50, ε = 0.01C, L f a = 1, and L miss = 1. We also upsampled each image by a factor of two so that smaller faces could be detected. Since our HOG filter box is 80x80 pixels in size this upsampling allows us to detect images that are larger than about 40x40 pixels in size. Additionally, we mirrored the dataset, effectively doubling the number of training images. This leads to training sizes of about 5000 images per fold and our optimizer requires approximately 25 minutes per fold.To perform detection, this single HOG filter is scanned over the image at each level of an image pyramid and any windows which pass a threshold test are output after nonmax suppression is performed. A ROC curve that compares this learned HOG filter against other methods is created by sweeping this threshold and can be seen in Figure 5. To create the ROC curve we followed the FDDB evaluation protocol of performing 10 fold cross-validation and combining the results in a single ROC curve using the provided FDDB evaluation software. Example images with detection outputs are also shown in Figure 6.In Figure 5 we see that the HOG filter learned via MMOD substantially outperforms a HOG filter learned with the typical linear SVM ""hard negative mining"" approach [12] as well as the classic Viola Jones method [19]. Moreover, our single HOG filter learned via MMOD gives a slightly better accuracy than the complex deformable part model of Zhu [22].",no,,,,no,,,,
367,https://github.com/mjsumpter/remod,mjsumpter_remod.tar.gz,REMOD: Relation Extraction for Modeling Online Discourse,,utils requests pandas csv sys tqdm classify gensim pathlib keras tensorflow re datetime json collections h5py node2vec enum os preproc matplotlib seaborn features pickle typing sklearn argparse joblib numpy config nodevectors shutil math rdflib networkx logging ratelimiter,cs.SI cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mjsumpter_remod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mjsumpter_remod.pdf,,,no,,,,no,,,,
368,https://github.com/sisl/latent_driver,sisl_latent_driver.tar.gz,Simultaneous Policy Learning and Latent State Inference for Imitating Driver Behavior,,"os, sys utils math bc_policy tensorflow collections h5py random vae argparse os dataloader numpy matplotlib time cPickle",cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sisl_latent_driver.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sisl_latent_driver.pdf,,,no,,,,no,,,,
372,https://github.com/helibenhamu/multichart3dgans,helibenhamu_multichart3dgans.tar.gz,Multi-chart Generative Surface Modeling,@staticmethod,utils data_loader scipy tensorflow bunch trainers multiprocessing json glob random layers tqdm argparse os numpy models shutil,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/helibenhamu_multichart3dgans.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\helibenhamu_multichart3dgans.pdf,,,no,,,,no,,,,
375,https://github.com/justinbellucci/cnn-visualizations-pytorch,justinbellucci_cnn-visualizations-pytorch.tar.gz,How convolutional neural networks see the world -- A survey of convolutional neural network visualization methods,,PIL helper_functions torch os numpy matplotlib,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/justinbellucci_cnn-visualizations-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\justinbellucci_cnn-visualizations-pytorch.pdf,Network Dissection  Interpretability  VGG-16  Dropout  Step Decay  Weight Decay  SGD with Momentum  Xavier Initialization  Color Jitter  Random Horizontal Flip  Random Resized Crop  Dense Connections  Rectified Linear Units  Max Pooling  Softmax  VGG  Convolution,,no,,,,no,,,,
377,https://github.com/google-research/si-score,google-research_si-score.tar.gz,"SI-SCORE: AN IMAGE DATASET FOR FINE-GRAINED ANALYSIS OF ROBUSTNESS TO OBJECT LOCATION, ROTATION AND SIZE",,PIL dataset_generator functools itertools tensorflow csv multiprocessing unittest operator label_str_to_imagenet_classes argparse os numpy time io,cs.CV cs.AI cs.LG cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/google-research_si-score.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\google-research_si-score.pdf,Contrastive Language-Image Pre-training,,no,,,,no,,,,
379,https://github.com/paolo-f/bcfind,paolo-f_bcfind.tar.gz,Cell identification in whole-brain multiview images of neural activation maps,@wraps(fun) @multi_kapur_timer.timed @ms_timer.timed @save_vaa3d_timer.timed @wraps(SubsetIterator.next) @mean_shift_timer.timed @functools.wraps(Dataset.iterator) @wraps(Layer.get_monitoring_channels) @patch_ms_timer.timed @deconvolver_timer.timed @pca_analysis_timer.timed @save_markers_timer.timed,"PIL copy parameters pandas warnings sys glob ujson sys,os,string,copy __future__ platform distutils pylab results_table banyan re sys, os uuid os,sys datetime collections json Cython volume manifold os matplotlib string progressbar plistlib scipy functools itertools tables pylearn2 SurfaceCleaner sklearn argparse mahotas pyparsing numpy Queue time clsm_registration timeit math bcfind setuptools traceback os,sys,math networkx errno theano operator random subprocess cPickle",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/paolo-f_bcfind.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\paolo-f_bcfind.pdf,,,no,,,,no,,,,
382,https://github.com/joffery/GC-GAN,joffery_GC-GAN.tar.gz,Geometry-Contrastive GAN for Facial Expression Transfer,@staticmethod,utils scipy tensorflow re vggface ops errno random pickle vaegan os numpy,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/joffery_GC-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\joffery_GC-GAN.pdf,,,no,,,,no,,,,
383,https://github.com/ZhenYangIACAS/unsupervised-NMT,ZhenYangIACAS_unsupervised-NMT.tar.gz,Unsupervised Neural Machine Translation with Weight Sharing,@function.Defun( @property @staticmethod @function.Defun(compiled=True),utils yaml sys tensor2tensor __future__ cnn_discriminator tensorflow re evaluate model collections tempfile os path six gzip itertools codecs argparse numpy cnn_text_discriminator time math share_function random logging operator subprocess data_iterator cPickle,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZhenYangIACAS_unsupervised-NMT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZhenYangIACAS_unsupervised-NMT.pdf,Convolution  Generative Adversarial Network,"|Unsupervised TrainingBased on the architecture proposed above, we train the NMT model with the monolingual corpora only using the following four strategies: Denoising auto-encoding Firstly, we train the two AEs to reconstruct their inputs respectively. In this form, each encoder should learn to compose the embeddings of its corresponding language and each decoder is expected to learn to decompose this representation into its corresponding language. Nevertheless, without any constraint, the AE quickly learns to merely copy every word one by one, without capturing any internal structure of the language involved. To address this problem, we utilize the same strategy of denoising AE (Vincent et al., 2008) and add some noise to the input sentences (Hill et al., 2016;Artetxe et al., 2017b). To this end, we shuffle the input sentences randomly. Specifically, we apply a random permutation ε to the input sentence, verifying the condition:||ε(i) − i|| ≤ min(k([ steps s ] + 1), n), ∀i ∈ {1, n}(5) where n is the length of the input sentence, steps is the global steps the model has been updated, k and s are the tunable parameters which can be set by users beforehand. This way, the system needs to learn some useful structure of the involved languages to be able to recover the correct word order. In practice, we set k = 2 and s = 100000.Back-translation In spite of denoising autoencoding, the training procedure still involves a single language at each time, without considering our final goal of mapping an input sentence from the source/target language to the target/source language. For the cross language training, we utilize the back-translation approach for our unsupervised training procedure. Back-translation has shown its great effectiveness on improving NMT model with monolingual data and has been widely investigated by (Sennrich et al., 2015a;Zhang and Zong, 2016). In our approach, given an input sentence in a given language, we apply the corresponding encoder and the decoder of the other language to translate it to the other language 3 . By combining the translation with its original sentence, we get a pseudo-parallel corpus which is utilized to train the model to reconstruct the original sentence from its translation.Local GAN Although the weight sharing constraint is vital for the shared-latent space assumption, it alone does not guarantee that the corresponding sentences in two languages will have the same or similar latent code. To further enforce the shared-latent space, we train a discriminative neural network, referred to as the local discriminator, to classify between the encoding of source sentences and the encoding of target sentences. The local discriminator, implemented as a multilayer perceptron with two hidden layers of size 256, takes the output of the encoder, i.e., H r calculated as equation 3, as input, and produces a binary prediction about the language of the input sentence. The local discriminator is trained to predict the language by minimizing the following crossentropy loss:L D l (θ D l ) = − E x∈xs [log p(f = s||Enc s (x))] − E x∈xt [log p(f = t||Enc t (x))] (6)where θ D l represents the parameters of the local discriminator and f ∈ {s, t}. The encoders are trained to fool the local discriminator:L Encs (θ Encs ) = − E x∈xs [log p(f = t||Enc s (x))] (7) L Enct (θ Enct ) = − E x∈xt [log p(f = s||Enc t (x))](8)where θ Encs and θ Enct are the parameters of the two encoders.Global GAN We apply the global GANs to fine tune the whole model so that the model is able to generate sentences undistinguishable from the true data, i.e., sentences in the training corpus. Different from the local GANs which updates the parameters of the encoders locally, the global GANs are utilized to update the whole parameters of the proposed model, including the parameters of encoders and decoders. The proposed model has two global GANs: GAN g1 and GAN g2 . In GAN g1 , the Enc t and Dec s act as the generator, which generates the sentence xt 4 from x t . The D g1 , implemented based on CNN, assesses whether the generated sentence xt is the true target-language sentence or the generated sentence. The global discriminator aims to distinguish among the true sentences and generated sentences, and it is trained to minimize its classification error rate. During training, the D g1 feeds back its assessment to finetune the encoder Enc t and decoder Dec s . Since the machine translation is a sequence generation problem, following , we leverage policy gradient reinforcement training to back-propagate the assessment. We apply a similar processing to GAN g2 (The details about the architecture of the global discriminator and the training procedure of the global GANs can be seen in appendix B and C).There are two stages in the proposed unsupervised training. In the first stage, we train the proposed model with denoising auto-encoding, backtranslation and the local GANs, until no improvement is achieved on the development set. Specifically, we perform one batch of denoising autoencoding for the source and target languages, one batch of back-translation for the two languages, and another batch of local GAN for the two languages. In the second stage, we fine tune the proposed model with the global GANs.|",no,,yes,"word embedding, dropout, beam size, penalty",no,,,,
385,https://github.com/nimily/impute,nimily_impute.tar.gz,Fixed Point and Bregman Iterative Methods for Matrix Rank Minimization,"@pytest.fixture( @pytest.fixture(params=[ @pytest.mark.usefixtures('re_dot_case') @pytest.mark.usefixtures('op_info', 'trace_op_case') @pytest.mark.usefixtures('svt') @abc.abstractmethod @property @staticmethod @pytest.mark.usefixtures('nae_case') @pytest.mark.usefixtures('rae_case') @pytest.mark.usefixtures('low_rank_matrix') @pytest.mark.usefixtures('imputer_cls', 'rae_case', 'alpha_ratio')",math scipy functools setuptools collections impute typing sklearn pytest numpy abc,math.OC cs.IT math.IT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nimily_impute.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nimily_impute.pdf,,,no,,,,no,,,,
390,https://github.com/alicanb/vae_novel_examples,alicanb_vae_novel_examples.tar.gz,Can VAEs Generate Novel Examples?,@property @staticmethod,math scipy mlp itertools datasets seaborn plot_utils glob vae tqdm torch os argparse numbers numpy matplotlib torchvision,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alicanb_vae_novel_examples.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alicanb_vae_novel_examples.pdf,AutoEncoder  Variational Autoencoder,,no,,,,no,,,,
394,https://github.com/lihengbit/Pytorch1.4-WCT,lihengbit_Pytorch1.4-WCT.tar.gz,Universal Style Transfer via Feature Transforms,,util PIL scipy modelsNIPS torch os argparse __future__ numpy Loader time torchvision,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lihengbit_Pytorch1.4-WCT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lihengbit_Pytorch1.4-WCT.pdf,,,no,,,,no,,,,
397,https://github.com/tsattler/visuallocalizationbenchmark,tsattler_visuallocalizationbenchmark.tar.gz,Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis,,utils camera matchers sys types tqdm argparse torch os subprocess numpy sqlite3 shutil,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tsattler_visuallocalizationbenchmark.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tsattler_visuallocalizationbenchmark.pdf,,,no,,,,no,,,,
400,https://github.com/adriapr/crowdairway,adriapr_crowdairway.tar.gz,Crowdsourcing Airway Annotations in Chest Computed Tomography Images,,"math parse scipy pandas tables figures json, csv seaborn data skimage statsmodels os analysis sklearn numpy matplotlib zipfile",cs.CV cs.HC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/adriapr_crowdairway.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\adriapr_crowdairway.pdf,,,no,,,,no,,,,
407,https://github.com/HYOJINPARK/Reuse_VOS,HYOJINPARK_Reuse_VOS.tar.gz,Learning Dynamic Network Using a Reuse Gate Function in Semi-supervised Video Object Segmentation,@classmethod @property @staticmethod @functools.wraps(op),"PIL copy warnings sys sys, pathlib lib skimage tqdm easydict torchvision os, sys pathlib model json collections visdom torch os functools argparse numpy time shutil math cv2 random",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HYOJINPARK_Reuse_VOS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HYOJINPARK_Reuse_VOS.pdf,,,no,,,,no,,,,
410,https://github.com/MitraDP/Pancreas-CT-image-and-volumetric-semantic-segmentation,MitraDP_Pancreas-CT-image-and-volumetric-semantic-segmentation.tar.gz,"TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning",@staticmethod,volume_patch_composer PIL pandas collections albumentations random nibabel torchio torch tqdm os metrics numpy matplotlib,eess.IV cs.AI cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MitraDP_Pancreas-CT-image-and-volumetric-semantic-segmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MitraDP_Pancreas-CT-image-and-volumetric-semantic-segmentation.pdf,,,no,,,,no,,,,
415,https://github.com/zt95/infinite-horizon-off-policy-estimation,zt95_infinite-horizon-off-policy-estimation.tar.gz,Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation,,Density_ratio_continuous generate_network sys traci __future__ sumolib tensorflow environment Q_learning os scipy quadprog Density_Ratio_discrete optparse argparse numpy time Easy_agent random subprocess,cs.LG cs.AI cs.SY stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zt95_infinite-horizon-off-policy-estimation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zt95_infinite-horizon-off-policy-estimation.pdf,,"Figure 3 :3Figure 3: Results on Pendulum. (a)-(b) show the results in the average reward case when we vary the mixing ratio α in the behavior policies and the truncated length T , respectively. (c)-(d) show the results of the discounted reward case when we vary mixing ratio α in the behavior policies and discount factor γ, respectively. The default parameters are n = 150, T = 1000, γ = 0.99, α = 1. Figure 4 :4Figure 4: Results on SUMO (a) with average reward, as we vary the number of trajectories (b), choose different behavior policies (c), and truncated size (d). When being fixed, the default parameters are n = 250, T = 400. The behavior policy in (c) with x-tick 2 is used in (b) and (d).",no,,,,no,,,,
420,https://github.com/Panda-Peter/visda2019-multisource,Panda-Peter_visda2019-multisource.tar.gz,Multi-Source Domain Adaptation and Semi-Supervised Domain Adaptation with Focus on Visual Domain Adaptation Challenge 2019,@classmethod @staticmethod,PIL copy yaml samplers sys types lib tqdm easydict __future__ pprint ast torchvision re collections torch os trainer functools pickle argparse numbers numpy math optimizer datasets random cv2 logging losses evaluation models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Panda-Peter_visda2019-multisource.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Panda-Peter_visda2019-multisource.pdf,Batch Normalization  Residual Connection  PatchGAN  Rectified Linear Units  Tanh Activation  Residual Block  Instance Normalization  Convolution  Leaky ReLU  Sigmoid Activation  GAN Least Squares Loss  Cycle Consistency Loss  CycleGAN,,no,,,,no,,,,
424,https://github.com/submission2019/cnn-quantization,submission2019_cnn-quantization.tar.gz,Post training 4-bit quantization of convolutional networks for rapid-deployment,@abc.abstractclassmethod @property @staticmethod,"utils hyperdash copy pandas mlflow warnings sys tqdm torchvision os, sys pathlib re uuid datetime collections json enum torch os matplotlib six scipy int_quantization itertools pickle sklearn argparse tensorboardX numpy bokeh time shutil abc pytorch_quantizer math setuptools random logging models",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/submission2019_cnn-quantization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\submission2019_cnn-quantization.pdf,,,no,,,,no,,,,
425,https://github.com/dataplayer12/Fly-LSH,dataplayer12_Fly-LSH.tar.gz,Improving Similarity Search with High-dimensional Locality-sensitive Hashing,,"scipy functools tensorflow collections lshutils sklearn os numpy bokeh pickle, time",cs.LG cs.AI cs.DS stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dataplayer12_Fly-LSH.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dataplayer12_Fly-LSH.pdf,,,no,,,,no,,,,
426,https://github.com/ngonhi/mmdetection,ngonhi_mmdetection.tar.gz,MMDetection: Open MMLab Detection Toolbox and Benchmark,"@contextlib.asynccontextmanager @DETECTORS.register_module() @patch('mmdet.datasets.CocoDataset._filter_imgs', MagicMock) @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses')) @weighted_loss @force_fp32(apply_to=('x', 'y')) @mmcv.jit(coderize=True) @force_fp32(apply_to=('cls_scores', 'bbox_preds')) @pytest.mark.parametrize( @force_fp32(apply_to=('bbox_pred', )) @pytest.mark.parametrize('dataset', @patch('mmdet.datasets.CustomDataset._filter_imgs', MagicMock) @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'bbox_preds_refine')) @mmcv.jit(derivate=True, coderize=True) @POSITIONAL_ENCODING.register_module() @patch('mmdet.apis.single_gpu_test', MagicMock) @force_fp32(apply_to=('x', 'y'), out_fp16=True) @patch('mmdet.datasets.CocoDataset.load_annotations', MagicMock()) @force_fp32(apply_to=('mask_iou_pred', )) @BACKBONES.register_module() @mock.create_autospec @NECKS.register_module() @force_fp32(apply_to=('all_cls_scores_list', 'all_bbox_preds_list')) @auto_fp16(apply_to=('x', 'y'), out_fp32=True) @patch('mmdet.datasets.CustomDataset.load_annotations', MagicMock()) @TRANSFORMER_LAYER_SEQUENCE.register_module() @auto_fp16(apply_to=('x', 'y')) @HEADS.register_module() @pytest.mark.parametrize('EvalHookParam', (EvalHook, DistEvalHook)) @force_fp32(apply_to=('center_heatmap_preds', 'wh_preds', 'offset_preds')) @auto_fp16(apply_to=('x', )) @force_fp32(apply_to=('segm_pred', )) @pytest.mark.parametrize('num_sample', [0, 1, 2]) @auto_fp16(apply_to=('img', )) @BBOX_CODERS.register_module() @BBOX_SAMPLERS.register_module() @TRANSFORMER_LAYER.register_module() @auto_fp16() @PRIOR_GENERATORS.register_module() @patch('mmdet.datasets.CityscapesDataset._filter_imgs', MagicMock) @pytest.mark.parametrize('config_path', @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'iou_preds')) @PIPELINES.register_module() @LOSSES.register_module() @property @pytest.mark.parametrize('EvalHookCls', (EvalHook, DistEvalHook)) @BBOX_ASSIGNERS.register_module() @abstractmethod @SHARED_HEADS.register_module() @patch('mmdet.datasets.XMLDataset._filter_imgs', MagicMock) @patch('mmdet.datasets.XMLDataset.load_annotations', MagicMock()) @TRANSFORMER.register_module() @classmethod @pytest.mark.parametrize('classes, expected_length', [(['bus'], 2), @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'coeff_preds')) @force_fp32(apply_to=('bbox_preds', )) @contextlib.contextmanager @force_fp32(apply_to=('x', )) @force_fp32() @ROI_EXTRACTORS.register_module() @force_fp32(apply_to=('feats', ), out_fp16=True) @functools.wraps(loss_func) @pytest.mark.parametrize('arch_name,arch,out_channels', regnet_test_data) @staticmethod @patch('mmdet.datasets.CityscapesDataset.load_annotations', MagicMock()) @LINEAR_LAYERS.register_module(name='NormedLinear') @pytest.mark.skipif( @MATCH_COST.register_module() @HOOKS.register_module() @force_fp32(apply_to=('cls_score', 'bbox_pred')) @force_fp32(apply_to=('mask_pred', )) @force_fp32(apply_to=('pred', )) @force_fp32( @IOU_CALCULATORS.register_module() @patch('mmdet.apis.multi_gpu_test', MagicMock) @DATASETS.register_module() @CONV_LAYERS.register_module(name='NormedConv2d')",imagecorruptions imageio copy PIL yaml warnings sys multiprocessing contextlib glob model_archiver tools test_forward onnx xlutils __future__ base64 platform mmcv ts pathlib onnxsim re asyncio json collections unittest resource current tempfile torch os pytest tensorrt matplotlib six scipy functools itertools xml seaborn bisect xlrd terminaltables urllib pickle typing sklearn argparse cityscapesscripts mmdet gather_models lvis numpy asynctest time shutil abc math kwarray pycocotools setuptools albumentations cv2 logging random subprocess onnxruntime inspect,cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ngonhi_mmdetection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ngonhi_mmdetection.pdf,,,no,,,,no,,,,
448,https://github.com/vveitch/causal-embeddings,vveitch_causal-embeddings.tar.gz,Using Embeddings to Correct for Unobserved Confounding in Networks,@classmethod @property @staticmethod,"copy bert mkl_random pandas multiprocessing sys glob __future__ distutils re tensorflow model json collections sys, os, random, glob os six scipy functools spacy dataset sklearn argparse numbers numpy relational_erm math ScienceParse unicodedata setuptools re,io,json,sys networkx dateutil random data_cleaning io",stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vveitch_causal-embeddings.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vveitch_causal-embeddings.pdf,Causal Inference,,no,,,,no,,,,
453,https://github.com/ivalab/grasp_primitiveShape,ivalab_grasp_primitiveShape.tar.gz,Using Synthetic Data and Deep Networks to Recognize Primitive Shapes for Object Grasping,"@C2_FORMAT_LOADER.register(""R-50-FPN-RETINANET"") @registry.BACKBONES.register(""R-101-FPN"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""FBNet.roi_head_keypoints"") @amp.float_function @C2_FORMAT_LOADER.register(""R-50-C5"") @C2_FORMAT_LOADER.register(""R-101-FPN"") @registry.ROI_BOX_PREDICTOR.register(""FastRCNNPredictor"") @registry.BACKBONES.register(""R-50-C5"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPNXconv1fcFeatureExtractor"") @registry.ROI_BOX_PREDICTOR.register(""FPNPredictor"") @registry.ROI_KEYPOINT_PREDICTOR.register(""KeypointRCNNPredictor"") @registry.BACKBONES.register(""R-50-FPN"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPN2MLPFeatureExtractor"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""FBNet.roi_head_mask"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNC4Predictor"") @C2_FORMAT_LOADER.register(""R-101-C4"") @registry.BACKBONES.register(""R-101-C5"") @property @once_differentiable @staticmethod @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FBNet.roi_head"") @registry.BACKBONES.register(""R-101-C4"") @C2_FORMAT_LOADER.register(""R-50-C4"") @registry.RPN_HEADS.register(""FBNet.rpn_head"") @registry.BACKBONES.register(""R-101-FPN-RETINANET"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""KeypointRCNNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-101-FPN-RETINANET"") @registry.BACKBONES.register(""R-152-FPN"") @registry.BACKBONES.register(""R-50-C4"") @registry.BACKBONES.register(""FBNet"") @registry.BACKBONES.register(""R-50-FPN-RETINANET"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""MaskRCNNFPNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-152-FPN"") @registry.RPN_HEADS.register(""SingleConvRPNHead"") @C2_FORMAT_LOADER.register(""R-50-FPN"") @C2_FORMAT_LOADER.register(""R-101-C5"") @unittest.skipIf(not TEST_CUDA, ""no CUDA detected"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNConv1x1Predictor"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""ResNet50Conv5ROIFeatureExtractor"")","utils PIL copy sys multiprocessing glob pdb skimage tqdm __future__ ctypes platform torchvision os, sys csHelpers parse re robot model datetime json collections unittest h5py fnmatch tempfile env_tests robot_New os torch socket matplotlib importlib pycococreatortools struct scipy apex IPython xml itertools bisect imp pickle argparse simulation cityscapesscripts logger numpy time shutil math pycocotools setuptools errno maskrcnn_benchmark cv2 random logging subprocess zmq select yacs predictor_kinect",cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ivalab_grasp_primitiveShape.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ivalab_grasp_primitiveShape.pdf,,,no,,yes,"learning rate, iterations, mini batch size",no,,,,
454,https://github.com/collwe/JEDI-Crowd-Teaching,collwe_JEDI-Crowd-Teaching.tar.gz,Unlearn What You Have Learned: Adaptive Crowd Teaching with Exponentially Decayed Memory Learners,@classmethod,memory pandas sys glob rest_framework re datetime os matplotlib scipy hashlib teacher sklearn joblib captcha numpy random logging django,stat.ML cs.HC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/collwe_JEDI-Crowd-Teaching.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\collwe_JEDI-Crowd-Teaching.pdf,,,no,,,,no,,,,
457,https://github.com/tomMoral/adopty,tomMoral_adopty.tar.gz,Learning step sizes for unfolded sparse coding,"@pytest.mark.parametrize('reg', [.1, .3, .5, 2]) @pytest.mark.parametrize('n_layers', [1, 10, 50, 100]) @pytest.mark.parametrize('learn_th', [True, False]) @delayed @pytest.mark.parametrize('parametrization', PARAMETRIZATIONS) @mem.cache @pytest.mark.parametrize('reg', [.1, .3, .5])",functions copy pandas warnings sys skimage __future__ re adopty autograd torch pytest setup os matplotlib sphinx_bootstrap_theme importlib scipy functools itertools sklearn argparse joblib numpy numpydoc time generation setuptools,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tomMoral_adopty.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tomMoral_adopty.pdf,,"|C Proof of Section 4's LemmasC.1 Proof of Lemma 4.2Lemma 4.2 (Stability of solutions around D j ). Let D ∈ R n×m be a dictionary with non-duplicated unit-normed columns. Let c max l =j ||D l D j || < 1 . Then for all j ∈ 1, m and ε ∈ R m such that ε < λ(1 − c) and D j ε = 0 , the vector (1 − λ)e j minimizes F x for x = D j + ε .Proof. Let j ∈ 1, m and let ε ∈ R m ∩ D ⊥ j be a vector such that ε < λ(1 − c) . For notation simplicity, we denote z Proof. For simplicity of the notation, we will drop the x variable whenever possible, i.e. z * = z * (x) and φ θ (z) = φ θ (z, x) . We denote z (t) = Φ Θ (t) (x) the output of the network with t layers. Let > 0 . By hypothesis (i), there exists T 0 such that for all t ≥ T 0 ,* = z * (D j − ε) . D j (Dz * − D j − ε) = D j (−λD j − ε) = −λ = −λ sign z * j , (19) since 1 − λ > 0 . For the other coefficients l ∈ 1, m \ {j} , we have ||D l (Dz * − D j − ε)|| = ||D l (−λD j − ε)|| ,(20)= ||λD l D j + D l ε)|| ,(21)W (t) − W * ≤ ||α (t) − α * || ≤ ||β (t) − β * || ≤ . (34)By hypothesis (ii), , there exists T 1 such that for all t ≥ T 1 and all x ∈ B ∞ ,z (t) − z * ≤ .(35)Let x ∈ B ∞ be an input vector and t ≥ max(T 0 , T 1 ) . Using (35), we havez (t+1) − z (t) ≤ z (t+1) − z * + z (t) − z * ≤ 2(36)By (i), there exist a compact set K 1 ⊂ R n×m × R + * × R + * s.t. θ (t) ∈ K 1 for all t ∈ N and θ * ∈ K . The input x is taken in a compact set B ∞ and as z * = arg min z F x (z) , we have λ z 1 ≤ F x (z * ) ≤ F x (0) = x thus z * is also in a compact set K 2 . We consider the function f (z, x, θ) = ST(z − αW (Dz − x), β) on the compact set K 2 ×B ∞ ×K 1 . This function is continuous and piece-wise linear on a compact set. It is thus L-Lipschitz and thus φ θ (t) (z (t) ) − φ θ (t) (z * ) ≤ L z (t) − z * ≤ L (37)φ θ * (z * ) − φ θ (t) (z * ) ≤ L θ (t) − θ * ≤ L(38)Using these inequalities, we getφ θ * (z * , x) − z * ≤ φ θ * (z * ) − φ θ (t) (z * )<L by ( 38)+ φ θ (t) (z * ) − φ θ (t) (z (t) )<L by (37)(39)+ φ θ (t) (z (t) ) − z (t)<2 by (36)+ z (t) − z * < by ( 35)≤ (2L + 3) . (40)As this result holds for all > 0 and all x ∈ B ∞ , we have φ θ * (z * ) = z * for all x ∈ B ∞ . We can apply the Lemma 4.3 to conclude this proof. We report the average number of iterations taken to reach a point z such that F x (z) < F * x + 10 −13 . The experiment is repeated 10 times, starting from random points in B ∞ . OISTA is always faster than ISTA, and is faster than FISTA for high regularization. Figure 2 :2Figure 2: Convergence curves of OISTA, ISTA, and FISTA on a toy problem with n = 10 , m = 50 , λ = 0.5 . The bottom figure displays the (normalized) steps taken by OISTA at each iteration. Full experimental setup described in Appendix D. Figure 4 :4Figure4: Steps learned with a 20 layers SLISTA network on a 10 × 20 problem. For each layer t and each training sample x, we compute the support S(x, t) of z (t) (x). The brown curves display the quantiles of the distribution of 1/L S(x,t) for each layer t . Full experimental setup described in Appendix D. Figure 5 :5Figure 5: Illustration of Theorem 4.4: for deep layers of LISTA, we have α (t) W (t) − β (t) D F → 0 , indicating that the network ultimately only learns a step size. Full experimental setup described in Appendix D.|",no,,,,no,,,,
458,https://github.com/SwiftieH/GFAttack,SwiftieH_GFAttack.tar.gz,A Restricted Black-box Adversarial Framework Towards Attacking Graph Embedding Models,,math scipy tensorflow numba sklearn tqdm GF_Attack os argparse numpy time,cs.SI cs.CR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/SwiftieH_GFAttack.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\SwiftieH_GFAttack.pdf,,,no,,yes,"hyperparameter commonly set (window size, number of negative sampling in skip-gramas",no,,,,
461,https://github.com/benjaminalt/shadow-program-inversion,benjaminalt_shadow-program-inversion.tar.gz,Robot Program Parameter Inference via Differentiable Shadow Program Inversion,@abstractmethod @staticmethod,requests rtde multiprocessing sys tqdm threading re uuid py7zr datetime json pyrobolearn queue enum tempfile torch os socket shadow_program_inversion matplotlib pyquaternion scipy itertools tables roslibpy typing natsort argparse numpy time abc math setuptools random logging pytorch_utils,cs.RO cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/benjaminalt_shadow-program-inversion.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\benjaminalt_shadow-program-inversion.pdf,,,no,,,,no,,,,
465,https://github.com/sumitsk/MOGP-AL,sumitsk_MOGP-AL.tar.gz,Near-Optimal Active Learning of Multi-Output Gaussian Processes,@property,utils ipdb pandas arguments agent env argparse torch gpytorch numpy models matplotlib,stat.ML cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sumitsk_MOGP-AL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sumitsk_MOGP-AL.pdf,,,no,,,,no,,,,
466,https://github.com/mmalekzadeh/privacy-preserving-bandits,mmalekzadeh_privacy-preserving-bandits.tar.gz,PRIVACY-PRESERVING BANDITS,,scipy keras tensorflow pandas skmultilearn sys iteround category_encoders bandipy random pickle sklearn pairing os numpy matplotlib,cs.LG cs.CR cs.MA stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mmalekzadeh_privacy-preserving-bandits.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mmalekzadeh_privacy-preserving-bandits.pdf,,,no,,,,no,,,,
471,https://github.com/charlottvallon/FlapPyBird-MPC,charlottvallon_FlapPyBird-MPC.tar.gz,Data-Driven Hierarchical Predictive Learning in Unknown Environments,,pygame math itertools sys datetime glob mip_hpl random pickle sklearn torch os gpytorch cvxpy joblib numpy matplotlib time,eess.SY cs.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/charlottvallon_FlapPyBird-MPC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\charlottvallon_FlapPyBird-MPC.pdf,,"5) Training:We use the Matlab Statistics and Machine Learning toolbox 2 to learn kernel hyperparameters that best match the training data (10). Specifically, we train a squaredexponential kernel with separate length scales for each predictor (see Appendix). New hyperparameters can be learned whenever more executions become available.",no,,,,no,,,,
472,https://github.com/guy-oren/OneShotTranslationExt,guy-oren_OneShotTranslationExt.tar.gz,One-Shot Unsupervised Cross Domain Translation,@staticmethod,requests PIL copy utee solver_baseline_online_svhn_to_mnist warnings sys solver_svhn_to_mnist_fixed model_svhn_mnist skimage bs4 tqdm solver_mnist_to_svhn mnist __future__ data_loader_svhn_mnist torchvision util solver_baseline_online_mnist_to_mnist_m solver_autoencoder_svhn_mnist solver_autoencoder_mnist_m_mnist collections data visdom torch os dominate scipy functools itertools argparse data_loader_mnist_m_mnist numpy svhn time tarfile ntpath random cv2 logging options models model_mnist_m_mnist zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/guy-oren_OneShotTranslationExt.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\guy-oren_OneShotTranslationExt.pdf,AutoEncoder,"Previous WorkUnsupervised domain translation methods receive two sets of samples, one from each domain, and learn a function that maps between a sample in one domain and the analogous sample in the other domain [2,3,4,5,6,7,8,9,10,11,12]. Such methods are unsupervised in the sense that the two sets are completely unpaired.The mapping between the domains can be recovered based on multiple cues. First, shared objects between domains can serve as supervised samples. This is the case in the early unsupervised cross-lingual dictionary translation methods [13,14,15,16], which identified international words ('computer', 'computadora','kompüter') or other words with a shared etymology by considering inter-language edit distances. These words were used as a seed set to bootstrap the mapping process.A second cue is that of object relations. It often holds that the pairwise similarities between objects in domain A are preserved after the transformation to domain B. This was exploited in [5] using the L2 distances between classes. In the work on unsupervised word to word translation [9,10,11,17], the relations between words in each language are encoded by word vectors [18], and translation is well approximated by a linear transformation of one language's vectors to those of the second.A third cue is that of inner object relations. If the objects of domain A are complex and contain multiple parts, then one can expect that after mapping, the counterpart in domain B would have a similar arrangement of parts. This was demonstrated by examining the distance between halves of images in [5] and it also underlies unsupervised NLP translation methods that can translate a sentence in one language to a sentence in another, after observing unmatched corpora [12].Another way to capture these inner-object relations is by constructing separate autoencoders for the two domains, which share many of the weights [6,7]. It is assumed that the low-level image properties, such as texture and color, are domain-specific, and that the mid-and top-level properties are common to both domains.The third cue is also manifested implicitly (in both autoencoder architectures and in other methods) by the structure of the neural network used to perform the cross-domain mapping [19]. The network's capacity constrains the space of possible solutions and the relatively shallow networks used, and their architecture dictate the form of a solution. Taken together with the GAN [20] constraints that ensure that the generated images are from the target domain, and restricted further by the circularity constraint [2,3,4], much of the ambiguity in mapping is eliminated.In the context of one-shot translation, it is not possible to find or to generate analogs in B to the given x ∈ A, since the domain-invariant distance between the two domains is not defined. One can try to use general purpose distances such as the perceptual distance, but this would make the work  E S ) and part of the decoder (G S ). These shared parts, marked with a snowflake, are frozen with respect to the sample x. For both phase I and phase II, we train a discriminator D B to ensure that the generated image belong to the distribution of domain B. P (x) and P (Λ) are translated to a common feature space, C E , using E UA and E U B respectively. C (resp C G ) is the space of features, constructed after passing C E (resp C) through the common encoder E S (resp common decoder G S ). R AB denotes the subspace of samples in B constructed from P (x), which is generated by augmenting x. R AA denotes the space of reconstructed samples from P (x). R ABA denotes the subspace of samples in A constructed by translating P (x) to domain B and then back to A.semi-supervised such as [21,22] (these methods are also not one-shot). Since there are no inter-object relations in domain A, the only cue that can be used is of the third type.We have made attempts to compare various image parts within x, thereby generalizing the imagehalves solution of [5]. However, this did not work. Instead, our work relies on the assumption that the mid-level representation of domain A is similar to that of B, which, as mentioned above, is the underlying assumption in autoencoder based cross-domain translation work [6,7]. DiscussionBeing a one-shot technique, the method we present is suitable for agents that survey the environment and encounter images from unseen domains. In phase II, the autoencoder of domain B changes in order to adapt to domain A. This is desirable in the context of ""life long"" unsupervised learning, where new domains are to be encountered sequentially. However, phase II is geared toward the success of translating x, and in the context of multi-one-shot domain adaptations, a more conservative approach would be required.In this work, we translate one sample from a previously unseen domain A to domain B. An interesting question is the ability of mapping from a domain in which many samples have been seen to a new domain, from which a single training sample is given. An analog two phase approach can be attempted, in which an autoencoder is trained on the source domain, replicated, and tuned selectively on the target domain. The added difficulty in this other direction is that adversarial training cannot be employed directly on the target domain, since only one sample of it is seen. It is possible that one can still model this domain based on the variability that exists in the familiar source domain. (Figure 1 :1Figure 1: Illustration of the two phases of training. (Phase I): Augmented samples from domain B, P (Λ), are used to train a variational autoencoder for domain B. R BB denotes the space of reconstructed samples from P (Λ). (Phase II): the variational autoencoder of phase I is cloned, while sharing the weights of part of the encoder (E S) and part of the decoder (G S ). These shared parts, marked with a snowflake, are frozen with respect to the sample x. For both phase I and phase II, we train a discriminator D B to ensure that the generated image belong to the distribution of domain B. P (x) and P (Λ) are translated to a common feature space, C E , using E UA and E U B respectively. C (resp C G ) is the space of features, constructed after passing C E (resp C) through the common encoder E S (resp common decoder G S ). R AB denotes the subspace of samples in B constructed from P (x), which is generated by augmenting x. R AA denotes the space of reconstructed samples from P (x). R ABA denotes the subspace of samples in A constructed by translating P (x) to domain B and then back to A.",no,,,,no,,,,
473,https://github.com/AlirezaJav/Projection-based-PC-Quality-Metric,AlirezaJav_Projection-based-PC-Quality-Metric.tar.gz,Joint Geometry and Color Projection-based Point Cloud Quality Metric,,PIL ProjQM pandas psnr_hvs_m piq skimage open3d collections torch os IQA_pytorch matplotlib configparser scipy argparse numpy time math cv2,eess.IV cs.MM,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AlirezaJav_Projection-based-PC-Quality-Metric.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AlirezaJav_Projection-based-PC-Quality-Metric.pdf,,,no,,,,no,,,,
481,https://github.com/enalisnick/dropout_icml2019,enalisnick_dropout_icml2019.tar.gz,Dropout as a Structured Shrinkage Prior,,tensorflow _pickle random sklearn os numpy models,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/enalisnick_dropout_icml2019.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\enalisnick_dropout_icml2019.pdf,,"ExperimentsWe performed experiments to test the practicality of the tail-adaptive importance sampling scheme (Section 5.2) for MC dropout and the variational EM algorithm (Section 5.3) for ARD, ADD, and ARD-ADD priors (Section 4). For both cases we used the same experimental setup as Gal & Ghahramani (2016b), testing the models on regression tasks from the UCI repository (Dheeru & Karra Taniskidou, 2017). The supplementary materials include details of the model and optimization hyperparameters as well as Python implementations 2 of both experiments.Tail-Adaptive Importance Sampling Table 2 reports test set root-mean-square error (RMSE) and log-likelihood for three MN objectives: the usual lower bound (Equation 3), the importance weighted objective (Equation 15), and the tail-adaptive method (Equation 17). The 'lower bound' columns are the results reported by Gal & Ghahramani (2016b). 3 The only difference between columns is in how the importance weights were set. We see that the tail-adaptive method results in the best RMSE in five out of seven data sets. However, the best log-likelihoods are achieved by regular importance sampling (five out of seven). We believe that this is due to L IW-MN being a less-biased estimate of the exact likelihood (unbiased as S → ∞). The tail-adaptive method, on the other hand, is most effective at regularizing for purposes of predictive accuracy. Notably, Table 3. We compare test set RMSE for UCI regression benchmarks. As baselines, we use previously reported two-hidden-layer results for dropout (Gal & Ghahramani, 2016b), probabilistic backpropagation (Hernández-Lobato & Adams, 2015), and deep Gaussian processes (Bui et al., 2016). ARD, ADD, and ARD-ADD use the Γ −1 (3, 3) scale prior in all cases. the tail-adaptive method performs worst on Power, the largest data set (N = 9568).",no,,,supplementary material with hyperparameter not available,no,,,,
497,https://github.com/amaiya/causalnlp,amaiya_causalnlp.tar.gz,CausalNLP: A Practical Toolkit for Causal Inference with Text,@ignore_warnings(category=ConvergenceWarning) @abstractmethod @property @staticmethod @abstractclassmethod,"copy packaging pandas warnings sys tqdm lightgbm setuptools,re,sys collections transformers sentence_transformers torch os statsmodels pkg_resources shap pygam matplotlib configparser importlib scipy pickle sklearn argparse numpy xgboost time abc math logging",cs.CL cs.LG cs.LG cs.CL stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amaiya_causalnlp.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amaiya_causalnlp.pdf,Causal Inference,"ConclusionIn this paper, we presented CausalNLP, a Python library for causal inference with text from observational data. Based on meta-learners, CausalNLP is a versatile toolkit supporting inclusion of text and its linguistic properties as treatments, outcomes, confounders, or mediators in a causal inference study. The intersection of causality and NLP is a newer research area with a rapidly changing landscape ripe with opportunities for future work. Examples include better explainability of causal inferences and characterizing and better understanding meta-learner performance with respect to text. For instance, while the S-Learner performed well, we did observe that other meta-learners (e.g., the T-Learner) were better able to identify heterogeneous treatment effects in terms of how causal impacts varied across observations.[ ' T_ac ' ] = ( df [ ' p o s i t i v e ' ] > 0 . 5 ) . astype ( ' i n t ' ) # t r a i n S− L e a r n e r f o r c a u s a l − i n f e r e n c e from causalnlp . causalinference i m p o r t ← CausalInferenceModel from sklearn . linear_model i m p o r t ← LogisticRegression base_learner = LogisticRegression ( solver= ' l i b l i n e a r ' , penalty= ' l 1 ' , fit_intercept=False ) cm = CausalInferenceModel ( df , metalearner_type= ' s − l e a r n e r ' , treatment_col= ' T_ac ' , outcome_col= ' Y_sim ' , text_col= ' t e x t ' , include_cols = [ ' C _ t r u e ' ] ,",no,,,,no,,,,
500,https://github.com/namanphy/style-transfer-through-back-translate,namanphy_style-transfer-through-back-translate.tar.gz,Style Transfer Through Back-Translation,,math codecs sys json random torch argparse onmt __future__ numpy time,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/namanphy_style-transfer-through-back-translate.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\namanphy_style-transfer-through-back-translate.pdf,,"StyleSentiment. To compare our work with the stateof-the-art approaches of style transfer for nonparallel corpus we perform sentiment transfer, replicating the models and experimental setups of Hu et al. (2017) and Shen et al. (2017). Given a positive Yelp review, a style transfer model will generate a similar review but with an opposite sentiment. We used Shen et al.'s (2017) corpus of reviews from Yelp. They have followed the standard practice of labeling the reviews with rating of higher than three as positive and less than three as negative. They have also split the reviews to sentences and assumed that the sentence has the same sentiment as the review.Dataset statistics. We summarize below corpora statistics for the three tasks: transferring gender, political slant, and sentiment. The dataset for sentiment modification task was used as described in (Shen et al., 2017)",no,,yes,"input size, hidden dimension, maximum length, filters, filter size",no,,,,
501,https://github.com/awsm-research/SQAPlanner-implementation,awsm-research_SQAPlanner-implementation.tar.gz,SQAPlanner: Generating Data-Informed Software Quality Improvement Plans,,math copy pathlib re pandas util_lomika defect_prep collections glob _pickle random pickle util_defect sklearn os numpy openpyxl,cs.SE cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/awsm-research_SQAPlanner-implementation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\awsm-research_SQAPlanner-implementation.pdf,,,no,,,,no,,,,
503,https://github.com/gchers/wfes,gchers_wfes.tar.gz,"Bayes, not Naïve: Security Bounds on Website Fingerprinting Defenses",,utils ctypes_utils pandas csv sys multiprocessing dyer ctypes evaluate collections json wang os scipy hayes sklearn dill argparse numpy editdistance math attacks random experiment_utils,cs.CR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gchers_wfes.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gchers_wfes.pdf,,,no,,,,no,,,,
504,https://github.com/PRBonn/semantic-kitti-api,PRBonn_semantic-kitti-api.tar.gz,SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences,@property @staticmethod,yaml sys tqdm auxiliary re glfw imgui collections json torch os matplotlib struct scipy argparse OpenGL numpy time shutil math vispy zipfile,cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/PRBonn_semantic-kitti-api.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\PRBonn_semantic-kitti-api.pdf,,,no,,,,no,,,,
505,https://github.com/ximenafernandez/intrinsicPH,ximenafernandez_intrinsicPH.tar.gz,INTRINSIC PERSISTENT HOMOLOGY VIA DENSITY-BASED METRIC LEARNING,,math scipy pandas gudhi fermat src random ripser sklearn mpl_toolkits numpy matplotlib,stat.ML cs.LG math.AT math.PR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ximenafernandez_intrinsicPH.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ximenafernandez_intrinsicPH.pdf,,,no,,,,no,,,,
510,https://github.com/gitsamshi/nli-image-caption,gitsamshi_nli-image-caption.tar.gz,Enhancing Descriptive Image Captioning with Natural Language Inference,,"PIL copy pyciderevalcap csv multiprocessing sys pdb skimage tqdm mmap zlib __future__ base64 ast torchvision os, sys atexit collections json h5py tempfile nltk torch os matplotlib string six scipy functools hashlib codecs lmdb dataloaderraw pickle subword_nmt argparse opts tensorboardX numpy eval_utils time math pycocotools traceback networkx random misc dataloader_raml dataloader models pycocoevalcap",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gitsamshi_nli-image-caption.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gitsamshi_nli-image-caption.pdf,,,no,,,,no,,,,
511,https://github.com/chen-xiong-yi/OwnBERT,chen-xiong-yi_OwnBERT.tar.gz,Semi-supervised Sequence Learning,@classmethod,math copy unicodedata tensorflow re optimization csv codecs modeling collections json random tempfile os __future__ tokenization six,cs.CL cs.LG cs.LG cs.CL cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/chen-xiong-yi_OwnBERT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\chen-xiong-yi_OwnBERT.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer  Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,"Object classification experiments with CIFAR-10In these experiments, we attempt to see if our pre-training methods extend to non-textual data. To do this, we train a LSTM on the CIFAR-10 image dataset, consisting of 60,000 32x32 colour images divided into 10 classes. The input at each timestep of the LSTM is an entire row of pixels and we predict the class of the image after reading the final row. We use the same method as in [15] to perform data augmentation. We also trained a LSTM to do next row prediction given the current row (we denote this as LM-LSTM) and a LSTM to autoencode the image by rows (SA-LSTM). The loss function during unsupervised learning is the Euclidean L2 distance between the predicted and the target row. We then fine-tune these on the classification task and present the classification results in Table 8. While we do not achieve the results attained by state of the art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN model [14] despite not using any convolutions and outperforms the non pre-trained LSTM.",no,,,,no,,,,
526,https://github.com/AlisonYao/NLP-Final-Project,AlisonYao_NLP-Final-Project.tar.gz,TableQA: a Large-Scale Chinese Text-to-SQL Dataset for Table-Aware SQL Generation,,re records json sqlnet collections modules net_utils lib tqdm torch argparse numpy matplotlib,cs.DB cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AlisonYao_NLP-Final-Project.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AlisonYao_NLP-Final-Project.pdf,,,no,,,,no,,,,
529,https://github.com/AIBluefisher/GraphSfM,AIBluefisher_GraphSfM.tar.gz,Graph-Based Parallel Large Scale Structure from Motion,@ClassProperty @layer,"sys cnn_wrapper glob tools __future__ threading tensorflow sys, os model datetime image_preprocessing thread tempfile os helper matplotlib progressbar struct pynvml functools scipy sys,pdb sklearn argparse numpy Queue time math random subprocess",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AIBluefisher_GraphSfM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AIBluefisher_GraphSfM.pdf,,,no,,,,no,,,,
534,https://github.com/IICNELAB/qpu_code,IICNELAB_qpu_code.tar.gz,Quaternion Product Units for Deep Learning on 3D Rotation Groups,@pytest.helpers.register @staticmethod,utils data_utils yaml sys glob tqdm __future__ qpu_ops pprint torchvision datetime collections h5py enum data torch os pytest matplotlib importlib scipy builtins dataset test typing pickle shlex argparse feeder tensorboardX numpy time shutil pointnet2 math etw_pytorch_utils setuptools random ntu_info subprocess mpl_toolkits qpu_layers models __builtin__,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IICNELAB_qpu_code.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IICNELAB_qpu_code.pdf,,"ConclusionIn this work, we proposed a novel quaternion product unit for deep learning on 3D rotation groups. This model can be used as a new module to construct quaternion-based neural networks, which presents encouraging generalization ability and flexible rotation robustness. Moreover, our implementation makes this model compatible with existing real-valued models, achieving end-to-end training through backpropagation. Besides skeleton classification, we plan to extend our QPU to more applications and deep learning. In particular, we have done a preliminary experiment on applying our QPU to point cloud classification task. We designed a quaternion representation for the neighbor point set of a centroid which first converts 3D coordinates of neighbor points into quaternion-based 3D rotations then cyclically sort them according to their rotation order around the vector from the origin to the centroid. We designed our models based on Pointnet++ [19,31] by replacing the first Set Abstraction layer with a QMLP module. We tested our model on ModelNet40 [32] and our rotation-invariant model achieved 80.1% test accuracy. Please refer to the supplementary file for more details of our quaternion-based point cloud representation, network architectures and experimental setups.",no,,,,no,,,,
536,https://github.com/janlepppa/graph_learn_mi,janlepppa_graph_learn_mi.tar.gz,Learning non-parametric Markov networks with mutual information,,copy pandas multiprocessing sltests fishertest graphutils os matplotlib mbalgs rpy2 scipy itertools datagenerator seaborn pickle sklearn argparse numpy structlearn huge networkx knnestimator rcit,cs.LG cs.IT math.IT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/janlepppa_graph_learn_mi.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\janlepppa_graph_learn_mi.pdf,,,no,,,,no,,,,
537,https://github.com/pragyak412/Improving-Voice-Separation-by-Incorporating-End-To-End-Speech-Recognition,pragyak412_Improving-Voice-Separation-by-Incorporating-End-To-End-Speech-Recognition.tar.gz,IMPROVING VOICE SEPARATION BY INCORPORATING END-TO-END SPEECH RECOGNITION,@click.group() @staticmethod @property @main.command(),espnet sys torch_complex pit_criterion tqdm __future__ feature_transform_for distutils soundfile get_power_spectral_density_matrix click model datetime json unigram_gen frontend_for pytorch_wpe torch os domainTranslation matplotlib six specific_config scipy itertools mutagen ETESpeechRecognition test conv_tasnet pickle typing argparse test_real apply_beamforming_vector pytorch_backend numpy sentencepiece shutil config editdistance train math chainer warpctc_pytorch librosa random logging tinytag dataloader get_mvdr_vector,cs.SD cs.LG eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pragyak412_Improving-Voice-Separation-by-Incorporating-End-To-End-Speech-Recognition.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pragyak412_Improving-Voice-Separation-by-Incorporating-End-To-End-Speech-Recognition.pdf,,,no,,,,no,,,,
539,https://github.com/ZZR8066/mmdetection-GaofenCompetition,ZZR8066_mmdetection-GaofenCompetition.tar.gz,MMDetection: Open MMLab Detection Toolbox and Benchmark,"@contextlib.asynccontextmanager @auto_fp16() @force_fp32() @DETECTORS.register_module() @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses')) @weighted_loss @ROI_EXTRACTORS.register_module() @force_fp32(apply_to=('x', 'y')) @force_fp32(apply_to=('cls_scores', 'bbox_preds')) @force_fp32(apply_to=('feats', ), out_fp16=True) @functools.wraps(loss_func) @CONV_LAYERS.register_module('DCN') @CONV_LAYERS.register_module('DCNv2') @pytest.mark.parametrize('arch_name,arch,out_channels', regnet_test_data) @pytest.mark.parametrize( @auto_fp16(apply_to=('x', 'y')) @pytest.mark.parametrize('dataset', @HEADS.register_module() @UPSAMPLE_LAYERS.register_module(name='carafe') @CONV_LAYERS.register_module('ConvAWS') @PIPELINES.register_module() @LOSSES.register_module() @property @staticmethod @CONV_LAYERS.register_module('ConvWS') @once_differentiable @pytest.mark.parametrize('cfg_file', [ @BBOX_ASSIGNERS.register_module() @abstractmethod @force_fp32(apply_to=('mask_pred', )) @SHARED_HEADS.register_module() @LOSSES.register_module @auto_fp16(apply_to=('x', )) @force_fp32(apply_to=('x', 'y'), out_fp16=True) @force_fp32( @functools.wraps(old_func) @DATASETS.register_module @auto_fp16(apply_to=('img', )) @classmethod @HEADS.register_module @force_fp32(apply_to=('mask_iou_pred', )) @BBOX_CODERS.register_module() @IOU_CALCULATORS.register_module() @contextlib.contextmanager @BACKBONES.register_module() @DATASETS.register_module() @force_fp32(apply_to=('x', )) @NECKS.register_module() @BBOX_SAMPLERS.register_module() @CONV_LAYERS.register_module('Conv', force=True) @CONV_LAYERS.register_module(name='SAC') @ANCHOR_GENERATORS.register_module() @auto_fp16(apply_to=('x', 'y'), out_fp32=True)",imagecorruptions PIL copy contextlib warnings sys multiprocessing instaboostfast glob roi_align test_forward tools onnx __future__ platform torchvision mmcv pathlib re asyncio json collections unittest resource tempfile torch os pytest matplotlib six functools itertools xml seaborn robustness_eval bisect terminaltables urllib pickle typing asynctest argparse cityscapesscripts mmdet lvis roi_pool numpy time abc shutil math kwarray pycocotools setuptools albumentations cv2 random logging subprocess inspect io,cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZZR8066_mmdetection-GaofenCompetition.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZZR8066_mmdetection-GaofenCompetition.pdf,,,no,,,,no,,,,
545,https://github.com/ayeshakhtar209/fashion-mnist,ayeshakhtar209_fashion-mnist.tar.gz,DENSER: Deep Evolutionary Network Structured Representation,,utils multiprocessing sys benchmark psutil __future__ ast threading tensorflow datetime json collections os matplotlib gzip itertools typing sklearn numpy time random logging subprocess configs,cs.NE cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ayeshakhtar209_fashion-mnist.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ayeshakhtar209_fashion-mnist.pdf,,"RepresentationEach solution encodes an ANN by means of an ordered sequence of feedforward layers and their respective parameters; the learning, data augmentation, and any other hyperparameters can be encoded with each individual too. The representation of the candidate solution is made at two different levels:GA Level -encodes the macro structure of the networks and is responsible for representing the sequence of  layers that later serves as an indicator to the grammatical starting symbol. It requires the definition of the allowed structure of the networks, i.e., the valid sequence of layers. For example, for evolving CNNs the following GA structure may be specified: [(features, 1, 10), (classification, 1, 2), (softmax, 1, 1), (learning, 1, 1)], where each tuple indicates the valid starting symbols, and the minimum and maximum number of times they can be used. Using the grammar of Figure 2, this GA structure can evolve networks with up to 10 convolution or pooling layers, followed by up to 2 fully-connected layers, and the classification layer softmax, that usually has a number of outputs that matches the number of problem classes; the learning tuple is responsible for codifying the parameters that should be used to train the network.DSGE Level -encodes the parameters associated to a layer. The parameters and their allowed values or ranges are codified in the grammar that must be defined by the user. Looking at the grammar of Figure 2, for the pooling layers we tune the kernel size, the stride, and the type of padding. The same exercise can be made to the remaining layers defined in the grammar. The parameters can have closed values (e.g., the padding that can be only valid or same), or can assume a value in an integer or real interval.The novel combination of a GA with DSGE enables a two-fold gain: (i) the GA level encapsulates the genetic material of each layer, making it easier to apply the variation operators (described next); and (ii) the DSGE makes the approach easily generalisable, as it is only needed to change the grammar to enable the evolution of different types of networks, or of networks to solve different tasks; it also One-point offspring:Bit-mask offspring:<F 2,A > <C 1,A > <C 2,A > <S 1,A > <L 1,A > <F 1,B > <C 1,B > <C 2,B > <C 3,B > <S 1,B > <L 1,B > <F 1,A > <F 2,A > <C 1,A > <C 2,B > <C 3,B > <S 1,A > <L 1,A > <F 1,B > <C 1,B > <C 2,A > <S 1,B > <L 1,B > <F 1,A > <F 2,A > <C 1,B > <C 2,B > <C 3,B > <S 1,B > <L 1,A > <F 1,B > <C 2,A > <S 1,A > <L 1,B > <C 1,A > cut-pointFigure 5. Example of the introduced crossover operators. The example focuses on the GA level of the genotype. For the bit-mask crossover the mask is 1001, which is associated to the features, classification, softmax and learning modules, respectively.facilitates the incorporation of domain specific knowledge.An example of the genotype is shown in Figure 3. This example is based on the grammar of Figure 2 and on the above detailed GA structure. Figure 4 depicts the phenotype corresponding to the layer which has the DSGE genotype detailed in Figure 3.",no,,yes,"epochs, learning rate, loss, batch size, momentum",no,,,,
547,https://github.com/daixiangzi/ImprovedGan-pytorch,daixiangzi_ImprovedGan-pytorch.tar.gz,Improved Techniques for Training GANs,,imageio PIL sys cifar10_data __future__ torchvision torch os matplotlib six pickle plotting tensorboardX numpy time Nets config shutil tarfile random,cs.LG cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/daixiangzi_ImprovedGan-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\daixiangzi_ImprovedGan-pytorch.pdf,Batch Normalization  Virtual Batch Normalization  GAN Feature Matching  Weight Normalization  Label Smoothing  Minibatch Discrimination  Convolution  Generative Adversarial Network,"SVHNFor the SVHN data set, we used the same architecture and experimental setup as for CIFAR-10.  HA, sample quality is considerably reduced (see ""-L""). ""-LS"" removes label smoothing and incurs a noticeable drop in performance relative to ""our methods."" ""-MBF"" removes the minibatch features and incurs a very large drop in performance, greater even than the drop resulting from removing the labels. Adding HA cannot prevent this problem.",no,,,hyperparameter can be found at supplementary website,no,,,,
559,https://github.com/yizhe-ang/detectron2-1,yizhe-ang_detectron2-1.tar.gz,Adversarial Examples for Semantic Segmentation and Object Detection,@classmethod @torch.no_grad(),PIL copy fvcore yaml tqdm pathlib json collections torch os matplotlib itertools typing argparse numpy math random cv2 logging wandb detectron2 detectron2_1,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yizhe-ang_detectron2-1.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yizhe-ang_detectron2-1.pdf,,,no,,,,no,,,,
563,https://github.com/geomstats/applications,geomstats_applications.tar.gz,Curvature effects on the empirical mean in Riemannian and affine Manifolds: a non-asymptotic high concentration expansion in the small-sample regime,"@tf_export(""initializers.local_variables"", ""local_variables_initializer"") @pytest.mark.skipif(sys.version_info < (3,), reason='Cannot catch warnings in python 2') @keras_test @deprecated(""2017-03-02"", ""Use `tf.variables_initializer` instead."") @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow backend') @pytest.mark.skipif((K.backend() == 'cntk'), @interfaces.legacy_pooling1d_support @interfaces.legacy_model_constructor_support @pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(), @interfaces.legacy_upsampling1d_support @pytest.mark.parametrize( @interfaces.legacy_prelu_support @interfaces.legacy_generator_methods_support @interfaces.legacy_zeropadding2d_support @pytest.mark.skipif((K.backend() in ['theano']), @interfaces.legacy_input_support @interfaces.legacy_pooling3d_support @interfaces.legacy_gaussiandropout_support @interfaces.legacy_recurrent_support @trainable_weights.setter @built.setter @use_spawn @pytest.mark.parametrize('layer_class,layer_args', [ @interfaces.legacy_spatialdropout1d_support @interfaces.legacy_batchnorm_support @tf_export(""trainable_variables"") @interfaces.legacy_dense_support @pytest.mark.skipif((K.backend() != 'tensorflow'), reason='Requires TensorFlow backend') @interfaces.legacy_conv2d_support @tf_export(""initializers.variables"", ""variables_initializer"") @interfaces.legacy_cropping3d_support @pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU']) @tf_export(""report_uninitialized_variables"") @pytest.mark.skipif((keras.backend.backend() != 'tensorflow'), @pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV']) @pytest.mark.parametrize('x_np,axis,keepdims', [ @pytest.fixture @pytest.mark.skipif(sys.version_info < (3, 3), reason=""requires python3.3"") @six.wraps(func) @pytest.mark.parametrize('merge_mode', ['sum', 'mul', 'ave', 'concat', None]) @interfaces.legacy_conv3d_support @pytest.mark.parametrize('merge_mode', ['sum', 'concat', None]) @tf_should_use.should_use_result @pytest.mark.skipif(not K.tensorflow_backend._get_available_gpus(), reason='Requires GPU') @tf_export(""initializers.global_variables"", ""global_variables_initializer"") @interfaces.legacy_spatialdropoutNd_support @interfaces.legacy_deconv2d_support @pytest.mark.skipif((K.backend() == 'theano'), @rnn_test @tf_export(""model_variables"") @pytest.mark.parametrize('validation_split,num_training', [ @tf_export(""initialize_all_variables"") @tf_export(""assert_variables_initialized"") @pytest.mark.parametrize(""layer"", [ @pytest.mark.parametrize('op,input_shape,kernel_shape,depth_multiplier,padding,data_format', [ @pytest.mark.parametrize('fn, name, accept_all, expected', [ @tf_export(""initialize_variables"") @pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.') @pytest.mark.skipif((K.backend() in ['cntk', 'theano']), @rnn_cell_test @pytest.mark.skipif(K.backend() != 'tensorflow', reason='sparse operations supported only by TensorFlow') @non_trainable_weights.setter @pytest.mark.parametrize('bidirectional', [False, True], ids=['single', 'bidirectional']) @interfaces.legacy_add_weight_support @interfaces.legacy_cropping2d_support @interfaces.legacy_global_pooling_support @property @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Not supported.') @tf_export(""is_variable_initialized"") @deprecated(""2017-03-02"", ""Use `tf.global_variables_initializer` instead."") @pytest.mark.xfail(sys.version_info < (3, 3), @interfaces.legacy_get_updates_support @abstractmethod @interfaces.legacy_upsampling2d_support @interfaces.legacy_separable_conv2d_support @classmethod @tf_export(""initialize_local_variables"") @pytest.mark.skipif(sys.version_info < (3,), @tf_export(""global_variables"") @normalize_conv @threadsafe_generator @interfaces.legacy_zeropadding3d_support @deprecated(""2017-03-02"", ""Use `tf.local_variables_initializer` instead."") @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [ @pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2']) @interfaces.legacy_embedding_support @pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend') @pytest.mark.parametrize('tensor_shape', [(100, 100), (1, 2, 3, 4)], ids=['FC', 'CONV']) @pytest.mark.parametrize('metrics_mode', ['list', 'dict']) @tf_export(""all_variables"") @deprecated(""2017-03-02"", ""Please use tf.global_variables instead."") @interfaces.legacy_conv1d_support @pytest.mark.skipif((K.backend() != 'tensorflow'), @interfaces.legacy_convlstm2d_support @interfaces.legacy_upsampling3d_support @interfaces.legacy_lambda_support @tf_export(""local_variables"") @staticmethod @interfaces.legacy_pooling2d_support @contextmanager @tf_export(""Variable"") @interfaces.legacy_gaussiannoise_support @states.setter @trainable.setter @pytest.mark.parametrize('op,input_shape,pool_size,strides,padding,data_format,pool_mode', [ @pytest.mark.parametrize('rnn_type', ['lstm', 'gru'], ids=['LSTM', 'GRU']) @pytest.mark.skipif(K.backend() == 'theano', reason='Not supported.') @tf_export(""moving_average_variables"") @interfaces.legacy_dropout_support @pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])",requests PIL copy imageio pandas csv contextlib warnings sys yaml types multiprocessing skimage tqdm __future__ pydot pylab cntk threading reference_operations keras re tensorflow zipfile datetime collections json queue h5py unittest tempfile os pytest matplotlib string six importlib scipy functools itertools gzip hashlib seaborn codecs optparse xlrd pickle sklearn argparse redis numpy Queue time shutil cairocffi editdistance abc tarfile binascii math marshal setuptools traceback geomstats mock random theano logging mpl_toolkits cPickle inspect io,math.DG math.ST stat.TH,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/geomstats_applications.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\geomstats_applications.pdf,,,no,,,,no,,,,
569,https://github.com/bismex/RLT-DIMP,bismex_RLT-DIMP.tar.gz,Robust Long-Term Object Tracking via Improved Discriminative Model Prediction,@model_constructor @wraps(f) @staticmethod @functools.wraps(op) @classmethod @tensor_operation,PIL copy pandas csv warnings sys multiprocessing glob ltr prroi_pool tqdm skimage torchvision pathlib re jactorch collections json unittest tempfile visdom torch os jpeg4py matplotlib importlib scipy functools itertools xml pickle argparse lvis trax tensorboardX numpy time shutil _collections math pytracking pycocotools tikzplotlib traceback cv2 random evaluation analysis gdown inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bismex_RLT-DIMP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bismex_RLT-DIMP.pdf,Random Search,,no,,,,no,,,,
570,https://github.com/drexelai/kcompleteness-in-binary-neural-nets,drexelai_kcompleteness-in-binary-neural-nets.tar.gz,Evaluating Online and Offline Accuracy Traversal Algorithms for k-Complete Neural Network Architectures,,math keras itertools pandas sys collections unittest pipeline sklearn argparse os mpl_toolkits numpy matplotlib time,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/drexelai_kcompleteness-in-binary-neural-nets.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\drexelai_kcompleteness-in-binary-neural-nets.pdf,,"C. DatasetsIn this study, we use two datasets, titanic dataset and customer churn dataset.1) Titanic dataset: This dataset is made public by Kaggle [8]. The dataset has 14 columns to indicate each passenger on the Titanic ship. The columns include sex, name, destination, fare, destination etc. The dataset has about 1000 rows. The label for each row has a 1 or 0 to indicate whether the passenger survived the crash or not, hence the binary classification. The architecture candidates that we search to predict this binary classification problem has all 11 dimensions in the input layer and 1 in the output layer.  2) Churn dataset: This dataset is made public by Drexel Society of Artificial Intelligence [9]. The dataset has information about customers that are using an imaginary contract and has labels to indicate whether the company has left the contract or not. The model that we build for this dataset tries to predict whether the customer is about to leave this contract or not, hence the binary classification. There are 14 columns and 10000 rows in the dataset. The dataset columns has features to indicate the customer revenue, customer contract cost, the type of the product the contract is about, the region of the customer, the fact that the customer whether renewed the contract in the previous 90 days or so etc. The dataset has features represented as categories as well as floating point numbers, therefore, we have to scale and apply categorical transformation when we train a model for this dataset. All the architectures that we look in the search space has 11 dimensions in the input layer and 1 in the output layer.",no,,,,no,,,,
576,https://github.com/SliverySky/MarioLevelRepairer,SliverySky_MarioLevelRepairer.tar.gz,A Novel CNet-assisted Evolutionary Level Repairer and Its Applications to Super Mario Bros,@staticmethod,"utils os, sys pygame copy GA CNet PIL os,sys json LevelGenerator random torch sys,os os numpy matplotlib shutil root",cs.AI cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/SliverySky_MarioLevelRepairer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\SliverySky_MarioLevelRepairer.pdf,,"|I. INTRODUCTIONProcedural content generation (PCG) refers to the generation of game content (including rules, levels, maps, sound, background stories and so on) automatically without or with limited help of human designers [1]. It has become a popular area in recent years. With the development of video games, the scale of game production teams is also expanding. This leads to increasingly expensive development costs of game production. PCG techniques can reduce the workload of game designers or give designers inspiration. That means, for big game companies, PCG techniques can decrease the costs and shorten cycle for game production. On the other hand, PCG can also lower the technology and capital threshold of complex game development so that those small teams, even singleperson developers, who have good ideas can realize their ideas with PCG techniques. What's more, some games need automatic content generation after their publication, including real-time content generation. Those games will be the biggest beneficiaries of development of PCG.Mario AI framework is designed for the game AI competition for Super Mario which includes a PCG track [2]. After T. Shu and Z. Wang contributed equally to this work. 2010, some level generation methods for Super Mario based on the Mario AI framework have been proposed (e.g., [3]- [8]). Some of those methods are based on human designed grammars [3] or patterns [4]- [6]. Those methods have their own advantages respectively. However, they still need human designers to specify the behaviors of generators.In 2018, Volz et al. [7] successfully applied latent variable evolution (LVE) [9] to level generation of Super Mario Bros. They trained a Deep Convolution Generative Adversarial Networks (DCGAN) and use its generator to generate Super Mario Bros levels. Their approach is powerful, but they mentioned GAN will generate broken pipes sometimes in [7]. This raised our interests in repairing the logic errors of levels generated by GAN.When the DCGAN generates the tiles to build a level, it cannot ""consider"" the surrounding neighbor tiles of each tile. We think this is the reason of broken pipes generated by GAN. Broken pipe is a microcosm of a stubborn problem of GANbased method: there are usually some rules for game level design but the model cannot learn them due to its own network structure. One may think of using a Recurrent Neural Network (RNN) as the model of GAN [10]. RNNs can partially handle the broken pipes problem since they take some connection with neurons in the same layer. But the constraints of level may be complex, and the correctness of a tile may be influenced by the neighbor tiles in multiple directions. Besides, in terms of level generation, the network model of DCGAN may have some potential superiority comparing to RNNs. In fact, we have tried to train a GAN on a dataset of more levels in Super Mario Bros with 35 types of tiles in total. New types of tiles bring more constraints and cause more logic errors made by the GAN. Therefore, an automatic algorithm to repair levels with logic errors is desired.A straightforward way to automatically repair logic errors is writing a script to edit the level files based on some predefined rules. However, as mentioned previously, a significant advantage of LVE approach is that it can learn how to design levels without expert knowledge. If we need to design a repairing rule for each different game and each different type of errors, why not designing a pattern-based or grammar-based generator directly? We'd like to devise an approach to repair the levels generated by GAN without the help of human designers. In this work we focus on using AI techniques to (i) exam the generated level and detect the logic errors, and then (ii) repair the logic errors in levels without human designers' help.In this work, we come up with an approach to learn the level constraints from real levels and then repair the defective levels generated by GAN. This approach is inspired by the thinking process of human when repairing broken pipes for Super Mario Bros levels. When one sees a broken pipe, this person will detect several possibly error tiles by observing their surrounding tiles. And then, this person may think of some schemes to replace those error tiles and analyze if this scheme is proper. After considering several replacement schemes, the best one and the resulted optimally repaired level will be determined. Our proposed approach imitates this process. First, we train a Multi-Layer Perceptron (MLP) model to judge whether a tile disobeys the constraint or not by taking its surrounding tiles as input. When repairing a defective level, we use the model to label the error tiles first, and then use a genetic algorithm (GA) to search for replacement schemes for those error tiles. In our case study, we have applied our approach to determining and repairing the broken pipes in Super Mario Bros levels generated by MarioGAN [7] and our experimental study has validated its effectiveness.The remainder of this paper is organized as follows. Some related work on PCG and Mario level generation are presented in Section II. Section III presents our proposed CNet for detecting error tiles in generated levels and recommending alternatives for repairing. Our novel evolutionary repairer is presented in Section IV. The experimental studies on these two approaches are described in Sections III-D and IV-D, respectively. Section VI concludes and discusses future work. D. Experimental setting and resultsWe design two levels with flaw to test the performance of our evolutionary repairer. In the first test, we train a GAN with the same structure of the one described in [7], while the same training data as for training the CNet (detailed previously in Section III). Then, some defective levels are collected from the levels generated by GAN (Fig. 2a). In the second test, different types of pipes from the training maps have been collected and put in one map. Then, tiles in this map are randomly selected to be destroyed. An example is shown in Fig. 3a.In all the experiments, the weights in the fitness function (Eg. 2) are set as ω 1 = 5, ω 2 = 3 and ω 3 = 1 after fine tuning. All the individuals in the population are initialized by randomly setting a tile type for the unstable tiles in solution space (cf. Section IV-A) using uniform distribution and applying repair once as described in Algorithm 1. The initial values of parameters are shown in the first line of Algorithm 2.When a tile is determined as a wrong tile, its alternative can be randomly selected from the Candidate Tiles following Algorithm 2 Evolution process for repairing a level l.1: Input: p m0 ← 0.8, p m1 ← 1 ||S l || , p r ← 0.3, n ← 20, RRT m ← 4 2:Initialize population X (cf. Section IV-D) 3: while time not elapsed do Choose different x i , x j from X according to probability P x (x ∈ X ) 9:x 1 , x 2 ←CROSSOVER(x i , x j ) Evaluate fitness of all x in X 21:X ← The best n individuals of X ∪ X selected using Round-robin Tournament.|",no,,,,no,,,,
579,https://github.com/matheuscnali/monocular_visual_odometry,matheuscnali_monocular_visual_odometry.tar.gz,Moving Objects Detection with a Moving Camera: A Comprehensive Review,@staticmethod,utils PIL copy yaml update sys glob torchvision threading pathlib re collections visualization torch os matplotlib scipy corr raft alt_cuda_corr sklearn argparse visual_odometry numpy time math extractor setuptools cv2 motion_detection random models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/matheuscnali_monocular_visual_odometry.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\matheuscnali_monocular_visual_odometry.pdf,,,no,,,,no,,,,
583,https://github.com/UKPLab/linspector-web,UKPLab_linspector-web.tar.gz,LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations,"@tag('fast', 'core') @receiver(post_delete, sender=Model) @unique @tag('slow', 'core', 'nn', 'contrastive') @receiver(pre_delete, sender=Model) @TokenEmbedder.register('sentence_embedding') @receiver(post_delete, sender=Epoch) @shared_task @register.filter @tag('fast', 'core', 'fixtures') @tag('core') @tag('core', 'nn', 'contrastive') @tag('slow', 'core', 'nn') @tag('fast') @tag('core', 'static') @tag('slow', 'nn', 'consistency') @tag('slow', 'core', 'nn', 'static') @tag('core', 'nn')",copy sys django_celery_results __future__ linspector ast uuid collections json transformers enum tempfile torch os codecs inspector urllib numpy time abc math random celery django inspect allennlp zipfile,cs.CL cs.CL cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UKPLab_linspector-web.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UKPLab_linspector-web.pdf,,,no,,,,no,,,,
588,https://github.com/Derpimort/VGGVox-PyTorch,Derpimort_VGGVox-PyTorch.tar.gz,VoxCeleb: a large-scale speaker identification dataset,,train math scipy signal_utils pandas torchsummary collections tqdm torch os argparse vggm numpy matplotlib time torchvision,cs.SD,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Derpimort_VGGVox-PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Derpimort_VGGVox-PyTorch.pdf,,"ExperimentsThis section describes the experimental setup for both speaker identification and verification, and compares the performance of our devised CNN baseline to a number of traditional state of the art methods on VoxCeleb.",no,,,"default values for hyperparameters (weight decay, learning rate)",no,,,,
591,https://github.com/dwofk/fast-depth,dwofk_fast-depth.tar.gz,NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications,@staticmethod,utils PIL csv warnings types __future__ torchvision accimage collections h5py torch os matplotlib scipy tvm argparse numbers imagenet numpy time shutil math random dataloaders metrics models,cs.CV cs.CV cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dwofk_fast-depth.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dwofk_fast-depth.pdf,Average Pooling  Depthwise Convolution  Pointwise Convolution  Rectified Linear Units  Batch Normalization  Global Average Pooling  1x1 Convolution  Convolution  Dense Connections  Softmax  Depthwise Separable Convolution  MobileNetV1  NetAdapt,,no,,no,"batch size, learning rate",no,,,,
593,https://github.com/jayleicn/TVCaption,jayleicn_TVCaption.tar.gz,TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval,@classmethod @abc.abstractmethod @property,utils decode_strategy sys tqdm easydict pprint json h5py nltk torch os string baselines pickle argparse tensorboardX numpy time abc math random logging subprocess pycocoevalcap zipfile,cs.CV cs.CL cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jayleicn_TVCaption.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jayleicn_TVCaption.pdf,,,no,,,empirically set,no,,,,
603,https://github.com/DengPingFan/PraNet,DengPingFan_PraNet.tar.gz,"Distribution-Free, Risk-Controlling Prediction Sets",,"utils imageio PIL lib torchvision datetime libtiff torch os os, argparse scipy argparse numpy time shutil thop math jittor random",cs.LG cs.AI cs.CV stat.ME stat.ML eess.IV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DengPingFan_PraNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DengPingFan_PraNet.pdf,Convolution,,no,,,,no,,,,
605,https://github.com/mf1832146/tree_transformer_2.0,mf1832146_tree_transformer_2.0.tar.gz,A SURVEY OF AUTOMATIC GENERATION OF SOURCE CODE COMMENTS: ALGORITHMS AND TECHNIQUES A PREPRINT,@staticmethod,"utils train module dataset pytorch_pretrained_bert pickle argparse torch os numpy math, copy time solver",cs.SE cs.AI cs.CL cs.SE cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mf1832146_tree_transformer_2.0.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mf1832146_tree_transformer_2.0.pdf,,,no,,,,no,,,,
610,https://github.com/MichiganCOG/devil,MichiganCOG_devil.tar.gz,The DEVIL is in the Details: A Diagnostic Evaluation Benchmark for Video Inpainting,@classmethod @abstractmethod @staticmethod,requests utils PIL flickrapi pandas yaml warnings sys update types glob ffmpeg tqdm pprint torchvision util threading parse re json unittest queue tempfile torch os matplotlib scipy itertools coloredlogs corr urllib alt_cuda_corr pickle sklearn argparse numpy time shutil abc math tarfile extractor setuptools traceback cv2 operator logging random subprocess detectron2 inspect io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MichiganCOG_devil.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MichiganCOG_devil.pdf,Inpainting,,no,,,,no,,,,
611,https://github.com/bayesgroup/semi-supervised-NFs,bayesgroup_semi-supervised-NFs.tar.gz,Semi-Conditional Normalizing Flows for Semi-Supervised Learning,@functools.wraps(conv) @property,utils PIL tabulate pandas yaml warnings sys tqdm algo torchvision configargparse pathlib datetime collections json torch os matplotlib datautils functools itertools strconv sklearn argparse filelock numbers logger numpy time myexman random models,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bayesgroup_semi-supervised-NFs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bayesgroup_semi-supervised-NFs.pdf,,,no,,,,no,,,,
617,https://github.com/cleopatra-itn/GeoWINE,cleopatra-itn_GeoWINE.tar.gz,"GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval","@app.route('/api/select_image_news_events', methods=['POST']) @app.route('/', methods=['GET']) @app.route('/api/upload_image_entities', methods=['POST']) @slim.add_arg_scope @app.route('/api/select_image_entities', methods=['POST']) @staticmethod","utils requests PIL pandas csv warnings sys glob events __future__ torchvision eventregistry pathlib tensorflow re json collections entity_retriever h5py pytorch_lightning torch os geolocation scipy s2sphere cairosvg urllib wikipediaapi argparse resizeimage numpy flask news geo_wine time waitress, api embedding cnn_architectures logging SPARQLWrapper io",cs.IR cs.MM,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cleopatra-itn_GeoWINE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cleopatra-itn_GeoWINE.pdf,,,no,,,,no,,,,
620,https://github.com/ccchang1023/maskrcnn-benchmark,ccchang1023_maskrcnn-benchmark.tar.gz,Is Heuristic Sampling Necessary in Training Deep Object Detectors?,"@C2_FORMAT_LOADER.register(""R-50-FPN-RETINANET"") @registry.BACKBONES.register(""R-101-FPN"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""FBNet.roi_head_keypoints"") @amp.float_function @C2_FORMAT_LOADER.register(""R-50-C5"") @C2_FORMAT_LOADER.register(""R-101-FPN"") @registry.ROI_BOX_PREDICTOR.register(""FastRCNNPredictor"") @registry.BACKBONES.register(""R-50-C5"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPNXconv1fcFeatureExtractor"") @registry.ROI_BOX_PREDICTOR.register(""FPNPredictor"") @registry.ROI_KEYPOINT_PREDICTOR.register(""KeypointRCNNPredictor"") @registry.BACKBONES.register(""R-50-FPN"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPN2MLPFeatureExtractor"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""FBNet.roi_head_mask"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNC4Predictor"") @C2_FORMAT_LOADER.register(""R-101-C4"") @registry.BACKBONES.register(""R-101-C5"") @property @once_differentiable @staticmethod @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FBNet.roi_head"") @registry.BACKBONES.register(""R-101-C4"") @C2_FORMAT_LOADER.register(""R-50-C4"") @registry.RPN_HEADS.register(""FBNet.rpn_head"") @registry.BACKBONES.register(""R-101-FPN-RETINANET"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""KeypointRCNNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-101-FPN-RETINANET"") @registry.BACKBONES.register(""R-152-FPN"") @registry.BACKBONES.register(""R-50-C4"") @registry.BACKBONES.register(""FBNet"") @registry.BACKBONES.register(""R-50-FPN-RETINANET"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""MaskRCNNFPNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-152-FPN"") @registry.RPN_HEADS.register(""SingleConvRPNHead"") @C2_FORMAT_LOADER.register(""R-50-FPN"") @C2_FORMAT_LOADER.register(""R-101-C5"") @unittest.skipIf(not TEST_CUDA, ""no CUDA detected"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNConv1x1Predictor"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""ResNet50Conv5ROIFeatureExtractor"")","utils PIL copy contextlib sys glob tqdm __future__ torchvision os, sys csHelpers re datetime json collections unittest h5py fnmatch tempfile env_tests torch os matplotlib importlib apex scipy IPython xml itertools bisect imp pickle argparse cityscapesscripts numpy time math pycocotools setuptools predictor maskrcnn_benchmark errno cv2 logging random yacs io",cs.CV cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ccchang1023_maskrcnn-benchmark.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ccchang1023_maskrcnn-benchmark.pdf,Focal Loss  Average Pooling  RetinaNet  ResNeXt Block  RoIAlign  Non Maximum Suppression  Step Decay  SGD with Momentum  Weight Decay  Self-Adjusting Smooth L1 Loss  RetinaMask  Feature Pyramid Network  Grouped Convolution  Bottleneck Residual Block  Global Average Pooling  Residual Block  Residual Connection  Rectified Linear Units  Kaiming Initialization  Max Pooling  1x1 Convolution  Convolution  Batch Normalization  Residual Network  ResNeXt  Group Normalization  Logistic Regression  Focal Loss  k-Means Clustering  Average Pooling  RetinaNet  ResNeXt Block  Darknet-53  YOLOv3  SSD  Weight Decay  SGD with Momentum  Feature Pyramid Network  Non Maximum Suppression  FCOS  Grouped Convolution  Bottleneck Residual Block  Global Average Pooling  Residual Block  Residual Connection  Rectified Linear Units  Kaiming Initialization  Max Pooling  1x1 Convolution  Batch Normalization  Residual Network  ResNeXt  Convolution  Softmax  Convolution  RoIAlign  Mask R-CNN  Focal Loss,"|C. Solutions for Foreground-Background ImbalanceAs we can see, the fg-bg imbalance always exists in training deep object detectors, which impedes deep object detectors from achieving higher accuracy as reported in [17], [24]. In previous works, there are three solutions:1) Sampling Methods: It is the most common solution for fg-bg imbalance, which has two groups [35] -hard sampling and soft sampling. The hard sampling method re-samples a set of training samples by some strategies. For example, biased sampling [4] randomly samples 256 examples with 1:1 fg-to-bg ratio during training RPN. OHEM [33] and IoUbalanced sampling [12] selects training samples according to loss and IoU values, respectively. The objectness [16], [19], [20] modules, generative methods [50], [51] can also be regarded as hard sampling methods. Soft sampling reweights training samples discriminatively by some strategies. Focal Loss [17] dynamically assigns higher weight to the hard training samples (i.e. with high loss value). Similar to Focal Loss, GHM [24] suppresses gradients originating from easy and very hard training samples (i.e. with low loss value). PISA [34] re-weights training samples according to the IoU between training samples and ground-truths.2) Classification to Ranking: As the fg-bg imbalance always exists in classification task in deep object detectors, AP Loss [40] and DR Loss [41] propose to convert the classification task into ranking task. These methods train a pair of samples rather than independent sample. Specifically, the predicted score of one training sample is transformed into the difference between the predicted scores of two training samples. These methods are also quite in line with the detection evaluation metric (average precision, AP).To date, almost all deep object detectors are equipped with sampling methods during training. The ranking task, however, trains pairs of N examples thus has O(N 2 ) computational cost, which is much higher than O(N ) cost of the classification task. Although sampling methods are popular, the re-sampling/re-weighting strategy is hard to design, and both sampling methods and ranking-based methods require laborious hyperparameter tuning. Our work overcomes these shortcomings, which discards sampling methods in the classification task without any hyperparameters introduced. B. AnalysisAs most deep object detectors are trained with mini-batch stochastic gradient descent (mini-batch SGD), we discuss here the effect of the fg-bg imbalance and heuristic sampling methods on mini-batch SGD training. For each iteration, the learnable parameters Θ of the detector will be updated in the direction of the gradient, i.e.Θ t+1 = Θ t − η ∂L ∂Θ t ,(4)where Θ t denotes the parameters in t-th step, and η is the learning rate. According to Equation 3, the gradient can be further expressed as∂L ∂Θ t = −s N i y i ∂[w f i log(p i )] ∂Θ t + (1 − y i ) ∂[w b i log(1 − p i )] ∂Θ t .(5)As the exact form of the parameters Θ t is unknown, a quantitative analysis for mini-batch SGD training seems impossible, especially after multiple training iterations. Therefore, in the following, we turn to analyze the case at the start of the training. We denote the learning parameters at the start of the training as Θ s . At this point, Θ s cannot distinguish foreground samples from background samples. In other words, the detector outputs similar confidence scores p i ≈ p for all samples. Then, we have∂L ∂Θ s ≈ −s N i y i ∂[w f i log(p)] ∂Θ s + (1 − y i ) ∂[w b i log(1 − p)] ∂Θ s ≈ −s N i y i [ ∂w f i ∂Θ s log(p) + w f i p ∂p ∂Θ s ] + (1 − y i )[ ∂w b i ∂Θ s log(1 − p) − w b i 1 − p ∂p ∂Θ s ].(6)If the weights w f i and w b i are constants, Equation 6 would be simple as∂w f i ∂Θ s = ∂w b i ∂Θ s = 0.However, in most soft sampling methods [17], [24], [34], the weight of a training sample is usually dynamic that may depend on confidence scores, IoU scores, and training iterations. Therefore, we would like to discuss the training cases of constant weights and dynamic weights, respectively.1) Constant Weights: In this case, we have w f i = w f , w b i = w b for any i, where w f and w b are constants. Thus, we also have∂w f i ∂Θ s = ∂w b i∂Θ s = 0, and Equation 6 can be derived as∂L ∂Θ s ≈ −s N i y i w f p ∂p ∂Θ s − (1 − y i ) w b 1 − p ∂p ∂Θ s = −s(N f w f p − N b w b 1 − p ) ∂p ∂Θ s ,(7)where N f and N b are the number of foreground and background training samples in a training iteration, respectively. Following [52], we use ||||•|||| to denote the L2-norm of a gradient vector, which represent its magnitude. From Equation 7, the gradient magnitude is|||| ∂L ∂Θ s |||| ≈ s||N f w f p − N b w b 1 − p || • |||| ∂p ∂Θ s ||||.(8)If Θ s is not biased, the initial estimate for both foreground and background samples are p = 0.5, then we have|||| ∂L ∂Θ s (p = 0.5)|||| ≈ 2s||N f w f − N b w b || • |||| ∂p ∂Θ s ||||.(9)When heuristic sampling methods are not used, i.e. w f i = w b i = 1, the gradient magnitude of the fg-bg imbalance case (N f N b ) will be much larger than that of the balanced case (N f ≈ N b ). Thus, if we train a detector without heuristic sampling methods, the fg-bg imbalance will result in a much larger gradient magnitude at the start of the training. If the scaling term s is not set properly, it may lead to training divergence. Compared with the gradient magnitude with heuristic sampling methods, the weighting terms w f and w b can alleviate the imbalance between N f and N b , thus leading to better stability at the start of the training.2) Dynamic Weights: In this case, we take the well-known Focal Loss [17] as an example, which proposes a unified representation of the weighting term as α t (1 − p t ) γ . When we apply it separately for foreground and background samples, we havew f i = α(1 − p i ) γ and w b i = (1 − α)p γ i .Here α and γ are the hyperparameters in Focal Loss for adaptively re-weighting training samples. As p i ≈ p at the start of the training, we havew f i ≈ α(1 − p) γ and w b i ≈ (1 − α)p γ . Then, Equation 6 can be derived as ∂F L ∂Θ s ≈ −s N i y i ∂p ∂Θ s [ ∂w f i ∂p log(p) + w f i p ] + (1 − y i ) ∂p ∂Θ s [ ∂w b i ∂p log(1 − p) − w b i 1 − p ] ≈ −s ∂p ∂Θ s N i y i α(1 − p) γ−1 [−γ log(p) + 1 − p p ] + (1 − y i )(1 − α)p γ−1 [γ log(1 − p) − p 1 − p ] ≈ −s ∂p ∂Θ s {N f α(1 − p) γ−1 [−γ log(p) + 1 − p p ] + N b (1 − α)p γ−1 [γ log(1 − p) − p 1 − p ]},(10)where F L denotes Focal Loss. As reported in [17], the best setting of Focal Loss is α = 0.25, γ = 2 on COCO dataset [43], and Focal Loss uses a biased initialization to ensure p ≈ 0.01. With these values, the gradient magnitude can be computed as|||| ∂F L ∂Θ s |||| ≈ 2s||10N f − 10 −4 N b || • |||| ∂p ∂Θ s ||||.(11)As we can see, the RHS (right hand side) of Equation 11is equal to the RHS of Equation 9 by setting w f = 10, w b = 10 −4 . Coincidentally, for COCO [43] dataset, a training anchor will learn 80 binary classifies for 80 object classes. In our observation, the fg-to-bg ratio of training anchor is 1 :10 3 , thus N b N f ≈ 8 × 10 4, which is close to w f w b = 10 5 . As we can see, Focal Loss also tries to alleviate the imbalance between N f and N b , to obtain a reasonable gradient magnitude.However, this does not mean that it is impossible to train the detector without heuristic sampling methods. In fact, we can reduce s to lower the excessive gradient magnitude. But this in turn creates the dilemma of too small gradient magnitude. Specifically, when we set a small s m as the scaling factor to train a detector without heuristic sampling methods, as N f N b , p will rapidly approach p ≈ N f N f +N b to achieve a minimal loss value, and the gradient magnitude|||| ∂L ∂Θ s (w f = w b = 1, s = s m )|||| ≈ s m || N f p − N b 1 − p || • |||| ∂p ∂Θ s ||||(12)will be greatly decreased. At this point, s m becomes unreasonable and we should set a large scaling factor. It is not surprising why most effective heuristic sampling methods are dynamic, like Focal Loss [17] and GHM [24].To sum up, it is essential to control the gradient magnitude in the classification task when training a detector without heuristic sampling methods. As illustrated in [42], [53], the gradient magnitude will have a significant impact on the performance of multi-task learning. Object detection usually involves two or more tasks, the unreasonable gradient magnitude on the classification task affects not only itself but also other tasks. But in fact, we have various ways to control the gradient magnitude, and the heuristic sampling method is not the only choice.Furthermore, the fg-bg imbalance, as we illustrated in Sec. III-A, also has the similar distribution in training and inference. If we use heuristic sampling methods during training, then it is equivalent to breaking the consistency of this distribution. In other words, Θ e is obtained from the weighted imbalance distribution, which may not perform well in the vanilla imbalance distribution. Next, we will experimentally explore how to train deep object detectors without heuristic sampling methods. IV. METHODOur investigation reveals that by tuning s and π, the detector without heuristic sampling methods can achieve a similar detection accuracy to that with heuristic sampling methods.However, tuning them is always laborious. In this section, we propose a novel Sampling-Free mechanism, which addresses the fg-bg imbalance by adaptively setting s and π, thus adaptively controlling the classification gradient magnitude.1) Discarding Heuristic Sampling Methods: As sampling methods are always a default part in training deep object detectors, the first step of our Sampling-Free mechanism is discarding heuristic sampling methods during training. For one-stage anchor-based object detectors [16]- [23], [39], soft sampling methods (e.g. Focal Loss [17], GHM [24], PISA [34]) is widely used for re-weighting training samples in the classification task. In our method, we use the standard CE loss to train the classification task, which treats all training samples equally.For two-stage anchor-based approaches [4]- [15], hard sampling methods (e.g. biased sampling [4], OHEM [33]) are widely used for re-sampling training samples. In our method, we train all training samples in RPN and RoI-subnet. For example, a common implementation of biased sampling [4] in training Faster R-CNN is: (1) RPN randomly selects 256 anchors with a biased 1:1 fg-to-bg ratio, (2) RoI-subnet randomly selects 512 proposals with a biased 1:3 fg-to-bg ratio. In our method, we train all examples in both RPN and RoI-subnet. That is to say, we train RPN with all foreground/background anchors (∼10 5 per-image) and train RoI-subnet with all foreground/background proposals (∼10 3 per-image). We use CE loss as the classification loss both in RPN and RoI-subnet.For anchor-free object detectors [11], [14], [25]- [32], they regard ""points"" as the training samples rather than ""anchors"" in anchor-based object detectors, and most of them use soft sampling methods to address the fg-bg imbalance. We follow the principle of Sampling-Free in anchor-based object detectors that equally use all training samples during training, i.e. we use the standard CE loss in the classification task.2) Optimal Bias Initialization: Sec. III-C has shown that adjusting π can help to avoid network diverging. However, it is difficult to determine π. We propose optimal bias initialization to compute π from data statistics rather than tuning it. Our idea is to initialize the bias of the last convolutional layer to obtain a minimal classification loss value. The derivative of L CE is∂L CE ∂π = − 1 π + ( N N f − 1) 1 1 − π . (17)When π = N f N , ∂L CE ∂π = 0, and L CE will attain the minimal value. As the predicted score is predicted with the sigmoid activation, we can obtain the optimal initial bias asb = − log 1 − π π = − log( N N f − 1). (18)Here N N f can be computed by pre-defined anchors, thus the computation is efficient as it does not require network forwarding. In our observation, N N f ≈ 10 5 , which corresponds to π = 10 −5 that performs best in our experiments. It is worth noting that the accuracy of the model is robust to our initialization strategy, as the model can ""utilize"" the imbalanced distribution to obtain a lower loss. We initialize the model to ensure the stability of the classification loss.3) Guided Loss Scaling: Usually, the overall loss function to train a deep object detector is composed of a localization loss term L LOC and a classification loss term L CLS . Let L t denote the overall loss in the t-th training step. When we use CE loss as the classification loss, we haveL t = (L LOC ) t + (L CLS ) t = (L LOC ) t + s t (L CE ) t , (19)where s t is used to scale the (L CE ) t as the CE loss scale is unreasonable under the fg-bg imbalance. As mentioned in III-C, it is essential to control the classification loss scale to be close to the localization loss scale. A straightforward way is to adjust s t . However, it results in a new hyperparameter.Our key idea is to adjust s t dynamically during training. That says, instead of using a constant s t , we define a guided termg t = (L LOC ) t (L CE ) t ,(20)and let s t = g t , which suggests using the localization loss scale of the current mini-batch as the target of the rescaled CE loss scale. Thus, this technique is termed ""guided loss scaling"". It is worth noting that g t is only used for scaling the classification loss, i.e. its gradient is ignored in the backpropagation. Therefore, the overall gradient is∂L t ∂Θ t = ∂(L LOC ) t ∂Θ t + g t ∂(L CE ) t ∂Θ t ,(21)which ignores the gradient calculation of s t . Our guided loss scaling can be interpreted threefold. First, according to Sec. III-C (especially Fig. 3), it appears a good choice to let the localization loss scale and the classification loss scale be similar, where the classification loss is either Focal Loss or CE loss. Second, it is convenient to use the localization loss as guidance, because the localization loss is already there for object detection. Third, the classification loss without sampling methods (i.e. CE loss) is greatly influenced by the fg-bg imbalance, but localization loss is little influenced as it is computed merely for foreground anchors. Thus, the localization loss is helpful to control the unreasonable classification loss due to the fg-bg imbalance.However, in our experiments, we find that the detector may not achieve the best detection accuracy when the classification loss is simply equal to the localization loss. Fortunately, the well-known uncertainty weighting [57] proposes a simple method to weigh two losses from the perspective of Bayesian uncertainty. When we apply the method to our case, the overall loss would beL t = 1 (σ t 1 ) 2 (L LOC ) t + 1 (σ t 2 ) 2 (L CE ) t + 2 log(σ t 1 σ t 2 ),(22)where σ t 1 and σ t 2 are learnable parameters, and they are initialized asσ 0 1 = σ 0 2 = 1. 2 log(σ t 1 σ t 2 )is the normalization term to avoid the degradation of 1 (σ t 1 ) 2 → 0 and 1 (σ t 2 ) 2 → 0. But if we train the detector in this way, the training would be quickly failed, as the classification loss would be much larger (∼10×) than the localization loss at the start of the training (see Figure 3). Hence, the guided term g t is necessary, and the overall loss should beL t = 1 (σ t 1 ) 2 (L LOC ) t + g t (σ t 2 ) 2 (L CE ) t + 2 log(σ t 1 σ t 2 ).(23)To keep the consistency with the original training loss, we can identify that the localization loss does not require weighting (σ t 1 = 1). We can also use δ t to denote 1 (σ t 2 ) 2 , then the overall loss would be very simple, i.e.L t = (L LOC ) t + g t δ t (L CE ) t − log δ t .(24)We notice that there have been several works [53], [57], [58] for adaptive multi-task loss scaling. Our guided loss scaling is different from them in three points: 1) it is aimed at controlling the classification loss under the fg-bg imbalance, which belongs to the single-task loss weighting rather than the multi-task loss weighting; 2) it is specifically designed for deep object detectors as it requires the localization loss to guide the classification loss; 3) it converts the class imbalance problem to the loss scaling problem, which seems not reported before in the literature, to our best knowledge.|",no,,,,no,,,,
621,https://github.com/rhythmswing/Fair-Representation-Learning,rhythmswing_Fair-Representation-Learning.tar.gz,Learning Fair Representations via an Adversarial Framework,@jit,copy pandas csv sys dumb_containers pdb pylab helpers_lfr model torch os helpers matplotlib scipy IPython numba pickle sklearn pyemd numpy time train random,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rhythmswing_Fair-Representation-Learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rhythmswing_Fair-Representation-Learning.pdf,,"EXPERIMENTAL SETUPIn this section, we describe the datasets, baselines, and evaluation metrics used in our experiments.Datasets. We conduct experiments on four real-world datasets:• Adult [28]. This dataset is extracted from the 1994 Census database. Each sample represents an individual and is classified based on whether the individual's annual income exceeds $50,000.• Statlog [28]. This is the German credit data. In this dataset, every individual is classified as either good or bad in terms of credit risks. • Fraud. This dataset is provided by PPDai, the largest unsecured micro-credit loan platform in China. It consists of over 200,000 registered users and over 37 million call logs between them. Each user's features are extracted from their basic information (e.g., age, gender, education, etc.) and call behavior (e.g., call number, call duration, etc.). We aim to determine each user's credit risk by deducing whether they will default pn a loan for more than 90 days. • Investor. This dataset is provided by PPDai. It consists of almost 10,000 registered investors, and each of which is classified as investing over $73,000. The attributes are the user's basic information (e.g., age, gender, residential place, house price) and behavior (e.g., frequency of use of the app, etc.)For all datasets, we choose Gender as the protected attribute. Female is the protected group in our tasks. Note that although gender is a binary attribute in all the four datasets, we recognize that gender may not be binary. Tasks. In our experiments, we first learn the latent representations according to the given feature matrix X, then we conduct the following classification tasks to validate the effectiveness of the learned representations:• Task I. We use the representation methods to learn the latent features of the data and estimate the information loss by MSE and the distributional distance between different groups of the protected attribute.Conference'17, July 2017, Washington, DC, USA Rui Feng 1 , Yang Yang • Task II. We further use the learned latent features to classify against the data label and examine the size of performance drop and whether the results meet fairness constraints.Baseline methods. In our experiments, we employ and compare the following different methods for representation learning.• Original. This method directly uses all features to train a classifier for task I. • Original-P. This process employs all features except for the protected one to train the classifier. • AutoEncoder. This process uses an autoencoder to learn representations according to all features. • AutoEncoder-P. This method uses an autoencoder to learn representations according to all features but the protected one. • LFR. This is a model for learning fair representations that was proposed by [42]. In this method, instances are assigned to certain prototypes as latent representations. We use the parameters advised by the authors [42]. • NRL. This is the proposed fair representation learning method.We also have compared NRL with the methods proposed by Madras et al. [30] and Edwards and Storkey [13]. However, we omit the detailed results considering their unstable performance. More specifically, these two methods often require a more complex network architecture than ours. In addition, when using a three-layer neural network for autoencoder and the adversary on the Adult dataset, we found the adversary in Madras et al. [30] and Edwards and Storkey [13]'s model indeed cannot predict the protected attribute. However, after the representation is learned, a classifier of the same architecture usually separates well the protected groups. Methods of evaluation. We evaluate the quality of the representation learning using the following metrics:• The mean square error of the autoencoder, which estimates the information loss of the representation. • The EMD distance between the representations of different groups.In addition, in order to investigate the actual effect of our method, classification based on the learned latent representations is also studied. To obtain the classification results, we employ RLR(ℓ 2regularized Logistic Regression, also known as the ridge logistic regression) on X and Z. The following metrics are used to quantify the quality of classification.• Classification performance against labels, evaluated by the F1score. F1-score is calculated by the harmonic average of the precision and recall scores of the prediction. • The statistical parity score of the classification result against labels, defined as max( 1 n 0 p i =0 Ψ(z i ), 1 n 1 p i =1 Ψ(z i )) min ( 1 n 0 p i =0 Ψ(z i ), 1 n 1 p i =1 Ψ(z i ))• Consistency score of the classification, defined asyN N = 1 − 1 n n i=1 ŷi − 1 k j ∈k N N (x i ) ŷjThis equation evaluates the average differences in classification scores between a point and its k-nearest neighbors. A similar calculation is also used in [42]. In our experiments, we use k = 1. Specifically, to evaluate performance, fair representations are first learned and then fixed. On the learned latent features (or the original features, if no representation is learned), the EMD distance between the groups of the protected attribute and the MSE reconstruction loss are computed.Then, we train a logistic regression on the latent features to predict labels of data. The logistic regression predicts for each sample a score between 0 and 1 representing its probability of being positively labeled. Using the predicted score, the statistical parity score and the consistency score are calculated.",no,,,,no,,,,
624,https://github.com/Sid2697/Word-recognition-and-retrieval,Sid2697_Word-recognition-and-retrieval.tar.gz,AGDC: Automatic Garbage Detection and Collection,,pdb pickle logging tqdm argparse torch sklearn numpy time Levenshtein,cs.RO cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Sid2697_Word-recognition-and-retrieval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Sid2697_Word-recognition-and-retrieval.pdf,,,no,,,,no,,,,
629,https://github.com/cosimoizzo/neutral-baseline-for-shapley-values,cosimoizzo_neutral-baseline-for-shapley-values.tar.gz,A Baseline for Shapley Values in MLPs: from Missingness to Neutrality,@staticmethod,"scipy tensorflow itertools explain pandas os, sys, inspect random sklearn tqdm numpy",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cosimoizzo_neutral-baseline-for-shapley-values.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cosimoizzo_neutral-baseline-for-shapley-values.pdf,,,no,,,,no,,,,
637,https://github.com/Voice-Privacy-Challenge/Voice-Privacy-Challenge-2020,Voice-Privacy-Challenge_Voice-Privacy-Challenge-2020.tar.gz,Introducing the VoicePrivacy Initiative,,"pandas sys performance __future__ ioTools os amfm_decompy matplotlib kaldi_io kaldiio scipy sklearn argparse sys,os numpy shutil imp, sys, argparse, os, math, subprocess math librosa operator random",cs.CL cs.CL cs.SD eess.AS cs.CR eess.AS eess.AS cs.CR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Voice-Privacy-Challenge_Voice-Privacy-Challenge-2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Voice-Privacy-Challenge_Voice-Privacy-Challenge-2020.pdf,,,no,,,,no,,,,
643,https://github.com/xiahaifeng1995/FAVAE-anomaly-detection-localization-master,xiahaifeng1995_FAVAE-anomaly-detection-localization-master.tar.gz,Anomaly localization by modeling perceptual features,,utils PIL skimage tqdm torchvision torch os matplotlib scipy func sklearn argparse numpy time math datasets torchsummary cv2 random models,cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xiahaifeng1995_FAVAE-anomaly-detection-localization-master.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xiahaifeng1995_FAVAE-anomaly-detection-localization-master.pdf,Variational Autoencoder  AutoEncoder,,no,,,same hyperparameter in all experiments but without optimization,no,,,,
646,https://github.com/apple/ml-multiple-futures-prediction,apple_ml-multiple-futures-prediction.tar.gz,Multiple Futures Prediction,@gin.configurable,multiple_futures_prediction math scipy gin datetime json glob cv2 typing pickle torch argparse os subprocess attrdict numpy time,cs.LG cs.CV cs.MA cs.RO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/apple_ml-multiple-futures-prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\apple_ml-multiple-futures-prediction.pdf,,,no,,,,no,,,,
648,https://github.com/pykao/ISLES2017-mRS-prediction,pykao_ISLES2017-mRS-prediction.tar.gz,Predicting Clinical Outcome of Stroke Patients with Tractographic Feature,,utils csv multiprocessing nibabel skimage medpy SimpleITK os matplotlib scipy itertools sklearn argparse natsort xgboost paths numpy shutil math logging subprocess utils_40,eess.IV cs.LG q-bio.QM stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pykao_ISLES2017-mRS-prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pykao_ISLES2017-mRS-prediction.pdf,,,no,,,,no,,,,
649,https://github.com/HDI-Project/BTB,HDI-Project_BTB.tar.gz,The Machine Learning Bazaar: Harnessing the ML Ecosystem for Effective System Development,"@patch('btb.selection.custom_selector.random') @patch('btb.session.np.random.choice') @patch('btb.tuning.tunable.BooleanHyperParam') @patch('btb.selection.hierarchical.HierarchicalByAlgorithm.bandit') @pytest.mark.skip(reason=""This is not implemented yet"") @patch('btb.tuning.hyperparams.base.np.array') @patch('btb.tuning.tunable.list') @patch('btb.tuning.hyperparams.base.np.isscalar') @patch('btb.tuning.tunable.CategoricalHyperParam') @patch('btb.tuning.metamodels.gaussian_process.super') @patch('btb.tuning.hyperparams.numerical.IntHyperParam._transform') @patch('btb.tuning.tunable.FloatHyperParam') @property @staticmethod @patch('btb.tuning.tunable.IntHyperParam') @patch('btb.tuning.hyperparams.base.np.asarray') @patch('random.choice') @patch('btb.tuning.hyperparams.numerical.IntHyperParam._inverse_transform') @patch('btb.tuning.hyperparams.numerical.np.random.random') @use_named_args(space) @patch('btb.session.Tunable') @abstractmethod @patch('btb.selection.best.BestKReward.bandit') @patch('btb.tuning.tuners.base.BaseTuner._check_proposals') @patch('btb.tuning.hyperparams.numerical.sys') @classmethod @patch('btb.tuning.hyperparams.categorical.OneHotEncoder') @patch('btb.tuning.tuners.base.super') @patch('btb.selection.ucb1.UCB1._shuffle') @patch('btb.session.isinstance') @patch('btb.tuning.hyperparams.boolean.np.random.random') @patch('btb.selection.recent.RecentKReward.bandit') @dask.delayed @patch('btb.selection.pure.PureBestKVelocity.bandit')",tabulate copy pandas ax yaml warnings sys btb_benchmark btb skopt ConfigSpace tqdm copulas re datetime json collections unittest smac tempfile dask_kubernetes os pytest socket sphinx_rtd_theme importlib scipy itertools hashlib kubernetes urllib sklearn argparse boto3 xgboost numpy abc dask distributed hyperopt setuptools random logging inspect io,cs.SE cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HDI-Project_BTB.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HDI-Project_BTB.pdf,,"INTRODUCTIONOnce limited to conventional commercial applications, machine learning (ML) is now being widely applied in physical and social sciences, in policy and government, and in a variety of industries. This diversification has led to difficulties in actually creating and deploying real-world systems, as key functionality becomes fragmented across ML-specific or domain-specific software libraries created by independent communities. In addition, the process of building problemspecific end-to-end systems continues to be marked by ML and data management challenges, such as formulating achievable learning problems [29], managing and cleaning data and metadata [6,38,49], scaling tuning procedures [17,34], and deploying models and serving predictions [3,12]. In practice, engineers and data scientists often spend significant effort developing ad hoc programs for new problems: writing ""glue code"" to connect components from different software libraries, processing different forms of raw input, and interfacing with external systems. These steps are tedious and error-prone and lead to the emergence of brittle ""pipeline jungles"" [43].These points raise the question, ""How can we make building ML systems easier in practical settings?"" A new approach is needed to designing and developing software systems that solve specific ML tasks. Such an approach should address a wide variety of input data modalities, such as images, text, audio, signals, tables, graphs, and learning problem types, such as regression, classification, clustering, anomaly detection, community detection, graph matching; it should cover the intermediate stages involved, such as data preprocessing, munging, featurization, modeling, and evaluation; and it should enable AutoML functionality to fine-tune solutions, such as hyperparameter tuning and algorithm selection. Moreover, it should offer coherent APIs, fast iteration on ideas, and easy integration of new ML innovations. In sum, this ambitious goal would allow almost all end-to-end learning problems to be solved or built using a single framework.To address these challenges, we present the Machine Learning Bazaar, 1 a framework for designing and developing ML and AutoML systems. We organize the ML ecosystem into composable software components, ranging from basic building blocks like individual classifiers to full AutoML systems. With our design, a user specifies a task, provides a raw dataset, and either composes an end-to-end pipeline out of pre-existing, annotated, ML primitives or requests a curated pipeline for their task (Figure 1). The resulting pipelines can be easily evaluated and deployed across a variety of software and hardware settings and tuned using a hierarchy of AutoML approaches. Using our own framework, we have created an AutoML system which we have entered in participation in DARPA's Data-Driven Discovery of Models (D3M) program; ours is the first end-to-end, modular, publicly released system designed to meet the program's goal.To preview the potential of development using our framework, we highlight the Orion project within MIT for MLbased anomaly detection in satellite telemetry (Figure 2), as one of several successful real-world applications that uses ML Bazaar for effective ML system development (Section 4.1). The Orion pipeline processes a telemetry signal using several custom preprocessors, an LSTM predictor, and a dynamic thresholding postprocessor to identify anomalies. The entire pipeline can be represented in a short Python snippet, custom processing steps are easily implemented as modular components, two external libraries are integrated without glue code, and the pipeline can be tuned using composable AutoML functionality.Our contributions in this paper include:A composable framework for representing and developing ML and AutoML systems: Our framework enables users to specify a pipeline for any ML task, ranging from image classification to graph matching, through a unified API (Sections 2 and 3).The first general-purpose automated machine learning system: Our system, AutoBazaar, is, to the best of our knowledge, the first open-source, publicly-available, system with the ability to reliably compose end-to-end, automaticallytuned, solutions for 15 data modalities and problem types (Section 3.3). AUTOML SYSTEM DESIGN AND ARCHITECTUREFrom the components of the ML Bazaar, data scientists can easily and effectively build ML pipelines with fixed hyperparameters for their specific problems. To improve the performance of these solutions, we introduce the more general pipeline templates and pipeline hypertemplates and then present the design and implementation of AutoML primitives which facilitate hyperparameter tuning and model selection, either using our own library for Bayesian optimization or external AutoML libraries. Finally, we describe AutoBazaar, one specific AutoML system we have built on top of these components. AutoML PrimitivesJust as primitives units components of ML computation, Au-toML primitives represent components of an AutoML system. We separate AutoML primitives into tuners and selectors. In our extensible AutoML library for developing AutoML systems, BTB, 6 we provide various instances of these AutoML primitives.3.2.1 Tuners. Given a pipeline template, an AutoML system must find a specific pipeline with fully-specified hyperparameter values to maximize some utility. Given pipeline template T and a function f that assigns a performance score to pipeline L λ with hyperparameters λ ∈ Λ, the tuning problem is defined as λ * = arg max λ ∈Λ f (L λ ). We introduce tuners, AutoML primitives which provide a record/propose interface in which evaluation results are recorded to the tuner by the user or by an AutoML controller and new hyperparameters are proposed in return.Hyperparameter tuning is widely studied and its effective use is instrumental to maximizing the performance of ML systems [4,5,18,46]. One widely used approach to hyperparameter tuning is Bayesian optimization (BO), a black-box optimization technique in which expensive evaluations of f are kept to a minimum by forming and updating a metamodel for f . At each iteration, the next hyperparameter configuration to try is chosen according to an acquisition function. We structure these meta-models and acquisition functions as separate BO-specific AutoML primitives that can be combined together to form a tuner. Researchers have argued for different formulations of meta-models and acquisition functions [39,46,51]. In our BTB library for AutoML, we implement the GP-EI tuner, which uses a Gaussian Process meta-model primitive and an Expected Improvement (EI) acquisition function primitive, among several other tuners. Many other tuning paradigms exist, such as those based on evolutionary strategies [37,40], adaptive execution [27,33], meta-learning [21], or reinforcement learning [16]. Though we have not provided implementations of these in BTB, one could do so using our common API. Selectors.For many ML task types, there may be multiple pipeline templates or pipeline hypertemplates available, each with their own tunable hyperparameters. The aim is to balance the exploration-exploitation tradeoff while selecting promising pipeline templates to tune. For a set of pipeline templates T , we define the selection problem as T * = arg max T ∈T max λ T ∈Λ T f (L λ T ). We introduce selectors, AutoML primitives which provide a compute_rewards/select API.Algorithm selection is often treated as a multi-armed bandit problem where the score returned from a selected template can be assumed to come from an unknown underlying probability distribution. In BTB, we implement the UCB1 selector, which uses the upper confidence bound method [2], among several other selectors. Users or AutoML controllers can use selectors and tuners together to perform joint algorithm selection and hyperparameter tuning. Building an AutoML systemUsing the ML Bazaar framework, we have built AutoBazaar, 7 an open-source, end-to-end, general-purpose, multi-task, automated machine learning system. It consists of several components: an AutoML controller; a pipeline execution engine; data stores for metadata and pipeline evaluation results; loaders and configuration for ML tasks, primitives, etc.; a Python language client; and a command-line interface. AutoBazaar is an open-source variant of the AutoML system we have developed for the DARPA D3M program.We focus here on the core pipeline search and evaluation algorithms (Algorithm 2). The input to the search is a computational budget and an ML task, which consists of the raw data and task and dataset metadata -dataset resources, problem type, dataset partition specifications, and an evaluation procedure for scoring. Based on these inputs, AutoBazaar searches through its catalog of primitives and pipeline templates for the most suitable pipeline that it can build. First, the controller loads the train and test dataset partitions, D (t r ain) and D (t est ) , following the metadata specifications. Next, it loads from its default catalog and the user's custom catalog a collection of candidate pipeline templates suitable for the ML task type. Using the BTB library, it initializes a UCB1 selector and a collection of GP-EI tuners for joint algorithm selection and hyperparameter tuning. The search process begins and continues for as long as the computation budget has not been exhausted. In each iteration, the selector is queried to select a template, the corresponding tuner is queried to propose a hyperparameter configuration, a pipeline is generated and scored using cross validation over D (t r ain) , and the score is reported back to the selector and tuner. The best overall pipeline found during the search, L * , is re-fit on D (t r ain) and scored over D (t est ) . Its specification is returned to the user alongside the score obtained, s * .  DiscussionThrough these applications using the components of the ML Bazaar, several advantages surfaced. 4.3.1 Composability. One important aspect of ML Bazaar is that it does not restrict the user to use a single monolithic system, rather users can pick and choose parts of the framework they want to use. For example, Orion uses only MLPrimitives/MLBlocks, Cardea uses MLPrimitives but integrates the hyperopt library for hyperparameter tuning, our D3M AutoML system submission mainly uses AutoML primitives and BTB, and AutoBazaar uses every component. RELATED WORKResearchers have developed numerous algorithmic and software innovations to make it possible to create ML and Au-toML systems in the first place.ML libraries. High-quality ML libraries have originated over a period of decades. For general ML applications, scikitlearn implements many different algorithms using a common API centered on the influential fit/predict paradigm [11]. For specialized analysis, libraries have been developed in separate academic communities, often with different and incompatible APIs [1,7,10,24,28,32]. In ML Bazaar, we connect and link components of these libraries, only creating missing functionality ourselves.ML systems. Prior work has provided several approaches for making it easier to develop ML systems. For example, caret [31] standardizes interfaces and provides utilities for the R ecosystem, but without enabling more complex pipelines. Recent systems have attempted to provide graphical interfaces, like [22] and Azure Machine Learning Studio. Development of ML systems is closely tied to the execution environments needed to train, deploy, and update the resulting models. In SystemML [9] and Weld [45], implementations of specific ML algorithms are optimized for specific runtimes. Velox [12] is an analytics stack component that efficiently serves predictions and manages model updates.AutoML libraries. AutoML research has often been limited to solving sub-problems of an end-to-end ML workflow, such as data cleaning [14], feature engineering [28,30], hyperparameter tuning [18,21,25,27,33,40,46,48], or algorithm selection [25,50]. Thus AutoML solutions are often not widely applicable or deployed in practice without human support. In contrast, ML Bazaar integrates many of these existing approaches and designs one coherent and configurable structure for joint tuning and selection of end-to-end pipelines.AutoML systems. These AutoML libraries, if deployed, are typically one component within a larger system that aims to manage several practical aspects such as parallel and distributed training, tuning, and model storage, and even serving, deployment, and graphical interfaces for model building. These include ATM [47], Vizier [20], and Rafiki [52], as well as commercial platforms like Google AutoML, DataRobot, and Azure Machine Learning Studio. While these systems provide many benefits, they have several limitations. First, they each focus on a specific subset of ML use cases, such as computer vision, NLP, forecasting, or hyperparameter tuning. Second, these systems are designed as proprietary applications and do not support community-driven integration of new innovations. ML Bazaar provides a new approach to developing such systems in the first place: it supports a wide variety of ML task types, and builds on top of a communitydriven ecosystem of ML innovations. Indeed, it could serve as the backend for such ML services or platforms.The DARPA D3M program [35], of which we are participants, aims to spur development of automated systems for model discovery for use by non-experts. Several differing approaches are being developed within this context. For example, Alpine Meadow [44] focuses on efficient search for producing interpretable ML pipelines with low latencies for interactive usage. It combines existing techniques from query optimization, Bayesian optimization, and multi-armed bandits to efficiently search for pipelines. AlphaD3M [16] formulates a pipeline synthesis problem and uses reinforcement learning to construct pipelines. In contrast, ML Bazaar is a framework to develop ML or AutoML systems in the first place. While we present our open-source AutoBazaar system, it is not the primary focus of our work and represents a single point in the design space of AutoML systems using our framework libraries. Indeed, one could use specific AutoML approaches like the ones described by Alpine Meadow or AlphaD3M for pipeline search within our own framework.",no,,,,no,,,,
650,https://github.com/Prithwiraj12/Bengali-Deep-News-Summarization,Prithwiraj12_Bengali-Deep-News-Summarization.tar.gz,Bengali Abstractive News Summarization (BANS): A Neural Attention Approach,,requests data_utils pandas sys bs4 __future__ seq2seq_model re tensorflow datetime nltk os six rouge configparser codecs numpy time math random,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Prithwiraj12_Bengali-Deep-News-Summarization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Prithwiraj12_Bengali-Deep-News-Summarization.pdf,Tanh Activation  Sigmoid Activation  Long Short-Term Memory  Sequence to Sequence,,no,,,,no,,,,
651,https://github.com/dreossi/analyzeNN,dreossi_analyzeNN.tar.gz,Counterexample-Guided Data Augmentation,@property,utils PIL copy analysis_nn vgg16_convDet sys resnet50_convDet glob easydict gen_utils squeezedet __future__ kitti_vgg16_config kitti_squeezeDetPlus_config threading update_library tensorflow kitti datetime collections im_obj image_generation_utils os caffe components matplotlib nets six scipy xml kitti_model_config image_mod_gen_utils dataset kitti_squeezeDet_config lib_obj pickle argparse pascal_voc joblib numpy nn_skeleton time config shutil train math squeezeDet squeezeDetPlus kitti_res50_config random cv2 subprocess cPickle,cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dreossi_analyzeNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dreossi_analyzeNN.pdf,,,no,,,,no,,,,
653,https://github.com/RowitZou/topic-dialog-summ,RowitZou_topic-dialog-summ.tar.gz,Topic-Oriented Spoken Dialogue Summarization for Customer Service with Saliency-Aware Topic Modeling,@classmethod @property @staticmethod,gc copy sys signal glob train_abstractive __future__ prepro gensim threading re datetime collections json nltk torch os rouge torchtext pytorch_transformers codecs pickle argparse tensorboardX numpy time math others unicodedata translate distributed traceback random logging models io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/RowitZou_topic-dialog-summ.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\RowitZou_topic-dialog-summ.pdf,,,no,,,supplementary material not available,no,,,,
654,https://github.com/correlllab/nn4mc,correlllab_nn4mc.tar.gz,Embedded Neural Networks for Robot Autonomy,,"keras tensorflow sys, os pandas sys collections __future__ numpy matplotlib",cs.RO eess.SP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/correlllab_nn4mc.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\correlllab_nn4mc.pdf,,,no,,,,no,,,,
655,https://github.com/FarnazAdib/Crash_course_on_RL,FarnazAdib_Crash_course_on_RL.tar.gz,A Crash Course on Reinforcement Learning,,scipy copy tensorflow warnings datetime collections setuptools random lq argparse os mpl_toolkits numpy matplotlib cartpole gym,cs.LG cs.SY eess.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/FarnazAdib_Crash_course_on_RL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\FarnazAdib_Crash_course_on_RL.pdf,,"Categorizing RL agentThere are many ways to categorize an RL agent, like model-free and model-based, online or offline agents, and so on. One possible approach is to categorize RL agents based on the main components that the RL agent is built upon. Then, we will have the following classification• Policy gradient.• Dynamic Programming (DP)-based solutions.• Model building.Policy gradient approaches are built upon defining a policy for the agent, DP-based solutions require estimating value functions and model-building approaches try to estimate a model of the environment. This is a coarse classification of approaches; indeed by combining different features of the approaches, we get many useful variations which we do not discuss in this handout.All aforementioned approaches reduce to some sort of function approximation from data obtained from the dynamical systems. In policy gradient, we fit a function to the policy; i.e. we consider policy as a function of state π = network(state). In DP-based approach, we fit a model to the value function to characterize the cost-to-go. In the model-building approach, we fit a model to the state transition of the environment.As you can see, in all approaches, there is a modeling assumption. The thing which makes one approach different from another is ""where"" to put the modeling assumption: policy, value function or dynamical system. The reader should not be confused by the term ""model-free"" and think that no model is built in RL. The term ""model-free"" in RL community is simply used to describe the situation where no model of the dynamical system is built.",no,,,,no,,,,
659,https://github.com/almost-matching-exactly/DAME-FLAME-Python-Package,almost-matching-exactly_DAME-FLAME-Python-Package.tar.gz,dame-flame: Interpretable Causal Inference dame-flame: A Python Library Providing Fast Interpretable Matching for Causal Inference,,dame_flame math itertools pandas setuptools unittest sys operator sklearn os numpy,cs.LG cs.MS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/almost-matching-exactly_DAME-FLAME-Python-Package.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\almost-matching-exactly_DAME-FLAME-Python-Package.pdf,Causal Inference,"Package UsageBasic Functionality. To run DAME or FLAME, the user provides the input data as a data frame or file, which must contain an outcome column, and a treatment column. Each algorithm by default produces a table consisting of the units that were matched to at least one other unit. For each unit that was matched, the table indicates which of the covariates were used for matching, and the covariate values that each unit was matched on. The covariates that were not used to match the unit are denoted with ""*"" as their values. Users can find matched groups of individual units and can estimate treatment effects, including the average treatment effect (ATE) and conditional average treatment effect (CATE) of a selected unit after either algorithm has been run. Comparison to Other Matching Packages. Many of the features previously described are unavailable in other matching packages. Table 1 compares the characteristics of dame-flame against popular alternatives. Most matching packages are implemented in R. R's cem package only supports ATT treatment effects (Iacus et al., 2009). The MatchIt package does not support any treatment effect calculations, recommending users compute these using a separate R package, Zelig (Stuart et al., 2011). Users of any propensity score matching algorithm can adjust matched group sizes only by entering a ratio of treatment to control units, forcing all matched groups to be of the same size. Python's PyMatch and DoWhy offer Propensity Score matching, but DoWhy doesn't emphasize matched groups, favoring treatment effects and other output (Sharma et al., 2019), (Miroglio et al., 2017) further advantage of dame-flame is the higher quality of the matched groups generated by DAME and FLAME relative to propensity score matching, as shown by Dieng et al. (2019). Additional Advanced Controls of DAME-FLAME. Users are offered several ways to control the matching procedure, including early stopping critera, and the option of placing units in matched groups with and without replacement. The default parameters for the package were chosen for their versatility and speed, so the algorithm can be relevant and easy-to-use for a range of users. Full descriptions of algorithm parameters are provided in the respective package documentations. As part of a suite of printing options, users can request that the predictive error and/or balancing factor at each iteration be returned.",no,,,,no,,,,
665,https://github.com/zuoym15/craves.ai,zuoym15_craves.ai.tar.gz,CRAVES: Controlling Robotic Arm with a Vision-based Economic System,@property,"utils imageio pose sys signal unreal __future__ torchvision unrealcv os, sys progress datetime json collections glob, os, json torch os matplotlib scipy argparse numpy time shutil math setuptools errno random cv2 logging",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zuoym15_craves.ai.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zuoym15_craves.ai.pdf,,"Detecting 2D KeypointsWe first evaluate 2D keypoint detection, and make use of a popular metric named PCK@0.2 [42]  ping. On top of this model, we consider several approaches to achieve domain transfer. One is to train an explicit model which transfers virtual data to fake real data on which we train a new model. In practice, we apply a popular generative model named CycleGAN [45]. We trained the Cycle-GAN network with synthetic image as the source domain and lab image as the target domain for 100 epochs. Other domain adaptation methods, i.e., ADDA [40] and its followup work CyCADA [9] are also applied and compared with our approach described in Section 3.3. We mix the synthetic and real images with a ratio of 6 : 4 and use the same hyper-parameters as for the baseline. Background clutters are added to the lab images in an online manner to facilitate variability (see Section 3.5).Results are summarized in Table 1. The baseline model works almost perfectly on virtual data, which reports a PCK@0.2 accuracy of 99.95%. However, this number drops significantly to 95.66% in lab data, and even dramatically to 80.05% in YouTube data, demonstrating the existence of domain gaps. These gaps are largely shrunk after domain adaptation algorithms are applied. Training with images generated by CycleGAN, we found that the model works better in its target domain, i.e. the lab dataset by a margin of 2.18%. However, this model failed to generalize to YouTube dataset, as the accuracy is even lower than the baseline model. The cases are similar for ADDA and CyCADA, which gains 0.48% and 2.43% improvement on the lab dataset respectively, but both of the approaches do not generalize well to the YouTube dataset. Our approach, on the other hand, achieves much higher accuracy, with a PCK@0.2 score of 99.55% in the lab data, and 87.01% in the YouTube, boosting the baseline performance by 6.96%. In the subset of visible YouTube keypoints, the improvement is even higher (7.28%). In addition, the refined model only produces a slightly worse PCK@0.2 accuracy (99.63% vs. 99.95%) on virtual data, implying that balance is achieved between ""fitting on virtual data"" and ""transferring to real data"".The results reveal that performance of explicit domain adaptation manners, i.e. CycleGAN, ADDA and CyCADA can be limited in several aspect. For instance, compared with our 3D geometric based domain adaptation method, although the models trained with these explicit domain adap-   Table 1 .12D keypoint detection accuracy (PCK@0.2, %) on three datasets. Models are tested on YouTube dataset when considering all keypoints and considering only the visible ones.ModelVirtual Lab YouTube YouTube-visSynthetic99.95 95.66 80.0581.61CycleGAN 99.86 97.84 75.2676.98ADDA99.89 96.14 79.1980.04CyCADA 99.84 98.09 73.4774.37Our Method 99.63 99.55 87.0188.89to evaluate the accuracy. For this purpose, we train a 2-stack hourglass network from scratch for 30 epochs in the virtual dataset. Standard data augmentation techniques are applied, including random translation, rotation, scaling, color shifting and flip-",no,,,,no,,,,
671,https://github.com/oishik75/CAGE,oishik75_CAGE.tar.gz,Data Programming using Continuous and Quality-Guided Labeling Functions,,numpy sklearn torch cage,cs.LG cs.CL stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oishik75_CAGE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oishik75_CAGE.pdf,,,no,,,"learning rate, epochs",no,,,,
675,https://github.com/vijaykeswani/Noisy-Fair-Classification,vijaykeswani_Noisy-Fair-Classification.tar.gz,Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees,@wraps(func) @abstractmethod @contextmanager @property @staticmethod @if_delegate_has_method('postprocessor_') @jit @if_delegate_has_method('estimator_'),"utils copy pandas contextlib warnings sys multiprocessing algorithms tqdm the __future__ platform tensorflow BlackBoxAuditing os,sys datetime collections json doctest tempfile os cvxpy site matplotlib scipy functools itertools numba aif360 pickle sklearn argparse numpy time abc math traceback fadm random logging subprocess mpl_toolkits",cs.LG cs.AI cs.CY cs.DS stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vijaykeswani_Noisy-Fair-Classification.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vijaykeswani_Noisy-Fair-Classification.pdf,,,no,,,,no,,,,
677,https://github.com/tatp22/linformer-pytorch,tatp22_linformer-pytorch.tar.gz,Linformer: Self-Attention with Linear Complexity,,math torchtext sys collections linformer_pytorch tqdm torch os numpy matplotlib,cs.CL cs.LG cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tatp22_linformer-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tatp22_linformer-pytorch.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer  Absolute Position Encodings  Position-Wise Feed-Forward Layer  Gaussian Error Linear Units  Linear Layer  Multi-Head Linear Attention  Linformer  Residual Connection  Label Smoothing  Multi-Head Attention  Adam  Dropout  Byte Pair Encoding  Dense Connections  Layer Normalization  Softmax  Scaled Dot-Product Attention  Transformer,,no,,,,no,,,,
678,https://github.com/nocotan/density_fixing,nocotan_density_fixing.tar.gz,Density-Fixing: Simple yet Effective Regularization Method based on the Class Priors,,utils csv warnings sys argparse torch os __future__ numpy models time torchvision,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nocotan_density_fixing.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nocotan_density_fixing.pdf,,,no,,,,no,,,,
680,https://github.com/deepmind/reverb,deepmind_reverb.tar.gz,REVERB: A FRAMEWORK FOR EXPERIENCE REPLAY,"@parameterized.parameters(['signatured'], ['bounded_spec_signatured']) @parameterized.parameters( @dataclasses.dataclass @parameterized.product( @tf.function @parameterized.named_parameters( @parameterized.parameters(1, 3, 7) @abc.abstractmethod @property @parameterized.parameters([1], [3], [10]) @staticmethod @classmethod",copy sys multiprocessing portpicker termcolor distutils threading tensorflow absl tree collections datetime unittest fnmatch tempfile dataclasses os concurrent socket google six functools itertools codecs reverb typing pickle argparse numpy time abc reverb_version setuptools subprocess,cs.LG cs.AI cs.DC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/deepmind_reverb.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\deepmind_reverb.pdf,Experience Replay,,no,,,,no,,,,
682,https://github.com/yifita/3PU,yifita_3PU.tar.gz,Patch-based Progressive 3D Point Set Upsampling,@ops.RegisterGradient('NnDistance') @tf.RegisterGradient('GroupPoint') @tf.RegisterGradient('GatherPoint'),utils csv sys glob pdb nibabel tqdm __future__ mixed_data_provider pprint plyfile re tensorflow open3d collections h5py os tf_ops socket matplotlib importlib curriculumn_record_provider itertools functools curriculum_data_provider builtins pickle sklearn argparse numpy time math model_utils traceback random mpl_toolkits,cs.CV cs.GR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yifita_3PU.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yifita_3PU.pdf,,"MethodGiven an unordered set of 3D points, our network generates a denser point set that lies on the underlying surface. This problem is particularly challenging when the point set is relatively sparse, or when the underlying surface has complex geometric and topological structures. In this paper, we propose an end-to-end progressive learning technique for point set upsampling. Intuitively, we train a multi-step patch-based network to learn the information from different levels of detail. As shown in Figures 2 and 3, our model consists of a sequence of upsampling network units. Each unit has the same structure, but we employ it on different levels of detail. The information of all levels is shared via our intra-level and inter-level connections inside and between the units. By progressively training all network units end-to-end, we achieve significant improvements over previous works. We first present the global design of our network and then elaborate on the upsampling units. Multi-step upsampling networkMulti-step supervision is common practice in neural image super-resolution [11,30,62]. In this section, we first discuss the difficulties in adapting multi-step learning to point set upsampling, which motivates the design of our multistep patch-based supervision method. Next, we illustrate the end-to-end training procedure for a cascade of upsampling network units for large upsampling ratios and highresolution outputs.Multi-step patch-based receptive field. Ideally, a point set upsampling network should span the receptive field adaptively for various scales of details to learn geometric information from multiple scales. However, it is challenging to apply a multi-scope receptive field on a dense irregular point set due to practical constraints. In contrast to im- ages, point sets do not have the regular structure, and the neighborhoods of points are not fixed sets. Neighborhood information must be collected by, e.g., k-nearest neighbors (kNN) search. This per-layer and per-point computation is rather expensive, prohibiting a naive implementation of a multi-step upsampling network to reach large upsampling ratios and dense outputs. Therefore, it is necessary to optimize the network architecture, such that it is scalable to a high-resolution point set.Our key idea is to use a multi-step patch-based network, and the patch size should be adaptive to the scope of receptive fields at the present step. Note that in neural point processing, the scope of a receptive field is usually defined by the kNN size used in the feature extraction layers. Hence, if the neighborhood size is fixed, the receptive field becomes narrower as the point set grows denser. This observation suggests that it is unnecessary for a network to process all the points when the receptive field is relatively narrow. As shown in Figure 2, our network recursively upsamples a point set while at the same time reduces its spatial span. This multi-step patch-based supervision technique allows for a significant upsampling ratio.Multi-step end-to-end training. Our network takes L steps to upsample a set of points by a factor of 2 L . For L levels of detail, we train a set of subnet units {U 1 , U 2 , . . . , U L }. We train such a sequence of upsampling units by progressively activating the training of units; it has been used in many multiscale neural image processing works [25,51].More specifically, our entire training process has 2L − 1 stages, i.e., every upsampling unit has two stages except the first one. We denote the currently targeted level of detail by L. In the first stage of U L we fix the network parameters of units U 1 to U L−1 and start the training of unit U L. In the second stage, we unleash the fixed units and train all the units simultaneously. This progressive training method is helpful because an immature unit can impose destructive gradient turbulence on the previous units [25].We denote the ground truth model, prediction patch and reference patch with T , P and Q respectively and use L and to denote the targeted level of detail and an intermediate level, as illustrated in Figure 2 and 6. In practice, we recursively shrink the spatial scope by confining the input patch to a fixed number of points (N ). For more technical detail about extracting such input patches on-the-fly and updating the reference patches accurately, please refer to Section 3.3. ResultsIn this section, we compare our method quantitatively and qualitatively with state-of-the-art point upsampling methods, and evaluate various aspects of our model. Please refer to the supplementary for further implementation details and extended experiments.The metrics used for evaluation are (i) Chamfer distance, (ii) Hausdorff distance [4] and (iii) point-to-surface distance computed against the ground truth mesh.Training and testing data. We generate two datasets for our experiments: MNIST-CP, Sketchfab and ModelNet10 [54]. MNIST-CP consists of 50K and 10K training and testing examples of 2D contour points extracted from the MNIST dataset [31]. Given a set of 2D pixel points, we apply Delaunay triangulation [5], Loop surface subdivision [38], boundary edge extraction, and WLOP [22] to generate a uniformly distributed point set lying on the contour curve of the image. The number of points in input P and ground truth point sets T 1 , T 2 and T 3 are 50, 100, 200 and 800, respectively. Sketchfab consists of 90 and 13 highly detailed 3D models downloaded from Sketch-Fab [48] for training and testing, respectively. ModelNet10 is comprised of 10 categories, containing 3991 and 908 CAD meshes for training and testing, respectively. We use the Poisson-disk sampling [7] implemented in Meshlab [6] to sample input and ground truth point sets with the number of points ranging from 625 to 80000. Our data augmentation includes random rotation, scaling and point perturbation with gaussian noise.Comparison. We compare our method on relatively sparse (625 points) and dense (5000 points) inputs with three state-of-the-art point set upsampling methods: EAR [23], PU-Net [59] and EC-Net [58] . The code of these methods is publicly available. For EAR, we set the parameter σ n = 35 • to favor sharp feature preservation. For PU-Net and EC-Net, we obtain 16× results by iteratively applying their 4×-upsampling model twice, as advised by the authors. As for comparison, we train a four-step 16× model using our method, where the initial patch size falls into a similar level of detail as PU-Net. For all experiments, we add to the input Gaussian noise with 0.25% magnitude of the model dimensions. Table 2: Quantitative comparison with state-of-the-art approaches on Mod-elNet10 dataset for 16× upsampling from 625 input points.Table 1 and 2 summarizes the quantitative comparison conducted using Sketchfab and ModelNet10. Note that because many models in ModelNet10 are not watertight, we omit the point-to-surface distance in Table 2. Examples of the upsampling results are provided in Figures 11 and 12 for visual comparison, where we apply surface reconstruction to the upsampled point sets using PCA normal estimation (neighborhood number = 25) [18] and screened Poisson reconstruction (depth = 9) [26]. As seen in Figures 11 and 12, EAR generates competitive results for denser inputs but struggles with sparse inputs. As shown in Table 1, the performance of PU-Net on sparse and dense inputs is similar, revealing its limitation for high levels of detail. For denser inputs, EC-Net produces clean and more well defined outputs than PU-Net, but also shows signs of over-sharpening. For sparse input though, EC-Net produces more artifacts, possibly because the geodesic KNN, which EC-Net is built upon, becomes unreliable under sparse inputs. In comparison, our method outperforms all these methods quantitatively by a large margin. Qualitatively, our results are less noisy and contain notably more details.Ablation study. An ablation study quantitatively evaluates the contribution of each of our proposed components:1. Multi-stage architecture: we train a 2×-upsampling model for all levels of detail and test by iteratively applying the model 4 times. Table 3: Ablation study with 16×-upsampling factor tested on the Sketchfab dataset using 625 points as input. We evaluate the contribution of each proposed component quantitatively with Chamfer distance (CD), Hausdorff distance (HD) and mean point-to-surface distance (P2F), and also report the number of parameters in the rightmost column. to the full model. In particular, removing multi-stage architecture significantly increased the difficulty of the task, resulting in artifacts shown in Figure 8b. We observe similar artifacts when the upsampling units are trained separately (Figure 8c), as the networks cannot counteract the mistakes made in previous stages. The proposed dense feature extraction, feature expansion, and inter-level skip-connections considerably improve the upsampling results. Moreover, the feature extraction and expansion unit contribute to significant parameter reduction. The results are shown in Figure 7. Both direct upsampling and single-stage model ((i) and (ii)) are unable to reconstruct faithful geometry in curvy regions, suggesting that a multi-stage architecture is necessary for capturing high levels of detail. The multi-stage PU-Net (iii) notably Real world data. To test our model on real scans, we acquire input data using a hand-held 3D scanner Intel Re-alSense SR300. Albeit dense, such data is severely ridden with noise and outliers. Therefore, we first employ WLOP [22], a point set denoising tool known to be robust against noise and outliers, to consolidate and simplify the point set. We then apply our model to the resulting, denoised yet sparse point set and obtain a dense and clean output, as shown in Figure 9c.input (i) (ii) (iii) (iv) (v) GT",no,,,,no,,,,
683,https://github.com/cmackenziek/tsfl,cmackenziek_tsfl.tar.gz,variables -data analysis -statistics,"@jit(float64[:, :](float64[:, :], float64[:], float64[:], float64[:],",pandas sys lightcurveutil lightcurves pylab datetime lc_models fnmatch os site matplotlib scipy numba infpy pickle sklearn argparse preprocess lightcurve numpy time tarfile twed random,astro-ph.SR cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cmackenziek_tsfl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cmackenziek_tsfl.pdf,,,no,,,,no,,,,
684,https://github.com/NeuralSec/Daedalus-attack,NeuralSec_Daedalus-attack.tar.gz,Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples,,keras tensorflow model sys keras_retinanet cv2 random skimage YOLOv3 os numpy matplotlib time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/NeuralSec_Daedalus-attack.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\NeuralSec_Daedalus-attack.pdf,,"A. Experiment settingsIn the experiments, YOLO-v3 is picked as the model to be attacked. A recommended objectness threshold of 0.5 is used in YOLO-v3. We first select the best loss function, and then we quantify the performance of the Daedalus attack based on YOLO-v3 with NMS. YOLO-v3 is employed as the substitute to generate 416×416 adversarial perturbations in the evaluations. During the binary search to find the best constant c, the maximum number of search steps is set to 5.L 0 and L 2 Daedalus examples of images in the COCO 2017val dataset [1] are crafted for the evaluation. For each attack, first, to investigate the effectiveness of different confidence values (γ), we sweep γ from 0.1 to 0.9 and perturb 10 COCO examples under each γ. Second, 100 COCO examples are randomly perturbed under both a low (0.3) and a high (0.7) value for γ. First, to unify the evaluation results, all of the 80 object categories from the COCO dataset are included in the target set Λ to be attacked. Second, the categories of 'person' and 'car' are selected to be attacked. The graphical results are included in the supplementary file of the paper.To prove that the attack can be applied on other detectors, the results of attacking RetinaNet-ResNet-50 and SSD are also presented. RetinaNet-ResNet-50 uses ResNet-50 [30] as the backbone. SSD employs VGG-16 as its backbone [2]. The backbones differ from that of YOLO-v3. The output feature maps are different as well. An ensemble of the three OD models are used as the substitute to craft universal adversarial examples. Furthermore, to demonstrate the effectiveness of the attack in the real world, Daedalus perturbations are made into physical posters to launch attacks against real-world OD applications. As a showcase, we show the success of the physical Daedalus attack in an indoor OD scene.",no,,,,no,,,,
685,https://github.com/iesl/expLinkage,iesl_expLinkage.tar.gz,Supervised Hierarchical Clustering with Exponential Linkage,,"utils csv sys csv,os,argparse,copy hier_clust pprint random, itertools pathlib uuid datetime json collections itertools, math, os csv, os, argparse, itertools, math torch os trainer matplotlib copy, time time, copy scipy itertools seaborn sklearn argparse csv, itertools numpy time math argparse, time, sys, os csv,os,argparse,copy,itertools,scipy PairFeatureTrainer random logging models itertools,csv eval",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/iesl_expLinkage.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\iesl_expLinkage.pdf,,,no,,,,no,,,,
688,https://github.com/yya518/FinBERT,yya518_FinBERT.tar.gz,Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts,,copy bertModel pandas datasets json pytorch_pretrained_bert sklearn torch os argparse __future__ numpy time,cs.CL cs.IR q-fin.CP cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yya518_FinBERT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yya518_FinBERT.pdf,Weight Decay  Softmax  Adam  Multi-Head Attention  Dropout  Gaussian Error Linear Units  Attention Dropout  Linear Warmup With Linear Decay  Dense Connections  Layer Normalization  Scaled Dot-Product Attention  WordPiece  Residual Connection  BERT,"Experiments and resultsThe purpose of the LPS algorithm is to detect the semantic orientation of news phrases in a manner that is optimal in terms of accuracy, precision and recall. In this section, we construct a series of experiments to evaluate the performance of the model with respect to a number of baseline algorithms. As dataset for the experiments, we have used the financial phrase-bank described in the previous section. Example phrases with missing annotation labels and phrases that have been labeled by the annotators as inconsistent have been discarded from the evaluations. The system used in the experiments was implemented using Java-based software on top of Stanford's CoreNLP framework, which provides tools for standard document preprocessing tasks and extraction of phrase-structure information. The SVM models used within our LPS-algorithm and some of the baselines where learned using the LIBSVM implementation for maximum margin classification available in JavaML library. In all estimations the default parameters were used without optimizing the models for any specific criteria.",no,,,default parameters,no,,,,
689,https://github.com/jwcalder/GraphLearning,jwcalder_GraphLearning.tar.gz,Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates,,"importlib scipy mayavi joblib datetime setuptools urllib ssl annoy sklearn numpy os sys, getopt, time, csv, torch, os, multiprocessing graphlearning matplotlib kymatio",cs.LG cs.CV cs.NA math.AP math.NA stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jwcalder_GraphLearning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jwcalder_GraphLearning.pdf,,,no,,,,no,,,,
692,https://github.com/cake-lab/PointAR,cake-lab_PointAR.tar.gz,PointAR: Efficient Lighting Estimation for Mobile Augmented Reality,@lru_cache(maxsize=3) @coefficients.setter @property @staticmethod,imageio PIL multiprocessing glob torch_cluster tqdm io __future__ zipfile model json h5py pytorch_lightning torch os trainer fire importlib functools pycuda sklearn numpy time math etc datasets configs,cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cake-lab_PointAR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cake-lab_PointAR.pdf,,,no,,,same hyperparameters similar to other study,no,,,,
706,https://github.com/cmu-enyac/Renofeation,cmu-enyac_Renofeation.tar.gz,Efficient Object Localization Using Convolutional Networks,,PIL sys glob tqdm torchvision model torch os advertorch scipy dataset argparse numpy time torchcontrib random cv2 tensorpack io,cs.CV cs.LG cs.AI cs.CV stat.ML cs.LG cs.CR cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cmu-enyac_Renofeation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cmu-enyac_Renofeation.pdf,SpatialDropout  Concatenated Skip Connection  Dense Block  DenseNet  Average Pooling  Zero-padded Shortcut Connection  Pyramidal Residual Unit  Pyramidal Bottleneck Residual Unit  Dropout  Dense Connections  Cosine Annealing  Weight Decay  PyramidNet  Wide Residual Block  WideResNet  Softmax  VGG  Shake-Shake Regularization  Stochastic Weight Averaging  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Stochastic Gradient Descent,,no,,,,no,,,,
714,https://github.com/rapidsai/gputreeshap,rapidsai_gputreeshap.tar.gz,GPUTreeShap: Massively Parallel Exact Calculation of SHAP Scores for Tree Ensembles,@memory.cache,pandas pytablewriter sklearn numpy argparse joblib xgboost time,cs.LG cs.DC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rapidsai_gputreeshap.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rapidsai_gputreeshap.pdf,Shapley Additive Explanations,"EVALUATIONWe train a range of decision tree ensembles using XGBoost on the datasets listed in Table 2. Our goal is to evaluate a wide range of models representative of different realworld settings, from simple exploratory models to large ensembles of thousands of trees. For each dataset, we train a small, medium, and large variant by adjusting the number of boosting rounds (10, 100, 1000) and maximum tree depth (3,8,16). The learning rate is set to 0.01 to prevent XGBoost learning the model in the first few trees and producing only stumps in subsequent iterations. Using a low learning rate is also common in practice to minimise generalisation error.Other hyperparameters are left as default. Summary statistics for each model variant are listed in Table 3, and our testing hardware is listed in Table 4.",no,,,"boosting rounds, max tree depth, learning rate",no,,,,
715,https://github.com/Jonas1312/CommunityDetection,Jonas1312_CommunityDetection.tar.gz,Spectral Clustering of Graphs with the Bethe Hessian,,scipy spectralClustering csv sys types networkx matrices permutation_generator stochasticBlockModel sklearn numpy matplotlib time,cond-mat.dis-nn cs.SI physics.soc-ph stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Jonas1312_CommunityDetection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Jonas1312_CommunityDetection.pdf,,,no,,,,no,,,,
721,https://github.com/MECLabTUDA/ACS,MECLabTUDA_ACS.tar.gz,Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation,@property @staticmethod,PIL copy pandas csv sys nibabel tqdm torchvision telegram re SimpleITK datetime json mp torch os matplotlib functools seaborn typing pickle torchio argparse numpy time shutil math torchsummary setuptools random args,eess.IV cs.CV cs.LG cs.CV cs.LG eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MECLabTUDA_ACS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MECLabTUDA_ACS.pdf,,,no,,,"batch size, epochs",no,,,,
727,https://github.com/ARiSE-Lab/Patch-as-translation,ARiSE-Lab_Patch-as-translation.tar.gz,Patching as Translation: the Data and the Metaphor,"@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, 4), dtype=tf.int32)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, 4), dtype=tf.int32)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, 4), dtype=tf.int32), tf.TensorSpec(shape=None, dtype=tf.bool)]) @property @tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, 4), dtype=tf.int32), tf.TensorSpec(shape=None, dtype=tf.bool)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=(None, None), dtype=tf.int32), tf.TensorSpec(shape=None, dtype=tf.bool)]) @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, None), dtype=tf.float32), tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)])",csv yaml sys glob util tensorflow data_reader transformer nltk os matplotlib tracker transformer_patching_model argparse numpy time shutil math javalang vocabulary random metrics,cs.SE cs.LG cs.PL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ARiSE-Lab_Patch-as-translation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ARiSE-Lab_Patch-as-translation.pdf,,,no,,,,no,,,,
728,https://github.com/jzhou316/Unsupervised-Sentence-Summarization,jzhou316_Unsupervised-Sentence-Summarization.tar.gz,Simple Unsupervised Summarization by Contextual Matching,@overrides @property,utils pre_word_list warnings sys tqdm sim_token_match model json h5py elmo_sequential_embedder nltk torch os beam_search importlib overrides torchtext elmo_lstm_forward lm_subvocab pickle typing argparse dataload numpy time train math pytorch_pretrained_bert logging pre_closetables sim_embed_score allennlp gpt2_sequential_embedder,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jzhou316_Unsupervised-Sentence-Summarization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jzhou316_Unsupervised-Sentence-Summarization.pdf,,,no,,,,no,,,,
733,https://github.com/Wouter-VDP/nuecc_python,Wouter-VDP_nuecc_python.tar.gz,Combined Neyman-Pearson Chi-square: An Improved Approximation to the Poisson-likelihood Chi-square,,gc pandas glob pathlib col_load os helpers matplotlib scipy pickle sklearn joblib xgboost numpy awkward time uproot subprocess enum_sample,physics.data-an hep-ex nucl-ex,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Wouter-VDP_nuecc_python.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Wouter-VDP_nuecc_python.pdf,,,no,,,,no,,,,
743,https://github.com/katiana22/GDM-PCE-surrogates,katiana22_GDM-PCE-surrogates.tar.gz,Manifold learning-based polynomial chaos expansions for high-dimensional surrogate models,@staticmethod,"electric_potential copy DimensionReduction sys skopt UQpy LoktaVoltera DiffusionEquation pylab os, subprocess rand_cmap matplotlib scipy functools itertools sklearn numpy time math datafold colorsys random mpl_toolkits",physics.data-an cs.LG physics.comp-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/katiana22_GDM-PCE-surrogates.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\katiana22_GDM-PCE-surrogates.pdf,,,no,,,,no,,,,
745,https://github.com/ZhaoyangLyu/VerifiablyRobustNN,ZhaoyangLyu_VerifiablyRobustNN.tar.gz,Towards Evaluating and Training Verifiably Robust Neural Networks,@staticmethod,utils copy eps_scheduler multiprocessing sys glob pdb model_defs torchvision bound_param_ramp datetime json bound_layers collections torch os importlib functools itertools argparse numpy time config shutil argparser datasets custom_attacks random logging,cs.CV cs.CR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZhaoyangLyu_VerifiablyRobustNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZhaoyangLyu_VerifiablyRobustNN.pdf,Rectified Linear Units,,no,,,,no,,,,
747,https://github.com/CDL-Quantum/Hackathon2020,CDL-Quantum_Hackathon2020.tar.gz,Machine Learning Advantages in Canadian Astrophysics,"@qml.qnode(dev) @dataclass @qml.qnode(qgan.disc_dev, interface='tf') @property @staticmethod @qml.qnode(qgan.gen_dev, interface='tf', shots=1)",utils PIL copy pandas csv quantum_aware_optims qiskit pennylane zquantum tabu geopandas pprint dwave_solution tensorflow re collections unittest strawberryfields json treelib enum dataclasses torch os plotly matplotlib scipy itertools typing more_itertools sklearn argparse numpy time dimod neal math setuptools planner networkx random dwave qGAN dwave_qbsolv,quant-ph astro-ph.IM quant-ph quant-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CDL-Quantum_Hackathon2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CDL-Quantum_Hackathon2020.pdf,,,no,,,,no,,,,
748,https://github.com/napakalas/NLIMED,napakalas_NLIMED.tar.gz,NLTK: The Natural Language Toolkit,@abstractmethod,"NLIMED json, time requests copy gc pandas urllib3 sys bs4 io nlimed stanza pprint benepar pathlib re json collections BeautifulSoup nltk os string struct scipy gzip itertools cc xml ssl pickle sklearn argparse lxml textwrap time abc shutil difflib math setuptools rdflib operator statistics SPARQLWrapper sklearn_crfsuite",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/napakalas_NLIMED.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\napakalas_NLIMED.pdf,,,no,,,,no,,,,
753,https://github.com/epfl-dlab/causal-distances,epfl-dlab_causal-distances.tar.gz,A Ladder of Causal Distances,@singledispatch @abstractmethod @to_serializable.register(np.float32),copy nuts pandas pgmpy pomegranate json collections cdt os CausalModel scipy functools itertools pickle sklearn pyemd argparse numpy abc networkx random,cs.AI cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/epfl-dlab_causal-distances.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\epfl-dlab_causal-distances.pdf,,"|B.3 Perturbing ID while keeping OD constantWe know that if two causal graphs are within the same Markov class they can support the same observational distribution [Verma and Pearl, 1991]. Thus, we take a causal model C with graph G and compute its Markov equivalence class M (G ).We then consider a causal graph H ∈ M (G ) from the Markov equivalence class and select the perturbation quantification as:= SID(H , G ) max{SID(H , G )||H ∈ M (G )} .(16)Then, we train an MLE parameter estimator using H to find the parameters that yield (almost) the same observational distribution as C. Thus, OD is expected to be (almost) constant while ID is perturbated. In particular, when = 0, ID is expected to be 0. P ( )N X := (1 − )P N X + GMM(k, µ, Σ)(17)Here, quantifies the perturbation. In order to preserve the conditional probability distribution P(X||PA X ) we fit a Gaussian process g X such that:g ( ) X (PA X , P ( ) N X ) ≈ f X (PA X , P N X )(18)Thus, ID is expected to stay (almost) fixed while CD is expected to be affected because the noise is changed. In particular, when is 0 the causal model is not modified and when is 1 the noise is fully replaced by the random Gaussian Mixture.|",no,,,,no,,,,
755,https://github.com/volvet/ARCNN,volvet_ARCNN.tar.gz,Compression Artifacts Reduction by a Deep Convolutional Network,,tarfile math tensorflow random cv2 tqdm argparse os numpy shutil,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/volvet_ARCNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\volvet_ARCNN.pdf,,,no,,,learning rate,no,,,,
757,https://github.com/facebookresearch/faiss,facebookresearch_faiss.tar.gz,Polysemous codes,"@unittest.skipIf(platform.system() != 'Linux', @contextlib.contextmanager @unittest.skipIf(platform.python_version_tuple()[0] < '3', \ @staticmethod @unittest.skipIf(platform.system() == 'Windows', \ @unittest.skipIf(platform.system() == 'Windows', @unittest.skipIf(faiss.get_num_gpus() < 2, ""Relevant for multiple GPU only."") @unittest.skipUnless(","gc contextlib warnings sys multiprocessing pdb faiss __future__ platform re collections unittest h5py scann resource tempfile torch os os,pdb,pickle,time,errno,sys,_thread,traceback,socket,threading,gc matplotlib neighbor_codec scipy rpc array typing pickle argparse common_faiss_tests numpy time shutil math search_server combined_index datasets logging inspect io",cs.CV cs.DB cs.IT cs.LG math.IT cs.CV cs.DB cs.DS cs.IR cs.CV cs.DB cs.DS cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/facebookresearch_faiss.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\facebookresearch_faiss.pdf,k-Nearest Neighbors  k-Nearest Neighbors,,no,,,,no,,,,
760,https://github.com/jessemzhang/dl_spectral_normalization,jessemzhang_dl_spectral_normalization.tar.gz,Generalizable Adversarial Training via Spectral Normalization,,pandas sys __future__ distutils tensorflow adversarial re os matplotlib scipy IPython itertools seaborn tflearn pickle sklearn numpy time dl_utils operator cPickle dl_spectral_normalization,cs.LG stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jessemzhang_dl_spectral_normalization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jessemzhang_dl_spectral_normalization.pdf,,,no,,,,no,,,,
761,https://github.com/YoYo000/MVSNet,YoYo000_MVSNet.tar.gz,MVSNet: Depth Inference for Unstructured Multi-view Stereo,@property @ClassProperty @layer,loss sys multiprocessing cnn_wrapper glob depthfusion tools __future__ pylab tensorflow re model datetime collections os matplotlib struct homography_warping scipy urllib argparse preprocess numpy time shutil photometric_augmentation math cv2 random convgru,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YoYo000_MVSNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YoYo000_MVSNet.pdf,,"Cost VolumeThe next step is to build a 3D cost volume from the extracted feature maps and input cameras. While previous works [14,15] divide the space using regular grids, for our task of depth map inference, we construct the cost volume upon the reference camera frustum. For simplicity, in the following we denote I 1 as the reference image, {I i } N i=2 the source images, and {K i , R i , t i } N i=1 the camera intrinsics, rotations and translations that correspond to the feature maps.Differentiable Homography All feature maps are warped into different frontoparallel planes of the reference camera to form N feature volumes {V i } N i=1 . The coordinate mapping from the warped feature map V i (d) to F i at depth d is determined by the planar transformation x ∼ H i (d) • x, where '∼' denotes the projective equality and H i (d) the homography between the i th feature map and the reference feature map at depth d. Let n 1 be the principle axis of the reference camera, the homography is expressed by a 3 × 3 matrix:H i (d) = K i • R i • I − (t 1 − t i ) • n T 1 d • R T 1 • K T 1 .(1)Without loss of generality, the homography for reference feature map F 1 itself is an 3 × 3 identity matrix. The warping process is similar to that of the classical plane sweeping stereo [5], except that the differentiable bilinear interpolation is used to sample pixels from feature maps {F i } N i=1 rather than images {I i } N i=1 . As the core step to bridge the 2D feature extraction and the 3D regularization networks, the warping operation is implemented in differentiable manner, which enables end-to-end training of depth map inference.Cost Metric Next, we aggregate multiple feature volumes {V i } N i=1 to one cost volume C. To adapt arbitrary number of input views, we propose a variancebased cost metric M for N-view similarity measurement. Let W, H, D, F be the input image width, height, depth sample number and the channel number of the feature map, andV = W 4 • H 4 • D • F the feature volume size, our cost metric defines the mapping M : R V × • • • × R V N → R V that: C = M(V 1 , • • • , V N ) = N i=1 (V i − V i ) 2 N (2)Where V i is the average volume among all feature volumes, and all operations above are element-wise. Most traditional MVS methods aggregate pairwise costs between the reference image and all source images in a heuristic way. Instead, our metric design follows the philosophy that all views should contribute equally to the matching cost and gives no preference to the reference image [11]. We notice that recent work [11] applies the mean operation with multiple CNN layers to infer the multi-patch similarity. Here we choose the 'variance' operation instead because the 'mean' operation itself provides no information about the feature differences, and their network requires pre-and post-CNN layers to help infer the similarity. In contrast, our variance-based cost metric explicitly measures the multi-view feature difference. In later experiments, we will show that such explicit difference measurement improves the validation accuracy.",no,,,,no,,,,
763,https://github.com/thomaskeck/FastBDT,thomaskeck_FastBDT.tar.gz,A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification,,"timeit scipy ROOT xgboost pandas FastBDT seaborn sys array traceback, pdb PyFastBDT pickle sklearn os numpy ctypes matplotlib distutils",hep-ex cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thomaskeck_FastBDT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thomaskeck_FastBDT.pdf,,,no,,,"depth trees, number of trees, number of features, number of training data points, sampling rate, shrinkage",no,,,,
767,https://github.com/tristandeleu/pytorch-meta,tristandeleu_pytorch-meta.tar.gz,Torchmeta: A Meta-Learning library for PyTorch,"@pytest.mark.parametrize('add_bias_kv', [True, False]) @pytest.mark.parametrize('dtype', [None, torch.float32]) @pytest.mark.parametrize('name', helpers.__all__) @pytest.fixture @pytest.mark.parametrize('kdim,vdim', [(None, None), (7, 11)]) @pytest.mark.parametrize('dataset_class', @pytest.mark.skipif(not is_local, reason='Requires datasets downloaded locally') @pytest.mark.parametrize('use_woodbury', [None, True, False]) @property @pytest.mark.parametrize('split', ['train', 'val', 'test']) @pytest.mark.parametrize('scale', [True, False]) @pytest.mark.parametrize('bias', [True, False]) @pytest.mark.parametrize('model', [linear_model(), model()]) @pytest.mark.parametrize('mode', ['sum', 'mean', 'max']) @pytest.mark.parametrize('reg_lambda', [0.1, 1.]) @pytest.mark.parametrize('name,dataset_class', @flaky @classmethod @pytest.mark.parametrize('shots', [1, 5]) @pytest.mark.parametrize('name', list_of_datasets) @pytest.mark.parametrize('padding_mode', [None, 'zeros', 'replicate', 'circular']) @pytest.mark.skipif(not is_multi_gpu, reason='Requires Multi-GPU support') @pytest.mark.parametrize('model,params', [ @pytest.mark.parametrize('padding_mode', [None, 'zeros', 'reflect', 'replicate', 'circular'])",utils requests PIL copy pandas yaml warnings sys munkres glob tqdm torchvision re zipfile model collections json h5py ordered_set torch os academictorrents pytest six importlib gzip flaky itertools pickle sklearn argparse numpy shutil torchmeta tarfile math version setuptools random logging inspect higher io,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tristandeleu_pytorch-meta.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tristandeleu_pytorch-meta.pdf,,,no,,,,no,,,,
772,https://github.com/cantordust/mpath,cantordust_mpath.tar.gz,Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis,,pathlib math tensorflow datetime mpath random cv2 enum argparse mpl_toolkits numpy matplotlib time distutils,cs.NE cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cantordust_mpath.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cantordust_mpath.pdf,,,no,,,,no,,,,
778,https://github.com/innvariant/deepgg,innvariant_deepgg.tar.gz,DeepGG: a Deep Graph Generator,@py_random_state(3) @py_random_state(2) @property @staticmethod,copy dgl itertools functools deepgg json networkx random pickle enum tqdm torch os numpy time,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/innvariant_deepgg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\innvariant_deepgg.pdf,,"ConclusionWe described a generative model for learning distributions of graph as a generalization of the model given in Li et al. (2018). The generalization made it possible to learn distributions of graphs based on the graph construction process of the Watts-Strogatz model and we are confident that DeepGG reduced bias towards scale-free graphs. Our experiments showed, that often an underlying construction process could not be learned although single instances showed promising resuls -visually and based on graph property distributions. We emphasize that generative models of graphs need to be evaluated on multiple levels to reach statistical significant evidence and focussed on providing such experimental setups.DeepGG was rendered under the perspective of a deep state machine, which can be seen as a state machine with learnable transitions and a memory. The decision process of such a state machine can be analysed more transparently as compared to other common end-to-end models.We hope to contribute valuable insights for learning distributions of graphs and expect to (1) further investigate generative models of graphs and (2) apply the recently developed models in promising domains.",no,,,same hyperparameters,no,,,,
803,https://github.com/viebboy/SIPL,viebboy_SIPL.tar.gz,SUBSET SAMPLING FOR PROGRESSIVE NEURAL NETWORK LEARNING,,"copy sys threading keras tensorflow pmlp_utility Utility sys, getopt os os, itertools, pickle pmlp PLN exp_configurations os, sys, getopt, Runners, pickle pickle sklearn dill joblib numpy time StackedELM random subprocess",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/viebboy_SIPL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\viebboy_SIPL.pdf,,"INTRODUCTIONProgressive Neural Network Learning (PNNL) [1,2,3,4,5,6,7,8] aims to build the network's topology incrementally depending on the training set given for the specific problem at hand. At each incremental training step, a PNNL algorithm adds a new set of neurons to the existing network topology and optimizes the new synaptic weights using the entire training set. Thus, throughout the network's topology progression, the number of times the PNNL algorithm iterates through the entire training set is very high. For large datasets, this approach leads to an enormous computational cost and long training process. In this paper, we propose to perform the optimization of each incremental training step using only a subset of the training data. Our motivation in doing so is two-fold. Firstly, optimizing with respect to a subset of the training data leads to lower overall computational cost; Secondly, the use of different subsets of data at each incremental step promotes specialization of different sets of neurons at capturing different patterns in the data.The idea of subset sampling for training machine learning methods has been proposed in different contexts in literature. With the motivation of reducing the expense of labeling data for training, methods following the active learning paradigm [9] seek to define a sampling strategy that selects a sample to be labeled among a large pool of unlabeled data for the next learning round. While the active learning paradigm considers the problem of data selection in an (initially) unsupervised setting, in the context of PNNL we take advantage of the available labeling information for subset selection. Directly related to our work are methods selecting a subset of data formed by the most representative samples [10,11,12,13]. These methods however only consider the data selection process once based on the input data representations and the available labels. The selected subset of data is then used to train a model with fixed capacity. Different from this line of works, we propose to perform subset sampling at every incremental step of the PNNL process with selection strategies that can also take into account the data representations learned by the current network's tolpology.When building a learning system, the development process often requires running multiple experiments to select the best values for the hyper-parameters associated with the learning model. For neural networks, such hyper-parameters correspond to the values used e.g. for the weight decay coefficient, or the dropout percentage. In existing PNNL algorithms, the value associated with each hyper-parameter is fixed throughout the entire training process, and the best combination of the hyper-parameter values is usually selected by following a grid search strategy training multiple models each corresponding to a different combination of hyper-parameter values. Different from that (traditional) approach, we propose to incorporate the hyper-parameter selection process into each incremental training step, enabling adaptive hyper-parameter assignment during the network's topology progression process. Coupled with the speed up gained from subset sampling, this further accelerates the overall training process and improves generalization performance as indicated by our experimental results.The remainder of the paper is organized as follows: Section 2 reviews Progressive Neural Network Learning and the subset sampling strategies in different learning contexts. Sec-tion 3 describes the proposed progressive network training method. In Section 4, we detail our experimental setup and present empirical results. Section 5 concludes our work.",no,,,,no,,,,
825,https://github.com/YadiLao/MM-Tag,YadiLao_MM-Tag.tar.gz,A Tree Search algorithm For Sequence Labeling,,gc sys util tensorflow torch_process_data datetime json collections treelib new_value torch os matplotlib cnn_lstm functools pg codecs pickle sklearn argparse numpy time math operator logging random,cs.CL cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YadiLao_MM-Tag.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YadiLao_MM-Tag.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Conditional Random Field,,no,,,"learning rate, tree search trade off, hidden units",no,,,,
829,https://github.com/wilson1yan/rlpyt,wilson1yan_rlpyt.tar.gz,Learning to Manipulate Deformable Objects without Demonstrations,@torch.no_grad()  # Hint: apply this decorator on overriding method. @torch.no_grad() @contextmanager @property @staticmethod,imageio copy csv contextlib sys multiprocessing tqdm psutil base64 ctypes platform distutils gym re posix_ipc datetime json collections queue enum rlpyt os torch string importlib pyprind functools itertools pydoc pickle shlex argparse skvideo numpy time math rllab setuptools errno random subprocess inspect mmap,cs.RO cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wilson1yan_rlpyt.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wilson1yan_rlpyt.pdf,,"|B. Learning with Composite Action SpacesThe straightforward approach to learning with a pick-place action space is to learn a policy π joint that directly outputs the optimal locations to pick and to place [a pick , a place ], i.e. π joint ≡ p(a pick ||o) • p(a place ||o) where o is the observation of the deformable object (Fig. 2(a)). However, this approach fails to capture the underlying composite and conditional nature of the action space, where the location to place a place is strongly dependent on the pick point a pick .One way to learn with conditional output spaces is to explicitly factor the output space during learning. This has provided benefits in several other learning problems from generating images [64] to predicting large dimensional robotic actions [40,62]. Hence instead of learning the joint policy, we factor the policy as:π f actor ≡ π pick (a pick ||o) • π place (a place ||o, a pick )(1)This factorization will allow the policy to reason about the conditional dependence of placing on picking (Fig. 2(b)). However, in the context of RL, we face another challenge: action credit assignment. Using RL, the reward for a specific behavior comes through the cumulative discounted reward at the end of an episode. This results in the temporal credit assignment problem where attributing the reward to a specific action is difficult. With our factored action spaces, we now have an additional credit assignment problem on the different factors of the action space. This means that if an action receives high reward, we do not know if it is due to π pick or π place . Due to this, training π f actor jointly is inefficient and often leads to the policy selecting a suboptimal pick location. This suboptimal π pick then does not allow π place to learn, since π place (a place ||o, a pick ) only sees suboptimal picking locations a pick during early parts of training. Thus, this leads to a mode collapse as shown in Sec. V-D.To overcome the action credit assignment problem, we propose a two-stage learning scheme. Here the key insight is that training a placing policy can be done given a fullsupport picking policy and the picking policy can be obtained from the placing policy by accessing the Value approximator for placing. Algorithmically, this is done by first training π place conditioned on picking actions from the uniform random distribution U pick . Using SAC, we train and obtain π place (a place ||o, a pick ), s.t. a pick ∼ U pick as well as the place value approximator V π place place (o, a pick ). Since the value is also conditioned on pick point a pick , we can use this to obtain our picking policy as:π pick ≡ arg max a pick V π place place (o, a pick )(2)We call this picking policy: Maximum Value under Placing (MVP). The arg max is computed by searching over all available pick location from the image of the object being manipulated. MVP allows us get an informed picking policy without having to explicitly train for picking. This makes training efficient for off-policy learning with conditional action spaces especially in the context of deformable object manipulation.|",no,,,network architecture,no,,,,
834,https://github.com/belakaria/USEMOC,belakaria_USEMOC.tar.gz,Uncertainty aware Search Framework for Multi-Objective Bayesian Optimization with Constraints,,math copy scipy acquisitions benchmarks model platypus sklearn os numpy,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/belakaria_USEMOC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\belakaria_USEMOC.pdf,,,no,,,,no,,,,
836,https://github.com/esclear/ph-nn,esclear_ph-nn.tar.gz,Port-Hamiltonian Approach to Neural Network Training,@tf.function,timeit keras tensorflow itertools typing PHNetworks numpy matplotlib,cs.NE cs.LG cs.SY eess.SY stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/esclear_ph-nn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\esclear_ph-nn.pdf,,,no,,,,no,,,,
840,https://github.com/eladhoffer/fix_your_classifier,eladhoffer_fix_your_classifier.tar.gz,FIX YOUR CLASSIFIER: THE MARGINAL VALUE OF TRAINING THE LAST WEIGHT LAYER,,math scipy torch,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eladhoffer_fix_your_classifier.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eladhoffer_fix_your_classifier.pdf,,,no,,,"hyperparameter used as in original work, ",no,,,,
842,https://github.com/jlibovicky/assess-multilingual-bert,jlibovicky_assess-multilingual-bert.tar.gz,On the Language Neutrality of Pre-trained Multilingual Representations,,utils sacremoses sys pytorch_revgrad qe_by_cosine collections transformers word_alignment torch os matplotlib scipy pytorch_transformers sklearn argparse joblib numpy lang_id time mwec pytorch_pretrained_bert logging sentence_retrieval,cs.CL cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jlibovicky_assess-multilingual-bert.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jlibovicky_assess-multilingual-bert.pdf,Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT  Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,,no,,,,no,,,,
846,https://github.com/IBM/OoD,IBM_OoD.tar.gz,Linear Regression Games: Convergence Guarantees to Approximate Out-of-Distribution Solutions,,models_crossval_Sep8 copy pandas sem_Sep8 tensorflow datetime cProfile torch matplotlib irm_block scipy IPython itertools sklearn argparse numpy time sem math models_v1 generate_synthetic_data metrics models,cs.LG stat.ML cs.LG cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IBM_OoD.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IBM_OoD.pdf,,,no,,,,no,,,,
848,https://github.com/ShenakhtPajouh/transposition-simple,ShenakhtPajouh_transposition-simple.tar.gz,Recognizing Arrow Of Time In The Short Stories,@autograph.convert() @classmethod @property,utils copy multiprocessing sys official bert_serving __future__ determiner CNNEncoder1 encoder ffn_layer GatedCNNEncoder1 make_simple_bert_data tensorflow re BERT transformer_transposition transformer_model modeling json collections entity_main_model attention_layer embedding_layer beam_search os matplotlib string six remote spacy transformer_transposition2 codecs CNNEncoder2 transformer_batch_generator2 pickle OS numpy brt time tokenization sent_encoder transformer_batch_generator math batch_generator unicodedata classifier Simpler_Models model_utils staticRecurrentEntNet random,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ShenakhtPajouh_transposition-simple.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ShenakhtPajouh_transposition-simple.pdf,Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,,no,,,,no,,,,
851,https://github.com/matthewmatero/affectivelanguageforecasting,matthewmatero_affectivelanguageforecasting.tar.gz,Autoregressive Affective Language Forecasting: A Self-Supervised Task,,math copy scipy pandas sys json collections sklearn argparse os dataloader numpy,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/matthewmatero_affectivelanguageforecasting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\matthewmatero_affectivelanguageforecasting.pdf,Gated Recurrent Unit,"Train-Test Setup: Time and UsersIn training a k-fold validation(k=5) is also performed, where for each fold the following sub-splits are generated: train users(weeks 1-14), test users 1 (weeks 15-19 of train users) referred to as out-of-sample time, test users 2 (weeks 15-19 of users held-out from training/validation) referred to as out-of-sample users, and validation users (weeks 15-19 of users held-out from training, different than test users 2).In more detail, for train users only their first 14 weeks are used to train our models. The remaining weeks of these users form our main test set(in-sample user, out-of-sample time(OOS Time)). Validation users are out-of-sample users where only weeks 15-19 of their time-series is used to tune hyperparameters. Lastly, our second test set is comprised of weeks 15-19 from users excluded from both training and validation(out-sample user, out-of-sample time (OOS User)). This is because in practice, there could be applications where users will not be available for training but only for testing. Alternative ApproachesDeep Averaging Network: One way to model this problem is to ignore the sequential information entirely but model the non-linearity's between the average change in dimensions over a time-span. However, this approach is naive and is not encoding the most important aspect: time. A deep averaging network(DAN) (Iyyer et al., 2015) with 2 dense layers and sigmoid activation between them is used. During univariate training there are only 2 hidden units at each layer, while there are 35 when doing multivariate.Support Vector & Gradient Boosting Regression: These 2 models are popular approaches for many machine learning tasks (Yoshimi and Kotani, 2019;Liu et al., 2019). The SVR chosen uses a linear kernel and both models are implementations from the sklearn python package (Pedregosa et al., 2011).Figure 2: Visualization of an autoregressive prediction with an order p=4. The first prediction is made using only the green(previously observed) data. For each proceeding prediction the lag moves forward 1 time-step, but still maintains 4 previous observations as input.Autoregressive Ridge (AR-Ridge): A L2regularized(ridge) linear regression is also considered. In the field of time-series analysis a traditional linear regression is often preferred for these types of tasks (Lydia et al., 2016;Balakrishna et al., 2018).Baselines: Simple Moving Average and Constants: As a new task we propose a collection of heuristic baseline models. For example, to predict the future one may reuse values from the past(i.e. someone's change in mood may be identical to their change last week).We employ standard ""zero rule"" baselines, with some modifications to time-series. These are intended to capture easy to implement or naive approaches for prediction (Andrzejak et al., 2008;Miglionico, 2019). The following 2 baselines are considered: (1) predict the last time step's change (last(1)) and ( 2) average of all available changes over a 14 time-step history (mean( 14)).An additional third baseline could be used where one would predict 0 change (no change), However, for our evaluation we stick with the previously mentioned 2 baselines due to using Pearson correlation (r) as our main evaluation metric. Where predicting 0 change would have no correlation with the actual time-series values. Correlation is chosen as it is resistant to the scale of the target variable. Thus allowing direct comparison of difficulty in forecasting across all of our affective dimensions Transformer Networks: A popular approach to many tasks in Natural Language Processing is to fine-tune a pre-trained language model (i.e. BERT) rather than train a smaller model from scratch. However, these language models are pre-trained over individual messages and would require alternative architectures for aggregating over multiple messages and capturing time. Since we are introducing the problem we focus on a more fundamental setup and suggest such architectures would be an interesting direction for future work. For example, one could explore various architectures for hierarchical models on top of standard pre-trained transformer language models.",no,,,,no,,,,
859,https://github.com/rloganiv/streaming-cdc,rloganiv_streaming-cdc.tar.gz,Benchmarking Scalable Methods for Streaming Cross Document Entity Coreference,@classmethod @staticmethod @dataclass,requests csv sys tqdm meercat pathlib re datetime json collections unittest transformers blink tempfile dataclasses torch os scipy itertools xml pickle typing sklearn argparse numpy time version setuptools random logging statistics io,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rloganiv_streaming-cdc.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rloganiv_streaming-cdc.pdf,,,no,,,"hyperparameter used from other work, ",no,,,,
860,https://github.com/kpolsen/SIGAME,kpolsen_SIGAME.tar.gz,SIMULATOR OF GALAXY MILLIMETER/SUBMILLIMETER EMISSION (S ÍGAME): THE [Cii] −SFR RELATIONSHIP OF MASSIVE Z=2 MAIN SEQUENCE GALAXIES,,"sympy copy pandas socket, getpass sys multiprocessing glob pdb aux csv, subprocess re astropy os matplotlib numexpr scipy pickle sigame sklearn argparse numpy time shutil periodictable linecache subprocess mpl_toolkits swiftsimio",astro-ph.GA astro-ph.GA astro-ph.GA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kpolsen_SIGAME.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kpolsen_SIGAME.pdf,,,no,,,,no,,,,
865,https://github.com/mschuessler/two4two,mschuessler_two4two.tar.gz,TWO4TWO: EVALUATING INTERPRETABLE MACHINE LEARNING -A SYNTHETIC DATASET FOR CONTROLLED EXPERIMENTS,@classmethod @dataclasses.dataclass @pytest.mark.slow @property @staticmethod @dataclasses.dataclass() @dataclasses.dataclass(frozen=True),imageio copy PIL pandas two4two sys skimage tqdm __future__ pprint torchvision pathlib tensorflow uuid json dataclasses tempfile torch os pytest matplotlib importlib scipy mathutils bpy optparse keras_preprocessing typing pickle argparse numbers numpy time shutil coverage math setuptools toml random subprocess,cs.AI cs.CV cs.HC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mschuessler_two4two.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mschuessler_two4two.pdf,,,no,,,,no,,,,
866,https://github.com/Video-Streaming-Pipeline/Video-Streaming-Pipeline,Video-Streaming-Pipeline_Video-Streaming-Pipeline.tar.gz,AI Benchmark: Running Deep Neural Networks on Android Smartphones,@outputToString @abstractmethod @staticmethod,"os, sys, multiprocessing, argparse, traceback csv multiprocessing glob, sys, os, re glob, os, os ctypes torchvision run_suite doxygen_scan run_android threading re tensorflow torch caffe urllib2 msvcrt sys, os, argparse, pathlib, traceback, contextlib, shutil codecs os, sys, re, json, shutil numbers numpy pascal_semsegm_test_fcn tarfile traceback _winreg statistics testlog_parser requests yaml __future__ pprint sys, re distutils pathlib imagenet_cls_test_alexnet cStringIO testlog_parser, sys, os, xml, glob, re unittest queue fnmatch os table_formatter google string glob, re, os, os sys, re, os socket,pickle xml hashlib run_tests svgfig time abc shutil getpass os, sys, subprocess, argparse, shutil, glob, re, multiprocessing os, os pycocotools operator subprocess testlog_parser, sys, os, xml, re tf_text_graph_common tf_text_graph_ssd run_long sys html_functions color datetime enum tempfile hdr_parser matplotlib run_utils common struct build_framework itertools shlex Queue human_parsing errno tf_text_graph_faster_rcnn io PIL re, codecs, os, platform, copy, itertools, math, cmath, random, sys, copy glob skimage bs4 platform ast os, sys, subprocess, argparse, shutil, re pylab os, sys build_docs json collections torchbench math, os, sys testlog_parser, sys, os, xml, re, glob optparse summary urllib argparse xlwt math templates trace_profiler cv2 logging cv_build_utils sys, os, re",cs.AI cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Video-Streaming-Pipeline_Video-Streaming-Pipeline.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Video-Streaming-Pipeline_Video-Streaming-Pipeline.pdf,Convolution  Average Pooling  Global Average Pooling  1x1 Convolution  Batch Normalization  Rectified Linear Units  Depthwise Convolution  Pointwise Convolution  Dense Connections  Softmax  Exponential Decay  Random Resized Crop  Random Horizontal Flip  Depthwise Separable Convolution  MobileNetV1,,no,,,,no,,,,
869,https://github.com/wilsonjr/humap,wilsonjr_humap.tar.gz,HUMAP: Hierarchical Uniform Manifold Approximation and Projection,,scipy setuptools sys humap sklearn numpy matplotlib pybind11 _hierarchical_umap,cs.LG cs.GR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wilsonjr_humap.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wilsonjr_humap.pdf,,,no,,,,no,,,,
871,https://github.com/denadai2/bayesian-crime-multiple-cities,denadai2_bayesian-crime-multiple-cities.tar.gz,"Socio-economic, built environment, and mobility conditions associated with crime: A study of multiple cities",,PIL pandas pystan csv yaml spatial_groups sys tqdm psycopg2 sqlalchemy os matplotlib scipy hashlib seaborn pickle sklearn argparse joblib numpy time math logging arviz io,cs.SI cs.CY physics.soc-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/denadai2_bayesian-crime-multiple-cities.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\denadai2_bayesian-crime-multiple-cities.pdf,,,no,,,,no,,,,
874,https://github.com/chatc/TriageSQL,chatc_TriageSQL.tar.gz,Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL,,utils copy name pandas re model json collections transformers torch os sqlite3 matplotlib itertools seaborn pyecharts sklearn numpy time config random models,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/chatc_TriageSQL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\chatc_TriageSQL.pdf,Multi-Head Attention  Layer Normalization  WordPiece  Softmax  Adam  Dense Connections  Weight Decay  Dropout  Linear Warmup With Linear Decay  Attention Dropout  Gaussian Error Linear Units  Scaled Dot-Product Attention  Residual Connection  BERT  RoBERTa,,no,,,,no,,,,
877,https://github.com/thomaspinder/SteinGP,thomaspinder_SteinGP.tar.gz,Stein Variational Gaussian Processes,@property @staticmethod @add_regression @add_classficiation,steingp pandas tqdm tensorflow datetime os matplotlib scipy urllib ssl pickle typing sklearn numpy time shutil tarfile setuptools logging mpl_toolkits tensorflow_probability gpflow zipfile,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thomaspinder_SteinGP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thomaspinder_SteinGP.pdf,Gaussian Process,,no,,,,no,,,,
880,https://github.com/ntubiolin/xcos,ntubiolin_xcos.tar.gz,xCos: An Explainable Cosine Metric for Face Verification Task,@property @abstractmethod,utils PIL copy pandas warnings sys glob pipeline skimage tqdm attrdict base64 torchvision model datetime collections json tempfile torch os matplotlib importlib coloredlogs seaborn sklearn argparse joblib numpy tensorboardX time abc bcolz shutil math data_loader worker errno cv2 random logging io,cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ntubiolin_xcos.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ntubiolin_xcos.pdf,Interpretability,,no,,,,no,,,,
882,https://github.com/samxmot/RANDGAN,samxmot_RANDGAN.tar.gz,RANDGAN: RANDOMIZED GENERATIVE ADVERSARIAL NETWORK FOR DETECTION OF COVID-19 IN CHEST X-RAY A PREPRINT,,math keras tensorflow SelfAttentionLayer anogan cv2 random skimage tqdm argparse os xlwt __future__ numpy matplotlib,eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/samxmot_RANDGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\samxmot_RANDGAN.pdf,,,no,,,,no,,,,
883,https://github.com/bottler/iisignature,bottler_iisignature.tar.gz,The iisignature library: efficient calculation of iterated-integral signatures and log signatures,@classmethod @staticmethod,"numpy, sys iisignature_torch sys iisignature_theano ctypes distutils os, sys keras tensorflow test_sig unittest theano, numpy, sys iisignature_recurrent_torch resource tabulate, sqlite3, numpy torch os sys, os, numpy sqlite3 matplotlib six iisignature_match_esig itertools subprocess, sqlite3 iisignature_tensorflow os, sys, numpy tkinter kivy iisignature_recurrent_keras numpy, os, sys numpy time math theano, numpy setuptools theano iisignature",cs.DS cs.MS math.RA math-ph math.MP math.RA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bottler_iisignature.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bottler_iisignature.pdf,,,no,,,,no,,,,
884,https://github.com/yangalan123/WM-SRA,yangalan123_WM-SRA.tar.gz,Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains,,"csv sys tqdm gensim re model json transformers os, pickle nltk torch os suicideData pickle sklearn argparse numpy config shutil loguru random",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yangalan123_WM-SRA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yangalan123_WM-SRA.pdf,,"Table 2 :2Results Task A test set. For each of tasks 7-11, the size of added data is 8% of training data. Metrics are all reported on macro-average. size 32. For users with too many posts and words, we only sample 100 passages for them. Table2shows our results on Macro-F1.No. ApproachSetupMacro (P/R/F1)1BaselineBERT0.436 / 0.424 / 0.4272TAPBERT0.439 / 0.445 / 0.4323MVLWord-Mask0.464 / 0.466 / 0.4634MVLSent-Mask0.380 / 0.409 / 0.3835MVLBegEd0.384 / 0.422 / 0.4016MVLK-Sum0.384 / 0.422 / 0.4017PLDepression (medium-risk)0.535 / 0.480 / 0.4988PLAnxiety (low-risk)0.495 / 0.469 / 0.4789PLDepression + Anxiety0.473 / 0.456 / 0.46310PLTask C (low-risk)0.475 / 0.462 / 0.460Task C11-(crowd-0.418 / 0.406 / 0.408labeled). Due to the limitation of GPU memory, we only use the base version.We split 20% of original training data to be the validation set and fix the split for all models. The model selection is made by early stopping and we train all models for 20 epochs with the batch Task-Adaptive Pre-training After applying task-adaptive pre-training on BERT, we see small performance gains over BERT (i.e., from 0.427 to 0.432). That might be because even we use the whole corpus provided by the shared task, it is still not large enough. adding meaningful pseudo-label data from relevant domains helps mitigate the problem of insufficient data in the intermediate classes (b and c). To verify this point, we show the class-wise classification results for PL-based models in Table3where we can",no,,,,no,,,,
888,https://github.com/czbiohub/noise2self,czbiohub_noise2self.tar.gz,Noise2Self: Blind Denoising by Self-Supervision,,numexpr scipy torch os subprocess argparse numpy models matplotlib,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/czbiohub_noise2self.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\czbiohub_noise2self.pdf,,"|IntroductionWe would often like to reconstruct a signal from highdimensional measurements that are corrupted, undersampled, or otherwise noisy. Devices like high-resolution cameras, electron microscopes, and DNA sequencers are capable of producing measurements in the thousands to millions of feature dimensions. But when these devices are pushed to their limits, taking videos with ultra-fast frame rates at very low-illumination, probing individual molecules with electron microscopes, or sequencing tens of thousands of cells simultaneously, each individual feature can become quite noisy. Nevertheless, the objects being studied are often very structured and the values of different features are highly correlated. Speaking loosely, if the ""latent dimension"" of the space of objects under study is much lower than the dimension of the measurement, it may be possible to implicitly learn that structure, denoise the measurements, and recover the signal without any prior knowledge of the signal or the noise.Traditional denoising methods each exploit a property of the noise, such as Gaussianity, or structure in the signal, such as spatiotemporal smoothness, self-similarity, or having low-rank. The performance of these methods is limited by the accuracy of their assumptions. For example, if the data are genuinely not low rank, then a low rank model will fit it poorly. This requires prior knowledge of the signal structure, which limits application to new domains and modalities. These methods also require calibration, as hyperparameters such as the degree of smoothness, the scale of self-similarity, or the rank of a matrix have dramatic impacts on performance.In contrast, a data-driven prior, such as pairs (x i , y i ) of noisy and clean measurements of the same target, can be used to set up a supervised learning problem. A neural net trained to predict y i from x i may be used to denoise new noisy measurements (Weigert et al., 2018). As long as the new data are drawn from the same distribution, one can expect performance similar to that observed during training. Lehtinen et al. demonstrated that clean targets are unnecessary (2018). A neural net trained on pairs (x i , x i ) of independent noisy measurements of the same target will, under certain distributional assumptions, learn to predict the clean signal. These supervised approaches extend to image denoising the success of convolutional neural nets, which currently give state-of-the-art performance for a vast range of image-to-image tasks. Both of these methods require an experimental setup in which each target may be measured multiple times, which can be difficult in practice.In this paper, we propose a framework for blind denoising based on self-supervision. We use groups of features whose noise is independent conditional on the true signal to predict one another. This allows us to learn denoising functions from single noisy measurements of each object, with performance close to that of supervised methods. The same approach can also be used to calibrate traditional image denoising methods such as median filters and non-local means, The box represents the dimensions of the measurement x. J is a subset of the dimensions, and f is a J-invariant function: it has the property that the value of f (x) restricted to dimensions in J, f (x)J , does not depend on the value of x restricted to J, xJ . This enables self-supervision when the noise in the data is conditionally independent between sets of dimensions. Here are 3 examples of dimension partitioning: (b) two independent image acquisitions, (c) independent pixels of a single image, (d) independently detected RNA molecules from a single cell. and, using a different independence structure, denoise highly under-sampled single-cell gene expression data.We model the signal y and its noisy measurement x as a pair of random variables in R m . If J ⊂ {1, . . . , m} is a subset of the dimensions, we write x J for x restricted to J.Definition. Let J be a partition of the dimensions {1, . . . , m} and letJ ∈ J . A function f : R m → R m is J-invariant if f (x) J does not depend on the value of x J . It is J -invariant if it is J-invariant for each J ∈ J .We propose minimizing the self-supervised lossL(f ) = E f (x) − x 2 ,(1)over J -invariant functions f . Since f has to use information from outside of each subset of dimensions J to predict the values inside of J, it cannot merely be the identity.Proposition 1. Suppose x is an unbiased estimator of y, i.e. E[x||y] = y, and the noise in each subset J ∈ J is independent from the noise in its complement J c , conditional on y.Let f be J -invariant. ThenE f (x) − x 2 = E f (x) − y 2 + E x − y 2 . (2)That is, the self-supervised loss is the sum of the ordinary supervised loss and the variance of the noise. By minimizing the self-supervised loss over a class of J -invariant functions, one may find the optimal denoiser for a given dataset.For example, if the signal is an image with independent, mean-zero noise in each pixel, we may choose J = {{1}, . . . , {m}} to be the singletons of each coordinate.Then ""donut"" median filters, with a hole in the center, form a class of J -invariant functions, and by comparing the value of the self-supervised loss at different filter radii, we are able to select the optimal radius for denoising the image at hand (See §3).The donut median filter has just one parameter and therefore limited ability to adapt to the data. At the other extreme, we may search over all J -invariant functions for the global optimum:Proposition 2. The J -invariant function f * J minimizing (1) satisfies f * J (x) J = E[y J ||x J c ] for each subset J ∈ J .That is, the optimal J -invariant predictor for the dimensions of y in some J ∈ J is their expected value conditional on observing the dimensions of x outside of J.In §4, we use analytical examples to illustrate how the optimal J -invariant denoising function approaches the optimal general denoising function as the amount of correlation between features in the data increases.In practice, we may attempt to approximate the optimal denoiser by searching over a very large class of functions, such as deep neural networks with millions of parameters. In §5, we show that a deep convolutional network, modified to become J -invariant using a masking procedure, can achieve state-of-the-art blind denoising performance on three diverse datasets.Sample code is available on GitHub 1 and deferred proofs are contained in the Supplement. Calibrating Traditional ModelsMany denoising models have a hyperparameter controlling the degree of the denoising-the size of a filter, the threshold for sparsity, the number of principal components. If ground truth data were available, the optimal parameter θ for a family of denoisers f θ could be chosen by minimizing f θ (x) − y 2 . Without ground truth, we may nevertheless . Calibrating a median filter without ground truth. Different median filters may be obtained by varying the filter's radius. Which is optimal for a given image? The optimal parameter for J -invariant functions such as the donut median can be read off (red arrows) from the self-supervised loss.compute the self-supervised loss f θ (x) − x 2 . For general f θ , it is unrelated to the ground truth loss, but if f θ is Jinvariant, then it is equal to the ground truth loss plus the noise variance (Eqn. 2), and will have the same minimizer.In Figure 2, we compare both losses for the median filter g r , which replaces each pixel with the median over a disk of radius r surrounding it, and the ""donut"" median filter f r , which replaces each pixel with the median over the same disk excluding the center, on an image with i.i.d. Gaussian noise. For J = {{1}, . . . , {m}} the partition into single pixels, the donut median is J -invariant. For the donut median, the minimum of the self-supervised loss f r (x) − x 2 (solid blue) sits directly above the minimum of the ground truth loss f r (x) − y 2 (dashed blue), and selects the optimal radius r = 3. The vertical displacement is equal to the variance of the noise. In contrast, the self-supervised loss g r (x) − x 2 (solid orange) is strictly increasing and tells us nothing about the ground truth loss g r (x) − y 2 (dashed orange). Note that the median and donut median are genuinely different functions with slightly different performance, but while the former can only be tuned by inspecting the output images, the latter can be tuned using a principled loss.More generally, let g θ be any classical denoiser, and let J be any partition of the pixels such that neighboring pixels are in different subsets. Let s(x) be the function replacing each pixel with the average of its neighbors. Then the function f θ defined byf θ (x) J := g θ (1 J • s(x) + 1 J c • x) J ,(3)for each J ∈ J , is a J -invariant version of g θ . Indeed, since the pixels of x in J are replaced before applying g θ , the output cannot depend on x J .In Supp. Figure 1, we show the corresponding loss curves for J -invariant versions of a wavelet filter, where we tune the threshold σ, and NL-means, where we tune a cut-off distance h (Buades et al., 2005a;Chang et al., 2000;. The partition J used is a 4x4 grid. Note that in all these examples, the function f θ is genuinely different than g θ , and, because the simple interpolation procedure may itself be helpful, it sometimes performs better.In Table 1, we compare all three J -invariant denoisers on a single image. As expected, the denoiser with the best selfsupervised loss also has the best performance as measured by Peak Signal to Noise Ratio (PSNR). |",no,,,"batch size, epochs",no,,,,
890,https://github.com/pawelmorawiecki/Fast-and-stable-IBP,pawelmorawiecki_Fast-and-stable-IBP.tar.gz,Fast and Stable Interval Bounds Propagation for Training Verifiably Robust Models,,numpy torch torchvision,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pawelmorawiecki_Fast-and-stable-IBP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pawelmorawiecki_Fast-and-stable-IBP.pdf,,"IntroductionDeep learning models achieve impressive performance in computer vision Krizhevsky et al. [2012], natural language processing , and many other domains. Although neural networks are able to outperform humans on various machine learning tasks, they are also vulnerable to adversarial examples Szegedy et al. [2013]. In particular, a slightly modified input can fool the neural model and change its prediction. This is a serious problem, which limits the use of NNs in many areas, such as autonomous cars Sitawarin et al. [2018] or malware detection Grosse et al. [2017], where security is a priority. In recent years, a lot of effort has been put on understanding deep learning models and making them more robust Salman et al. [2019], Mirman et al. [2019].Adversarial attacks rely on creating such input data points, which are visually indistinguishable from 'normal' examples, but drastically change the prediction of the model Goodfellow et al. [2014]. One remedy is to construct adversarial examples and add them to the training set Madry et al. [2017]. While such models become robust to many adversarial attacks, there are no guarantees that another adversarial scheme exists. To formally verify the robustness of the model against normbounded perturbations, one can find the outer bound on the so-called 'adversarial polytope ' Wong and Kolter [2017]. These techniques give loose bounds on the output activations, but guarantee that no adversary within a given norm can change the class label. Unfortunately, most of these techniques are computationally demanding and do not scale well to large networks, which makes them difficult to use in practice.In this paper, we consider the framework of interval bounds propagation (IBP) proposed by Gowal et al. Gowal et al. [2018] for constructing provably robust classifiers. IBP uses the interval arithmetic to minimize the upper bound on the maximum difference between any pair of logits when the input is perturbed within the norm-bounded ball. Direct application of interval arithmetic in a layer-wise fashion leads to the well-known wrapping effect Moore [1979], because bounds are reshaped to be axis-aligned with bounding boxes that always encompass the adversarial polytope, see Figure 1. To overcome this limitation, the authors starts from a typical classification loss to pretrain the network and gradually increases the importance of adversarial loss together with increasing the size of the input perturbation. Unfortunately, too sudden change of these trade-off factors results in the lack of convergence, which makes the training process cumbersome and time consuming.In this contribution, we show that the training procedure of IBP can be significantly simplified, which results in more stable training and faster convergence. Our key idea relies on combining the IBP loss with an additional term, which controls the size of adversarial polytope across layers. More precisely, we minimize the size of outer bound of adversarial polytope at each layer jointly with the IBP cost function, see Figure 2 for the illustration. As a result, our model is less sensitive to the change of the aforementioned IBP hyper-parameters, which makes it easier to use in practice.Our contribution is the following:1. We introduce a new term to the IBP loss function. Our modification allows to use larger perturbations at the initial stage of training and helps to stabilize the training. Moreover, it requires a lower number of epochs to obtain comparable performance to IBP. In consequence, our model can be seen as a very efficient technique for constructing provable robust models, which can be applied to large networks. The proposed idea is not limited to IBP and can be incorporated in other robust training methods, such as the convex-optimization-based approaches Wong and Kolter [2017], Dvijotham et al. [2018b]. It also helps to reduce hyperparameter tuning (particularly dynamics of during the training).2. We give an insight on instability of IBP and show that this effect is correlated with a lack of minimization of interval bounds in hidden layers. Looking from a different perspectivewe observe that IBP (implicitly) minimizes the interval bounds in hidden layers when the training is convergent.3. Conducted experiments support the research hypothesis, that the additional term in the loss function stabilizes the training, improves its efficiency and guides the network in the early stage of training. In the most challenging settings for the CIFAR-10, we are able to get better results (verified test error) even using much smaller network than the one used in the IBP's best performance. We show concrete examples, where IBP fails (or gets stuck in local minimum for a long time), whereas the new loss function allows to train the model in a stable fashion.Figure 2: The scheme of the proposed method. The original IBP loss is supplied with an additional term controlling the errors across layers.",no,,,"hyperparameter from other study, model architecture",no,,,,
892,https://github.com/realsonalkumar/Mish-Mash-Hackathon,realsonalkumar_Mish-Mash-Hackathon.tar.gz,Content Enhanced BERT-based Text-to-SQL Generation,@classmethod @property,"copy tabulate bert ujson lib tqdm __future__ stanza sqlova torchvision tensorflow re os, sys, argparse, re, json wikisql records modeling json collections corenlp torch os matplotlib six sqlnet argparse babel numpy math unicodedata os, json random os, sys, json",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/realsonalkumar_Mish-Mash-Hackathon.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\realsonalkumar_Mish-Mash-Hackathon.pdf,,,no,,,,no,,,,
893,https://github.com/jackyzyb/bayesian-coresets-optimization,jackyzyb_bayesian-coresets-optimization.tar.gz,Bayesian Coresets: Revisiting the Nonconvex Optimization Perspective,,secrets pandas pystan warnings sys mcmc accelerated_iht skimage inference __future__ abstract_dummies model_lr autograd torch os scipy model_linreg cairosvg pickle plotting numpy bokeh stan_code time bayesiancoresets model_poiss setuptools logging gaussian,stat.ML cs.LG stat.CO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jackyzyb_bayesian-coresets-optimization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jackyzyb_bayesian-coresets-optimization.pdf,Coresets,"|Bayesian logistic and Poisson regressionWe consider how IHT performs when used in real applications where the closed-form expressions are unattainable. Moreover, large-scale datasets are considered to test running time of each algorithm. As the true posterior is unknown, a Laplace approximation is used for GIGA and IHT to derive the finite projection of the distribution, i.e., ĝi . Further, Monte Carlo sampling is used to derive gradients of D KL for SparseVI. We compare different algorithms estimating the posterior distribution for logistic regression and Poisson regression. The reverse KL and forward KL between the coreset posterior and true posterior are estimated using another Laplace approximation. The mode of the Laplace approximation is derived by maximizing the corresponding posterior density. The experiment was proposed by Campbell & Broderick (2019), and is used in (Campbell & Broderick, 2018) and (Campbell & Beronov, 2019). Due to space limitations, we refer to section G in the appendix for details of the experimental setup, and extensive additional results.For logistic regression, given a dataset{(x n , y n ) ∈ R D × {1, −1} || n ∈ [N ]}, we aim to infer θ ∈ R D+1based on the model:y n || x n , θ ∼ Bern 1 1 + e −z n θ ,where z n = [x n , 1] . We set N = 500 by uniformly We present two sets of experiments, i.e., logistic regression using the synthetic dataset and the phishing dataset, in Figure 3. One other set of experiments on logistic regression, and three sets of experiments on Poisson regression are deferred to section G in appendix.It is observed that A-IHT and A-IHT II achieve state-ofthe-art performance. The IHT algorithms often obtain coresets with smaller KL between the coreset posterior and true posterior than GIGA and SparseVI, with computing time comparable to GIGA and significantly less than SparseVI. We conjecture that GIGA and SparseVI perform worse than our methods due to their greedy nature: they can be ""short-sighted"" and do not rectify past decisions. The experiments indicate that IHT outperforms the previous methods, improving the trade-off between accuracy and performance.Large-scale Datasets. Two large datasets are considered: i) the large synthetic dataset for logistic regression is generated following the same procedure as before, but with dataset size N = 9000; ii) the original phishing dataset has size N = 11055 and dimension D = 68. The maximal iteration number of the two IHT algorithms is 500. Symmetrized KL, i.e., the sum of forward and reverse KL, is reported.Results are shown in Figure 4. We have to omit Spar-seVI due to its prohibitively high cost (e.g., as shown in Figure 3, SparseVI needs ×10 4 more time than IHT and GIGA). As our complexity analysis of the algorithms in subsection 3.2, the running time of GIGA grows linearly with respect to the coreset size k, while that is almost free for IHT. GIGA begins to cost more time than IHT at k ≈ 200, i.e., about only 2% of the dataset.Additional evaluation. For large-scale datasets, it is often necessary to ""batch"" the algorithms. We test the performance of IHT using a stochastic gradient estimator. The gradient estimator is calculated with random batches in each iteration, where we use a batch size of 20% of the full dataset size. Results on six datasets are defer to section G in appendix.Moreover, as an alternative evaluation of the quality of constructed coresets, we test the 2 -distance between the maximum-a-posteriori (MAP) estimation of the full-dataset posterior and coreset posterior. Results on six datasets are deferred to section G in appendix.|",no,,,,no,,,,
894,https://github.com/isi-vista/face-completion,isi-vista_face-completion.tar.gz,Does Generative Face Completion Help Face Recognition?,@staticmethod,math torch,cs.CV cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/isi-vista_face-completion.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\isi-vista_face-completion.pdf,,"Encoder-Decoder for Face CompletionA key component for developing an effective face inpainter relies on having a clean (i.e. unoccluded) face dataset that can be used as a basis set for generating synthetic face occlusions for learning the face completion task. However, harvesting faces in the wild does not satisfy this requirement since those could be contaminated by occlusions and other obstructions. In order to properly supervise the proposed face completion encoder-decoder, we leverage large face collections borrowed from the face recognition community, where data is abundant. We used images from CASIA WebFaces [45], VGG Faces [31] and MS-Celeb-1M [13] datasets for training the encoder-decoder inpainter. To mitigate the presence of occlusions, we initially run a face detector [43] and retain a face image for training if and only if the face detector is highly confident of its prediction (confidence ≥ 0.985). This process leads to a training data of 197,081 clean images, while maintaining most of the tough variability present in the original data sets. Another 21,121 validation images are used for hyperparameter tuning. Faces are aligned by expanding the detector box into a square from its center to compensate for scale and translation.",no,,,,no,,,,
897,https://github.com/rougier/VSOM,rougier_VSOM.tar.gz,Randomized Self Organizing Map,,"som, mnist, plot imageio gudhi sys tqdm os, struct sys, imageio shapely mnist forbiddenfruit plot plot_persistent_homology som, plot vsom os matplotlib struct scipy spatial pickle umap sklearn numpy persistence_diagram math som networkx os, sys, struct",cs.NE cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rougier_VSOM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rougier_VSOM.pdf,,,no,,,"epochs, learning rate, sigma, epochs",no,,,,
910,https://github.com/Majeed7/L1LR,Majeed7_L1LR.tar.gz,An efficient projection neural network for ℓ 1 -regularized logistic regression,@staticmethod,timeit scipy model_LR_NN_PR sklearn torch numpy matplotlib,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Majeed7_L1LR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Majeed7_L1LR.pdf,Logistic Regression,"C. Comparison of real data setsWe now compare the performance of the proposed neural solution with other state-of-the-art solvers on several real data sets. The comparison is made in terms of execution time, classifier accuracy, ROC, as well as AUROC. We first introduce the methods and data sets being used.Methods: For the comparison study, we select several methods, each with a different approach to handle the ℓ 1 norm. The methods are Gauss-Seidel [25], Shooting [26], Gauss-Southwell [27], Grafting [28], Subgradient [7], epsL1 [29], Log-Barrier [7], SmoothL1 [5], SQP [29], ProjectionL1 [7], InteriorPoint [4], Orthant-wise [7], and Pattern-Search [7], all implemented in MATLAB and are freely available [30]. We also use the implementation for logistic regression in scikit-learn, and refer to it as sklearn [31]. The core of scikit-learn methods have been implemented in C/C + +, and the implementation of logistic regression takes advantage of an inherent feature and subset selection, which makes it significantly faster.Data sets and experimental setup: We subject the proposed method and other methods to eight data sets: leukemia, live-disorders, madelon, splice, gisette, ijcnn1, a1a, and a9a. The data sets have their own training and test partitions, which are used for the training and testing of all the methods. The information regarding these data sets is tabulated in Table I.Execution time comparison: We first compare the methods in terms of the time they consumed to solve problem (1). For doing so, we set the stopping criterion of all the methods to 10 −6 . Table II shows the execution time of the methods to solve problem (1) for different data sets. For those of data sets with a limited number of features and training samples, all the methods behave efficiently and converge to an optimal in a relatively fast manner, e.g., the performance of methods over the data sets splice, madelon, liver-disorders, and a1a. Even for a data set with a large number of the training samples like ijcnn1, the performance of the solvers in terms of their execution time is quite competitive, mainly because the dimensions of the solvers are typically linear in the number of features, which is 22 for the ijcnn1 data set. But when the number of features and training samples increases together, many of the solvers fail to produce a solution in a reasonable time frame. For example, it takes more than three and seven hours for the InteriorPoint method to generate a result on the leukemia and gisette data sets, respectively. Or it takes even more for the Orthant-Wise method to generate results for the same data sets. Among other methods, SubGradient has a very acceptable performance in terms of execution time in generating acceptable results, but we also need to investigate how good its result is regarding accuracy and AUROC. The proposed neural solution, on the other hand, shows a significantly better performance compared to other methods in terms of execution time and demonstrates a fast performance regardless of the size of the data set. In particular, it is especially competitive with sklearn despite its efficient implementation (which includes feature and subset selection in C/C + +). A surprising point is also on the execution time of sklearn for the a9a data set, which shows that a significant increase in both feature and training samples number would adversely affect the performance of this solver. The proposed method demonstrates a stable performance on different data sets with a different number of features and/or training samples.Accuracy comparison: While the execution time of the proposed method is quite superior to other solvers, it is also essential to compare its performance in terms of accuracy. Table III shows the accuracy of different methods on different data sets 2 . According to this table, the accuracy of the proposed method is better than or equivalent to other solvers on five data sets: splice, liver-disorders, ijcnn1, a9a, and leukemia. For the remaining three data sets, the accuracy of the proposed method is quite competitive with the best-performing solvers. In particular, the accuracy difference between the proposed neural solution and best-performing solvers for the three data sets (i.e., Gauss-Seidel for madelon, Gauss-Southwell for a1a, and Orthant-Wise for gisette) is less than 1%, making the performance of the proposed neural solution very competitive with the state-of-the-art solvers in terms of accuracy. At the same, these methods require plenty of time to generate a result. For example, Orthant-wise takes more than 15 hours to generate results on the gisette data set, while its accuracy is only 0.3% higher than that of the proposed neural solution.ROC curve and AUROC comparison: We now compare the proposed neural solution in terms of ROC curve and AUROC. Figure 5 plots the ROC curves of the methods over eight data sets, the legend of which shows the AUROC of each method as well. As in the ROC curve, a deviation from diagonal shows a better performance, this figure also supports that the performance of the proposed neural solution is competitive with other solvers. In particular, the proposed neural network is the best-performing method on a9a and ijcnn1. In addition, its difference with the top-performing method on a1a, splice, and gisette data sets are very infinitesimal with a difference in AUROC less than 0.004. However, Gauss-Seidel has shown better performance on the madelon data set with a higher margin. This experiment also upholds the reasonable performance of the proposed neural network compared to the state-of-the-art solvers. Therefore, the conclusion can be drawn that the proposed neural solution can provide a reliable solution to the logistic regression with ℓ 1 regularization, while it is significantly fast and is scalable for large-scale problems. TABLE II :IIThe methods' execution time (in seconds) over eight data sets.Method's namesplicemadelonliver-disordersijcnn1a1aa9aleukemiagisetteGauss-Seidel4.1327.670.057.792.9058.4221.90977.58Shooting2.4120.640.171.531.9543.7314.37698.52Gauss-Southwell3.5510.890.121.682.3859.2520.11846.80Grafting1.0820.640.252.612.3150.8221.63649.00SubGradient0.081.460.080.650.100.2561.0618.57epsL10.041.070.060.360.185.6825.80401.36Log-Barrier49.752.660.170.870.162.20457.35398.98SmoothL10.144.640.011.472.197.21144.022214.89SQP0.063.400.191.200.621.061591.701585.94ProjectionL10.045.200.041.512.5145.251951.456483.31InteriorPoint0.2212.340.090.530.422.6711677.1227362.16Orthant-Wise0.0520.090.061.833.1646.15143543.2554686.79Pattern-Search0.164.330.211.340.576.0329.52700.61sklearn0.014.630.010.530.6257.320.045.94Proposed method0.220.790.010.170.401.790.141.5 TABLE III :IIIAccuracy of the methods over eight data sets.Method's namesplicemadelonliver-disordersijcnn1a1aa9aleukemiagisetteGauss-Seidel85.4759.3359.589.2882.9483.1991.1884.8Shooting85.4752.5059.090.5083.4582.4664.7168Gauss-Southwell85.4758.1759.090.4483.9085.0185.2995.2Grafting85.4757.8359.091.0083.8484.9982.3594.4SubGradient85.4756.6759.09.5024.0523.6258.8250.0epsL185.4756.6759.091.3483.8484.9976.4798.0Log-Barrier85.4756.3359.091.3483.8484.9885.2997.9SmoothL185.4756.3359.091.2083.8479.6361.7695.7SQP85.4756.3359.091.3483.8484.9985.2997.9ProjectionL185.4756.3359.091.3383.8484.9885.2998.1InteriorPoint85.4756.3359.091.3483.8484.9985.2997.9Orthant-Wise85.4756.3359.091.3383.8484.9785.2998.3Pattern-Search85.4756.3359.091.3483.8484.9988.2498.2sklearn85.4756.6759.0088.2383.8384.9988.2397.9Proposed method85.4759.061.5091.9083.6285.0394.1198.0proposed method. Figure",no,,,,no,,,,
912,https://github.com/rpryzant/causal-bert-pytorch,rpryzant_causal-bert-pytorch.tar.gz,Adapting Text Embeddings for Causal Inference,,math scipy keras pandas collections transformers pickle sklearn torch os tqdm numpy,cs.LG cs.CL stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rpryzant_causal-bert-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rpryzant_causal-bert-pytorch.pdf,,,no,,,,no,,,,
914,https://github.com/Maluuba/nlg-eval,Maluuba_nlg-eval.tar.gz,Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation,@property,"copy sys pdb psutil __future__ xdg sys, math, re threading atexit gensim re click collections unittest json nltk os six scipy sklearn numpy math pip nlgeval setuptools theano logging subprocess",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Maluuba_nlg-eval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Maluuba_nlg-eval.pdf,,,no,,,,no,,,,
922,https://github.com/facebookresearch/fbcdgraph,facebookresearch_fbcdgraph.tar.gz,Cumulative deviation of a subpopulation from the full population,,math PIL subpop multiprocessing subpop_weighted calibr random torch os subprocess numpy matplotlib string torchvision,stat.ME cs.CY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/facebookresearch_fbcdgraph.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\facebookresearch_fbcdgraph.pdf,,,no,,,,no,,,,
924,https://github.com/vaksakalli/spfsr,vaksakalli_spfsr.tar.gz,SPSA-FSR: Simultaneous Perturbation Stochastic Approximation for Feature Selection and Ranking An Introduction and Accuracy Performance Comparison with Other Wrapper Methods,@staticmethod,logging sklearn joblib numpy spFSR time,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vaksakalli_spfsr.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vaksakalli_spfsr.pdf,,,no,,,,no,,,,
929,https://github.com/ibiscp/Synthetic-Plants,ibiscp_Synthetic-Plants.tar.gz,Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming,@property,"utils main imageio yaml sys glob tqdm ast torchvision vgg19_keras tensorflow re wgangp datetime torch os keras_segmentation matplotlib ot os, random scipy functools pickle sklearn argparse numpy gan dcgan time shutil spade timeit math gridSearch ops cv2 random help pytorchMetrics inspect",cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ibiscp_Synthetic-Plants.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ibiscp_Synthetic-Plants.pdf,,,no,,,,no,,,,
932,https://github.com/anglyan/spikingtorch,anglyan_spikingtorch.tar.gz,Coarse scale representation of spiking neural networks: backpropagation through spikes and application to neuromorphic hardware,@staticmethod,math torch argparse spikingnet numpy torchvision,cs.NE cs.ET cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/anglyan_spikingtorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\anglyan_spikingtorch.pdf,,,no,,,,no,,,,
946,https://github.com/StephanieWyt/NMN,StephanieWyt_NMN.tar.gz,Neighborhood Matching Network for Entity Alignment,,math scipy tensorflow json include pickle argparse os numpy,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/StephanieWyt_NMN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\StephanieWyt_NMN.pdf,,"|Entity Alignment and TrainingPre-training. As discussed in Sec. 3.3, our neighborhood sampling is based on the GCNoutput entity embeddings. Therefore, we first pretrain the GCN-based KG embedding model to produce quality entity representations. Specifically, we measure the distance between two entities to determine whether they should be aligned:d(e 1 , e 2 ) = h e 1 − h e 2 L 1(9)The objective of the pre-trained model is:L = (i,j)∈L (i ,j )∈L max{0, d(i, j) − d(i , j ) + γ} (10)where γ > 0 is a margin hyper-parameter; L is our alignment seeds and L is the set of negative aligned entity pairs generated by nearest neighbor sampling (Kotnis and Nastase, 2017).Overall training objective. The pre-training phase terminates once the entity alignment performance has converged to be stable. We find that after this stage, the entity representations given by the GCN are sufficient for supporting the neighborhood sampling and matching modules. Hence, we replace the loss function of NMN after the pretraining phase as:L = (r,t)∈L (r ,t )∈C max{0, d(r, t) − d(r , t ) + γ} (11) d(r, t) = h match r − h match t L 1 (12)where the negative alignments setC = {(r , t )||(r = r ∧ t ∈ C r ) ∨ (t = t ∧ r ∈ C t )} is made up of the alignment candidate sets of r and t, C r and C t are generated in the candidate selection stage described in Sec. 3.4. Note that our sampling process is nondifferentiable, which corrupts the training of weight matrix W s in Eq. 2. To avoid this issue, when training W s , instead of direct sampling, we aggregate all the neighbor information by intuitive weighted summation:g w i = ( p∈N i α ip • σ( ĥp W gate ) • ĥp )W N (13)where α ip is the aggregation weight for neighbor p, and is the sampling probability p(h p ||h i ) for p given by Eq. 2. Since the aim of training W s is to let the learned neighborhood representations of aligned entities to be as similar as possible, the objective is:L w = (r,t)∈L g w r − g w t L 1(14)In general, our model is trained end-to-end after pre-training. During training, we use Eq. 11 as the main objective function, and, every 50 epochs, we tune W s using Eq. 14 as the objective function.|",no,,,"epochs, learning rate",no,,,,
947,https://github.com/ycd2016/acaioc2,ycd2016_acaioc2.tar.gz,Highly Efficient Memory Failure Prediction using Mcelog-based Data Mining and Machine Learning,"@app.errorhandler(500) @app.route(""/tccapi"", methods=[""GET"", ""POST""]) @app.route(""/shutdown"", methods=[""GET"", ""POST""])",scipy pandas sys json ai_hub _pickle logging sklearn os pytorch_tabnet numpy flask,cs.DB cs.LG cs.PF cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ycd2016_acaioc2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ycd2016_acaioc2.pdf,,,no,,,,no,,,,
950,https://github.com/arobey1/RobustNN,arobey1_RobustNN.tar.gz,Safety Verification and Robustness Analysis of Neural Networks via Quadratic Constraints and Semidefinite Programming,@rect.setter @property @timing,utils scipy functools tensorflow itertools robust_program robust_tools network os rectangle cvxpy joblib numpy matplotlib time,math.OC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/arobey1_RobustNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\arobey1_RobustNN.pdf,,,no,,,,no,,,,
956,https://github.com/cyberpunk317/Action_detection,cyberpunk317_Action_detection.tar.gz,Tube Convolutional Neural Network (T-CNN) for Action Detection in Videos,@staticmethod,utils pathlib PIL warnings model collections dataset glob setuptools pdb cv2 torch numpy matplotlib torchvision,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cyberpunk317_Action_detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cyberpunk317_Action_detection.pdf,3D Convolution  Convolution,,no,,,"learning rate, batch size",no,,,,
957,https://github.com/PierrickPochelu/word_tree_label,PierrickPochelu_word_tree_label.tar.gz,Deep Neural Decision Trees,,tarfile keras tensorflow sys urllib pickle os get_cifar10 get_model numpy zipfile,cs.CV cs.LG stat.ML cs.CV cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/PierrickPochelu_word_tree_label.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\PierrickPochelu_word_tree_label.pdf,Fast-YOLOv2  Non Maximum Suppression  SSD  Softmax  Polynomial Rate Decay  Step Decay  Darknet-19  Color Jitter  Random Resized Crop  SGD with Momentum  Weight Decay  YOLOv2  Affine Coupling  Normalizing Flows,"NNDT Random ForestsNeural Decision Trees is weak to dimensionality (i.e. number of features) increase, as the size of the Kron Product result grows exponentially with the number of features.To This tree is very comparable to the one created by a Decision Tree Classifier. For the same dataset, the trained sklearn DecisionTreeClassifier() can create (without any additional parameters) the tree in Figure 5.2. The attribute ""Petal Length"" corresponds to feature index 0 and ""Petal Width"" to index 1. trained on a random subset of features. A better solution that does not require an uninterpretable forest is to exploit the sparsity of the final binning during learning: the number of non-empty leaves grows much slower than the total number of leaves. But this somewhat complicates the otherwise simple implementation of DNDT.X[1] <= 0.7 gini = 0.444 samples = 6 value = [[4, 2] [4, 2] [4, 2]] gini = 0.0 samples = 2 value = [[0, 2] [2, 0] [2, 0]] True X[0] <= 4.75 gini = 0.333 samples = 4 value = [[4, 0] [2, 2] [2, 2]] False gini = 0.0 samples = 2 value = [[2, 0] [0, 2] [2, 0]] gini = 0.0 samples = 2 value = [[2, 0] [2, 0] [0, 2]]",no,,,,no,,,,
961,https://github.com/ilyakava/pyfst,ilyakava_pyfst.tar.gz,EXPLORING THE HIGH DIMENSIONAL GEOMETRY OF HSI FEATURES,,tang_feat PIL copy fst3d_feat csv salt_data sys googleapiclient Tkinter glob rgb_pixelNN pdb lib skimage tqdm window_plot salt_binary windows __future__ salt_pixelNN hdf5storage tensorflow re pydub datetime collections h5py tf_unet os hyper_pixelNN cvxpy plotly matplotlib numpngw audio_load scipy hsi_data itertools salt_baseline DFFN pickle sklearn argparse numpy time shutil kaggle_prec ntpath librosa random operator logging AP,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ilyakava_pyfst.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ilyakava_pyfst.pdf,,,no,,,,no,,,,
963,https://github.com/zouharvi/kb-shrink,zouharvi_kb-shrink.tar.gz,KILT: a Benchmark for Knowledge Intensive Language Tasks,,sys tqdm faiss collections json scann transformers torch model_normalization_data matplotlib scipy itertools pickle umap sklearn argparse kilt numpy timeit pympler datasets random misc reduce_dim,cs.CL cs.AI cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zouharvi_kb-shrink.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zouharvi_kb-shrink.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Sequence to Sequence,,no,,,,no,,,,
964,https://github.com/wmkouw/cc-smoothprior,wmkouw_cc-smoothprior.tar.gz,A cross-center smoothness prior for variational Bayesian brain tissue segmentation,,vGMI pandas hPottsMRF nibabel skimage util keras pystruct tomopy os VGGUnet matplotlib scipy vis vGMM sklearn numpy time math hPotts mpl_toolkits,stat.ML cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wmkouw_cc-smoothprior.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wmkouw_cc-smoothprior.pdf,,"Segmentation methodsWe will compare the following methods: firstly, a U-net consisting of a mirrored VGG16 architecture pre-trained on ImageNet and fine-tuned on labeled data from the source medical center [19,20]. This method represents the performance of a state-of-the-art tissue segmentation model without taking centerbased variation into account. Secondly, we take both an unsupervised and a semi-supervised variational Gaussian mixture model (UGM, SGM), initialized using k-means and 1-nearest-neighbours respectively. Thirdly, an unsupervised and a semi-supervised hidden Potts Gaussian mixture (UHP, SHP) are taken, also initialized using k-means and 1-nearest-neighbours. Comparing these with the previous two models shows the influence of smoothing the segmentations. Lastly, we train a 1-nearest-neighbours (1NN) based on the labeled voxels (1 per tissue) in the target image, as a baseline supervised tissue classifier. The maximum number of training iterations is set to 30 for all methods.",no,,,,no,,,,
965,https://github.com/mchandak29/IQ-VQA,mchandak29_IQ-VQA.tar.gz,IQ-VQA: Intelligent Visual Question Answering,@staticmethod,gc copy IQ yaml sys global_variables glob _pickle tqdm build_post_combine_transform re dataset_utils datetime json collections torch os build_image_feature_encoding scipy bisect top_down_bottom_up pickle sklearn argparse tensorboardX numpy train_model config shutil timeit demjson random logging,cs.CV cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mchandak29_IQ-VQA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mchandak29_IQ-VQA.pdf,,,no,,no,hyperparamter from other studies,no,,,,
967,https://github.com/jia2lin3yuan1/2020-instanceSeg,jia2lin3yuan1_2020-instanceSeg.tar.gz,Deep Variational Instance Segmentation,"@Param: mask_logits -- instance mask logits, in size [bs, 1, ht, wd] @torch.no_grad() @script_method_wrapper @Output: @staticmethod @torch.jit.script","utils functions refineNet sys types dvis_network nvinfo pdb layers skimage torch, torchvision dcn_v2 torchvision pathlib re datetime collections json data torch os matplotlib pymeanshift scipy math, random pickle typing sklearn argparse pyximport tensorboardX numpy time shutil math backbone ptflops pycocotools train_settings cv2 random subprocess colorama",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jia2lin3yuan1_2020-instanceSeg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jia2lin3yuan1_2020-instanceSeg.pdf,Max Pooling  Region Proposal Network  RoIPool  Softmax  Convolution  Faster R-CNN  Fully Convolutional Network,"DatasetsPASCAL VOC 2012 consists of 20 object classes and one background class. It has been the benchmark challenge for segmentation over the years. The original dataset contains 1,464, 1,449, and 1,456 images for training, validation, and testing. It is augmented by extra annotations from [17], resulting in 10,582 training images. The metric we use to evaluate on PASCAL is average precision (AP) with pixel intersection-over-union (IoU) thresholds at 0.5, 0.6, 0.7, 0.8 and 0.9 averaged across the 20 object classes. As there is no ground truth on the testing set, we use the val set to test.PASCAL SBD is a different split on the PASCAL VOC dataset. In order to compare with [25,6], we train a separate model on the training set of SBD and evaluate on its 5,732 validation images.COCO is a very challenging dataset for instance segmentation and object detection. It has 115,000 images and 5,000 images for training and validation, respectively. 20,000 images are used as test-dev from the split of 2017. There are 80 instance classes for instance segmentation and object detection challenge. There are more objects in each image than PASCAL VOC. We train our model on the train 2017 subset and run prediction on val 2017 and test-dev 2017 subsets respectively. We adopt the public cocoapi to report the performance metrics AP , AP 50 , AP 75 , AP S , AP M , and AP L .",no,,,,no,,,,
970,https://github.com/benbogin/obverter,benbogin_obverter.tar.gz,COMPOSITIONAL OBVERTER COMMUNICATION LEARNING FROM RAW VISUAL INPUT,,PIL obverter model random pickle data vapory argparse os torch numpy matplotlib,cs.AI cs.CL cs.LG cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/benbogin_obverter.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\benbogin_obverter.pdf,,,no,,,,no,,,,
971,https://github.com/ku2482/discor.pytorch,ku2482_discor.pytorch.tar.gz,DisCor: Corrective Feedback in Reinforcement Learning via Distribution Correction,@property @abstractmethod,discor yaml datetime collections torch os argparse numpy abc,cs.LG stat.ML cs.LG cs.AI stat.ML cs.LG cs.AI cs.RO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ku2482_discor.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ku2482_discor.pytorch.pdf,Q-Learning  Experience Replay  Dense Connections  Rectified Linear Units  Adam  Soft Actor Critic  Experience Replay  Dense Connections  Rectified Linear Units  Adam  Soft Actor-Critic (Autotuned Temperature),"E.1 DisCor in PracticeIn this section, we provide details on the experimental setup and present the pseudo-code for the practical instantiation of our algorithm, DisCor.The pseudocode for the practical algorithm is provided in Algorithm 3. Like any other ADP algorithm, such as DQN or SAC, our algorithm maintains a pair of Q-functions -the online Q-network Q θ and a target network Qθ. For continuous control domains, we use the clipped double Q-learning trick [5], which is also referred to as the ""twin-Q"" trick, and it further parametrizes another pair of online and target Q-functions, and uses the minimum Q-value for backup computation. In addition to Q-functions, in a continuous control domain, we parametrize a separate policy network π ψ similar to SAC. In a discrete action domain, the policy is just given by a greedy maximization of the online Q-network.DisCor further maintains a model for accumulating errors ∆ φ parameterized by φ and the corresponding target error network ∆ φ. In the setting with two Q-functions, DisCor models two networks, one for modelling error in each Q-function. At every step, a few (depending upon the algorithm) gradient steps are performed on Q and ∆, and π -if it is explicitly modeled, for instance in continuous control domains. This is a modification of generalized ADP Algorithm 2 and the corresponding DisCor version (Algorithm 1), customized to modern deep RL methods.",no,,yes,"temperature, archtecture, target net updates, learning rate, ",no,,,,
973,https://github.com/hanglearning/blockfl_implementation_incomplete,hanglearning_blockfl_implementation_incomplete.tar.gz,Blockchained On-Device Federated Learning,"@app.route('/within_miner_wait_time', methods=['GET']) @app.route('/running', methods=['GET']) @app.route('/register_with', methods=['POST']) @app.route('/receive_updates_from_miner', methods=['POST']) @app.route('/new_transaction', methods=['POST']) @app.route('/receive_propagated_block', methods=['POST']) @app.route('/get_role', methods=['GET']) @app.route('/') @app.route('/register_node', methods=['POST']) @app.route('/get_chain_meta', methods=['GET']) @app.route('/get_peers', methods=['GET']) @app.route('/get_miner_epoch', methods=['GET']) @app.route('/download_block_from_miner', methods=['POST']) @app.route('/get_rewards_from_miner', methods=['POST']) @app.route('/get_worker_data', methods=['GET']) @app.route('/chain', methods=['GET']) @app.route('/get_worker_epoch', methods=['GET'])",utils requests copy block sys pdb threading re datetime json app torch os matplotlib hashlib argparse flask time blockchain binascii device random logging,cs.IT cs.NI math.IT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hanglearning_blockfl_implementation_incomplete.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hanglearning_blockfl_implementation_incomplete.pdf,,,no,,,,no,,,,
984,https://github.com/zhuangdizhu/FedGen,zhuangdizhu_FedGen.tar.gz,Communication-Efficient Learning of Deep Networks from Decentralized Data,@staticmethod,utils copy multiprocessing tqdm torchvision collections json h5py torch os matplotlib string importlib FLAlgorithms seaborn argparse numpy time random mpl_toolkits,cs.LG stat.ML cs.LG cs.LG cs.DC cs.LG cs.DC cs.IT cs.NI math.IT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zhuangdizhu_FedGen.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zhuangdizhu_FedGen.pdf,Knowledge Distillation  Knowledge Distillation,"Experimental ResultsWe are motivated by both image classification and language modeling tasks where good models can greatly enhance the usability of mobile devices. For each of these tasks we first picked a proxy dataset of modest enough size that we could thoroughly investigate the hyperparameters of the FedAvg algorithm. While each individual training run is relatively small, we trained over 2000 individual models for these experiments. We then present results on the benchmark CIFAR-10 image classification task. Finally, to demonstrate the effectiveness of FedAvg on a real-world problem with a natural partitioning of the data over clients, we evaluate on a large language modeling task.Our initial study includes three model families on two datasets. The first two are for the MNIST digit recognition task [26]: 1) A simple multilayer-perceptron with 2-hidden layers with 200 units each using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN.2) A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters). To study federated optimization, we also need to specify how the data is distributed over the clients. We study two ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. This is a pathological non-IID partition of the data, as most clients will only have examples of two digits. Thus, this lets us explore the degree to which our algorithms will break on highly non-IID data. Both of these partitions are balanced, Table 1: Effect of the client fraction C on the MNIST 2NN with E = 1 and CNN with E = 5. Note C = 0.0 corresponds to one client per round; since we use 100 clients for the MNIST data, the rows correspond to 1, 10 20, 50, and 100 clients. Each table entry gives the number of rounds of communication necessary to achieve a test-set accuracy of 97% for the 2NN and 99% for the CNN, along with the speedup relative to the C = 0 baseline. Five runs with the large batch size did not reach the target accuracy in the allowed time. however. 4 For language modeling, we built a dataset from The Complete Works of William Shakespeare [32]. We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients.2NN IID NON-IID C B = ∞ B = 10 B = ∞ B =For each client, we split the data into a set of training lines (the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters 5 in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.On this data we train a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character [22]. The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters. SGD is sensitive to the tuning of the learning-rate parameter η. The results reported here are based on training over a sufficiently wide grid of learning rates (typically 11-13 values for η on a multiplicative grid of resolution 10 ). We checked to ensure the best learning rates were in the middle of our grids, and that there was not a significant difference between the best learning rates. Unless otherwise noted, we plot metrics for the best performing rate selected individually for each x-axis value. We find that the optimal learning rates do not vary too much as a function of the other parameters.Increasing parallelism We first experiment with the client fraction C, which controls the amount of multi-client parallelism. Table 1 shows the impact of varying C for both MNIST models. We report the number of communication rounds necessary to achieve a target test-set accuracy. To compute this, we construct a learning curve for each combination of parameter settings, optimizing η as described above and then making each curve monotonically improving by taking the best value of test-set accuracy achieved over all prior rounds. We then calculate the number of rounds where the curve crosses the target accuracy, using linear interpolation between the discrete points forming the curve. This is perhaps best understood by reference to Figure 2, where the gray lines show the targets.With B = ∞ (for MNIST processing all 600 client examples as a single batch per round), there is only a small advantage in increasing the client fraction. Using the smaller batch size B = 10 shows a significant improvement in using C ≥ 0.1, especially in the non-IID case. Based on these results, for most of the remainder of our experiments we fix C = 0.1, which strikes a good balance between computational efficiency and convergence rate. Comparing the number of rounds for the B = ∞ and B = 10 columns in  2. Plots for the 2NN are given as Figure 7 in Appendix A.Table 1 shows a dramatic speedup, which we investigate next.Increasing computation per client In this section, we fix C = 0.1, and add more computation per client on each round, either decreasing B, increasing E, or both. Figure 2 demonstrates that adding more local SGD updates per round can produce a dramatic decrease in communication costs, and Table 2 quantifies these speedups. The expected number of updates per client per round is u = (E[n k ]/B)E = nE/(KB), where the expectation is over the draw of a random client k. We order the rows in each section of Table 2 by this statistic. We see that increasing u by varying both E and B is effective. As long as B is large enough to take full advantage of available parallelism on the client hardware, there is essentially no cost in computation time for lowering it, and so in practice this should be the first parameter tuned.For the IID partition of the MNIST data, using more computation per client decreases the number of rounds to reach the target accuracy by 35× for the CNN and 46× for the 2NN (see Table 4 in Appendix A for details for the 2NN). The speedups for the pathologically partitioned non-IID data are smaller, but still substantial (2.8 -3.7×). It is impressive that averaging provides any advantage (vs. actually diverging) when we naively average the parameters of models trained on entirely different pairs of digits. Thus, we view this as strong evidence for the robustness of this approach.The unbalanced and non-IID distribution of the Shakespeare (by role in the play) is much more representative of the kind of data distribution we expect for real-world applications. Encouragingly, for this problem learning on the non-IID and unbalanced data is actually much easier (a 95× speedup vs 13× for the balanced IID data); we conjecture this is largely due to the fact some roles have relatively large local datasets, which makes increased local training particularly valuable.For all three model classes, FedAvg converges to a higher level of test-set accuracy than the baseline FedSGD models. This trend continues even if the lines are extended beyond the plotted ranges. For example, for the CNN the B = ∞, E = 1 FedSGD model eventually reaches 99.22% accuracy after 1200 rounds (and had not improved further after 6000 rounds), while the B = 10, E = 20 FedAvg model reaches an accuracy of 99.44% after 300 rounds. We conjecture that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout [36].We are primarily concerned with generalization performance, but FedAvg is effective at optimizing the training loss as well, even beyond the point where test-set accuracy plateaus. We observed similar behavior for all three model classes, and present plots for the MNIST CNN in Figure 6 in Appendix A.Can we over-optimize on the client datasets? The current model parameters only influence the optimization performed in each ClientUpdate via initialization. Thus, as E → ∞, at least for a convex problem eventually the initial conditions should be irrelevant, and the global minimum would be reached regardless of initialization. Even for a non-convex problem, one might conjecture the algo-  rithm would converge to the same local minimum as long as the initialization was in the same basin. That is, we would expect that while one round of averaging might produce a reasonable model, additional rounds of communication (and averaging) would not produce further improvements.Figure 3 shows the impact of large E during initial training on the Shakespeare LSTM problem. Indeed, for very large numbers of local epochs, FedAvg can plateau or diverge. 6 This result suggests that for some models, especially in the later stages of convergence, it may be useful to decay the amount of local computation per round (moving to smaller E or larger B) in the same way decaying learning rates can be useful. Figure 8 in Appendix A gives the analogous experiment for the MNIST CNN. Interestingly, for this model we see no significant degradation in the convergence rate for large values of E. However, we see slightly better performance for E = 1 versus E = 5 for the large-scale language modeling task described below (see Figure 10 in Appendix A). CIFAR experimentsWe also ran experiments on the CIFAR-10 dataset [24] to further validate FedAvg. The dataset consists of 10 classes of 32x32 images with three RGB channels. There are 50,000 training examples and 10,000 testing examples, which we partitioned into 100 clients each containing 500 training and 100 testing examples; since there isn't a natural user partitioning of this data, we considered the balanced and IID setting. The model architecture was taken from the TensorFlow tutorial [38], which consists of two convolutional layers followed by two fully connected layers and then a linear transformation layer to produce logits, for a total of about 10 6 parameters. Note For all algorithms, we tuned a learning-rate decay parameter in addition to the initial learning rate. Table 3 gives the number of communication rounds for baseline SGD, FedSGD, and FedAvg to reach three different accuracy targets, and Figure 4 gives learning-rate curves for FedAvg versus FedSGD.By running experiments with minibatches of size B = 50 for both SGD and FedAvg, we can also look at accuracy as a function of the number of such minibatch gradient calculations. We expect SGD to do better here, because a sequential step is taken after each minibatch computation. However, as Figure 9 in the appendix shows, for modest values of C and E, FedAvg makes a similar amount of progress per minibatch computation. Further, we see that both standard SGD and FedAvg with only one client per round (C = 0), demonstrate significant oscillations in accuracy, whereas averaging over more clients smooths this out.",no,,,,no,,,,
985,https://github.com/shuuchen/video_autoencoder,shuuchen_video_autoencoder.tar.gz,Unsupervised Learning of Video Representations using LSTMs,,torch torchvision,cs.LG cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shuuchen_video_autoencoder.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shuuchen_video_autoencoder.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory,,no,,,,no,,,,
989,https://github.com/Kaixhin/imitation-learning,Kaixhin_imitation-learning.tar.gz,Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation,"@hydra.main(config_path='conf', config_name='config')","utils yaml environments glob hydra tqdm omegaconf sys, os collections torch os matplotlib seaborn training argparse numpy time math evaluation models",cs.LG stat.ML cs.LG stat.ML cs.LG cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Kaixhin_imitation-learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Kaixhin_imitation-learning.pdf,Generative Adversarial Imitation Learning,"|Imitation LearningWe briefly review the main methods for imitation learning:Behavioral Cloning (BC) learns a control policy π : S → A directly from expert trajectories via supervised learning. Despite its simplicity, BC is prone to compounding errors: small mistakes in the policy cause the agent to deviate from the state distribution seen during training, making future mistakes more likely. Mistakes accumulate, leading to eventual catastrophic errors Ross et al. (2011). While several strategies have been proposed to address this Ross & Bagnell (2010); Sun et al. (2017), they often require access to the expert policy during the entire training process, rather than a finite set of expert trajectories. BC is commonly used to initialize control policies for reinforcement learning Rajeswaran et al. (2017); Nagabandi et al. (2018).Inverse Reinforcement Learning (IRL) models the expert trajectories with a Boltzmann distribution Ziebart et al. (2008), where the likelihood of a trajectory is defined as:p θ (τ ) = 1 Z exp(−c θ (τ ))(1)where τ = {s 1 , a 1 , s 2 , a 2 ...} is a trajectory, c θ (τ ) = t c θ (s t , a t ) a learned cost function parametrized by θ, and the partition function Z exp(−c θ (τ ))d(τ ) the integral over all trajectories consistent with transition function of the MDP. The main computational challenge of IRL is the efficient estimation the partition function Z. IRL algorithms typically optimize the cost function by alternating between updating the cost function and learning an optimal policy with respect to the current cost function with reinforcement learning Abbeel & Ng (2004); Ng & Russell (2000).Imitation Learning via Distribution Matching frames imitation learning as distribution matching between the distribution of state-action pairs of the expert policy and that of the learned policy. In Ho & Ermon (2016);Finn et al. (2016), the authors connect IRL to distribution matching via Generative Adversarial Networks (GANs) Goodfellow et al. (2014). Known as Generative Adversarial Imitation Learning (GAIL), the method imitates an expert policy by formulating the following minimax game:min π max D∈(0,1) E π (log D(s, a)) + E π E (log(1 − D(s, a))) − λH(π)(2)where the expectations E π and E π E denote the joint distributions over state-action pairs of the learned policy and the expert policy, respectively, and H(π) E π (− log π(a||s)) is the entropy of the learned policy. GAIL has been successfully applied to various control tasks in the Mujoco environment Ho & Ermon (2016); Baram et al. (2017). However, GAIL inherits the challenges of GANs, including possible training instability such as vanishing or exploding gradients, as well as overfitting to training data ; Brock et al. (2018). While numerous theoretical and practical techniques have been proposed to improve GANs (e.g ; Salimans et al. (2016)), a large-scale study of GANs show that many GAN algorithms and architectures achieve similar performance with sufficient hyperparameter tuning and random restarts, and no algorithm or network architecture stands out as the clear winner on all tasks Lucic et al. (2018). Similar to GAIL, Kim & Park (2018) proposed generative moment matching imitation learning (GMMIL) by minimizing the maximum mean discrepancy between the expert policy and the learned policy. Though GMMIL avoids the difficult minimax game, the cost of each reward function evaluation grows linearly with the amount of training data, which makes scaling the method to large dataset potentially difficult. In addition, we demonstrate in our experiments that GMMIL may fail to estimate the appropriate reward functions.|",no,,,,no,,,,
994,https://github.com/NVlabs/SCOPS,NVlabs_SCOPS.tar.gz,SCOPS: Self-Supervised Co-Part Segmentation,@property @constrain.setter,"utils PIL imageio loss warnings sys pdb __future__ torchvision web_vis_html model absl collections json doctest visdom torch os os, argparse matplotlib pydensecrf importlib dominate scipy tps_stn_pytorch ipdb itertools visualize dataset pickle scops_trainer sklearn argparse tps tensorboardX numpy shutil math random cv2",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/NVlabs_SCOPS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\NVlabs_SCOPS.pdf,,"Overall FrameworkFigure 2 shows the overall framework of our proposed method. Given an image collection {I} of the same object category, we train a part segmentation network F parameterized by θ f , which is a fully convolutional neural network (FCN [36]) with a channel-wise softmax layer in the end, to generate the part response maps R = F(I; θ f ) ∈ [0, 1] (K+1)×H×W , where K denotes the number of parts and H × W is the image resolution. Our network predicts K + 1 channels with an additional channel indicating the background. To obtain the final part segmentation results, we first normalize each part map with it maximum response value in the spatial dimensions R(k, i, j) = R(k, i, j)/ max u,v (R(k, u, v)), and we set the background map as constant with value 0.1. The purpose of this normalization is to enhance weak part responses. Then the part segmentation is obtained with the arg max function along the channel dimension. We use DeepLab-V2 [8] with ResNet50 [17] as our part segmentation network.Since we do not assume the availability of any ground truth segmentation annotations, we formulate several constraints as differentiable loss functions to encourage the above mentioned desired properties of a part segmentation, such as geometry concentration and semantic consistency. The overall loss function for part segmentation network is a weighted sum of different loss functions which we describe next. Contrary to several co-segmentation approaches [24,32,19,43,33], which require multiple images during test-time inference, our network only takes a single image as input during the test time resulting in better portability of our trained model to unseen test images. ExperimentsThroughout the experiments, we refer to our technique as ""SCOPS"" (Self-supervised Co-Part Segmentation). Since SCOPS is self-supervised, the segmentation does not necessarily correspond to the human annotated object parts. Therefore, we quantitatively evaluate SCOPS with two different proxy measures on different object categories, including CelebA [29], AFLW [22] (human faces), CUB [44] (birds), and PASCAL [11] (common objects) datasets.On CelebA, AFLW, and CUB datasets, we convert our part segmentation into landmarks by taking part centers (Eqn. 1) and evaluate against groundtruth annotations. Following recent works [50,42], we fit a linear regressor that learns to map the detected landmarks to groundtruth landmarks and evaluate the resulting model on test data. On PASCAL, we aggregate the part segmentations and evaluate them with the foreground segmentation IOU. Implementation Details We implement SCOPS 1 with Py-Torch, and we train the networks with a single Nvidia GPU. We use relu5 2 concatenated with relu5 4 from VGG- 19 [39] as the pre-trained features V for the semantic consistency loss.",no,,,,no,,,,
998,https://github.com/hsd1121/PointCloudProcessing,hsd1121_PointCloudProcessing.tar.gz,Crop Height and Plot Estimation for Phenotyping from Unmanned Aerial Vehicles using 3D LiDAR,,sympy collada math scipy xml UliEngineering csv open3d random cv2 sklearn os numpy matplotlib,cs.RO cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hsd1121_PointCloudProcessing.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hsd1121_PointCloudProcessing.pdf,,,no,,,,no,,,,
999,https://github.com/LittleYUYU/Interactive-Semantic-Parsing,LittleYUYU_Interactive-Semantic-Parsing.tar.gz,Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study,,"utils lam_rule_agent gc copy csv pickle, pdb, random sys hierarchical_agent pdb pprint tensorflow re environment collections nltk interactive_setting os hierarchical_env os, random, sys pickle argparse numpy time config agent random operator",cs.CL cs.AI cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/LittleYUYU_Interactive-Semantic-Parsing.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\LittleYUYU_Interactive-Semantic-Parsing.pdf,,"Extend to Complex SQL GenerationA remarkable characteristic of MISP-SQL is its generalizability, as it makes the best use of the base semantic parser and requires no extra model training. To verify it, we further experiment MISP-SQL on the more complex text-to-SQL dataset ""Spider"" (Yu et al., 2018c). The dataset consists of 10,181 questions on multi-domain databases, where SQL queries can contain complex keywords such as GROUP BY or join several tables. We extend the NLG lexicon and grammar (Section 4.3) to accommodate this complexity, with details shown in Appendix A. We adopt SyntaxSQLNet (Yu et al., 2018b) as the base parser. 3 In our experiments, we follow the same database split as in (Yu et al., 2018c) and report the Exact Matching accuracy (""Acc em "") on Dev set. 4 Other experimental setups remain the same as when evaluating MISP-SQL on WikiSQL. Table 4 shows the results.We first observe that, via interactions with simulated users, MISP-SQL improves SyntaxSQL-Net by 10% accuracy with reasonably 3 questions per query. However, we also realize that, unlike on WikiSQL, in this setting, the probabilitybased error detector requires more questions than the Bayesian uncertainty-based detector. This can be explained by the inferior performance of the base SyntaxSQLNet parser (merely 20% accuracy without interaction). In fact, the portion of questions that the probability-based detector spends on right predictions (Q r ) is still half of that the dropout-based detector asks (12.8% vs. 24.8%). However, it wastes around 60% of questions on unsolvable wrong predictions. This typically hap- pens when the base parser is not strong enough, i.e., cannot rank the true option close to the top, or when there are unsolved wrong precedent predictions (e.g., in ""WHERE col op val"", when col is wrong, whatever op/val following it is wrong). This issue can be alleviated when more advanced base parsers are adopted in the future. C Error Detector ComparisonAs a supplementary experiment to Figure 4, in this section, we show the performance of different error detectors under the same average number of questions (""target budget""). Specifically, for each base semantic parser and each kind of error detector, we tune its decision threshold (i.e., p * and s * ) such that the resulting average number of questions (""actual budget"") is as close to the target as possible. In practice, we relax the actual budget to be within ±0.015 of the target budget, which empirically leads to merely negligible variance. The results are shown in Figure 1 :1Figure 1: Model-based Interactive Semantic Parsing (MISP) framework.",no,,,,no,,,,
1003,https://github.com/DFKI-Interactive-Machine-Learning/py-faster-rcnn-ft,DFKI-Interactive-Machine-Learning_py-faster-rcnn-ft.tar.gz,Fine-tuning deep CNN models on specific MS COCO categories,"@roidb_handler.setter @app.route('/') @app.route('/classify_url', methods=['GET']) @property @staticmethod @app.route('/classify_upload', methods=['POST'])","copy csv multiprocessing generate_anchors tools threading re h5py caffe six codecs rpn pickle time, os, sys numpy unicodedata random roi_data_layer inspect mmap extract_seconds utils getopt yaml pdb __future__ pprint distutils sys, os cStringIO os mask google string xml hashlib sre_compile os, sys, argparse time shutil werkzeug customToolbox pycocotools tornado subprocess pandas sys exifutil easydict atexit gflags _init_paths datetime mincepie matplotlib prototxt_mod scipy IPython itertools sklearn datasets setuptools errno cPickle caffe, os, sys, cv2 fast_rcnn PIL nms skimage voc_eval ast pylab os, sys uuid Cython json collections optparse urllib argparse flask math cv2 logging",cs.CV cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DFKI-Interactive-Machine-Learning_py-faster-rcnn-ft.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DFKI-Interactive-Machine-Learning_py-faster-rcnn-ft.pdf,,,no,,,,no,,,,
1006,https://github.com/gittar/k-means-u-star,gittar_k-means-u-star.tar.gz,The k-means-u* algorithm: non-local jumps and greedy retries improve k-means++ clustering,@classmethod,"math scipy math, random copy bottleneck collections inspect random bfutil sklearn os numpy matplotlib",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gittar_k-means-u-star.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gittar_k-means-u-star.pdf,,"Figure 32: Cloud data from UCI (https://archive.ics.uci.edu/ml/data sets/Cloud), (all pairs of dimensions displayed), data preprocessed with sklearn.preprocessing.StandardScaler to have unit variance in each direction, d = 10, n = 1024, g =(unknown) Figure 34 :34Figure 34: Propulsion data from UCI (https://archive.ics.uci.edu/ml/data sets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants), (all pairs of dimensions displayed), data preprocessed with sklearn.preprocessing.StandardScaler to have unit variance in each direction, d = 16, n = 11934, g =(unknown)",no,,,,no,,,,
1009,https://github.com/hkzhang95/DynamicRCNN,hkzhang95_DynamicRCNN.tar.gz,Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training,@once_differentiable @property @staticmethod,"PIL copy sys glob tqdm easydict __future__ torchvision re datetime collections json tempfile network torch os os, getpass dynamic_rcnn itertools xml bisect dataset pickle argparse numpy time config math pycocotools setuptools errno cv2 logging random",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hkzhang95_DynamicRCNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hkzhang95_DynamicRCNN.pdf,Average Pooling  Non Maximum Suppression  Residual Connection  Rectified Linear Units  Batch Normalization  Soft-NMS  1x1 Convolution  Feature Pyramid Network  Region Proposal Network  Softmax  RoIPool  Faster R-CNN  Deformable Convolution  Dynamic SmoothL1 Loss  Dynamic R-CNN  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Residual Network  Convolution,,no,,,,no,,,,
1013,https://github.com/Line290/FeatureAttack,Line290_FeatureAttack.tar.gz,Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training,@property @staticmethod,utils PIL copy pgd_attack robustml sys tqdm __future__ torchvision tensorflow model datetime json collections torch os ot scipy pickle cifar10_input argparse numpy time math setuptools random attack_methods models,cs.CV cs.CR cs.LG cs.LG cs.CV cs.PF stat.ML cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Line290_FeatureAttack.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Line290_FeatureAttack.pdf,,,no,,,"learning rate, epochs, random crops, perturbation budget, label smoothing, attack iteration",no,,,,
1016,https://github.com/oriern/SuperPAL,oriern_SuperPAL.tar.gz,SuperPAL: Supervised Proposition ALignment for Multi-Document Summarization and Derivative Sub-Tasks,"@slow @unittest.skip(""This test is failing intermittently. Skipping it until we resolve."") @add_start_docstrings_to_callable(TRANSFO_XL_INPUTS_DOCSTRING) @sep_token.setter @PreTrainedTokenizer.eos_token.setter @bos_token.setter @add_start_docstrings_to_callable(OPENAI_GPT_INPUTS_DOCSTRING) @n_words.setter @lru_cache() @PreTrainedTokenizer.bos_token.setter @add_start_docstrings_to_callable(XLM_INPUTS_DOCSTRING) @add_start_docstrings_to_callable(XLNET_INPUTS_DOCSTRING) @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING) @custom_tokenizers @add_start_docstrings_to_callable(FLAUBERT_INPUTS_DOCSTRING) @require_torch @require_tf @add_start_docstrings_to_callable(GPT2_INPUTS_DOCSTRING) @pad_token.setter @property @staticmethod @add_start_docstrings(AutoModel.__doc__) @cls_token.setter @add_start_docstrings(""""""RoBERTa Model with a `language modeling` head on top. """""", ROBERTA_START_DOCSTRING) @unittest.skip(""Passing inputs_embeds not implemented for Bart."") @unittest.skip(""This is just too slow"") @abstractmethod @contextmanager @PreTrainedTokenizer.additional_special_tokens.setter @add_start_docstrings( @PreTrainedTokenizer.pad_token.setter @PreTrainedTokenizer.mask_token.setter @add_start_docstrings_to_callable(CTRL_INPUTS_DOCSTRING) @add_start_docstrings_to_callable(ROBERTA_INPUTS_DOCSTRING) @add_start_docstrings(AutoConfig.__doc__) @add_start_docstrings(AutoModelForSequenceClassification.__doc__) @classmethod @additional_special_tokens.setter @PreTrainedTokenizer.cls_token.setter @pl.data_loader @n_token.setter @PreTrainedTokenizer.sep_token.setter @wraps(func) @mask_token.setter @contextlib.contextmanager @s3_request @tf.function @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING) @PreTrainedTokenizer.unk_token.setter @eos_token.setter @add_start_docstrings_to_callable(DISTILBERT_INPUTS_DOCSTRING) @unk_token.setter",fastapi utils_hans copy tensorflow_datasets csv git multiprocessing seqeval torchvision lm_seqs_dataset tensorflow re run_generation h5py torch six rouge spacy a bert_score pickle boto3 tensorboardX numpy transformer_base sentencepiece tarfile configuration_bertabs unicodedata annotation2MRPC_Aligner random starlette utils_mmimdb colorama utils requests pdb Aligner psutil __future__ pprint uvicorn distutils pathlib unittest pythainlp botocore tests transformers fnmatch nltk os supervised_oie_wrapper socket string ftfy functools hashlib utils_squad filterContained time shutil utils_summarization abc getpass docSum2MRPC_Aligner run_glue operator pydantic subprocess models docopt Mykytea pandas run_squad sys model_bertabs regex tqdm tokenizers datetime absl distiller tempfile scipy itertools torchtext typing sklearn filelock timeit pplm_classification_head utils_ner setuptools allennlp hans_processors zipfile utils_multiple_choice PIL packaging io modeling_bertabs contextlib sacremoses glob fairseq ast fastprogress grouped_batch_sampler json collections pytorch_lightning utils_squad_evaluate apex bisect ptvsd jieba urllib argparse math MeCab logging,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oriern_SuperPAL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oriern_SuperPAL.pdf,,,no,,,,no,,,,
1021,https://github.com/UditSinghParihar/RoRD,UditSinghParihar_RoRD.tar.gz,RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching,,"PIL copy imageio pandas csv warnings sys lib skimage tqdm torchvision os, sys open3d h5py torch os matplotlib scipy argparse joblib numpy time shutil math cv2 random pydegensac",cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UditSinghParihar_RoRD.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UditSinghParihar_RoRD.pdf,,,no,,,,no,,,,
1023,https://github.com/drorsimon/image_barycenters,drorsimon_image_barycenters.tar.gz,Barycenters of Natural Images -Constrained Wasserstein Barycenters for Image Morphing,"@jit(""float64[:,:](int64,int64,int64[:,:,:])"",nopython=True) @jit(""float64[:,:](int64,int64,float64[:,:,:,:],float32)"",nopython=True)",utils PIL sys tqdm pix2pix generate_h5 torchvision h5py torch os generate_pix_dataset matplotlib numba typing argparse train_dcgan numpy shutil random dataloader dcgan_models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/drorsimon_image_barycenters.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\drorsimon_image_barycenters.pdf,,"Generative Adversarial NetworksIn the GAN setting [6,7], a generative network G(z) : R m → R n and a discriminative one D(y) : R n → {0, 1} contest against each other. Given a dataset, the former is trained to generate samples from it when given a random input vector z, while the latter is trained to distinguish the generated data from the original one. This approach leads to a model that is able to generate new data samples with statistical properties that are similar to the training set, by sampling random vectors z from the latent space of the model, and passing them through G.In order to use the generative network for image morphing, an inverse mapping, i.e. a mapping from an input image x to its latent representation vector z, is required. To obtain this mapping, we follow the approach described in [9] that we now briefly describe here. Once the generative network is trained, we train an encoder E(x) : R n → R m , such that G(E(x)) is similar to the input x:E(x) = z * = min z L (x, G(z(x))) , (14)where L is a pixel-wise 2 loss in simple cases such as MNIST [29]. For more complex images such as Zappos50k [2], the loss L is extended to a weighted sum of pixel-wise 2 and features extracted from AlexNet [30] trained on Im-ageNet [31]. This encoder-decoder scheme G • E : R n → R n may be perceived as a projection of the input signal onto the manifold of natural images. Therefore, To use GAN as a prior in our approach, the second step in Algorithm 1 is implemented by a simple feedforward activation of the obtained encoder-decoder. Shoe Images -UT Zappos50KThe Zappos50K [2] is a much more complicated dataset compared to the previous two. Specifically, it contains more details and higher resolution. To train a GAN capable of generating such images, we split the training process in two, somewhat similar to the training scheme described in Stack-GAN [34]. First, we downscale the images to 64 × 64, and train a DCGAN model, as well as an encoder, as described in Section 4.2. The output images of this model are very smooth and lack fine high-frequency details. To add these details, we train an additional generative model. To this end, we generate a dataset of input-output image pairs as follows: the input images are the output of the encoderdecoder scheme, upsampled to 128 × 128, whereas the output images are the original ones downscaled to 128 × 128. This dataset is used as a training set to a pix2pix model [8]. To summarize, our projection scheme consists of the following stages: (i) project the input image to the DCGAN's latent space using a trained encoder; (ii) generate a lowfrequency 64 × 64 image using a DCGAN; (iii) upscale the Figure 6: Morphing English character images using: Wasserstein barycenters, our approach using DCGAN as an image prior and DCGAN latent space linear interpolation in the 1-st, 2-nd and 3-rd row of every subfigure respectively. image to 128 × 128; and (iv) feed the image into a pix2pix model to generate high frequency details.Once the models are trained, we compare the three following methods. The first is a standard Wasserstein barycenter solution (applied on each color channel separately). The second approach is our proposed algorithm, i.e. project each of the Wasserstein barycenters to the manifold of natural images, using the trained generative models, and the third is the common transition achieved using GANs as follows. Each input image is projected onto the latent space, using the encoder. Then, to obtain a transition, we linearly interpolate the two latent vectors and pass the interpolated vectors through the DCGAN and pix2pix models. From our experiments, iterating once produces the best results. The results of our experiments are presented in Figures 2 and  7. Both our method and the GAN alternative provide natural images most of the time. Furthermore, in cases where the two input images are similar in shape and color the difference between the two approaches seems mild (see the last example in Figure 7). However, when the contour or the hue of the two input images differ significantly, our approach brings on a much more steady and straightforward transition to both the shape and the colors of the images.",no,,,,no,,,,
1026,https://github.com/hanchaoleng/shapeconv,hanchaoleng_shapeconv.tar.gz,ShapeConv: Shape-aware Convolutional Layer for Indoor RGB-D Semantic Segmentation,@LR_SCHEDULERS.register_module @wraps(func) @abstractmethod @UTILS.register_module @BACKBONES.register_module @lr.setter @DISTRIBUTED_SAMPLERS.register_module @NON_DISTRIBUTED_SAMPLERS.register_module @TRANSFORMS.register_module @METRICS.register_module @ENHANCE_MODULES.register_module @staticmethod @DATASETS.register_module @property @BRICKS.register_module @epoch.setter @DECODERS.register_module @HEADS.register_module,getopt PIL copy warnings sys multiprocessing weakref data_preparation rgbd_seg torchvision json collections h5py torch os matplotlib importlib scipy functools itertools addict argparse numpy time shutil abc math albumentations cv2 logging random inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hanchaoleng_shapeconv.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hanchaoleng_shapeconv.pdf,ShapeConv  Convolution,,no,,,,no,,,,
1028,https://github.com/jiali-ms/JLM,jiali-ms_JLM.tar.gz,Using the Output Embedding to Improve Language Models,@ex.automain,utils japanese copy sys tqdm __future__ re tensorflow decoder_dynamic model collections json data os model_ngram functools decoder_ngram pickle sklearn argparse decoder numpy time config train math operator random sacred,cs.CL cs.CL cs.LG cs.CL cs.LG cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jiali-ms_JLM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jiali-ms_JLM.pdf,Weight Tying  Softmax  Adaptive Softmax  VGG-16  1x1 Convolution  Convolution  Local Response Normalization  Grouped Convolution  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  AlexNet,,no,,,"same hyperparameter as in other study, model architecture",no,,,,
1039,https://github.com/IanChan711/ATM_ide,IanChan711_ATM_ide.tar.gz,Attention-based multi-task learning for speech-enhancement and speaker-identification in multi-speaker dialogue scenario,,"utils scipy soundfile pandas os, pdb model random pdb tqdm torch os Model numpy matplotlib time torchvision",eess.AS cs.SD,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IanChan711_ATM_ide.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IanChan711_ATM_ide.pdf,,"IV. EXPERIMENTS AND ANALYSESIn the following subsections, we first introduce the experimental setup and then provide the experimental results along with a discussion about our findings.",no,,,,no,,,,
1040,https://github.com/BaiLiping/WirelessPowerTransfer,BaiLiping_WirelessPowerTransfer.tar.gz,Multiagent Reinforcement Learning based Energy Beamforming Control,,"utils envs itertools pandas shared_adam tensorflow math, os sys collections setuptools lib torch os mpl_toolkits numpy matplotlib gym",eess.SP cs.SY eess.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BaiLiping_WirelessPowerTransfer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BaiLiping_WirelessPowerTransfer.pdf,,,no,,,,no,,,,
1047,https://github.com/jocpae/clDice,jocpae_clDice.tar.gz,clDice -a Novel Topology-Preserving Loss Function for Tubular Structure Segmentation,,keras tensorflow skimage torch numpy,cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jocpae_clDice.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jocpae_clDice.pdf,,"DatasetsWe employ five public datasets for validating clDice and soft-clDice as a measure and an objective function, respectively. In 2D, we evaluate on the DRIVE retina dataset [45], the Massachusetts Roads dataset [28] and the CREMI neuron dataset [12]. In 3D, a synthetic vessel dataset with an added Gaussian noise term [41] and the Vessap dataset of multi-channel volumetric scans of brain vessels is used [50,35]. For the Vessap dataset we train different models for one and two input channels. For all of the datasets, we perform three fold cross-validation and test on held-out, large, and highly-variant test sets. Details concerning the experimental setup can be found in the supplementary material. Evaluation MetricsWe compare the performance of various experimental setups using three types of metrics: volumetric, topologybased, and graph-based. Results and DiscussionWe trained two segmentation architectures, a U-Net and an FCN, for the various loss functions in our experimental setup. As a baseline, we trained the networks using soft-dice and compared it with the ones trained using the proposed loss (Eq. 3), by varying α from (0.1 to 0.5).Quantitative: We observe that including soft-clDice in any proportion (α > 0) leads to improved topological, volumetric and graph similarity for all 2D and 3D datasets, see Table 1. We conclude that α can be interpreted as a hyper parameter which can be tuned per-dataset. Intuitively, increasing the α improves the clDice measure for most experiments. Most often, clDice is high or highest when the graph and topology based measures are high or highest, particularly the β 1 Error, Streetmover distance and Opt-J F1 score; quantitatively indicating that topological properties are indeed represented in the clDice measure.In spite of not optimizing for a high soft-clDice on the background class, all of our networks converge to superior segmentation results. This not only reinforces our assumptions on dataset-specific necessary conditions but also validates the practical applicability of our loss. Our findings hold for the different network architectures, for 2D or 3D, and for tubular or curvilinear structures, strongly indicating its generalizability to analogous binary segmentation tasks.Observe that CREMI and the synthetic vessel dataset (see Supplementary material) appear to have the smallest increase in scores over the baseline. We attribute this to them being the least complex datasets in the collection, with CREMI having an almost uniform thickness of radii and the synthetic data having a high signal-to-noise ratio and insignificant illumination variation. More importantly, we observe larger improvements for all measures in case of the more complex Vessap and Roads data see Figure 5. In direct comparison to performance measures reported in two recent publications by Hu et al. and Mosinska et al. [17,29], we find that our approach is on par or better in terms of Accuracy and Betti Error for the Roads and CREMI dataset. It is important to note that we used a smaller subset of training data for the Road dataset compared to both while using the same test set.Hu et al. reported a Betti error for the DRIVE data, which exceeds ours; however, it is important to consider that their approach explicitly minimizes the mismatch of the persistence diagram, which has significantly higher computational complexity during training, see the section below. We find that our proposed loss performs superior to the baseline in almost every scenario. The improvement appears to be pronounced when evaluating the highly relevant graph and topology based measures, including the recently Qualitative: In Figure 5, typical results for our datasets are depicted. Our networks trained on the proposed loss term recover connections, which were false negatives when trained with the soft-Dice loss. These missed connections appear to be particularly frequent in the complex road and DRIVE dataset. For the CREMI dataset, we observe these situations less frequently, which is in line with the very high quantitative scores on the CREMI data. Interestingly, in the real 3D vessel dataset, the soft-Dice loss oversegments vessels, leading to false positive connections. This is not the case when using our proposed loss function, which we attribute to its topology-preserving nature. Additional qualitative results can be inspected in the supplementary.Computational Efficiency: Naturally, inference times of CNNs with the same architecture but different training losses are identical. However, during training, our soft-skeleton algorithm requires O(kn 2 ) complexity for an n × n 2D image where k is the number of iterations. As a comparison, [17] needs O(c 2 mlog(m)) (see [15]) complexity to compute the 1d persistent homology where d is the number of points with zero gradients in the prediction and m is the number of simplices. Roughly, c is proportional to n 2 , and m is of O(n 2 ) for a 2D Euclidean grid. Thus, the worst complexity of [17] is O(n 6 log(n)).Additionally, their approach requires an O(clog(c)) complexity to find an optimal matching of the birth-death pairs. We note that the total run-time overhead for soft-clDice compared to soft-Dice is marginal, i.e., for batch-size of 4 and 1024x1024 image resolution, the former takes 1.35s while the latter takes 1.24s on average (<10% increase) on an RTX-8000.Future Work: Although our proposed soft-skeleton approximation works well in practice, a better differentiable skeletonization can only improve performance, which we reserve for future research. Any such skeletonization can be readily plugged into our approach. Furthermore, theoretical and experimental multi-class studies would sensibly extend our study. H. Evaluation MetricsAs discused in the text, we compare the performance of various experimental setups using three types of metrics: volumetric, graph-based and topology-based.",no,,,,no,,,,
1049,https://github.com/InterDigitalInc/CompressAI,InterDigitalInc_CompressAI.tar.gz,Joint Autoregressive and Hierarchical Priors for Learned Image Compression,"@pytest.mark.parametrize(""backend"", (""matplotlib"", ""plotly"")) @pytest.fixture @pytest.mark.parametrize(""quality"", (""1"", ""4"", ""8"")) @abc.abstractmethod @pytest.mark.parametrize(""model"", (""bmshj2018-factorized"",)) @pytest.mark.parametrize(""metric"", (""mse"", ""ms-ssim"")) @pytest.mark.parametrize( @pytest.mark.pretrained @pytest.mark.parametrize(""v"", (""inf"", ""-inf"", ""nan"")) @torch.no_grad() @pytest.mark.slow @property @staticmethod @pytest.mark.parametrize(""arch,N"", itertools.product(archs, [1, 2, 3])) @pytest.mark.parametrize(""codec"", (""jpeg"",)) @pytest.mark.parametrize(""model_name"", (""factorized-prior"", ""bmshj2018-factorized"")) @pytest.mark.parametrize(""model_entrypoint"", (cheng2020_anchor, cheng2020_attn)) @pytest.mark.parametrize(""func"", (rgb2ycbcr, ycbcr2rgb)) @pytest.mark.parametrize(""entropy_estimation"", (False, True)) @pytest.mark.parametrize(""metric"", (""psnr"", ""ms-ssim"")) @classmethod @torch.jit.unused",PIL copy contextlib warnings multiprocessing sys range_coder platform torchvision pathlib re json collections tempfile torch os pytest plotly matplotlib importlib struct scipy itertools hashlib pytorch_msssim typing argparse numpy time abc shutil math setuptools random compressai subprocess pybind11 io,cs.CV cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/InterDigitalInc_CompressAI.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\InterDigitalInc_CompressAI.pdf,,,no,,,,no,,,,
1050,https://github.com/kooBH/VFWS,kooBH_VFWS.tar.gz,VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking,,"utils wave yaml multiprocessing glob tqdm soundfile model torch os mir_eval matplotlib array argparse os,glob tensorboardX numpy time math datasets librosa traceback random logging",eess.AS cs.LG eess.SP stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kooBH_VFWS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kooBH_VFWS.pdf,,"IntroductionRecent advances in speech recognition have led to performance improvement in challenging scenarios such as noisy and farfield conditions. However, speech recognition systems still perform poorly when the speaker of interest is recorded in crowded environments, i.e., with interfering speakers in the foreground or background.One way to deal with this issue is to first apply a speech separation system on the noisy audio in order to separate the voices from different speakers. Therefore, if the noisy signal contains N speakers, this approach would yield N outputs with a potential additional output for the ambient noise. A classical speech separation task like this needs to cope with two main challenges. First, identifying the number of speakers N in the recording, which in realistic scenarios is unknown. Secondly, the optimization of a speech separation system may be required to be invariant to the permutation of speaker labels, as the order of the speakers should not have an impact during training [1]. Leveraging the advances in deep neural networks, several successful works have been introduced to address these problems, such as deep clustering [1], deep attractor network [2], and permutation invariant training [3].This work addresses the task of isolating the voices of a subset of speakers of interest from the commonality of all the other speakers and noises. For example, such subset can be formed by a single target speaker issuing a spoken query to a personal mobile device, or the members of a house talking to a shared home device. We will also assume that the speaker(s) of interest can be individually characterized by previous reference recordings, e.g. through an enrollment stage. This task is closely related to classical speech separation, but in a way that it is speaker-dependent. In this paper, we will refer to the task of speaker-dependent speech separation as voice filtering (some literature call it speaker extraction). We argue that for voice filtering, speaker-independent techniques such as those presented in [1,2,3] may not be a good fit. In addition to the challenges described previously, these techniques require an extra step to determine which output -out of the N possible outputs of the speech separation system -corresponds to the target speaker(s), by e.g. choosing the loudest speaker, running a speaker verification system on the outputs, or matching a specific keyword.A more end-to-end approach for the voice filtering task is to treat the problem as a binary classification problem, where the positive class is the speech of the speaker of interest, and the negative class is formed by the combination of all foreground and background interfering speakers and noises. By speaker-conditioning the system, this approach suppresses the three aforementioned challenges: unknown number of speakers, permutation problem, and selection from multiple outputs. In this work, we aim to condition the system on the speaker embedding vector of a reference recording. The proposed approach is the following. We first train a LSTM-based speaker encoder to compute robust speaker embedding vectors. We then train separately a time-frequency mask-based system that takes two inputs: (1) the embedding vector of the target speaker, previously computed with the speaker encoder; and (2) the noisy multi-speaker audio. This system is trained to remove the interfering speakers and output only the voice of the target speaker. 1 This approach can be easily extended to more than one speaker of interest by repeating the process in turns, for the reference recording of each target speaker.Similar related literature exists for the task of voice filtering. For example, in [4,5], the authors achieved impressive results by doing an indirect speaker conditioning of the system on the visual information (lips movement). However, a solution like that would require simultaneously using speech and visual information, which may not be available in certain type of applications, where a reference speech signal may be more practical. The systems proposed in [6,7,8,9] are also very similar to ours, with a few major differences: (1) Unlike using one-hot vectors, i-vectors or speaker posteriors derived from a crossentropy classification network, our speaker encoder network is specifically designed for large-scale end-to-end speaker recognition [10], which proves to perform much better in speaker recognition tasks, especially for unseen speakers. (2) Instead of using a GEV beamformer [6,8], our system directly optimizes the power-law compressed reconstruction error between the clean and enhanced signals [11]. (3) In addition to sourceto-distortion ratio [6,7], we focus on Word Error Rate improvements for ASR systems. (4) We use dilated convolutional layers to capture low-level acoustic features more effectively. (5) We prefer separately trained speaker encoder network over joint training like [8,9], due to the very different requirements for data in speaker recognition and source separation tasks.The rest of this paper is organized as follows. In Section 2, we describe our approach to the problem, and provide the details of how we train the neural networks. In Section 3, we describe our experimental setup, including the datasets we use and the evaluation metrics. The experimental results are presented in Section 4. We draw our conclusions in Section 5, with discussions on future work directions. Experimental setupIn this section, we describe our experimental setup: the data used to train the two components of the system separately, as well as the metrics to assess the systems. DatasetsSpeaker encoder: Although our speaker encoder network has exactly the same network topology as the text-independent model described in [10], we use more training data in this system. Our speaker encoder is trained with two datasets combined by the MultiReader technique introduced in [10]. The first dataset consists of anonymized voice query logs in English from mobile and farfield devices. It has about 34 million utterances from about 138 thousand speakers. The second dataset consists of LibriSpeech [16], VoxCeleb [17], and VoxCeleb2 [18]. This model (referred to as ""d-vector V2"" in [13]) has a 3.06% equal error rate (EER) on our internal en-US phone audio test dataset, compared to the 3.55% EER of the one reported in [10]. VoiceFilter: We cannot use a ""standard"" benchmark corpus for speech separation, such as one of the CHiME challenges [19], because we need a clean reference utterance of each target speaker in order to compute speaker embeddings. Instead, we train and evaluate the VoiceFilter system on our own generated data, derived either from the VCTK dataset [20] or from the LibriSpeech dataset [16]. For VCTK, we randomly take 99 speakers for training, and 10 speakers for testing. For LibriSpeech, we used the training and development sets defined in the protocol of the dataset: the training set contains 2338 speakers, and the development set contains 73 speakers. These two datasets contain read speech, and each utterance contains the voice of one speaker. We explain in the next section how we generate the data used to train the VoiceFilter system.",no,,,,no,,,,
1053,https://github.com/yiyiliao/deep_marching_cubes,yiyiliao_deep_marching_cubes.tar.gz,Deep Marching Cubes: Learning Explicit Surface Representations,,utils functions loss _ext sys skimage parse_args __future__ val model shape json resource data torch os loss_autograd matplotlib scipy argparse pointTriangleDistance numpy time ConfigParser modules cube table mpl_toolkits ellipsoid mcubes,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yiyiliao_deep_marching_cubes.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yiyiliao_deep_marching_cubes.pdf,,,no,,,supplementary material not available,no,,,,
1055,https://github.com/ShengkaiWu/IoU-aware-single-stage-object-detector,ShengkaiWu_IoU-aware-single-stage-object-detector.tar.gz,IoU-aware Single-stage Object Detector for Accurate Localization,@DETECTORS.register_module @abstractmethod @BACKBONES.register_module @ROI_EXTRACTORS.register_module @LOSSES.register_module @NECKS.register_module @SHARED_HEADS.register_module @property @staticmethod @once_differentiable @HEADS.register_module,copy warnings sys roi_align __future__ mmcv re Cython collections json resource tempfile torch os matplotlib six functools xml seaborn terminaltables argparse mmdet roi_pool numpy time abc shutil math pycocotools setuptools logging subprocess,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ShengkaiWu_IoU-aware-single-stage-object-detector.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ShengkaiWu_IoU-aware-single-stage-object-detector.pdf,,,no,,,,no,,,,
1057,https://github.com/hadrien-pouget/Ranking-Policy-Decisions,hadrien-pouget_Ranking-Policy-Decisions.tar.gz,Ranking Policy Decisions,@abstractmethod,utils requests imageio elements csv counting environments tqdm datetime json torch os interpolating matplotlib scoring pickle argparse visualisation numpy time shutil abc polrank random,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hadrien-pouget_Ranking-Policy-Decisions.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hadrien-pouget_Ranking-Policy-Decisions.pdf,,,no,,yes,,no,,,,
1058,https://github.com/JdeRobot/NeuralBehaviors,JdeRobot_NeuralBehaviors.tar.gz,An LSTM-Based Autonomous Driving Model Using Waymo Open Dataset,@abstractmethod @contextlib.contextmanager @property @staticmethod @type.setter,"memory rospkg torchvision curses threading keras tensorflow torch liveplot importlib pilot six settings pickle environs gym_gazebo numpy rosgraph_msgs random inspect geometry_msgs utils yaml bagpy sensor_msgs rospy __future__ gazebo_msgs dqn distutils gym std_msgs pathlib robot sys, os os, json, numpy os cv_bridge xml rosbag time abc shutil nav_msgs drone_wrapper subprocess pandas sys brains webbrowser sensors datetime tempfile roslib matplotlib qlearn scipy itertools shlex setuptools albumentations std_srvs PIL ui contextlib skimage npyscreen jderobotTypes json collections agents argparse pynput math PyQt5 cv2 logging",cs.CV cs.LG cs.RO cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/JdeRobot_NeuralBehaviors.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\JdeRobot_NeuralBehaviors.pdf,,,no,,yes,"length of previous frames, length of future frames, epochs",no,,,,
1060,https://github.com/bbc/ColorGAN,bbc_ColorGAN.tar.gz,End-to-End Conditional GAN-based Architectures for Image Colourisation,@staticmethod,importlib threading keras tensorflow csv cv2 skimage data argparse os numpy models,eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bbc_ColorGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bbc_ColorGAN.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,no,,,,no,,,,
1061,https://github.com/longnguyen2/blazeface,longnguyen2_blazeface.tar.gz,BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,,tensorflow blazeface sys cv2 torch numpy,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/longnguyen2_blazeface.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\longnguyen2_blazeface.pdf,,,no,,,,no,,,,
1064,https://github.com/AndriyMulyar/sklearn-oblique-tree,AndriyMulyar_sklearn-oblique-tree.tar.gz,A System for Induction of Oblique Decision Trees,@classmethod,"sklearn_oblique_tree numpy,sys setuptools unittest shlex sklearn pytest",cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AndriyMulyar_sklearn-oblique-tree.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AndriyMulyar_sklearn-oblique-tree.pdf,,,no,,,keyword search not possible,no,,,,
1074,https://github.com/amiryanj/crowdGAN,amiryanj_crowdGAN.tar.gz,Data-Driven Crowd Simulation with Generative Adversarial Networks,,util math copy scipy pandas csv yaml datetime sys operator entrypointGAN predictorGAN torch os shapely numpy matplotlib time,cs.GR cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amiryanj_crowdGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amiryanj_crowdGAN.pdf,,,no,,,,no,,,,
1075,https://github.com/ColingPaper2018/DialogueAct-Tagger,ColingPaper2018_DialogueAct-Tagger.tar.gz,ISO-Standard Domain-Independent Dialogue Act Tagging for Conversational Agents,@classmethod @staticmethod @dataclass,utils pandas sys pathlib re trainers datetime collections json transformers enum dataclasses torch os torchtext spacy corpora typing pickle sklearn argparse numpy time config taggers random logging,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ColingPaper2018_DialogueAct-Tagger.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ColingPaper2018_DialogueAct-Tagger.pdf,,,no,,,,no,,,,
1076,https://github.com/microsoft/art,microsoft_art.tar.gz,MosAIc: Finding Artistic Connections across Culture with Conditional Image Retrieval,"@app.route('/', methods=['GET']) @app.route('/share', methods=['GET']) @app.route('/upload', methods=['POST'])",requests werkzeug PIL flask_cors pandas sys pickle tqdm subprocess os torch azure base64 numpy flask torchvision,cs.LG cs.CV cs.GR cs.IR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/microsoft_art.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\microsoft_art.pdf,,,no,,,,no,,,,
1083,https://github.com/jhwjhw0123/Imbalance-XGBoost,jhwjhw0123_Imbalance-XGBoost.tar.gz,Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost,,xgboost sys setuptools sklearn numpy imxgboost,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jhwjhw0123_Imbalance-XGBoost.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jhwjhw0123_Imbalance-XGBoost.pdf,,,no,,,,no,,,,
1087,https://github.com/Guepardow/Visual-features,Guepardow_Visual-features.tar.gz,An Empirical Analysis of Visual Features for Multiple Object Tracking in Urban Scenes,,pandas AIC2018_iamai sys tqdm affinity platform torchvision efficientnet_pytorch re json torch os matplotlib appearances scipy xml seaborn deep_person_reid dataset argparse numpy time cv2 inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Guepardow_Visual-features.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Guepardow_Visual-features.pdf,,"A. Visual featuresThere are many ways to obtain a description (a numerical vector) from an image of an object enclosed by a BB. We selected eight popular visual descriptors among four categories: color histograms-based, gradient-based, CNN-based and ReID-based models.a) Color and grayscale histograms: the RGB color histogram descriptor consists in counting the number of occurrences of each color inside a BB. We elected to use quantified histograms because they are more resilient to noise. For grayscale 1D-histogram, we used 32 bins. For color histogram, similarly, each channel of the image gives an histogram; after concatenation, we obtain a vector of size 96.b) Histograms of oriented gradients (HOG): contrarily to color-based histograms, gradient-based descriptors are more robust to illumination change. HOG [24] is one famous example. It captures texture information in the image as well as shape information about the object. To obtain an HOG descriptor, the image is first convolved with kernels to extract vertical and horizontal gradients. From these gradients, their angles and magnitudes can be obtained. The angles are typically quantified between 0 and 180°, as it was shown experimentally that ignoring the sign of the angle gives better results. Histograms are then constructed for several overlapping cells by counting the occurrence of quantified angles weighted by their gradient magnitudes. Since in our experimental setup, the datasets contain either vehicles or pedestrians, the HOG vectors are the same size for all candidate objects in each dataset : the BB is resized to 64 × 128 pixels for pedestrians and 64 × 64 for vehicles, with cells of size 8 × 8 and blocks of size 16 × 16. By quantifying angles into nine bins every 20°, this results in a vector of 1764 elements for vehicles and 3528 for pedestrians. c) CNN-based features: since their breakthrough in 2012 [25], CNNs are commonly used in computer vision for classification tasks. We used four different architectures which have great performance on ImageNet to extract visual features. For each network, we removed the last fully-connected layer to obtain a descriptor [26]. We evaluated VGG-19 [27], ResNet-18 [28], DenseNet-121 [29] and EfficientNet-B0 [30] architectures, respectively providing a description vector of size 4096, 512, 1024 and 1280, having respectively 140, 11, 7 and 4 millions parameters and resulting ImageNet top-1 error rates of respectively 27.6%, 30.2%, 25.4% and 23.4%.d) Re-identification network: in the case of pedestrian tracking, we evaluated a re-identification method named OSNet-AIN [22] containing 3 millions parameters giving ReID features of size 512. For vehicles, we extracted ReID features from the model of [31] containing 24 millions parameters providing vectors of size 2048.",no,,,,no,,,,
1088,https://github.com/pixelite1201/agora_evaluation,pixelite1201_agora_evaluation.tar.gz,AGORA: Avatars in Geography Optimized for Regression Analysis,,pandas sys glob tqdm pathlib re collections torch os psbody matplotlib itertools seaborn pickle argparse numpy math smplx setuptools cv2 logging agora_evaluation zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pixelite1201_agora_evaluation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pixelite1201_agora_evaluation.pdf,,"Method: Obtaining reference dataTo construct AGORA, we purchased high-quality textured 3D scans from 3DPeople [1], AXYZ [2], Human Alloy [4] and Renderpeople [8]. We selected 4240 scans for inclusion in the dataset spanning more than 350 unique subjects. A scan S comprises a set of 3D points S ⊂ R 3 and their connectivity F S , S = {S, F S }. To each scan S we fit a parametric SMPL-X body model M = {M, F M }, whose vertex locations M (θ, β, ψ) ⊂ R 3 are controlled by parameters for pose θ, shape β, and facial expression ψ [39]. θ consists of body pose θ b and hand pose θ h . Hand pose θ h is a function θ h (Z h ) of a PCA latent vector Z h ∈ R 6 .Fitting a SMPL-X mesh M to a scan S amounts to solving for the optimal parameters (θ, β, ψ) such that M resembles S. The fitting process takes into account that SMPL-X explains the body in minimal clothing while the scans are typically clothed. In this process we exploit the fact that a person may appear in multiple scans and their shape parameter, β, should be the same across scans.We first initialize the parameters by an approach that extends the single-view SMPLify-X fitting [39] to multi-view images rendered using C pre-defined virtual cameras. The initial mesh, M , obtained by multi-view SMPLify-X fitting is only approximately aligned with S. While sparse 2D landmarks constrain the 3D pose, they provide little information about body shape. To refine the shape and pose, we fit SMPL-X to the 3D scan surface. However, this is challenging because SMPL-X cannot model things like hair and clothing that are present in the scans. To address this, we use the idea of fitting body shape under clothing [11,58].Similar to [58], we define energy terms E skin and E cloth for skin and clothing, respectively. Both aim to bring the model surface close to the scan, whereas E cloth additionally penalizes body vertices being outside the clothing. In other words, our objective function tries to move the model as close as possible to the scan near the visible skin while discouraging the clothing vertices from penetrating the model. We label skin and cloth vertices on scan using Graphonomy [17]. We keep the 2D landmark data term E c J that penalizes differences between projected and observed keypoints from multi-view fitting, as they provide information complementary to E skin and E cloth . See Sup. Mat. for more details.We fit each model M i (β i , θ i , ψ i ) to the corresponding scan S i in parallel, for scans, i, of the same identity. We optimize jointly for θ i , ψ i and β i while minimizing the shape (inter-beta) distance E ib between scans of the same identity. The objective function is:E(β 1 , . . . , β N , θ 1 , . . . , θ N , ψ 1 , . . . , ψ N ) = N i=1 λ J C c=1 E c,i J + λ s E i skin + λ c E i cloth + E i reg + λ ib E ib , E ib = N i=1 N j=i+1 β i − β j 2 2 , E reg = λ θ b E θ b (θ b ) + λ θ h E θ h (θ h ) + λ β E β (β) + λ E E E (ψ),where E reg contains L 2 priors used to constrain the body shape, pose and expression, as defined in [39]. Different weights denoted by λ are used for each term. This approach exploits semantic information (2D landmarks, skin/clothing segmentation) as well as geometry (3D shapes) to obtain accurate fits as demonstrated in Sec. 4.1.",no,,,"same hyperparameters as in other paper, ",no,,,,
1093,https://github.com/vale-salvatelli/sdo-autocal_pub,vale-salvatelli_sdo-autocal_pub.tar.gz,Using U-Nets to Create High-Fidelity Virtual Observations of the Solar Corona,@abstractmethod,pandas yaml sys multiprocessing glob sdo pprint configargparse pathlib torch os pkg_resources pytest sphinx matplotlib scipy seaborn argparse numpy shutil abc math setuptools contexttimer random logging operator inspect,astro-ph.SR astro-ph.IM cs.LG physics.space-ph astro-ph.SR astro-ph.IM cs.LG physics.space-ph astro-ph.SR astro-ph.IM cs.LG physics.data-an physics.space-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vale-salvatelli_sdo-autocal_pub.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vale-salvatelli_sdo-autocal_pub.pdf,,,no,,,,no,,,,
1097,https://github.com/vered1986/NC_Embeddings,vered1986_NC_Embeddings.tar.gz,A Systematic Comparison of English Noun Compound Representations,"@DatasetReader.register(""nc_paraphrases_data_reader_single_words"") @Model.register(""paraphrase_composition_model"") @Model.register(""composition_model"") @SimilarityFunction.register(""add_composition"") @DatasetReader.register(""nc_data_reader_single_words"") @SimilarityFunction.register(""matrix_composition"") @DatasetReader.register(""nc_data_reader"") @DatasetReader.register(""nc_paraphrases_data_reader"") @DatasetReader.register(""nc_paraphrases_data_reader_for_words"") @SimilarityFunction.register(""full_add_composition"") @overrides @Model.register(""single_word_composition_model"")",source tqdm gensim re collections json torch os string gzip overrides itertools spacy codecs typing sklearn argparse numpy tarfile word_forms random logging subprocess allennlp,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vered1986_NC_Embeddings.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vered1986_NC_Embeddings.pdf,,,no,,,"window size, embedding dimension",no,,,,
1098,https://github.com/proy3/NLAP-net_VAD,proy3_NLAP-net_VAD.tar.gz,Predicting Next Local Appearance for Video Anomaly Detection,,copy pandas warnings sys skimage tqdm keras tensorflow datetime json src os matplotlib scipy keras_contrib detectors sklearn argparse natsort opts numpy time shutil cv2 subprocess inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/proy3_NLAP-net_VAD.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\proy3_NLAP-net_VAD.pdf,,,no,,,,no,,,,
1099,https://github.com/DeepLearnXMU/CG-RL,DeepLearnXMU_CG-RL.tar.gz,Exploring Dynamic Selection of Branch Expansion Orders for Code Generation,@Registrable.register('ifttt') @Registrable.register('word_cnt') @Registrable.register('default_evaluator') @Registrable.register('parser_rl') @Registrable.register('python2') @Registrable.register('sql') @Registrable.register('reconstructor') @cached_property @Registrable.register('is_2nd_hyp_and_paraphrase_score_margin_with_top_hyp') @Registrable.register('normalized_parser_score_by_action') @property @staticmethod @Registrable.register('cached_evaluator') @Registrable.register('reranker') @Registrable.register('wikisql_parser') @Registrable.register('prolog') @Registrable.register('parser_score') @Registrable.register('is_2nd_hyp_and_margin_with_top_hyp') @Registrable.register('default_parser') @classmethod @Registrable.register('normalized_parser_score') @Registrable.register('lambda_dcs') @Registrable.register('parser_pre') @Registrable.register('python3') @Registrable.register('paraphrase_identifier'),"copy astor sys multiprocessing pdb tqdm parser __future__ ast re sys, os cStringIO model collections asdl json queue tokenize torch os components six common itertools pickle argparse token numpy xgboost time math sys, traceback datasets traceback random evaluation cPickle io",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DeepLearnXMU_CG-RL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DeepLearnXMU_CG-RL.pdf,,"BaselinesTo facilitate the descriptions of experimental results, we refer to the enhanced TRANX model as TRANX-RL. In addition to TRANX, we compare our enhanced model with several competitive models:• TRANX (w/ pre-train). Table 2: The performance of our model in comparison with various baselines. We report the mean performance and standard deviation over five random runs. † indicates the scores are previously reported ones. Note that we only report the result of TREEGEN on ATIS, since it is the only dataset with released code for preprocessing.compare with it because our model involves a pre-training stage. • COARSE2FINE (Dong and Lapata, 2018).This model adopts a two-stage decoding strategy to produce the action sequence. It first generates a rough sketch of its meaning, and then fills in missing detail. • TREEGEN (Sun et al., 2020). It introduces the attention mechanism of Transformer (Vaswani et al., 2017), and a novel AST reader to incorporate grammar and AST structures into the network. • TRANX-R2L. It is a variant of the conventional TRANX model, which deals with multibranch AST nodes in a right-to-left manner. • TRANX-RAND. It is also a variant of the conventional TRANX model dealing with multi-branch AST nodes in a random order. • TRANX-RL (w/o pre-train). In this variant of TRANX-RL, we train our model from scratch. By doing so, we can discuss the effect of pre-training on our model training.To ensure fair comparisons, we use the same experimental setup as TRANX . Concretely, the sizes of action embedding, field embedding and hidden states are set to 128, 128 and 256, respectively. For decoding, the beam sizes for GEO, ATIS, DJANGO and CONALA are 5, 5, 15 and 15, respectively. We pre-train models in 10 epochs for all datasets. we determine the λs as 1.0 according to the model performance on validation sets. As in previous studies (Alvarez-Melis and Jaakkola, 2017;Neubig, 2018, 2019), we use the exact matching accuracy (Acc) as the evaluation metric for all datasets. For CONALA, we use the corpus-level BLEU  as a complementary metric.",no,,,,no,,,,
1104,https://github.com/jshilong/FisherPruning,jshilong_FisherPruning.tar.gz,Group Fisher Pruning for Practical Network Compression,@HOOKS.register_module(),mmcv copy re warnings collections types mmdet fisher_pruning torch os argparse numpy time,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jshilong_FisherPruning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jshilong_FisherPruning.pdf,Pointwise Convolution  Depthwise Convolution  Residual Connection  Rectified Linear Units  Average Pooling  Grouped Convolution  Depthwise Separable Convolution  Kaiming Initialization  1x1 Convolution  Global Average Pooling  Inverted Residual Block  Residual Block  ResNeXt Block  Bottleneck Residual Block  MobileNetV2  Max Pooling  Residual Network  Convolution  Batch Normalization  ResNeXt,,no,,,,no,,,,
1110,https://github.com/jayjaynandy/RBF_CNN-v2,jayjaynandy_RBF_CNN-v2.tar.gz,Approximate Manifold Defense Against Multiple Adversarial Perturbations,@tf_export('keras.preprocessing.image.random_brightness') @tf_export('keras.preprocessing.image.random_channel_shift') @tf_export('keras.preprocessing.image.ImageDataGenerator') @tf_export('keras.preprocessing.image.NumpyArrayIterator') @tf_export('keras.preprocessing.image.apply_transform') @tf_export('keras.preprocessing.image.load_img') @tf_export('keras.preprocessing.image.array_to_img') @tf_export('keras.preprocessing.image.random_rotation') @tf_export('keras.preprocessing.image.img_to_array') @tf_export('keras.preprocessing.image.DirectoryIterator') @staticmethod @tf_export('keras.preprocessing.image.random_shear') @tf_export('keras.preprocessing.image.random_zoom') @tf_export('keras.preprocessing.image.Iterator') @tf_export('keras.preprocessing.image.flip_axis') @tf_export('keras.preprocessing.image.random_shift'),threading PIL scipy keras tensorflow functools re tf_agumentation multiprocessing setup_data os __future__ numpy matplotlib,cs.CR cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jayjaynandy_RBF_CNN-v2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jayjaynandy_RBF_CNN-v2.pdf,,,no,,,,no,,,,
1111,https://github.com/choicelab/grasping-invisible,choicelab_grasping-invisible.tar.gz,A Deep Learning Approach to Grasping the Invisible,,"utils PIL warnings sys glob skimage __future__ ctypes platform distutils torchvision util os, sys threading re robot Cython collections json datetime lwrf_infer torch os trainer matplotlib six struct scipy itertools miou_utils networks argparse simulation logger numpy time config math datasets light_weight_refinenet colorsys random cv2 logging policies models",cs.RO cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/choicelab_grasping-invisible.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\choicelab_grasping-invisible.pdf,,"G. Additional TrainingTo further investigate the importance of the color channel in our approach, we train a depth+mask variant of our approach. We make minimal modifications on the motion critic to accept only the depth and the target mask while keeping all other parts unchanged. We evaluate Depth+Mask and report the results in Table IV. Our approach outperforms Depth+Mask. We conjecture that the color input helps the networks to distinguish the target from surrounding objects, especially in generalization. As shown in Fig. 12, when we test Depth+Mask on the challenging arrangements never seen during training, the Q maps shift from the target compared to the Q maps of our approach. This observation is consistent with Appendix IX-F.Fig. 1 :1Fig. 1: Example configuration of the ""grasping the invisible"" problem. The target object is the green cylinder and initially invisible to the robot. We propose to solve the problem with an exploration-singulation-grasping procedure.",no,,,,no,,,,
1115,https://github.com/501Good/lexicon-enhanced-lemmatization,501Good_lexicon-enhanced-lemmatization.tar.gz,Enhancing Sequence-to-Sequence Neural Lemmatization with External Resources,@sentences.setter @text.setter @xpos.setter @lemma.setter @conll_file.setter @parent_token.setter @dependency_relation.setter @upos.setter @property @tokens.setter @pos.setter @feats.setter @staticmethod @governor.setter @dependencies.setter @classmethod @index.setter @words.setter,copy sys apertium __future__ pathlib re datetime collections json unittest torch os importlib estnltk pickle typing argparse numpy time shutil pymorphy2 lexenlem math unicodedata random logging io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/501Good_lexicon-enhanced-lemmatization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\501Good_lexicon-enhanced-lemmatization.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Sequence to Sequence,,no,,,epochs,no,,,,
1119,https://github.com/brightyoun/Video-Style-Transfer,brightyoun_Video-Style-Transfer.tar.gz,Instance Normalization: The Missing Ingredient for Fast Stylization,,utils PIL sys onnx_caffe2 onnx pprint torchvision re datetime json collections torch os pickle argparse numpy time onvif cv2 vgg transformer_net,cs.CV cs.LG cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/brightyoun_Video-Style-Transfer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\brightyoun_Video-Style-Transfer.pdf,Instance Normalization  Batch Normalization,,no,,,,no,,,,
1127,https://github.com/jordipons/musicnn,jordipons_musicnn.tar.gz,musicnn: Pre-trained convolutional neural networks for music audio tagging,,tensorflow librosa setuptools musicnn argparse os numpy,cs.SD cs.CL eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jordipons_musicnn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jordipons_musicnn.pdf,,"These models are trained with two different datasets: the MagnaTagATune dataset (the MTT of 19k training songs [3]) 1 and the Million Song Dataset (the MSD of 200k training songs [1]) 2 .Which pre-trained models are available? Although the main focus of the library is to release pre-trained musically motivated convolutional neural networks, we also provide several vgg-like models 3 (as baselines for comparison). A high-level depiction of the musicnn architecture 4 is depicted in the following figure: What's the musicnn library for? Out-of-the-box music audio tagging. From within python, one can estimate the top-10 tags by simply running: from musicnn.tagger import top_tags top_tags('music_file.mp3', model='MTT_musicnn', topN=10)From the command-line, one can also print the top-N tags on the screen (top) or save them to a file (bottom): python -m musicnn.tagger music.au --model 'MTT_musicnn' --topN 10 --print python -m musicnn.tagger audio.wav -m 'MTT_vgg' --topN 5 --save out.tags What's the musicnn library for? Music feature extraction. Out of the extractor, see the example below, one gets the output of the model (the taggram and its associated tags) and all the intermediate representations of it (we refer to those as features). The features are packed in a dictionary and, for the musicnn models, you can extract timbral, temporal, cnn1, cnn2, cnn3, mean_pool, max_pool, and penultimate features 4 . For the vgg models, you can extract pool1, pool2, pool3, pool4, and pool5 features 3 . from musicnn.extractor import extractor output = extractor(file, model='MTT_musicnn', extract_features=True) taggram, tags, features = output What's the musicnn library for? Transfer learning. Our pre-trained deep learning models can be finetuned, together with an output neural-network that acts as a classifier, to perform any other music task. To assess the utility of our embeddings, we build SVM classifiers on top of several pre-trained models that act as music feature extractors. The tools used to run this simple transfer learning experiment are accessible online: https://github.com/jordipons/sklearn-audio-transfer-learningWe report accuracy results on the test set of the GTZAN (fault-filtered) dataset, and our processing pipeline consists of ""feature extraction"" + 128 PCA + SVM. The feature extraction can be based on VGGish audioset features (77.58% accuracy), OpenL3 audioset features (74.65% accuracy), MTT_musicnn features (71.37% accuracy), MTT_vgg features (72.75% accuracy), or MSD_musicnn features (77.24% accuracy). Note that our MSD pre-trained models outperform the MTT ones. Besides, the MSD_musicnn achieves similar results than the VGGish audioset features (that are trained with a much larger dataset: 2M audios).How did you train musicnn models? The code we employed to train the models above is also accessible: But the musicnn-training framework also allows to implement other models. For example, a similar architecture than musicnn but with an attention-based output layer (instead of the temporal pooling layer) can achieve 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset -and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset. You can find further details about this new architecture online.Figure 1 :1Figure 1: The musicnn architecture: a musically motivated convolutional neural network [4, 5].",no,,,,no,,,,
1128,https://github.com/UVa-NLP/HEDGE,UVa-NLP_HEDGE.tar.gz,Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection,@classmethod,copy csv sys utils_glue glob tqdm __future__ gensim re hedge_bert load_data torch os matplotlib scipy itertools torchtext pytorch_transformers sklearn argparse numpy eval_utils time hedge random logging io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UVa-NLP_HEDGE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UVa-NLP_HEDGE.pdf,,,no,,,,no,,,,
1129,https://github.com/closest-git/QuantumForest,closest-git_QuantumForest.tar.gz,Deep differentiable forest with sparse attention for the tabular data,,"gc pandas glob catboost qhoptim lightgbm os, sys main_tabular_data torch quantum_forest matplotlib GBDT_test IPython torch, torch litemort pickle sklearn argparse xgboost numpy time math io",cs.LG stat.ML cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/closest-git_QuantumForest.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\closest-git_QuantumForest.pdf,,"Table 22listed the accuracy of all libraries. All libraries use default parameters. For each dataset, QuantumForest uses 2048 trees, and the batch size is 512. It's clear that mGBDT needs much more memory than other libs. mGBDT always failed because out or memory for most large datasets. Both NODE and QuantumForest have higher accuracy than CatBoost and XGBoost. It is a clear sign that differentiable forest model has more potential than classical GBDT models.",no,,,,no,,,,
1131,https://github.com/yu20103983/FOTS,yu20103983_FOTS.tar.gz,FOTS: Fast Oriented Text Spotting with a Unified Network,@slim.add_arg_scope,"csv multiprocessing sys glob recognizer shapely threading tensorflow collections queue FOTS os icdar matplotlib nets RoiRotate scipy xml numpy Queue time sharedConv traceback data_util os, sys, csv cv2 random locality_aware_nms detector",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yu20103983_FOTS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yu20103983_FOTS.pdf,Convolution,,no,,,,no,,,,
1133,https://github.com/pangjian123/ISM-ReID,pangjian123_ISM-ReID.tar.gz,HAZY RE-ID: AN INTERFERENCE SUPPRESSION MODEL FOR DOMAIN ADAPTATION PERSON RE-IDENTIFICATION UNDER INCLEMENT WEATHER CONDITION,@torch.no_grad() @DATASET_REGISTRY.register() @functools.lru_cache(maxsize=None) @contextmanager @functools.lru_cache()  # so that calling setup_logger multiple times won't add many handlers @REID_HEADS_REGISTRY.register() @META_ARCH_REGISTRY.register() @property @staticmethod @functools.lru_cache() @BACKBONE_REGISTRY.register() @classmethod,PIL copy imageio tabulate fvcore yaml warnings sys contextlib weakref glob engine tqdm termcolor onnx __future__ fastreid pprint torchvision distutils atexit re tensorflow datetime collections json Cython h5py unittest modeling tempfile data torch os caffe socket google pytorch_to_caffe matplotlib importlib scipy functools ipdb itertools a bisect lmdb onnx_tf typing pickle sklearn argparse Caffe numpy time shutil config timeit solver math rfconv traceback predictor errno cv2 logging random fastai subprocess detectron2 gdown yacs io,cs.CV cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pangjian123_ISM-ReID.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pangjian123_ISM-ReID.pdf,,,no,,,"epochs, learning rate",no,,,,
1134,https://github.com/mahsa91/py_mgcn,mahsa91_py_mgcn.tar.gz,MGCN: Semi-supervised Classification in Multi-layer Graphs with Graph Convolutional Networks,,utils tensorboardX math scipy random layers sklearn torch argparse os __future__ numpy models time,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mahsa91_py_mgcn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mahsa91_py_mgcn.pdf,Graph Convolutional Network,,no,,,,no,,,,
1135,https://github.com/Shikib/schema_attention_model,Shikib_schema_attention_model.tar.gz,Schema-Guided Paradigm for Zero-Shot Dialog,"@flask.route(""/<task>"", methods=[""POST""]) @flask.route(""/mock/<task>"", methods=[""POST""]) @property @staticmethod @flask.errorhandler(InvalidUsage) @flask.route(""/mock/clear"", methods=[""GET""])",copy data_readers tqdm tokenizers json collections transformers nltk torch os api itertools typing pickle sklearn argparse numpy flask random models,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Shikib_schema_attention_model.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Shikib_schema_attention_model.pdf,,,no,,,,no,,,,
1143,https://github.com/ruiliu-ai/DivCo,ruiliu-ai_DivCo.tar.gz,DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network,@abstractmethod @staticmethod,PIL saver sys __future__ torchvision util model collections data visdom torch os dominate importlib functools itertools dataset pickle networks argparse tensorboardX numpy time abc ntpath random subprocess options models,cs.LG cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ruiliu-ai_DivCo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ruiliu-ai_DivCo.pdf,Convolution  Leaky ReLU  Rectified Linear Units  Batch Normalization  Support Vector Machine  Weight Decay  Adam  Deep Convolutional GAN  Multi-source Sentiment Generative Adversarial Network,,no,,,,no,,,,
1153,https://github.com/mdmustafizurrahman/An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection,mdmustafizurrahman_An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection.tar.gz,An Information Retrieval Approach to Building Datasets for Hate Speech Detection,,copy pandas warnings multiprocessing glob tqdm both_dataset_plots jsonlines gensim re json collections queue nltk os systemReader matplotlib scipy functools itertools seaborn preprocessor simpledorff pickle sklearn argparse xgboost numpy Queue time math topic_description random operator global_definition,cs.CL cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mdmustafizurrahman_An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mdmustafizurrahman_An-Information-Retrieval-Approach-to-Building-Datasets-for-Hate-Speech-Detection.pdf,,,no,,,,no,,,,
1154,https://github.com/ShawnDing1994/ACNet,ShawnDing1994_ACNet.tar.gz,ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks,@property,utils sys glob tqdm torchvision ding_test re datetime collections json h5py custom_layers data torch model_map os ndp_test constants ding_train builder coloredlogs bisect dataset acnet nobn_builder pickle base_config typing argparse numpy base_model time math errno logging ndp_train,cs.CV cs.LG cs.NE cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ShawnDing1994_ACNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ShawnDing1994_ACNet.pdf,Convolution  Weight Decay,"ACB enhances the skeletons of square kernelsIntuitively, as adding the horizontal and vertical kernels onto the square kernel can be viewed as a means to explicitly enhance the skeleton part, we seek to explain the effec- tiveness of ACNet by investigating the difference between the skeleton and the weights at the corners.Inspired by the CNN pruning methods [9,11,12], we start from removing some weights at different spatial locations and observing the performance drop using ResNet-56 on CIFAR-10. Concretely, we randomly set some individual weights in the kernels to zero and test the model. As shown in Fig. 5a, for the curve labeled as corner, we randomly select the weights from the four corners of every 3 × 3 kernel and set them to zero in order to attain a given global sparsity ratio of every convolutional layer. Note that as 4/9 = 44.4%, a sparsity ratio of 44% means removing most of the weights at the four corners. For skeleton, we randomly select the weights only from the skeleton of every kernel. For global, every individual weight in the kernel has an equal chance to be chosen. The experiments are repeated five times with different random seeds, and the mean±std curves are depicted.As can be observed, all the curves show a tendency of decreasing as the sparsity ratio increases, but not monotonically, due to the random effects. It is obvious that removing the weights from the corners causes less damage to the model, but pruning the skeletons does more harm. This phenomenon suggests that the skeleton weights are more important to the model's representational capacity.We continue to verify if this observation holds for AC-Net. We convert the ACNet counterpart via BN and branch fusion, then conduct the same experiments on it. As shown in Fig. 5b, we observe an even more significant gap, e.g., pruning almost all the corner weights only degrades the model's accuracy to above 60%. On the other hand, pruning the skeletons causes more damage, as the model is destroyed when the global sparsity ratio attained by pruning the skeletons merely reaches 13%, i.e., 13%×9/5 = 23.4% weights of the skeletons are removed.Then we explore the cause of the above observations by investigating the numeric values of the kernels. We use the    magnitude (i.e., absolute value) as the metric for the importance of parameters, which is adopted by many prior CNN pruning works [7,9,12,21]. Specifically, we add up all the fused 2D kernels in a convolutional layer, perform a layerwise normalization by the max value, and finally obtain an average of the normalized kernels of all the layers. Formally, let F (i,j) be the 3D kernel of the j-th filter at the i-th  3 × 3 layer, L be the number of all such layers, max and abs be the max and element-wise absolute value, respectively, the average kernel magnitude matrix is computed asA = 1 L L i=1 S (i) max(S (i) ) ,(9)where the sum of absolute kernels of layer i isS (i) = Di j=1 Ci k=1 abs(F (i,j) :,:,k ) .(10)We present the A values of the normally trained ResNet-56 and the fused ACNet counterpart in Fig. 6a and Fig. 6b, where the numeric value and color at a certain grid indicate the average relative importance of the parameter on the corresponding position across all the 3 × 3 layers, i.e., a larger value and darker background color indicates a higher average importance of the parameter.As can be observed from Fig. 6a, the normally trained ResNet-56 distributes the magnitude of the parameters in an imbalance manner, i.e., the central point has the largest magnitude, and the points at the four corners have the smallest. Fig. 6b shows that ACNet aggravates such imbalance, as the A values of the four corners are decreased to below 0.400, and the skeleton points have the A values above 0.666. In particular, the central point has an A value of 1.000, which means that this location has a dominant importance consistently in every single 3 × 3 layer. It is noteworthy that the weights on the corresponding positions of the square, horizontal and vertical kernels have a possibility to grow opposite in sign, thus summing them up may result in a larger or smaller magnitude. But we have observed a consistent phenomenon that the model always learn to enhance the skeletons at every layer.We continue to study how the model will behave if we add the asymmetric kernels onto the other positions rather than the central skeletons. Specifically, we train an ACNet counterpart of ResNet-56 using the same training configurations as before, but shift the horizontal convolutions one pixel towards the bottom on the inputs and shift the vertical convolutions towards the right. Accordingly, during branch fusion, we add the BN-fused asymmetric kernels to the bottom-right borders of the square kernels (Fig. 6c) in order for an equivalent resulting network. It is observed that such ACBs can also enhance the borders, but not as intensively as the regular ACBs do to the skeletons. The model delivers an accuracy of 94.67%, which is 0.42% lower than the regular ACNet (Table. 1). Moreover, similar pruning experiments are conducted on the fused model (Fig. 5c). As observed, pruning the corners still delivers the best accuracy, and pruning the enhanced bottom-right borders gives no better results than the top-left 2 × 2 squares, i.e., though the magnitudes of the borders have increased, the other parts remain essential to the whole kernels.In summary: 1) the skeletons are inherently more important than the corners in standard square kernels; 2) ACB can significantly enhance the skeletons, resulting in improved performance; 3) adding the horizontal and vertical kernels to the borders degrades the model's performance compared to regular ACBs; 4) doing so can also increase the magnitude of the borders but cannot diminish the importance of the other parts. Therefore, we partly attribute the effectiveness of ACNet to its capability of further strengthening the skeletons. Intuitively, ACNet follows the nature of the square convolution kernels.",no,,,"learning rate, padding, random cropping, left-right flipping",no,,,,
1164,https://github.com/dakot/probal,dakot_probal.tar.gz,Toward Optimal Probabilistic Active Learning Using a Bayesian Approach,@property @abstractmethod,math copy scipy itertools pandas warnings sys src sklearn argparse os joblib numpy time abc,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dakot_probal.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dakot_probal.pdf,,"Experimental EvaluationTo evaluate the quantitative performance of xPAL, we conduct experiments on real-world datasets. 5 We provide information on the used datasets, algorithms, and the experimental setup. We compare xPAL to state-of-the-art methods and show how the prior parameter affects the results. C. More Experimental ResultsIn this section, we provide more plots from our experimental evaluation. Please refer to the original paper for the detailed explanation of the experimental setup and the discussion of the results.",no,,,,no,,,,
1165,https://github.com/sashank06/CCNLG-emotion,sashank06_CCNLG-emotion.tar.gz,Emotional Neural Language Generation Grounded in Situational Contexts,,utils math apex tarfile itertools pytorch_transformers ignite collections json pytorch_pretrained_bert logging tempfile argparse os torch pprint,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sashank06_CCNLG-emotion.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sashank06_CCNLG-emotion.pdf,,,no,,,,no,,,,
1172,https://github.com/aayushbansal/Recycle-GAN,aayushbansal_Recycle-GAN.tar.gz,Recycle-GAN: Unsupervised Video Retargeting,@staticmethod,"requests PIL warnings sys bs4 zlib __future__ torchvision util collections data visdom torch os inspect, re dominate struct functools itertools argparse numpy time tarfile ntpath random options models zipfile",cs.CV cs.GR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/aayushbansal_Recycle-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\aayushbansal_Recycle-GAN.pdf,,,no,,,,no,,,,
1178,https://github.com/savageyusuff/MobilePose-Pi,savageyusuff_MobilePose-Pi.tar.gz,ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design,@staticmethod,copy csv scripts warnings sys glob skimage tqdm __future__ distutils torchvision mobilenetv2 re datetime json collections Cython coco_utils imgaug estimator torch os matplotlib alog scipy functools itertools urllib networks pickle argparse numpy ShuffleNetV2 time math pycocotools dataset_factory cv2 random logging dataloader,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/savageyusuff_MobilePose-Pi.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\savageyusuff_MobilePose-Pi.pdf,Convolution  Average Pooling  Channel Shuffle  Global Average Pooling  Sigmoid Activation  1x1 Convolution  Batch Normalization  Rectified Linear Units  Depthwise Convolution  Dense Connections  Softmax  Step Decay  Weight Decay  ShuffleNet v2  Residual Connection  Squeeze-and-Excitation Block  ShuffleNet V2 Downsampling Block  ShuffleNet V2 Block,,no,,,some hyperparameter are same as in other study,no,,,,
1179,https://github.com/cvxgrp/cvxstrat,cvxgrp_cvxstrat.tar.gz,A Distributed Method for Fitting Laplacian Regularized Stratified Models,,utils scipy pandas strat_models warnings datetime sys setuptools networkx multiprocessing sklearn torch os cvxpy numpy matplotlib time,cs.LG math.OC stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cvxgrp_cvxstrat.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cvxgrp_cvxstrat.pdf,,"|Gaussian distribution.Here Y = R m , and we fit a density to the observed values of y, given z. For example θ can parametrize a Gaussian on R m , N (µ, Σ). The standard parametrization uses the parameters θ= (Σ −1 , Σ −1 µ), with Θ = S m ++ × R m (S n++ denotes the set of n × n positive definite matrices.) The probability density function of y isp(y || θ) = (2π) −m/2 det(Σ) −1/2 exp(− 1 2 (y − µ) T Σ −1 (y − µ))The loss function (in the standard parametrization) isl(θ, y) = − log det S + y T Sy − 2y T ν + ν T S −1 νwhere θ = (S, ν) = (Σ −1 , Σ −1 µ). This loss function is jointly convex in S and ν; the first three terms are evidently convex and the fourth is the matrix fractional function (see [10, p76]).Some common (convex) choices for the local regularization function include the trace of Σ −1 (encourages the harmonic mean of the eigenvalues of Σ, and hence volume, to be large), sum of squares of Σ −1 (shrinks the overall conditional dependence between the variables),1 -norm of Σ −1 (encourages conditional independence between the variables) [69], or a prior on Σ −1 or Σ −1 µ. When m = 1, this model corresponds to the standard normal distribution, with mean µ and variance σ 2 . A stratified Gaussian distribution model has a separate mean and covariance of y for each value of the stratification parameter z. This model can be validated by evaluating the average log density of the observed values y i , under the predicted distributions, over a test set of data. We fit a non-parametric discrete distribution to y, which has the form A common (convex) regularization function is the negative entropy, given by M i=1 θ i log(θ i ).p(y = k || θ) = θ k , k = 1, . . . , M, where θ ∈ Θ = {p ∈ R M || 1 T p = 1, p 0}, i.Exponential families. A probability distribution that can be expressed asp(y || θ) = e M (θ,y)where M : R n × Y → R is a concave function of θ, is an exponential family [70,71]. Some important special cases include the Bernoulli, multinomial, Gaussian, and Poisson distributions. A probability distribution that has this form naturally leads to the following (convex) loss function l(θ, y) = − log p(y || θ) = −M (θ, y). Star graph.A star graph has one vertex with edges to every other vertex. The vertex that is connected to every other vertex is sometimes called the internal vertex. In a stratified model with a star regularization graph, the parameters of all of the non-internal vertices are encouraged to be similar to the parameter of the internal vertex. We refer to the internal vertex as the common model. A common use of a star graph is when the stratification feature relates many vertices only to the internal vertex. It is possible to have no data associated with the common model or internal vertex. In this case the common model parameter is a weighted average of other model parameters; it serves to pull the other parameter values together.It is a simple exercise to show that a stratified model with the complete graph can also be found using a star graph, with a central or internal vertex that is connected to all others, but has no data. (That is, it corresponds to a new fictitious value of the stratification feature, that does not occur in the real data.) Path graph. A path graph, or linear/chain graph, is a graph whose vertices can be listed in order, with edges between adjacent vertices in that order. The first and last vertices only have one edge, whereas the other vertices have two edges. Path graphs are a natural choice for when the stratification feature corresponds to time, or location in one dimension. When the stratification feature corresponds to time, a stratified model with a path regularization graph correspond to a time-varying model, where the model varies smoothly with time.Cycle graph. A cycle graph or circular graph is a graph where the vertices are connected in a closed chain. Every vertex in a cycle graph has two edges. A common use of a cycle graph is when the stratification feature corresponds to a periodic variables, e.g., the day of the week; in this case, we fit a separate model for each day of the week, and the model parameter for Sunday is close to both the model parameter for Monday and the model parameter for Saturday. Other examples include days of the year, the season, and non-time variables such as angle.Tree graph. A tree graph is a graph where any two vertices are connected by exactly one path (which may consist of multiple edges). Tree graphs can be useful for stratification features that naturally have hierarchical structure, e.g., the location in a corporate hierarchy, or in finance, hierarchical classification of individual stocks into sectors, industries, and subindustries. As in the star graph, which is a special case of a tree graph, internal vertices need not have any data associated with them, i.e., the data is only at the leaves of the tree.Grid graph. A grid graph is a graph where the vertices correspond to points with integer coordinates, and two vertices are connected if their maximum distance in each coordinate is less than or equal to one. For example, a path graph is a one-dimensional grid graph.Grid graphs are useful for when the stratification feature corresponds to locations. We discretize the locations into bins, and connect adjacent bins in each dimension to create a grid graph. A space-stratified model with grid graph regularization is encouraged to have model parameters that vary smoothly through physical space.Entity graph. There can be a vertex for every value of some entity, e.g., a person. The entity graph has an edge between two entities if they are perceived to be similar. For example, two people could be connected in a (friendship) entity graph if they were friends on a social networking site. In this case, we would fit a separate model for each person, and encourage friends to have similar models. We can have multiple relations among the entities, each one associated with its own graph; these would typically be weighted by separate hyperparameters.|",no,,,,no,,,,
1180,https://github.com/xiaozai/det,xiaozai_det.tar.gz,DepthTrack: Unveiling the Power of RGBD Tracking,@model_constructor @wraps(f) @staticmethod @functools.wraps(op) @classmethod @tensor_operation,PIL copy pandas csv warnings sys multiprocessing glob ltr prroi_pool tqdm skimage __future__ torchvision pathlib re jactorch collections json unittest tempfile visdom torch os jpeg4py matplotlib importlib scipy functools itertools xml pickle argparse lvis trax tensorboardX numpy time shutil _collections math pytracking pycocotools tikzplotlib traceback cv2 random gdown inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xiaozai_det.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xiaozai_det.pdf,,,no,,,parameter setting from original authors that have tuned the parameters,no,,,,
1192,https://github.com/Zehui127/SQUAD_BERT,Zehui127_SQUAD_BERT.tar.gz,Semi-supervised Sequence Learning,@classmethod,copy csv __future__ tensorflow_hub tensorflow re modeling collections json tempfile os six codecs numpy tokenization math unicodedata run_classifier optimization random,cs.LG cs.CL cs.CL cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Zehui127_SQUAD_BERT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Zehui127_SQUAD_BERT.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer  Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,"Object classification experiments with CIFAR-10In these experiments, we attempt to see if our pre-training methods extend to non-textual data. To do this, we train a LSTM on the CIFAR-10 image dataset, consisting of 60,000 32x32 colour images divided into 10 classes. The input at each timestep of the LSTM is an entire row of pixels and we predict the class of the image after reading the final row. We use the same method as in [15] to perform data augmentation. We also trained a LSTM to do next row prediction given the current row (we denote this as LM-LSTM) and a LSTM to autoencode the image by rows (SA-LSTM). The loss function during unsupervised learning is the Euclidean L2 distance between the predicted and the target row. We then fine-tune these on the classification task and present the classification results in Table 8. While we do not achieve the results attained by state of the art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the baseline convolutional DBN model [14] despite not using any convolutions and outperforms the non pre-trained LSTM.",no,,,,no,,,,
1201,https://github.com/jayheo/IAL,jayheo_IAL.tar.gz,Cost-effective Interactive Attention Learning with Neural Attention Process,"@app.route('/patient_search', methods=['GET', 'POST']) @app.route('/save_result', methods=['GET', 'POST']) @app.route('/read_non_hide', methods=['GET', 'POST']) @app.route('/start_check', methods=['GET', 'POST']) @app.route('/disease_search', methods=['GET', 'POST']) @app.route('/') @app.route('/button_year', methods=['GET', 'POST']) @app.route('/cf_estimation', methods=['GET', 'POST']) @property","utils pandas sys multiprocessing pdb __future__ smtplib tensorflow re datetime collections json os socket matplotlib six scipy IPython seaborn email pickle sklearn numpy flask time abc math random os, re, sys, linecache models",cs.LG cs.HC stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jayheo_IAL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jayheo_IAL.pdf,,,no,,,,no,,,,
1203,https://github.com/dougsm/egad,dougsm_egad.tar.gz,EGAD! an Evolved Grasping Analysis Dataset for diversity and reproducibility in robotic manipulation,@classmethod @property @staticmethod @diversity.setter,"pytorch_neat copy imageio sys multiprocessing autolab_core pathlib collections json tempfile torch os pathlib2 matplotlib progressbar scipy java itertools dexnet pickle neat jpype argparse trimesh, trimesh numpy time shutil setuptools random trimesh subprocess egad graphviz",cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dougsm_egad.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dougsm_egad.pdf,,,no,,,,no,,,,
1204,https://github.com/cjsg/PopSkipJump,cjsg_PopSkipJump.tar.gz,PopSkipJump: Decision-Based Attack for Probabilistic Classifiers,,PIL warnings sys popskip infomax_gpu tqdm torchvision adversarial re cifar10_models datetime collections torch os img_utils matplotlib tracker scipy abstract_attack pytorchmodels hopskip argparse numpy time model_factory math random logging defaultparams model_interface,cs.LG cs.CR cs.CV math.OC stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cjsg_PopSkipJump.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cjsg_PopSkipJump.pdf,,,no,,,,no,,,,
1210,https://github.com/Smilels/TeachNet_Teleoperation,Smilels_TeachNet_Teleoperation.tar.gz,Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network,@mlab.animate(delay=200) @jit(nopython=True),utils PIL copy pandas csv sys multiprocessing glob sensor_msgs rospy __future__ torchvision std_msgs mayavi model moveit_commander os torch cv_bridge matplotlib pyquaternion IPython numba seaborn pickle argparse tensorboardX numpy time shutil math sr_robot_commander cv2 ros_numpy mpl_toolkits shadow_teleop,cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Smilels_TeachNet_Teleoperation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Smilels_TeachNet_Teleoperation.pdf,,,no,,,,no,,,,
1214,https://github.com/visym/keynet,visym_keynet.tar.gz,Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors,"@cuda.jit @numba.jit(nopython=True, parallel=False, nogil=True) @numba.jit(nopython=True) @staticmethod @numba.jit(nopython=True, parallel=True, fastmath=True) @numba.jit(nopython=True, parallel=False, nogil=False)",PIL copy warnings multiprocessing sys cupy xxhash tqdm torchvision uuid collections tempfile torch os cupyx scipy itertools numba keynet vipy sklearn numpy time math dask setuptools,cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/visym_keynet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\visym_keynet.pdf,,,no,,,,no,,,,
1216,https://github.com/ulissigroup/Enabling-Robust-Offline-Active-Learning-for-MLPs,ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.tar.gz,Enabling robust offline active learning for machine learning potentials using simple physics-based priors,,copy amptorch multiprocessing sys random torch os numpy ase,physics.comp-ph cond-mat.mtrl-sci,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.pdf,,,no,,,,no,,,,
1217,https://github.com/steven7woo/Accuracy-First-Differential-Privacy,steven7woo_Accuracy-First-Differential-Privacy.tar.gz,Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM,,"output_pert sys noise_reduc run_ridge os, sys ridge sparsevec sys, os naive_covar os covar os, sys, traceback matplotlib theory sklearn run_logist naive_outputpert logist numpy sgd_ridge project math traceback random",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/steven7woo_Accuracy-First-Differential-Privacy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\steven7woo_Accuracy-First-Differential-Privacy.pdf,,,no,,,"number of data points, regularization parameters, failure probability",no,,,,
1224,https://github.com/silvanmelchior/CBF-SSM,silvanmelchior_CBF-SSM.tar.gz,Structured Variational Inference in Partially Observable Unstable Gaussian Process State Space Models,@abc.abstractmethod @property @staticmethod @constrain.setter,warnings sys tqdm __future__ tensorflow cbfssm doctest os matplotlib importlib scipy functools sklearn numpy time shutil abc math setuptools random,cs.LG cs.SY eess.SY stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/silvanmelchior_CBF-SSM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\silvanmelchior_CBF-SSM.pdf,Gaussian Process,,no,,,,no,,,,
1226,https://github.com/UrbsLab/Pancreatic_Cancer_ML_Notebook_Analysis,UrbsLab_Pancreatic_Cancer_ML_Notebook_Analysis.tar.gz,A RIGOROUS MACHINE LEARNING ANALYSIS PIPELINE FOR BIOMEDICAL BINARY CLASSIFICATION: APPLICATION IN PANCREATIC CANCER NESTED CASE-CONTROL STUDIES WITH IMPLICATIONS FOR BIAS ASSESSMENTS,,copy pandas exstracs_timer sys optuna exstracs_offlineenv tqdm lightgbm ast RBA skrebate os plotly matplotlib scipy exstracs_output exstracs_classifierset exstracs_at pickle exstracs_data sklearn Online_Learning exstracs_prediction numpy xgboost time exstracs_classifier exstracs_constants math exstracs_ek exstracs_rc random exstracs_algorithm exstracs_configparser exstracs_onlineenv exstracs_classaccuracy,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UrbsLab_Pancreatic_Cancer_ML_Notebook_Analysis.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UrbsLab_Pancreatic_Cancer_ML_Notebook_Analysis.pdf,,"ML Pipeline OverviewOne major goal of this paper is to develop, apply, and disseminate an accessible but rigorous ML analysis pipeline focused on binary (e.g. case/control) biomedical classification problems in both simulated and real world data. Through this work, we seek to coalesce and explain essential ML best practices for avoiding pitfalls and offer a complete binary class ML pipeline with many convenient features that can be applied 'as is' or used as a starting point for further customization and improvement. At a high level this pipeline includes: a) exploratory analysis, b) data cleaning (e.g. missing data) and transformation, c) feature selection, d) application of 9 established ML modeling approaches, each with hyperparameter optimization, and e) evaluation, including appropriate metrics, statistical analyses, and novel visualizations. This pipeline is best suited for a) comparing the performance of different algorithms for ML modeling on a given dataset, b) evaluating the importance of potentially relevant features in the dataset; an essential part of model interpretation, c) training a predictive model for downstream application, and/or d) running the pipeline on different datasets to compare achievable performance between them. This pipeline is organized within a well documented Jupyter notebook and coded in Python using well known packages such as pandas, scikit-learn, and scipy.Each individual method utilized in this pipeline has been previously published, thus this work focuses on what 'methods to include', and 'how they should be ordered or utilized' as a proposed basic standard of biomedical ML analysis rigor. Novel focuses of this pipeline include: a) utilizing a sequence of methods that preserve the potential to detect and model complex associations, i.e. epistasis [35] and/or genetic heterogeneity [62], b) adopting collective feature selection [70], c) applying a broader selection (i.e. more than 1 to 3) of well-established and cutting-edge ML modeling algorithms, that ideally have complementary strengths and weaknesses providing multiple 'perspectives' on modeling associations, d) identifying important hyperparameters for optimizing each ML algorithm with suggested value options/ranges for conducting a comprehensive hyperparameter sweep e) performing the first comparison of our cutting edge, interpretable rule-based machine learning algorithm (i.e. ExSTraCS [68]) to other well known ML algorithms, f) automatically reporting a wide variety of appropriate evaluation metrics with statistical comparisons and relevant visualizations, g) proposing a novel composite bar plot to illustrate feature importance consensus across different ML algorithms, and h) incorporating a number of convenient pipeline features including; automated generation of figures, the use of 'Optuna' for automated Bayesian hyperparameter optimization [1], and the use of Python's 'pickle' module applied to save/store all ML models for downstream replication analysis or application to make predictions on subjects with an unknown outcome. We also offer suggestions on how this pipeline can be further improved or adapted to other target applications and data challenges. This work does not: a) provide a comprehensive comparison of all ML best practices, b) suggest this pipeline is the 'best' or 'only' valid way to assemble an ML pipeline, c) focus on largely dataset-specific elements of an ML pipeline such as feature engineering, algorithm-specific data transformations, or the handling of outliers, or d) replace the need for careful data collection and the design of a study. In fact, utilizing the standardized statistical assessments and data visualizations of this proposed pipeline, we are able to demonstrate how bias introduced by study design within a data source could impact both feature and ML selection, and subsequent study findings. ML ModelingThe heart of this pipeline is ML modeling itself. ML refers to a large family of algorithmic methodologies that differ with respect to knowledge representation and inductive learning approaches as well as to the types of data and associations they are most effectively applied to and how interpretable their respective models will be. Many biomedical studies presented as ML analyses only utilize 1-3 different algorithms, most commonly applying either logistic regression (LR) [49], decision trees (DT) [36], random forests (RF) [43], and/or support vector machines (SVM) [32]. Deep learning (DL) has become extremely popular in recent years, often mistaken by novices as being equivalent to ML itself. In summary, DL is simply an elaboration of an artificial neural network (ANN), that typically includes more than 4 or 5 'hidden-layers' within the model's architecture [31]. While DL methods have been extremely successful in certain ML applications such as image classification [45], they are notoriously computationally intensive, difficult to interpret, and demand a large sample size for training [37]. It is for these reasons, that while ANN is included in this pipeline, DL is not. In addition to LR, DT, RF, SVM, and ANN, this pipeline also utilizes Naive Bayes (NB) [56], XGBoost [8],LGBoost [19], and our own rule-based ML algorithm called ExSTraCS [68]. ExSTraCS is a Learning Classifier System (LCS) algorithm which combines evolutionary learning with a 'piece-wise' knowledge representation comprised of a set of IF:THEN rules [67,63]. Unlike most ML methods, LCS algorithms learn iteratively from instances in the training data and adapt a population of IF:THEN rules to cover unique parts of the problem space. Notably, DT, RF, XGBoost, and LGBoost are a set of increasingly sophisticated tree-based ML algorithms. XGBoost and LGBoost are currently two of the most popular and successful ML algorithms within the ML research community. We selected this assortment of 9 ML algorithms to strike a balance between variety and computational expense. We expect future work to expand this assortment and believe that ultimately there is merit in identifying a minimal subset of ML algorithms with complementary strengths and weaknesses to apply in future studies.ExSTraCS and other related rule-based ML algorithms are not well-known within the ML or biomedical research communities. However, ExSTraCS has been demonstrated to have the ability to interpretably detect, model, and characterize complex associations such as epistasis and genetic heterogeneity. Uniquely, ExSTraCS not only offers a strategy to model heterogeneous patterns of associations but to identify candidate instance subgroups defined by respective predictive features. To date, ExSTraCS has only been compared to other rule-based ML algorithms, thus it is implemented here so that other researchers may easily apply and compare it to other well known methodologies.Based on previous work, we expect ExSTraCS to outperform other algorithms when the data includes heterogeneous associations, and when model interpretability is paramount. Notably in this analysis we evaluate ExSTraCS performance before and after a rule-compaction procedure (i.e. QRF [55]) has been applied to trained models.According to current best practices, the first step in ML modeling is to conduct a hyperparameter optimization sweep, a.k.a. 'tuning' [48]. Hyperparameters refer to the run parameters of a given ML algorithm that controls its functioning. Too often, ML algorithms are applied using their 'default' hyperparameter settings. This can lead to unfair ML algorithm comparisons, and a missed opportunity to obtain the best performing model possible. The optimization effectively 'tries out' different hyperparameter settings on a subset of the training instances, ultimately selecting those yielding the best performance to train the final model. The first consideration, is what hyperparameters to explore for each algorithm, as well as the range or selection of hyperparameter values to consider for each. As there is no real consensus regarding this first consideration, we have surveyed a number of online sources, publications, and consulted colleagues in order to select the wide variety of hyperparameters and value ranges incorporated in this pipeline that are suited to binary classification, and detailed in the aforementioned Jupyter notebook. For example, this notebook considers 8 hyperparameters for the optimization of the RF algorithm including 'number of estimators', i.e. the number of trees in the 'forest', with potential values ranging from 10 to 1000. Notebook users can easily adjust the values considered in the hyperparameter optimization of each ML algorithm within the Notebook run parameters. The second consideration, is what approach to take in conducting the hyperparameter sweep. Commonly approaches include a 'grid search' or a 'random search' [61] which either exhaustively or randomly hyperparameter value combinations. This pipeline adopts a new 'automated' package called 'Optuna', which applies Bayesian optimization of the hyperparameter space given user defined computing constraints [1]. In this pipeline, Optuna is afforded 100 optimization attempts, applying a nested 3-fold CV procedure and balanced accuracy [68] as the evaluation metric. In other words during Optuna optimization the target training data is further partitioned into 3 sub-training sets paired with hold out validation sets. This way, hyperparameters are selected based on their ability to generalize to unseen data (i.e. their average balanced accuracy on the 3 validation sets). Note, that this optimization does not see that hold out testing data. With Optuna, this pipeline also includes visualizations of each hyperparameter sweep to illustrate how different settings combinations impact performance. Figure 3 illustrates one such visualization applying SVM to our simulated data example. Note how the sweep suggests that the 'kernel' hyperparameter performs poorly when set to 'linear', performs better when set to 'poly' and best when set to 'rbf', i.e. radial basis function when applied to this data.Optuna optimization is conducted on 7 of the algorithms in this pipeline. Naive Bayes does not have any hyperparameters to optimize, and ExSTraCS is a computationally expensive evolutionary algorithm, making extensive hyperparameter Once 'optimal' hyperarameters have been selected for each of the k training datasets and ML algorithms, a 'final' model is trained using the full training set for each algorithm. These models are 'pickled', as described in section 2.1.2 so that they can be easily utilized in the future.Next, this pipeline further examines feature importance, now from the perspective of each ML model. These estimate offer useful insights into the high level interpretation of models. Specifically, which features were most important for making accurate predictions. Some algorithm implementations, including DT, RF, and LCS have built in mechanisms to report feature importance estimates after model training, without the need for further computation. In order to obtain feature importance estimates for the other 6 algorithms this pipeline implements a 'leave-one-out' approach were the target algorithm is trained again with the same 'optimal' hyperparameter settings an additional n times, on the training data with one of each of the n features left out. A relative feature importance estimate is calculated based on how leaving out each feature impacted the model's balanced accuracy.The last step in ML modeling is to evaluate model performance. It is essential to select appropriate evaluation metrics to fit the characteristics and goals of the given analysis. This pipeline ensures that a comprehensive selection of binary classification metrics and visualizations are employed that offer a holistic perspective of model performance. Earlier aspects of this pipeline demanded that a single evaluation metric be selected. We selected balanced accuracy as the primary evaluation metric of this notebook, as it equally emphasizes accurate predictions within either class. Balanced accuracy should replace traditional accuracy in the presence of imbalanced data. In addition to accuracy, and balanced accuracy this notebook calculates and reports the following evaluation metrics: a) F1-Score, b) recall, c) specificity, d) precision, e) receiver operating characteristic (ROC) area under the curve (AUC), f) precision-recall curve (PRC) AUC, g) PRC average precision score (APS), h) true positive count, i) true negative count, j) false positive count, and k) false negative count. Additionally for each algorithm, this pipeline outputs an ROC and PRC plot summarizing performance of each of the k trained models along side the mean ROC or PRC, respectively. PRC plots are preferable to ROC plots as class imbalance becomes more extreme. This pipeline automatically sets the 'no-skill' lines of PRC plots based on the class ratios in the dataset. Figure 4 gives the ROC (left) and PRC (right) plots generated by this pipeline for the LGBoost algorithm.  Predictive Performance of ML ModelsTables 2 and 3 summarize average balanced accuracy and F1 Score, respectively, for each ML algorithm and pancreatic cancer dataset. Both metrics are suited to evaluate classification performance within imbalanced data, however balanced accuracy equally weights the importance of both classes, while F1 Score focuses on the prediction of the 'case' minority class. Top performing algorithms for each dataset are highlighted in blue, while those whose performance was not significantly worse than the best (p < 0.05 with pairwise Mann-Whitney U-test) are highlighted in yellow. Thus, all highlighted cells are considered to have yielded 'similar' performance for the given dataset. Some high level observations from these tables include: (1) the 'best' performing ML algorithm was different for each dataset, and differed based on the evaluation metric examined, (2) multiple ML algorithms performed similarly well for each dataset, (3) ANN and LCS (i.e. ExSTraCS) with QRF both consistently under-performed other algorithms in this study, (4) LR and RF were the only algorithms to perform 'significantly well' in each dataset of this study; it's possible LR performed so well because the majority of established risk factors for pancreatic cancer were identified in previous studies using logistic regression approaches, and (5) despite LCS never performing 'best', and only sometimes performing 'significantly well', it's performance was never far behind other ML algorithms performing 'significantly well' (in contrast to XGBoost and LGBoost performing best in datsets 1 and 2, but performing poorly in dataset 3).Regarding the third observation, ANN underperformed despite a fairly extensive hyperparameter sweep that included variations in the depth and width of hidden layers. The application of QRF rule compaction to a trained LCS model [55] was designed to improve model interpretability, which clearly had a negative impact on model performance in this context. Also keep in mind that LCS, with and without QRF was the only algorithm in this study to use default run parameters thus not benefiting from the optimization of a hyperparameter sweep. Anecdotally, an earlier version of this pipeline was implemented and applied to these same datasets, adopting a much simpler 'grid-search-based' hyperparameter sweep. This preliminary analysis yielded significantly worse performance for all other ML algorithms, and suggested LCS performed best in both Datasets 1 and 2. Given that our LCS algorithm, (ExSTraCS) uniquely offers the benefits of model interpretability and the ability to characterize heterogeneous associations, it seems well worth inclusion in other ML analysis pipelines and in our future work that will add genetic variables into this analysis. Despite the computational expense, LCS might strongly benefit from hyperparameter optimization in the future.Importantly, the fourth observation above should not imply the general superiority of those methods more broadly, as can be clearly seen in contrast with the results on our example SNP dataset in Figure 5 where LR performs very badly.  3: F1 Score (with standard deviation) averaged over 10-fold CV for each dataset and ML algorithm. Best performing algorithms are highlighted in blue, while those that did not perform significantly worse (p < 0.05) are highlighted in yellow. Conclusions and Ongoing WorkIn this study we designed, implemented, and applied a rigorous ML analysis pipeline for binary classification that emphasizes ML best practices, transparency, reproducibility, automation, justification, and the ability to detect complex associations. This pipeline may be applied as-is or expanded upon for future applications. This pipeline was organized as 4 basic stages including: 1) data preprocessing and feature transformation, 2) feature importance and feature selection with collective FS, 3) ML modeling with 9 established algorithms, and 4) post-analysis including compound feature importance bar plots (CFIBPs). Through a simulated genetic example with complex associations and followup application to 3 different case-control pancreatic cancer datasets we demonstrate the efficacy and utility of this pipeline as a proposed rigorous standard for biomedical ML. We expect this automated pipeline to offer a more transparent and customizable alternative to AutoML, that will serve to educate and encourage less biased and more rigorous applications of ML to biomedical analyses in the future.Included in this pipeline is our own ExSTraCS algorithm, rule-based 'LCS' machine learner that had previously been demonstrated to proficiently detect and characterize epistatic and heterogeneous patterns of association. We utilize this pipeline to compare the performance of ExSTraCS to other well-known ML algorithms, despite being at the disadvantage of using default run parameters rather than optimizing via a hyperparameter sweep. While performing best on the simulated genetic example, in this particular study of pancreatic cancer did not always achieve the best predictive performance, but still performed well in the greatest variety of situations (i.e. dataset and evaluation metric). Given LCS's interpretability advantages and ability to detect complex associations, including genetic heterogeneity, we submit that ExSTraCS and LCS algorithms are valuable to include for comparison within future ML analysis pipelines. While most methods in this pipeline are scikit-learn compatible, our ExSTraCS algorithm is a stand-alone Python implementation. This illustrates how a user might adapt this pipeline to compare their own novel ML algorithms that may or may not be scikit-learn compatible.Analyses on three target pancreatic cancer datasets a) demonstrated the impact that bias may have on ML and feature selection, b) confirmed known risk factors, c) confirmed the efficacy of case/control matching of confounding variables to eliminate their effect, d) suggested that dietary variables may contribute a small predictive advantage when included Ongoing work will seek to expand these efforts in a number of ways: a) add other appropriate ML algorithms for comparison, e.g. nearest neighbors classifier [17] and other scikit-learn compatible LCS algorithms being developed, b) add other appropriate FS algorithms to expand the capabilities of collective feature selection, c) expand FS with appropriate wrapper algorithms to improve scalability of this pipeline to datasets with > 10,000 features, e) utilize this pipeline as a blueprint to construct similar pipelines for multi-class and quantitative (i.e. regression) outcomes, f) compartmentalize and set up parallelization of this pipeline outside of a Jupyter notebook so that it can be more efficiently run on larger-scale datasets, g) expand these pancreatic cancer analysis to include genetic variables, h) utilize the unique capabilities of ExStraCS to characterize potential heterogeneous associations, and i) expand the model interpretation capabilities of this pipeline beyond feature importance assessment wherever possible.Figure 1 :1Figure 1: Schematic overview of our proposed ML analysis pipeline. Starting from a single 'raw' dataset the pipeline proceeds through 4 main stages towards the output of 'knowledge'. Arrows indicate pipeline flow, i.e. the specific sequence of steps. The raw dataset and subsequent training and testing datasets are presented as yellow boxes. More specific pipeline elements are highlighted in light pink boxes. Specific algorithms are highlighted in white boxes. The arrows leaving box 1 correspond to the arrows entering box 2 below it. Figure 3 :3Figure 3: A hyperparameter sweep visualization output by Optuna for the SVM algorithm on a randomly chosen training partition of the simulated SNP dataset. The objective value in this figure has been set to the evaluation metric, 'balanced accuracy'.",no,,,,no,,,,
1228,https://github.com/sandeepsoni/semantic-progressiveness,sandeepsoni_semantic-progressiveness.tar.gz,FOLLOW THE LEADER: DOCUMENTS ON THE LEADING EDGE OF SEMANTIC CHANGE GET MORE CITATIONS,@classmethod @staticmethod @plac.annotations ( @plac.annotations(,"requests pandas sys glob ujson bs4 gensim similarity_measures langid datetime collections tempfile torch os scipy spacy gensim, logging, os, plac, ujson sklearn argparse numpy heapq time shutil math propernames modules logging random plac",cs.CL cs.SI physics.soc-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sandeepsoni_semantic-progressiveness.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sandeepsoni_semantic-progressiveness.pdf,,"Multivariate analysisThere are many factors that drive citation counts, such as age, length, and content [Fowler et al., 2007, Van Opijneni, 2012. Some of these factors may be correlated with semantic progressiveness, confounding the analysis: for example, older documents have more chances to be cited, but are unlikely to lead a semantic change that would be captured by our metrics. To control for these additional predictors, we formulate the problem as a multivariate regression. The dependent variable is the number of citations, and the predictors include our measure of semantic progressiveness, as well as a set of controls. As the number of citations is a count variable, we fit a Poisson regression model. 10 Regression modelsTo assess the relevance of semantic progressiveness, we compare against two baseline models, which include covariates that capture structural information about each document: the number of outgoing references that a document makes; its age; its length, operationalized as the number of unique types; and the number of authors for the document (available only for scientific articles). The baseline also incorporates a lightweight model of document content, to account for the fact that some topics may get cited more than others. Specifically, we fit a bag-of-words regression model on a small subset of documents [similar to Yogatama et al., 2011], and use its prediction as a covariate in the multivariate regression. We refer to this covariate as BoWs. This baseline is referred to as M1.The second baseline, M2, includes all the covariates from M1, and an additional covariate for the number of unique semantic innovations present in the document. This is aimed to tease out the effect of the presence of words with changing semantics from the extent to which the document employs the more contemporary meaning, as captured by our measure of semantic progressiveness. We refer to this covariate as # Innovs.To test the effect of semantic progressiveness, we create two experimental models, M3 and M4, which use the z-scores described earlier. In M3, the z-score is included as a raw value; in M4 it is binned into quartiles. Note that for M4, the bottom quartile (Q1) receives a coefficient of zero by default, so that the model is not underdetermined.We compare these models by goodness-of-fit, which is a standard technique from quantitative social science [Greene, 2003]. We compute the log-likelihood for each model; under the null hypothesis that the more complex model is no better than the baseline, the log-likelihood ratio has a χ 2 distribution with degrees of freedom equal to the number of parameters in the more expressive model. If the observed log-likelihood ratio is unlikely to arise under this distribution, then we can reject the null hypothesis. This approach is similar in spirit to the Akaike Information Criterion (AIC), which also penalizes the log-likelihood by the number of parameters. We can also measure the effect size by examining  the regression coefficients: the value of each coefficient corresponds to an increase in the log of the expected number of citations under the Poisson model.",no,,,most hyperparameter values were set the defaults,no,,,,
1230,https://github.com/dlmacedo/isotropic-maximization-loss-and-entropic-score,dlmacedo_isotropic-maximization-loss-and-entropic-score.tar.gz,"Isotropy Maximization Loss and Entropic Score: Accurate, Fast, Efficient, Scalable, and Turnkey Neural Networks Out-of-Distribution Detection Based on The Principle of Maximum Entropy",,utils pandas csv sys torchnet tools torchmetrics __future__ torchvision net torch os loaders agents pickle sklearn argparse calculate_log numpy time math data_loader random statistics losses models,cs.LG cs.CV stat.ML cs.LG cs.AI cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dlmacedo_isotropic-maximization-loss-and-entropic-score.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dlmacedo_isotropic-maximization-loss-and-entropic-score.pdf,Softmax  Softmax,,no,,,,no,,,,
1231,https://github.com/AMGrobelnik/MultiCOMET,AMGrobelnik_MultiCOMET.tar.gz,ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning,@contextlib.contextmanager,utils main_atomic copy main_conceptnet pandas contextlib sys tqdm demo_bilinear distutils re json src nltk torch os ftfy IPython spacy pickle argparse tensorboardX numpy time math random,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AMGrobelnik_MultiCOMET.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AMGrobelnik_MultiCOMET.pdf,,,no,,,,no,,,,
1232,https://github.com/Fzaero/BRDPN,Fzaero_BRDPN.tar.gz,Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects,,"copy keras IPython tensorflow os,sys DatasetLoader Blocks random sklearn tqdm os numpy matplotlib",cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Fzaero_BRDPN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Fzaero_BRDPN.pdf,,"IV. EXPERIMENTAL SETUPWe evaluate our model in simulation and on a real robot through a set of experiments. In the following, we give the details of the experimental setups designed to assess the generalization performance to the changing number of objects and time steps, transferability of our model to different object-relation distributions and to the real world setting.",no,,,,no,,,,
1237,https://github.com/XuefengBUPT/Tiny-Obstacle-Discovery-ROS,XuefengBUPT_Tiny-Obstacle-Discovery-ROS.tar.gz,A Novel Multi-layer Framework for Tiny Obstacle Discovery,,gc PIL copy sys multiprocessing tools sensor_msgs rospy __future__ psutil platform distutils std_msgs stat json pbcvt os pytest runpy matplotlib scipy functools tod itertools catkin bisect sklearn argparse joblib numpy time math setuptools errno cv2 logging test_project gmssl,cs.CV cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/XuefengBUPT_Tiny-Obstacle-Discovery-ROS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\XuefengBUPT_Tiny-Obstacle-Discovery-ROS.pdf,,,no,,,,no,,,,
1240,https://github.com/manishb89/interpretable_sentence_similarity,manishb89_interpretable_sentence_similarity.tar.gz,Logic Constrained Pointer Networks for Interpretable Textual Similarity,@_load_pretrained @_read @_init_training,"constraints requests copy scripts sys subprocess, os tqdm __future__ pprint gensim re stanfordcorenlp model datetime json collections syntactic json, sys ppdb nltk torch os spacy, re embed functools itertools xml spacy codecs training sklearn argparse lxml json, codecs, sys numpy corpus time tempfile, platform pytorch_pretrained_bert torch, os",cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/manishb89_interpretable_sentence_similarity.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\manishb89_interpretable_sentence_similarity.pdf,Attention Dropout  Adam  Dense Connections  Linear Warmup With Linear Decay  Residual Connection  Dropout  Additive Attention  Gaussian Error Linear Units  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  WordPiece  Weight Decay  Softmax  BERT  Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Pointer Network,"Cross-domain ExperimentsWhile the proposed approach achieves significant improvements over the previous baselines, a natural question arises -how would this approach be applicable on a new sentence similarity dataset to provide interpretability, when no chunk alignments are available to train the model in the target domain? To answer this, we attempted a cross-domain experiment, where we train on headlines, test on images, and vice versa (i.e., do not utilize training examples from the target domain). Remarkably, even in this setting, using our best model M4, we achieve F1 scores of 96.16 and 94.80 on headlines and images datasets, respectively, outperforming the previous SOTA results. Note that no hyperparameter tuning was performed on the target domain.",no,,,,no,,,,
1244,https://github.com/paniquex/msu_mmp_research,paniquex_msu_mmp_research.tar.gz,VoiceFilter: Targeted Voice Separation by Speaker-Conditioned Spectrogram Masking,,PIL timm pandas tqdm psutil torchvision pathlib model torch os matplotlib IPython dataset pickle sklearn numpy preprocessing config time librosa random metrics,eess.AS cs.LG eess.SP stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/paniquex_msu_mmp_research.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\paniquex_msu_mmp_research.pdf,,"IntroductionRecent advances in speech recognition have led to performance improvement in challenging scenarios such as noisy and farfield conditions. However, speech recognition systems still perform poorly when the speaker of interest is recorded in crowded environments, i.e., with interfering speakers in the foreground or background.One way to deal with this issue is to first apply a speech separation system on the noisy audio in order to separate the voices from different speakers. Therefore, if the noisy signal contains N speakers, this approach would yield N outputs with a potential additional output for the ambient noise. A classical speech separation task like this needs to cope with two main challenges. First, identifying the number of speakers N in the recording, which in realistic scenarios is unknown. Secondly, the optimization of a speech separation system may be required to be invariant to the permutation of speaker labels, as the order of the speakers should not have an impact during training [1]. Leveraging the advances in deep neural networks, several successful works have been introduced to address these problems, such as deep clustering [1], deep attractor network [2], and permutation invariant training [3].This work addresses the task of isolating the voices of a subset of speakers of interest from the commonality of all the other speakers and noises. For example, such subset can be formed by a single target speaker issuing a spoken query to a personal mobile device, or the members of a house talking to a shared home device. We will also assume that the speaker(s) of interest can be individually characterized by previous reference recordings, e.g. through an enrollment stage. This task is closely related to classical speech separation, but in a way that it is speaker-dependent. In this paper, we will refer to the task of speaker-dependent speech separation as voice filtering (some literature call it speaker extraction). We argue that for voice filtering, speaker-independent techniques such as those presented in [1,2,3] may not be a good fit. In addition to the challenges described previously, these techniques require an extra step to determine which output -out of the N possible outputs of the speech separation system -corresponds to the target speaker(s), by e.g. choosing the loudest speaker, running a speaker verification system on the outputs, or matching a specific keyword.A more end-to-end approach for the voice filtering task is to treat the problem as a binary classification problem, where the positive class is the speech of the speaker of interest, and the negative class is formed by the combination of all foreground and background interfering speakers and noises. By speaker-conditioning the system, this approach suppresses the three aforementioned challenges: unknown number of speakers, permutation problem, and selection from multiple outputs. In this work, we aim to condition the system on the speaker embedding vector of a reference recording. The proposed approach is the following. We first train a LSTM-based speaker encoder to compute robust speaker embedding vectors. We then train separately a time-frequency mask-based system that takes two inputs: (1) the embedding vector of the target speaker, previously computed with the speaker encoder; and (2) the noisy multi-speaker audio. This system is trained to remove the interfering speakers and output only the voice of the target speaker. 1 This approach can be easily extended to more than one speaker of interest by repeating the process in turns, for the reference recording of each target speaker.Similar related literature exists for the task of voice filtering. For example, in [4,5], the authors achieved impressive results by doing an indirect speaker conditioning of the system on the visual information (lips movement). However, a solution like that would require simultaneously using speech and visual information, which may not be available in certain type of applications, where a reference speech signal may be more practical. The systems proposed in [6,7,8,9] are also very similar to ours, with a few major differences: (1) Unlike using one-hot vectors, i-vectors or speaker posteriors derived from a crossentropy classification network, our speaker encoder network is specifically designed for large-scale end-to-end speaker recognition [10], which proves to perform much better in speaker recognition tasks, especially for unseen speakers. (2) Instead of using a GEV beamformer [6,8], our system directly optimizes the power-law compressed reconstruction error between the clean and enhanced signals [11]. (3) In addition to sourceto-distortion ratio [6,7], we focus on Word Error Rate improvements for ASR systems. (4) We use dilated convolutional layers to capture low-level acoustic features more effectively. (5) We prefer separately trained speaker encoder network over joint training like [8,9], due to the very different requirements for data in speaker recognition and source separation tasks.The rest of this paper is organized as follows. In Section 2, we describe our approach to the problem, and provide the details of how we train the neural networks. In Section 3, we describe our experimental setup, including the datasets we use and the evaluation metrics. The experimental results are presented in Section 4. We draw our conclusions in Section 5, with discussions on future work directions. Experimental setupIn this section, we describe our experimental setup: the data used to train the two components of the system separately, as well as the metrics to assess the systems. DatasetsSpeaker encoder: Although our speaker encoder network has exactly the same network topology as the text-independent model described in [10], we use more training data in this system. Our speaker encoder is trained with two datasets combined by the MultiReader technique introduced in [10]. The first dataset consists of anonymized voice query logs in English from mobile and farfield devices. It has about 34 million utterances from about 138 thousand speakers. The second dataset consists of LibriSpeech [16], VoxCeleb [17], and VoxCeleb2 [18]. This model (referred to as ""d-vector V2"" in [13]) has a 3.06% equal error rate (EER) on our internal en-US phone audio test dataset, compared to the 3.55% EER of the one reported in [10]. VoiceFilter: We cannot use a ""standard"" benchmark corpus for speech separation, such as one of the CHiME challenges [19], because we need a clean reference utterance of each target speaker in order to compute speaker embeddings. Instead, we train and evaluate the VoiceFilter system on our own generated data, derived either from the VCTK dataset [20] or from the LibriSpeech dataset [16]. For VCTK, we randomly take 99 speakers for training, and 10 speakers for testing. For LibriSpeech, we used the training and development sets defined in the protocol of the dataset: the training set contains 2338 speakers, and the development set contains 73 speakers. These two datasets contain read speech, and each utterance contains the voice of one speaker. We explain in the next section how we generate the data used to train the VoiceFilter system.",no,,,,no,,,,
1249,https://github.com/PedroFerreiradaCosta/FaceFitOpt,PedroFerreiradaCosta_FaceFitOpt.tar.gz,"Bayesian Optimization for real-time, automatic design of face stimuli in human-centred research",@tf.custom_gradient,ffhq_dataset requests PIL copy pwd dnnlib warnings sys types glob tqdm encoder ctypes pprint platform distutils threading pathlib keras tensorflow re zipfile uuid datetime json collections h5py enum fnmatch os matplotlib html six importlib gzip functools scipy hashlib training lmdb pickle typing sklearn argparse numpy dlib time shutil config train ipywidgets tensorboard traceback bz2 cv2 mpl_toolkits metrics inspect io,cs.LG cs.HC stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/PedroFerreiradaCosta_FaceFitOpt.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\PedroFerreiradaCosta_FaceFitOpt.pdf,,,no,,,,no,,,,
1253,https://github.com/amitport/Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting,amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.tar.gz,Towards Federated Learning With Byzantine-Robust Client Weighting,"@computations.federated_computation() @computations.tf_computation(server_state_type) @computations.federated_computation( @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.int64), shakespeare_train.element_type_structure]) @computations.tf_computation(model_weights_type, model_weights_type.trainable, @computations.tf_computation(dataset_with_byzflag_type, model_weights_type) @tf.function @abc.abstractmethod @tff.tf_computation((tf.string, tf.bool)) @computations.tf_computation @staticmethod @property @computations.federated_computation(new_param_type) @classmethod @dataclass(frozen=True) @abc.abstractproperty @attr.s(eq=False, frozen=True)",utils tensorflow_federated pathlib tensorflow absl json collections enum dataclasses os matplotlib wquantiles functools itertools shared typing argparse numpy abc math optimization attr experiments random,cs.LG cs.CR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amitport_Towards-Federated-Learning-With-Byzantine-Robust-Client-Weighting.pdf,,,no,,,"hyperparameters from other study, ",no,,,,
1254,https://github.com/SamuelHorvath/Compressed_SGD_PyTorch,SamuelHorvath_Compressed_SGD_PyTorch.tar.gz,A BETTER ALTERNATIVE TO ERROR FEEDBACK FOR COMMUNICATION-EFFICIENT DISTRIBUTED LEARN-ING,,math glob random pickle torch os optim numpy matplotlib torchvision,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/SamuelHorvath_Compressed_SGD_PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\SamuelHorvath_Compressed_SGD_PyTorch.pdf,,,no,,,batch size,no,,,,
1258,https://github.com/HanjieChen/HEDGE,HanjieChen_HEDGE.tar.gz,Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection,@classmethod,copy csv sys utils_glue glob tqdm __future__ gensim re hedge_bert load_data torch os matplotlib scipy itertools torchtext pytorch_transformers sklearn argparse numpy eval_utils time hedge random logging io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HanjieChen_HEDGE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HanjieChen_HEDGE.pdf,,,no,,,,no,,,,
1259,https://github.com/alan-turing-institute/signet,alan-turing-institute_signet.tar.gz,SPONGE: A generalized eigenproblem for clustering signed networks,,block_models math scipy setuptools sys networkx signet sklearn subprocess os numpy,stat.ML cs.LG math.ST stat.TH,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alan-turing-institute_signet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alan-turing-institute_signet.pdf,,,no,,,,no,,,,
1264,https://github.com/AsmaBALAMANE/tiramisu,AsmaBALAMANE_tiramisu.tar.gz,TIRAMISU: A Polyhedral Compiler for Expressing Fast and Portable Code,@deadline(1) @staticmethod,re csv islpy sys signal tensor_comprehensions gdb torch isl os distributed_module subprocess ctypes time,cs.PL cs.DC cs.MS cs.NE cs.PF,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AsmaBALAMANE_tiramisu.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AsmaBALAMANE_tiramisu.pdf,,,no,,,,no,,,,
1277,https://github.com/danielzuegner/gnn-meta-attack,danielzuegner_gnn-meta-attack.tar.gz,ADVERSARIAL ATTACKS ON GRAPH NEURAL NETWORKS VIA META LEARNING,,scipy tensorflow metattack setuptools tqdm sklearn numpy,cs.LG cs.CR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/danielzuegner_gnn-meta-attack.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\danielzuegner_gnn-meta-attack.pdf,,,no,,,,no,,,,
1279,https://github.com/cryu854/SinGAN,cryu854_SinGAN.tar.gz,Instance Normalization: The Missing Ingredient for Fast Stylization,@tf.function,utils train PIL tensorflow model datetime argparse os inference numpy time,cs.CV cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cryu854_SinGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cryu854_SinGAN.pdf,Convolution  Generative Adversarial Network  Instance Normalization  Batch Normalization  Residual Connection  Average Pooling  1x1 Convolution  Layer Normalization  Max Pooling  Global Average Pooling  Bottleneck Residual Block  Residual Block  Kaiming Initialization  Residual Network  Adam  RMSProp  Rectified Linear Units  WGAN-GP Loss  Leaky ReLU  Convolution  Batch Normalization  Wasserstein GAN (Gradient Penalty),,no,,,,no,,,,
1282,https://github.com/ElMehdiBouamama/MBTI-Tweetouilles,ElMehdiBouamama_MBTI-Tweetouilles.tar.gz,Controlled Experiments for Word Embeddings,,ConfigManager DatabaseManager pandas sys multiprocessing psutil TwitterManager tensorflow re collections json nltk os matplotlib string configparser functools html5lib urllib pickle sklearn numpy time DataManager math text_helper Content random TweetToType,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ElMehdiBouamama_MBTI-Tweetouilles.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ElMehdiBouamama_MBTI-Tweetouilles.pdf,,,no,,,"hidden layer size, window size, minimum frequency",no,,,,
1286,https://github.com/alexa/bort,alexa_bort.tar.gz,An Algorithm for Learning Smaller Representations of Models With Scarce Data,"@register(segment=['train', 'dev', 'test']) @register(segment=['train', 'dev_matched', 'dev_mismatched',",utils bort warnings multiprocessing sys glob tqdm io __future__ psutil re collections json fnmatch data os mxnet horovod create_pretraining_data gluonnlp functools itertools sklearn argparse numpy time mxboard unicodedata random logging zipfile,cs.LG cs.AI cs.DS cs.CL cs.LG cs.LG cs.DS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alexa_bort.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alexa_bort.pdf,Bort  Dense Connections  Multi-Head Attention  Dropout  Gaussian Error Linear Units  Linear Warmup With Linear Decay  Attention Dropout  Weight Decay  Residual Connection  Scaled Dot-Product Attention  WordPiece  Adam  Softmax  Layer Normalization  BERT,,no,,,,no,,,,
1289,https://github.com/HazyResearch/fonduer,HazyResearch_fonduer.tar.gz,Fonduer: Knowledge Base Construction from Richly Formatted Data,"@pytest.fixture @pytest.mark.skipif(""CI"" not in os.environ, reason=""Only run e2e on GitHub Actions"") @pytest.mark.dependency(depends=[""test_save_label_model""]) @pytest.mark.dependency(depends=[""test_save_subclasses""]) @pytest.mark.parametrize( @lru_cache(maxsize=1024) @declared_attr @deprecation.deprecated( @lru_cache(maxsize=256) @staticmethod @pytest.mark.skipif( @abstractmethod @lru_cache(maxsize=2) @classmethod @pytest.mark.dependency(depends=[""test_save_model""]) @labeling_function(name=""LF_storage_row"") @pytest.mark.dependency() @functools.lru_cache(maxsize=16) @pytest.fixture()",packaging mlflow pandas csv yaml warnings sys cloudpickle multiprocessing glob bs4 the tqdm treedlib psycopg2 threading fonduer pathlib sqlalchemy re datetime collections unittest queue tests tempfile nltk torch os snorkel pytest string importlib wand scipy functools itertools IPython spacy codecs builtins urllib typing pickle lxml tensorboardX numpy shutil editdistance abc difflib deprecation setuptools operator logging emmental random subprocess io,cs.DB,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HazyResearch_fonduer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HazyResearch_fonduer.pdf,,,no,,,,no,,,,
1290,https://github.com/wOOL/DNDT,wOOL_DNDT.tar.gz,Deep Neural Decision Trees,,functools tensorflow unittest neural_network_decision_tree numpy,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wOOL_DNDT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wOOL_DNDT.pdf,Affine Coupling  Normalizing Flows,"NNDT Random ForestsNeural Decision Trees is weak to dimensionality (i.e. number of features) increase, as the size of the Kron Product result grows exponentially with the number of features.To This tree is very comparable to the one created by a Decision Tree Classifier. For the same dataset, the trained sklearn DecisionTreeClassifier() can create (without any additional parameters) the tree in Figure 5.2. The attribute ""Petal Length"" corresponds to feature index 0 and ""Petal Width"" to index 1. trained on a random subset of features. A better solution that does not require an uninterpretable forest is to exploit the sparsity of the final binning during learning: the number of non-empty leaves grows much slower than the total number of leaves. But this somewhat complicates the otherwise simple implementation of DNDT.X[1] <= 0.7 gini = 0.444 samples = 6 value = [[4, 2] [4, 2] [4, 2]] gini = 0.0 samples = 2 value = [[0, 2] [2, 0] [2, 0]] True X[0] <= 4.75 gini = 0.333 samples = 4 value = [[4, 0] [2, 2] [2, 2]] False gini = 0.0 samples = 2 value = [[2, 0] [0, 2] [2, 0]] gini = 0.0 samples = 2 value = [[2, 0] [2, 0] [0, 2]]",no,,,,no,,,,
1291,https://github.com/plaidml/plaidml,plaidml_plaidml.tar.gz,Stripe: Tensor Compilation via the Nested Polyhedral Model,"@opTest([[m(2, 3)]]) @opTest([[m(2, 3) + 3], [m(2, 3, 4) + 3]]) @opTest([[m(3, 3), m(3, 3)], [m(2, 3), m(3)]]) @opTest([[m(1, 14, 14, 256), m(3, 3, 256, 256)]], @click.option('--kernel-timing/--no-kernel-timing', default=True, help='Emit kernel timing info') @opTest([[m(1, 3, 3, 1), m(1, 3, 3, 1) - 2]], skip_tensorflow=True, skip_theano=True) @opTest([[m(1, 3, 2)]]) @device_ids.setter @opTest([[m(2, 5)]]) @opTest([[m(3, 3, 2, 10)]], skip_theano=True, tol=0.01) @opTest([[m(1, 28, 28, 512), m(1, 1, 512, 128)]], @opTest([[m(1000, 1000)]], do_grads=False) @opTest([[m(4, 3, 3, 2, 5)]]) @opTest([[m(1, 96, 96, 192), m(3, 3, 192, 192)]], @click.option('--tc-autotune/--no-tc-autotune', default=True) @compareMultiple([[3, 4, 2], [7]]) @opTest([[m(1, 96, 96, 192), m(8, 8, 192, 192)]], @opTest([[m(2, 3, 2, 4)], [m(1, 1, 2, 1) + 1]], do_grads=False) @opTest([[m(3, 3), m(3, 3)]]) @opTest([[np.array([[0, 0, 0], [0, 0, 1], [1, 1, 0]]), (m(3, 3) + 3)]]) @opTest([[m(1, 3, 4)], [m(7, 19) - 10.]]) @click.option('--plaid-edsl', @unittest.skip(""We use a float16 as an accumulator for the matrix elements. "" @compareForwardClose(.1, skip_theano=True) @opTest([[m(2, 4, 3)]]) @opTest([[m(1, 224, 224, 3), m(7, 7, 3, 64)]], do_grads=False) @opTest([_conv_inp(IN=1, IC=1, OC=1, IS=[1, 6], KS=[1, 1], data_format='channels_last')], @click.option('--fp16/--no-fp16', @opTest([[m(3, 3, 10)]], skip_theano=True, tol=0.01) @cuda.reduce @unittest.skip(""The moving_average_update calls in this test are causing issues with TF"") @session.setter @click.option('--tvm', 'backend', flag_value='tvm', help='Use TVM as the backend') @opTest([[m(1, 14, 14, 256), m(1, 1, 256, 1024)]], @opTest([[m(10)], [m(2, 2, 2, 3)]], 1e-2) @unittest.skipIf(""darwin"" in platform.system().lower(), @opTest([[m(1, 56, 56, 256), m(1, 1, 256, 512)]], @opTest([[m(3, 3, 2, 1, 10)]], skip_theano=True, tol=0.01) @cuda.jit(device=True) @opTest([[m(3, 4) - 3.3, m(3, 4) / 2.0]]) @opTest([[n(2, 3), np.array([3., 4., .7]), @click.option('--tc', 'backend', flag_value='tc', help='Use TensorComprehensions as the backend') @unittest.skipIf(platform.system() == 'Darwin', ""Fails on metal"") @opTest([[m(3), m(3) + 1]], skip_tensorflow=True, skip_theano=True) @opTest([[m(20)], [m(7, 3)]]) @click.option('--epochs', type=int, default=1, help=""Number of epochs per test"") @offset.setter @opTest([[m(4, 3) - 3.1, m(4, 3) / 2.0]]) @opTest([[m(1, 8, 8, 16), m(3, 3, 16, 16)]], @unittest.skip(""T1193: Skip until there is a fake HAL or use single device is implmemented."") @_keras_shape.setter @opTest([[m(34)]]) @opTest([[m(2, 4, 5)]]) @opTest([[m(1, 56, 56, 256), m(1, 1, 256, 64)]], @click.option('--fix-learn-phase/--no-fix-learn-phase', @opTest([[m(1, 7, 7, 2048), m(1, 1, 2048, 512)]], @opTest([[m(1, 14, 14, 1024), m(1, 1, 1024, 512)]], @click.option('--print-stacktraces/--no-print-stacktraces', @click.option('--timeout-secs', type=int, default=None) @opTest([[m(1, 28, 28, 128), m(3, 3, 128, 128)]], @opTest([[m(1, 4, 4, 1)], [m(1, 7, 5, 1)], [m(2, 11, 13, 3)]], skip_theano=True) @opTest([[m(2, 3, 2, 4)], [m(1, 1, 2, 1) + 1], [m(1, 2, 3, 1) + 4]], do_grads=False) @opTest([[m(3, 10)]], skip_theano=True, tol=0.001) @opTest( @telemetry.setter @click.pass_context @opTest([[m(1, 28, 28, 128), m(1, 1, 128, 512)]], @click.argument('networks', nargs=-1, type=click.Choice(Frontend.NETWORK_NAMES)) @click.option('--tc-at-generations', default=13) @cuda.jit(""void(float64[:,:,:],float64[:,:,:],float64[:], float64[:], float64[:])"") @opTest([[m(1, 64, 64, 256), m(3, 3, 256, 256)]], @opTest([[m(1, 56, 56, 64), m(1, 1, 64, 64)]], @config_file.setter @compareForwardExact(skip_tensorflow=True) @opTest([[m(1, 14, 14, 1024), m(1, 1, 1024, 2048)]], @relative_start_time.setter @compareForwardClose(epsilon=0.2) @click.option('--callgrind/--no-callgrind', @opTest([[np.array([15])]], skip_theano=True) @opTest([[m(1, 56, 56, 64), m(3, 3, 64, 64)]], do_grads=False) @click.option('--tc-cachedir', default=""~/.plaidbench-tccache"") @opTest([[m(10, 10)]], do_grads=False) @opTest([[m(1, 56, 56, 256), m(1, 1, 256, 128)]], @click.command(cls=core.FrontendCommand, networks=Frontend.NETWORK_NAMES) @opTest([[np.array([100])]], skip_theano=True) @compareForwardClose(skip_tensorflow=True) @compareForwardExact() @opTest([[m(3, 3), m(3, 3), m(3, 3)]]) @_log_call @opTest([[m(10)], [m(2, 2, 2, 3)]], 1e-2, 1e-7) @click.command(cls=_PlaidbenchCommand) @opTest([[m(2, 3, 4, 5)], [m(2, 1, 2)]]) @click.option('--plaid', @opTest([[m(20)], [m(2, 2, 2)]]) @click.option('-v', '--verbose', count=True) @opTest([[m(10, 10)]], skip_theano=True) @property @click.option('--train/--no-train', @opTest([[m(2, 3, 2)]]) @click.option('--warmup/--no-warmup', default=True, help='Do warmup runs before main timing') @opTest([[m(1, 7, 7, 512), m(3, 3, 512, 512)]], @opTest([[m(2, 4, 5)], [m(4)], [m(2, 3)]]) @click.option( @opTest([[m(3, 6)]]) @opTest([[m(20) + 3], [m(10, 3)]]) @abstractmethod @compareMultiple([[10], [-2, 5, 2, 'float32']]) @opTest([[m(1, 56, 56, 256), m(3, 3, 256, 128)]], do_grads=False) @opTest([[m(3)]], skip_tensorflow=True, skip_theano=True) @opTest([[m(1, 14, 14, 1024), m(1, 1, 1024, 256)]], @click.option('--large-ops/--no-large-ops', default=True) @click.option('--tvm-driver', default='cuda') @opTest([[m(3, 3) - 0.0001]]) @click.option('--cuda-profile/--no-cuda-profile', default=False) @unittest.skip(""Unknown instability issues"") @cuda.jit(""void(float64[:,:,:],float64[:,:,:],float64[:,:,:], float64)"") @click.option('--results', @unittest.skipIf(os.environ.get(""PLAIDML_USE_STRIPE"", ""0"") == ""1"", ""Stripe fails this test"") @classmethod @click.option('--tf', 'backend', flag_value='tf', help='Use tensorflow as the backend') @opTest([[m(2, 4, 7)]]) @opTest([ @contextlib.contextmanager @opTest([[m(3, 3), 2.0001, 5.0001]]) @opTest([[m(3, 3)]]) @click.option('--tensorflow', @unittest.skip(""Broken on some drivers. TODO: periodically check if this is resolved"") @torch.jit.script @enable_winograd.setter @relative_end_time.setter @compareForwardExact(skip_theano=True) @compareMultiple([ @opTest([[m(3, 2, 4, 5, 6)]]) @click.option('--tc-at-population', default=13) @opTest([[m(1, 28, 28, 512), m(1, 1, 512, 256)]], @config.setter @unittest.skipIf( @setup.setter @atexit.register @opTest([[m(1, 64, 64, 128), m(3, 3, 128, 128)]], @opTest([[m(1, 7, 7, 512), m(1, 1, 512, 2048)]], @staticmethod @opTest([[m(1, 56, 56, 64), m(1, 1, 64, 256)]], @opTest([[m(4, 6, 7, 1)], [m(2, 4, 8, 3), ((1, 3), (0, 2))], @opTest([[m(3, 5)]]) @opTest([[m(1, 28, 28, 512), m(1, 1, 512, 1024)]], @opTest([[ @opTest([[np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]), (m(3, 3) + 3) / 15.0], @click.option('-n', @opTest([[m(4, 5, 3, 1, 2)], [m(3, 8, 1, 5, 6), ((0, 2), (3, 5), (1, 4))], @contextmanager @compareMultiple([[10], [3, 'int8']]) @opTest([[m(1, 3, 10)]], skip_theano=True, tol=0.01) @compareForwardClose(.1) @compareForwardClose() @opTest([[np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]), (m(3, 3) + 3) / 15.0]], @opTest([[m(1, 56, 56, 64), m(1, 1, 64, 64)]], do_grads=False) @opTest([[m(1, 2048), m(2048, 1000)]], do_grads=False) @opTest([[np.sqrt(m(5, 5, 10) + 2) - 3], [np.sin(m(4, 3, 2, 1, 6))]], 1e-02, skip_theano=True) @unittest.skip( @opTest([[m(4, 7, 1)], [m(2, 8, 3), (1, 3)], [m(2, 5, 7), (0, 0)]]) @click.option('--batch-size', type=int, default=1) @opTest([[m(10, 10), n(10, 10), 0], [m(10, 10), n(10, 10), -1]]) @opTest([[m(2, 6)], [m(2, 9, 9) - 3.1]]) @unittest.skipIf(is_llvm_cpu(), ""This test fails on llvm_cpu"") @experimental.setter @opTest([[m(1, 56, 56, 64), m(3, 3, 64, 64)]], @opTest([[(m(2, 2) + 3) / 10.0, np.array([[0., 0.], [1., 2.]])]])",csv warnings multiprocessing weakref types tools cmake_parser ctypes threading keras tensorflow re mimetypes torch plaidbench six importlib msvcrt numba example_correctness_test_utils pickle ngraph_bridge networks numpy cairocffi tarfile ipywidgets traceback networkx random win32event inspect requests yaml signal tile __future__ fcntl harness pprint distutils util pathlib click unittest cProfile os google gzip functools hashlib testing asq tensor_comprehensions pystache plaidml2 time abc shutil ci livereload cffi operator subprocess analysis pandas sys atexit datetime enum tempfile pkg_resources matplotlib scipy IPython itertools seaborn sklearn timeit setuptools errno cPickle io zipfile PIL keras_applications contextlib pcpp glob TransferLearningDemo skimage base64 platform uuid json collections urllib defaultlist argparse editdistance math min_parallel hyperopt theano logging plaidml base,cs.DC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/plaidml_plaidml.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\plaidml_plaidml.pdf,,,no,,,,no,,,,
1294,https://github.com/stavros0/bitcoin-price-prediction,stavros0_bitcoin-price-prediction.tar.gz,Bayesian regression and Bitcoin,,requests pytz bigfloat datetime setuptools bitcoin_price_prediction sklearn apscheduler numpy pymongo,cs.AI math.ST stat.TH,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stavros0_bitcoin-price-prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stavros0_bitcoin-price-prediction.pdf,,,no,,,,no,,,,
1297,https://github.com/hila-chefer/Transformer-MM-Explainability,hila-chefer_Transformer-MM-Explainability.tar.gz,Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers,"@test_utils.skip_if_no_network @registry.register_model(""cnn_lstm"") @registry.register_model(""movie_mcan"") @registry.register_trainer(""mmf"") @zoo_variation.setter @lru_cache() @registry.register_metric(""caption_bleu4"") @registry.register_processor(""evalai_answer"") @registry.register_fusion(""mutan"") @registry.register_metric(""r@5"") @registry.register_builder(""masked_sbu"") @registry.register_loss(""cross_entropy"") @registry.register_metric(""textvqa_accuracy"") @registry.register_model(""pythia_image_only"") @registry.register_encoder(""text_embedding"") @functools.lru_cache(maxsize=32) @registry.register_builder(""hateful_memes"") @registry.register_metric(""r@10"") @registry.register_fusion(""mfh"") @registry.register_builder(""vizwiz"") @registry.register_model(""visual_bert"") @registry.register_processor(""masked_token"") @registry.register_transformer_backend(""huggingface"") @registry.register_metric(""micro_ap"") @registry.register_metric(""micro_f1"") @registry.register_builder(""vqa_hat_test"") @zoo_config_path.setter @registry.register_model(""unimodal_image"") @registry.register_trainer(""mmf_pert"") @registry.register_decoder(""nucleus_sampling"") @registry.register_scheduler(""warmup_cosine"") @registry.register_model(""vilbert"") @registry.register_model(""m4c"") @registry.register_model(""pythia"") @unittest.skip(""CI doesn't have enough memory"") @registry.register_trainer(""base"") @torch.jit.unused @registry.register_builder(""vqa_hat"") @registry.register_fusion(""linear_sum"") @dataset_name.setter @registry.register_builder(""vqa2"") @registry.register_fusion(""mfb"") @registry.register_metric(""binary_ap"") @Registry.register_builder(""stvqa"") @registry.register_metric(""mean_r"") @registry.register_metric(""mean_rr"") @registry.register_builder(""masked_visual_genome"") @registry.register_loss(""softmax_kldiv"") @registry.register_encoder(""finetune_faster_rcnn_fpn_fc7"") @registry.register_metric(""r@1"") @registry.register_builder(""masked_mmimdb"") @registry.register_processor(""prediction.argmax"") @registry.register_metric(""binary_f1"") @registry.register_builder(""okvqa"") @registry.register_processor(""m4c_caption"") @registry.register_builder(""vqa_hat_train_val"") @registry.register_model(""multihead"") @registry.register_scheduler(""multi_step"") @Registry.register_builder(""vqa2_ocr"") @registry.register_builder(""gqa"") @functools.lru_cache(maxsize=None) @skip_if_no_network @registry.register_loss(""logit_bce"") @pytest.mark.parametrize('model_name', clip.available_models()) @registry.register_loss(""multi"") @annotation_db.setter @current_loader.setter @registry.register_metric(""multilabel_micro_f1"") @registry.register_builder(""visual_entailment"") @registry.register_builder(""masked_conceptual_captions"") @registry.register_builder(""textvqa"") @registry.register_processor(""masked_region"") @registry.register_builder(""vqacp_v2"") @registry.register_metric(""multilabel_f1"") @add_start_docstrings( @registry.register_builder(""masked_coco"") @META_ARCH_REGISTRY.register() @registry.register_metric(""stvqa_anls"") @registry.register_builder(""masked_flickr30k"") @current_dataset.setter @registry.register_metric(""accuracy"") @transform.setter @registry.register_model(""ban"") @registry.register_processor(""multi_sentence_bert_tokenizer"") @registry.register_processor(""transformer_bbox"") @registry.register_metric(""vqa_accuracy"") @current_index.setter @registry.register_processor(""GrayScaleTo3Channels"") @registry.register_scheduler(""warmup_linear"") @registry.register_processor(""soft_copy_answer"") @s3_request @registry.register_processor(""multi_hot_answer_from_vocab"") @batch_size.setter @registry.register_encoder(""resnet152"") @registry.register_fusion(""mcb"") @registry.register_encoder(""transformer"") @registry.register_builder(""mmimdb"") @registry.register_processor(""copy"") @name.setter @registry.register_loss(""nll_loss"") @registry.register_builder(""nlvr2"") @registry.register_fusion(""block_tucker"") @registry.register_builder(""clevr"") @registry.register_builder(""masked_q_vqa2"") @registry.register_model(""concat_bert"") @registry.register_metric(""macro_f1"") @registry.register_processor(""fasttext"") @registry.register_task(""pythia"") @registry.register_model(""top_down_bottom_up"") @registry.register_builder(""coco"") @dataclass @registry.register_decoder(""beam_search"") @skip_if_macos @registry.register_loss(""m4c_decoding_bce_with_mask"") @property @registry.register_metric(""textcaps_bleu4"") @registry.register_metric(""macro_ap"") @registry.register_builder(""vqa2_train_val"") @registry.register_metric(""f1"") @add_start_docstrings_to_model_forward(LXMERT_INPUTS_DOCSTRING.format(""batch_size, sequence_length"")) @abstractmethod @registry.register_processor(""bbox"") @metadata.setter @registry.register_loss(""weighted_softmax"") @registry.register_model(""lorra"") @Registry.register_builder(""ocrvqa"") @registry.register_processor(""glove"") @registry.register_builder(""masked_localized_narratives"") @registry.register_model(""late_fusion"") @registry.register_metric(""micro_roc_auc"") @classmethod @registry.register_encoder(""identity"") @registry.register_processor(""vqa_answer"") @registry.register_processor(""torchvision_transforms"") @registry.register_fusion(""tucker"") @wraps(func) @dataset_config.setter @dataset_class.setter @registry.register_metric(""macro_roc_auc"") @registry.register_builder(""masked_gqa"") @registry.register_builder(""visual_dialog"") @registry.register_model(""pythia_question_only"") @registry.register_processor(""multi_class_from_file"") @registry.register_metric(""vqa_evalai_accuracy"") @registry.register_loss(""caption_cross_entropy"") @registry.register_processor(""m4c_answer"") @contextlib.contextmanager @registry.register_model(""m4c_captioner"") @add_code_sample_docstrings( @Registry.register_builder(""textcaps"") @registry.register_metric(""ap"") @registry.register_metric(""stvqa_accuracy"") @registry.register_metric(""ocrvqa_accuracy"") @registry.register_processor(""simple_word"") @registry.register_model(""concat_bow"") @registry.register_loss(""attention_supervision"") @registry.register_task(""logit_bce"") @registry.register_model(""mmf_bert"") @test_utils.skip_if_macos @registry.register_builder(""vqa2_test"") @registry.register_processor(""bert_tokenizer"") @registry.register_builder(""masked_coco2017"") @registry.register_loss(""bce_kl_combined"") @registry.register_builder(""conceptual_captions"") @registry.register_fusion(""mlb"") @registry.register_builder(""masked_vqa2"") @torch.no_grad() @registry.register_model(""lxmert-old"") @staticmethod @lru_cache(maxsize=5000) @registry.register_fusion(""concat_mlp"") @registry.register_metric(""r@pk"") @registry.register_model(""unimodal_text"") @registry.register_builder(""visual_genome"") @registry.register_processor(""vocab"") @registry.register_model(""mmf_transformer"") @registry.register_fusion(""block"") @contextmanager @is_pretrained.setter @registry.register_loss(""wrong"") @registry.register_scheduler(""pythia"") @unittest.skipIf(onnxruntime is None, 'ONNX Runtime unavailable') @functools.lru_cache() @registry.register_processor(""simple_sentence"") @registry.register_processor(""caption"") @registry.register_processor(""phoc"") @registry.register_model(""butd"") @registry.register_loss(""triple_logit_bce"") @skip_if_no_cuda @iterators.setter @test_utils.skip_if_no_cuda @registry.register_metric(""multilabel_macro_f1"") @registry.register_metric(""roc_auc"") @test_utils.skip_if_windows @registry.register_loss(""bce"")","main copy csv warnings multiprocessing git types wget tools lib termcolor torchvision pretrain re tensorflow h5py dataclasses torch pytest caffe lxrt importlib hubconf codecs pickle boto3 numpy tarfile unicodedata clip demjson random fastText requests utils yaml fairscale __future__ d2 pprint util pathlib submitit unittest botocore tests transformers fnmatch nltk os socket ftfy gzip functools DETR hashlib time abc shutil mmf pycocotools maskrcnn_benchmark operator different subprocess detectron2 models pycocoevalcap gc fasttext pandas sys regex tqdm param omegaconf _init_paths datetime enum tempfile pkg_resources matplotlib html scipy itertools torchtext seaborn lmdb typing shlex sklearn filelock urlparse datasets setuptools onnxruntime panopticapi VisualBERT zipfile io fast_rcnn PIL packaging fvcore contextlib glob base64 platform ast os, sys uuid collections json src numexpr bisect builtins urllib argparse lxmert editdistance math run tasks colorsys cv2 logging",cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hila-chefer_Transformer-MM-Explainability.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hila-chefer_Transformer-MM-Explainability.pdf,,,no,,,,no,,,,
1301,https://github.com/choyingw/GAIS-Net,choyingw_GAIS-Net.tar.gz,Geometry-Aware Instance Segmentation with Disparity Maps,"@C2_FORMAT_LOADER.register(""R-50-FPN-RETINANET"") @registry.BACKBONES.register(""R-101-FPN"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""FBNet.roi_head_keypoints"") @amp.float_function @C2_FORMAT_LOADER.register(""R-50-C5"") @C2_FORMAT_LOADER.register(""R-101-FPN"") @registry.ROI_BOX_PREDICTOR.register(""FastRCNNPredictor"") @registry.BACKBONES.register(""R-50-C5"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPNXconv1fcFeatureExtractor"") @registry.ROI_BOX_PREDICTOR.register(""FPNPredictor"") @registry.ROI_KEYPOINT_PREDICTOR.register(""KeypointRCNNPredictor"") @registry.BACKBONES.register(""R-50-FPN"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPN2MLPFeatureExtractor"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""FBNet.roi_head_mask"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNC4Predictor"") @C2_FORMAT_LOADER.register(""R-101-C4"") @registry.BACKBONES.register(""R-101-C5"") @property @once_differentiable @staticmethod @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FBNet.roi_head"") @registry.BACKBONES.register(""R-101-C4"") @C2_FORMAT_LOADER.register(""R-50-C4"") @registry.RPN_HEADS.register(""FBNet.rpn_head"") @registry.BACKBONES.register(""R-101-FPN-RETINANET"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""KeypointRCNNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-101-FPN-RETINANET"") @registry.BACKBONES.register(""R-152-FPN"") @registry.BACKBONES.register(""R-50-C4"") @registry.BACKBONES.register(""FBNet"") @registry.BACKBONES.register(""R-50-FPN-RETINANET"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""MaskRCNNFPNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-152-FPN"") @registry.RPN_HEADS.register(""SingleConvRPNHead"") @C2_FORMAT_LOADER.register(""R-50-FPN"") @C2_FORMAT_LOADER.register(""R-101-C5"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNConv1x1Predictor"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""ResNet50Conv5ROIFeatureExtractor"")","PIL copy sys glob tqdm __future__ torchvision os, sys csHelpers re datetime collections json h5py tempfile torch os importlib apex scipy itertools xml bisect imp pickle argparse cityscapesscripts numpy time math pycocotools setuptools maskrcnn_benchmark errno cv2 logging random yacs",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/choyingw_GAIS-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\choyingw_GAIS-Net.pdf,,,no,,,,no,,,,
1311,https://github.com/ppope/dimensions,ppope_dimensions.tar.gz,THE INTRINSIC DIMENSION OF IMAGES AND ITS IMPACT ON LEARNING,,"utils gc PIL pandas warnings sys types glob imagehash __future__ torchvision accimage Cython json collections sys, argparse h5py datetime data torch os estimators matplotlib scipy functools pickle sklearn argparse numbers numpy gsp time math setuptools random pytorch_pretrained_biggan dataloader cPickle models",cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ppope_dimensions.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ppope_dimensions.pdf,,,no,,,,no,,,,
1313,https://github.com/JunMa11/ADAM2020,JunMa11_ADAM2020.tar.gz,Loss Ensembles for Extremely Imbalanced Segmentation,@abstractmethod @property @staticmethod,requests PIL copy pandas multiprocessing sys nibabel skimage tqdm io __future__ ast medpy SimpleITK datetime collections json unittest h5py tempfile nnunet torch os pkgutil hiddenlayer matplotlib cremi importlib scipy apex itertools functools hashlib _warnings urllib pickle http sklearn argparse dicom2nifti typing batchgenerators numpy time shutil abc math setuptools subprocess unittest2 meddec inspect zipfile,eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/JunMa11_ADAM2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\JunMa11_ADAM2020.pdf,Adam,,no,,,,no,,,,
1314,https://github.com/allenxiangx/snowflakenet,allenxiangx_snowflakenet.tar.gz,SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer,@unique @classmethod @staticmethod,utils warnings sys glob config_pcn tqdm easydict pprint core chamfer_3D datetime json open3d h5py enum torch os matplotlib transforms3d importlib pointnet2_ops Chamfer3D typing argparse tensorboardX numpy time math setuptools config_c3d random logging cv2 mpl_toolkits mc models io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/allenxiangx_snowflakenet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\allenxiangx_snowflakenet.pdf,,,no,,,,no,,,,
1319,https://github.com/bhargavajs07/Packed_WGAN_GP_Example,bhargavajs07_Packed_WGAN_GP_Example.tar.gz,PacGAN: The power of two samples in generative adversarial networks,,__future__ numpy matplotlib torch,cs.LG stat.ML stat.ML cs.LG cs.LG cs.IT math.IT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bhargavajs07_Packed_WGAN_GP_Example.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bhargavajs07_Packed_WGAN_GP_Example.pdf,Residual Connection  Average Pooling  1x1 Convolution  Layer Normalization  Max Pooling  Global Average Pooling  Bottleneck Residual Block  Residual Block  Kaiming Initialization  Residual Network  Adam  RMSProp  Rectified Linear Units  WGAN-GP Loss  Leaky ReLU  Convolution  Batch Normalization  Wasserstein GAN (Gradient Penalty)  Convolution  Wasserstein GAN,"Challenges in training GANsThe literature on GANs has documented three primary, closely-related challenges: (i) they are unstable to train, (ii) they are challenging to evaluate, and (iii) they exhibit mode collapse (more broadly, they do not generalize). Much research has emerged in recent years addressing these challenges. Our work explicitly addresses the challenge (iii). We give a brief overview of the related work on each of these challenges, and its relation to our work.Training instability. GANs' alternating generator and discriminator updates can lead to significant instability during training. This instability manifests itself as oscillating values of the loss function that exceed variations caused by minibatch processing [18]. Such variability makes it challenging to evaluate when training has converged, let alone which model one should choose among those obtained throughout the training process. This phenomenon is believed to arise because in practice, the learned distribution and the true distribution lie on disjoint manifolds in a highdimensional space [1]. As such, the discriminator can often learn to perfectly distinguish generated and real samples. On real data, the discriminator (correctly) learns to output '1', and vice versa on generated data. This is believed in GAN literature to cause the generator loss function to have a negligible gradient, leading to unstable parameter updates. In our work, we do not explicitly tackle instability; our theoretical results assume an optimal discriminator and infinite data samples. However, a better understanding of instability is of both practical and theoretical interest, albeit orthogonal to the question of packing.Fundamentally, it is not well-understood why training instability occurs. In [38], Li et al. take a step towards theoretically understanding GAN training dynamics, suggesting that optimization techniques are partially responsible for instability. [38] shows that for a toy distribution model (i.e., a Gaussian mixture), GANs using optimal discriminators are able to learn the underlying distribution, whereas first-order optimization methods exhibit oscillatory dynamics. Empirically, this observation seems to hold even with improved GAN optimization techniques, such as unrolled GANs [46], despite recent work showing that gradient-descent-based optimization of GANs is locally stable [50].Building on this intuition, several papers have proposed methods for mitigating instability, generally taking one of two approaches. The first relies on changing the optimization objective function. Regular GANs optimize the Jensen-Shannon divergence between the true distribution and the learned one [19]. Jensen-Shannon divergence can behave poorly in regions where the two distributions have nonoverlapping support [1], so other works have proposed alternative distance metrics, including Wasserstein distance [1], f-divergences [53,52], asymmetric KL divergences [51], kernel maximum mean discrepancy for two-sample testing [37], and neural network distance [2]. In practice, challenges can arise from trying to approximate these distance metrics; for instance, [1] uses the Kantorovich-Rubinstein dual to compute the Wasserstein-1 distance, which requires optimization over the set of 1-Lipschitz functions. This constraint is approximated in [1] by clipping the critic weights, which can lead to artificially stunted critic functions-a fact that was later tackled by using gradient regularization in WGAN-GP [20]. In a similar vein, [39] altered the objective function by transforming the discriminator optimization into its dual form, which improves stability as we have only minimizations in both the outer and inner optimizations.Another method of changing the objective function is to introduce regularization. In a sense, the fact that generators and discriminators are not trained to completion in practice is a simple form of regularization [19]. Recently, other innovative choices for regularizations have been proposed, including weight clipping [1], gradient regularization [20,45,50], Tikhonov regularizer for trainingwith-noise [58], adding noise to the gradient [21], and spectral-norm regularization [49].A conceptually different approach for improving training stability is to propose architectural changes that empirically improve training stability. For example, Salimans et al. proposed a number of heuristic tricks for improving the training of GANs, including minibatch discrimination, reference batch normalization, and feature mapping [60]. Our work most closely resembles minibatch discrimination from [60], which also inputs multiple images to the discriminator. We provide a detailed comparison between this proposed minibatch discriminator and ours later in this section.Evaluation Techniques. Generative models (including GANs) are notoriously difficult to evaluate. Ideally, one would measure the distance between the true distribution and the learned one. However, typical generative models can only produce samples from a learned distribution, and on real datasets, the true distribution is often unknown. As such, prior work on GANs has used a number of heuristic evaluation techniques.The most common evaluation technique is visual inspection. Many papers produce a collection of generated images, and compare them to the underlying dataset [19,55,18], or ask annotators to evaluate the realism of generated images [60]. This approach can be augmented by interpolating between two points in the latent space and illustrating that the GAN produces a semantically meaningful interpolation between the generated images [15]. This approach is useful to the extent that some GANs produce visually unrealistic images, but it is expensive, unreliable, and it does not help identify generalization problems [66]. The most common attempt to estimate the visual quality of an image is the inception score, which describes how precisely a classifier can classify generated images, thereby implicitly capturing some measure of image quality [60]; this has become a de facto evaluation technique for GANs trained on realistic data [60,17,20,49].Another common approach involves estimating the likelihood of a holdout set of test data under the learned distributions. The learned distribution is estimated using a standard kernel density estimator (KDE) [72]. However, KDEs are known to have poor performance in high dimensions, and in practice, the error in KDE is often larger than the distance between real and learned distributions [72]. Hence, it is unclear how meaningful such estimates are. One proposed approach uses annealed importance sampling (AIS) instead of KDE to estimate log-likelihoods [72], with significantly increased accuracy levels.An increasing number of papers are using classification-based evaluation metrics. Naively, GANs trained on labelled datasets can pass their outputs through a pre-trained classifier. The classifier outputs indicate which modes are represented in the generated samples [15,60,63]. This is useful for measuring the first type of mode collapse (missing modes), but it cannot reveal the second type (partial collapse within a mode). To provide a more nuanced view of the problem, [61] recently proposed a more general classification-based evaluation metric, in which they train a classifier on generated data and real data, and observe differences in classifier performance on a holdout set of test data. While this approach does not directly evaluate partial mode collapse, it is more likely to implicitly measure it by producing weaker classifiers when trained on generated data. On datasets that are not labelled, some papers have relied on human classification, asking human annotators to 'discriminate' whether an image is real or generated [13].In a recent work in [57], it was empirically shown that Gaussian mixture models (GMM) can also generates realistic samples if trained efficiently, although the images are not as sharp as GAN generated samples. However, trained GMMs do not suffer from mode collapse, capture the underlying distribution more faithfully, and provide interpretable representation of the statistical structures. One of the main contribution is a new evaluation technique. The domain of the samples is partitioned into bins in a data dependent manner on the training data. The histograms of the training data and the generated data on the bins are compared to give a measure on how close those two distributions are.Mode Collapse/Generalization. Mode collapse collectively refer to the phenomenon of lack of divergence in the generated samples. This includes trained generators assigning low probability mass to significant subsets of the data distribution's support, and hence losing some modes. This also includes the phenomenon of trained generators mapping two latent vectors that are far apart to the same or similar data samples. Mode collapse is a byproduct of poor generalization-i.e., the generator does not learn the true data distribution; this phenomenon is a topic of recent interest [2,4]. Prior work has observed two types of mode collapse: entire modes from the input data are missing from the generated data (e.g., in a dataset of animal pictures, lizards never appear), or the generator only creates images within a subset of a particular mode (e.g., lizards appear, but only lizards that are a particular shade of green) [18,68,4,14,46,56]. These phenomena are not well-understood, but a number of explanatory hypotheses have been proposed:1. The objective function is ill-suited to the problem [1], potentially causing distributions that exhibit mode collapse to be local minima in the optimization objective function.2. Weak discriminators cannot detect mode collapse, either due to low capacity or a poorlychosen architecture [46,60,2,38].3. The maximin solution to the GAN game is not the same as the minimax solution [18].The impact and interactions of these hypotheses are not well-understood, but we show in this paper that a packed discriminator can significantly reduce mode collapse, both theoretically and in practice. In particular, the method of packing is simple, and leads to clean theoretical analyses. We compare the proposed approach of packing to three main approaches in the literature for mitigating mode collapse:(1) Joint Architectures. The most common approach to address mode collapse involves an encoder-decoder architecture, in which the GAN learns an encoding G −1 (X) from the data space to a lower-dimensional latent space, on top of the usual decoding G(Z) from the latent space to the data space. Examples include bidirectional GANs [15], adversarially learned inference (ALI) [14], and VEEGAN [63]. These joint architectures feed both the latent and the high-dimensional representation of each data point into the discriminator: {(Z i , G(Z i ))} for the generated data and {(G −1 (X i ), X i )} for the real data. In contrast, classical GANs use only the decoder, and feed only high-dimensional representations into the discriminator. Empirically, training these components jointly seems to improve the GAN performance overall, while also producing useful feature vectors that can be fed into downstream tasks like classification. Nonetheless, we find experimentally that using the same generator architectures and discriminator architectures, packing captures more modes than these joint architectures, with significantly less overhead in the architecture and computation. Indeed, recent work shows theoretically that encoder-decoder architectures may be fundamentally unable to prevent mode collapse [3].(2) Augmented Discriminators. Several papers have observed that discriminators lose discriminative power by observing only one (unlabeled) data sample at a time [18,60]. A natural solution for labelled datasets is to provide the discriminator with image labels. This has been found to work well in practice [9], though it does not generalize to unlabelled data. A more general technique is minibatch discrimination [60]. Like our proposed packing architecture, minibatch discrimination feeds an array of data samples to the discriminator. However, unlike packing, minibatch discrimination proposed in [60] is complicated both computationally and conceptually, and highly sensitive to the delicate hyper-parameter choices. At a high level, the main idea in minibatch discrimination is to give the discriminator some side information coming from a minibatch, and use it together with each of the individual examples in the minibatch to classify each sample. The proposed complex architecture to achieve this goal is as follows.Let f (X i ) denote a vector of (latent) features for input X i produced by some intermediate layer in the discriminator. A tensor T is learned such that the tensor product T [I, I, f (X i )] gives a latent matrix representation M i of the input X i . The notation T [I, I, f (X i )] indicates a tensor to matrix linear mapping, where you take the third dimension and apply a vector f (X i ). The L 1 distance across the rows of the M i 's are computed for each pair of latent matrices in the minibatch to give a measure c b (X i , X j ) = exp(− M i,b − M j,b L 1 )). This minibatch layer outputs o(X i ) b = n j=1 c b (X i , X j ). This is concatenated with the original latent feature f (X i ) to be passed through the upper layers of the discriminator architecture. While the two approaches start from a similar intuition that batching or packing multiple samples gives stronger discriminator, the proposed architectures are completely different. PacGAN is easier to implement, quantitatively shows strong performance in experiments, and is principled: our theoretical analysis rigorously shows that packing is a principled way to use multiple samples at the discriminator.More recently, a breakthrough in training GANs was achieved in [29]. By progressively training GANs of increasing resolutions, the authors were able to train, for the first time, on high quality CelebA datasets with size 1024 × 1024. This produces by far the most realistic looking faces. One of the main innovations in the paper is to compute a new feature ""minibatch std"" that intuitively captures how diverse the minibatch is, and to append it to the rest of your features for the discriminator to see. This is a much simpler way of capturing minibatch statistics, that resolves the issue of sensitivity to hyperparameter tuning of the original minibatch idea of [60].(3) Optimization-based solutions. Another potential source of mode collapse is imperfect optimization algorithms. Exact optimization of the GAN minimax objective function is computationally intractable, so GANs typically use iterative parameter updates between the generator and discriminator: for instance, we update the generator parameters through k 1 gradient descent steps, followed by k 2 discriminator parameter updates. Recent work has studied the effects of this compromise, showing that iterative updates can lead to non-convergence in certain settings [38]-a worse problem than mode collapse. Unrolled GANs [46] propose a middle ground, in which the optimization takes k (usually five) gradient steps into account when computing gradients. These unrolled gradients affect the generator parameter updates by better predicting how the discriminator will respond. This approach is conjectured to spread out the generated samples, making it harder for the discriminator to distinguish real and generated data. The primary drawback of this approach is computational cost; packing achieves better empirical performance with smaller computational overhead and training complexity.",no,,,,no,,,,
1326,https://github.com/biomedia-mira/istn,biomedia-mira_istn.tar.gz,Image-and-Spatial Transformer Networks for Structure-Guided Image Registration,,math pandas pymira yaml SimpleITK json attrdict tqdm torch argparse os tensorboardX numpy matplotlib torchvision,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/biomedia-mira_istn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\biomedia-mira_istn.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,,no,,,supplementary material not available,no,,,,
1327,https://github.com/CrawlScript/tf_geometric,CrawlScript_tf_geometric.tar.gz,Efficient Graph Deep Learning in TensorFlow with tf_geometric,@classmethod @tf_utils.function @property,warnings sys multiprocessing tf_geometric ogb_lite tqdm tensorflow json os sphinx_rtd_theme scipy pickle sklearn numpy time shutil tf_sparse setuptools networkx,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CrawlScript_tf_geometric.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CrawlScript_tf_geometric.pdf,,,no,,,,no,,,,
1328,https://github.com/jspenmar/SAND_features,jspenmar_SAND_features.tar.gz,Scale-Adaptive Neural Dense Features: Learning via Hierarchical Context Aggregation,@classmethod @property @staticmethod,utils pathlib imageio warnings sys losses torch argparse os sklearn numpy models matplotlib time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jspenmar_SAND_features.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jspenmar_SAND_features.pdf,,,no,,no,,no,,,,
1329,https://github.com/kindredresearch/arp,kindredresearch_arp.tar.gz,Autoregressive Policies for Continuous Control Deep Reinforcement Learning,@contextmanager,common envs tensorflow baselines rl_experiments contextlib setuptools collections arp ar_ppo sys ar_trpo numpy argparse mpi4py matplotlib time gym,cs.LG cs.AI cs.RO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kindredresearch_arp.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kindredresearch_arp.pdf,,,no,,yes,"batch size, step size, epochs, clip, hidden layers, hidden sizes",no,,,,
1330,https://github.com/gpernelle/spindles_with_gap_junctions,gpernelle_spindles_with_gap_junctions.tar.gz,Gap Junction Plasticity can Lead to Spindle Oscillations,,fns tensorflow numpy matplotlib time,q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gpernelle_spindles_with_gap_junctions.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gpernelle_spindles_with_gap_junctions.pdf,,,no,,,,no,,,,
1331,https://github.com/johannbrehmer/goldmine,johannbrehmer_goldmine.tar.gz,Mining gold from implicit models to improve likelihood-free inference,,scipy goldmine re itertools sys collections logging pickle autograd argparse os torch sklearn __future__ numpy inspect,stat.ML cs.LG hep-ph physics.data-an,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/johannbrehmer_goldmine.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\johannbrehmer_goldmine.pdf,,"|) allows us to extract sufficient statistics from an intractable, non-differentiable simulator, at least in the neighborhood of θ ref .Moreover, this local model can be estimated by running the simulator at a single value θ ref -it does not require scanning the θ space, and thus avoids the curse of dimensionality. Based on this observation, we introduce two further inference strategies: SALLY (Score Approximates Likelihood LocallY): By minimizing the squared error with respect to the joint score, see Eq. (8), we train a score estimator t(x||θ ref ). In a next step, we estimate the density p( t(x||θ ref )||θ) through standard multivariate density estimation techniques. This calibration procedure implicitly includes the effect of the normalizing constant Z(θ).|",no,,,,no,,,,
1339,https://github.com/DearCaat/ioplin,DearCaat_ioplin.tar.gz,An Iteratively Optimized Patch Label Inference Network for Automatic Pavement Disease Detection,,ioplin PIL keras tensorflow sys setuptools random cv2 sklearn argparse os efficientnet numpy io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DearCaat_ioplin.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DearCaat_ioplin.pdf,,,no,,,,no,,,,
1344,https://github.com/soorya19/sparsity-based-defenses,soorya19_sparsity-based-defenses.tar.gz,Sparsity-based Defense against Adversarial Attacks on Linear Classifiers,@property,tqdm __future__ sp_func_nn tensorflow collections os six mnist_workaround gzip sklearn sp_func_svm numpy sp_func_cnn shutil locally_linear_attacks mnist_data_pair attacks pywt models cleverhans,stat.ML cs.IT cs.LG math.IT stat.ML cs.IT cs.LG math.IT stat.ML cs.IT cs.LG math.IT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/soorya19_sparsity-based-defenses.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\soorya19_sparsity-based-defenses.pdf,,,no,,,,no,,,,
1345,https://github.com/smallflyingpig/speech-to-image-translation-without-text,smallflyingpig_speech-to-image-translation-without-text.tar.gz,Direct Speech-to-Image Translation,@abstractmethod @staticmethod @torch.no_grad() @property,utils PIL speech_encoder copy pandas yaml sys multiprocessing glob tqdm easydict __future__ pprint torchvision re model datetime json collections Audio_to_Image h5py torch os caffe jel trainer matplotlib string six scipy functools lmdb pickle miscc argparse tensorboardX numpy time abc math datasets librosa aip dateutil errno random logging StackGAN_v2 cPickle,cs.MM cs.SD eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/smallflyingpig_speech-to-image-translation-without-text.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\smallflyingpig_speech-to-image-translation-without-text.pdf,,,no,,,,no,,,,
1348,https://github.com/gomezalberto/pretus,gomezalberto_pretus.tar.gz,PRETUS: A plug-in based platform for real-time ultrasound imaging research,@staticmethod,"utils PIL csv sys nibabel pdb skimage tqdm __future__ pprint platform torchvision SimpleITK datetime json collections h5py torch os torchsample inspect, re spd_utils scipy functools os, sys, numpy sklearn argparse numbers numpy time math setuptools standardplanedetection_worker cv2 dataio models",physics.med-ph cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gomezalberto_pretus.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gomezalberto_pretus.pdf,,,no,,,,no,,,,
1349,https://github.com/pidahbus/deepCBIR,pidahbus_deepCBIR.tar.gz,CBIR USING FEATURES DERIVED BY DEEP LEARNING,"@app.route(""/"") @app.route(""/retrieve"", methods=[""GET"", ""POST""]) @app.after_request @app.route('/uploads/<path:filename>')",werkzeug PIL tensorflow pandas loguru glob app tqdm os numpy flask matplotlib,cs.IR cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pidahbus_deepCBIR.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pidahbus_deepCBIR.pdf,Convolution,,no,,,,no,,,,
1356,https://github.com/lucidrains/STAM-pytorch,lucidrains_STAM-pytorch.tar.gz,"An Image is Worth 16x16 Words, What is a Video Worth?",,einops setuptools stam_pytorch torch,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lucidrains_STAM-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lucidrains_STAM-pytorch.pdf,Contrastive Language-Image Pre-training  Gaussian Error Linear Units  3D Convolution  Convolution,,no,,,,no,,,,
1358,https://github.com/PruneTruong/PDCNet,PruneTruong_PDCNet.tar.gz,Learning Accurate Dense Correspondences and When to Trust Them,@wraps(func) @preserve_channel_dim @model_constructor @wraps(f) @property @staticmethod @cupy.util.memoize(for_each_device=True),utils_flow gc imageio copy PIL pandas warnings sys cupy glob types skimage tqdm termcolor __future__ psutil torchvision pathlib third_party re uuid collections json h5py validation torch os jpeg4py model_selection matplotlib importlib scipy functools admin training pickle argparse numbers tensorboardX numpy time shutil math datasets pycocotools traceback random cv2 moviepy subprocess pydegensac utils_data models inspect,cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/PruneTruong_PDCNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\PruneTruong_PDCNet.pdf,,"|Implementation DetailsWe adopt the recent GLU-Net-GOCor [63,64] as our base architecture. It consists in a four-level pyramidal network operating at two image resolutions and employing a VGG-16 network [5] pre-trained on ImageNet for feature extraction. At each level, we add our uncertainty decoder (Sec. 3.3) and propagate the uncertainty prediction to the next level. We model the probability distribution p (y||ϕ) with a constrained mixture (Sec. 3.2) with M = 2 Laplace components, where the first is fixed to σ 2 1 = β − 1 = β + 1 = 1 to represent the very accurate predictions, while the second models larger errors and outliers, as2 = β − 2 ≤ σ 2 2 ≤ β + 2 , where β +2 is set to the square of the training image size. Our training consists of two stages. First, we follow the self-supervised training procedure of [63,64]. Random homography transformations are applied to images compiled from different sources to ensure diversity. For better compatibility with real 3D scenes and moving objects, the data is further augmented with random independently moving objects from the COCO [36] dataset. We further apply our perturbation strategy described in Sec. 3.4. In the second stage, we extend the self-supervised data with real image pairs with sparse ground-truth correspondences from the MegaDepth dataset [34]. We additionally finetune the backbone feature extractor. For fair comparison, we also train a version of GLU-Net-GOCor, denoted GLU-Net-GOCor*, using the same settings and data.For datasets with very extreme geometric transformations, we also report using a multi-scale strategy. In particular, we extend our two-stage refinement approach (Sec. 3.5) by resizing the reference image to different resolutions. The resulting image pairs are passed through the network and we fit a homography for each pair, using our predicted flow and uncertainty map. We select the homography with the highest percentage of inliers, and scale it to the images original resolutions. The original image pair is then coarsely aligned and from there we follow the same procedure, as explained in Sec. 3.5. We refer to this option as Multi Scale (MS). C.4. Inference multi-scaleWe then give additional details about our multi-scale strategy (MS). We extend our two-stage refinement approach (Sec. 3.5) by resizing the reference image to different resolutions. Specifically, following [53], we use seven scales: 0.5, 0.88, 1, 1.33, 1.66 and 2.0. As for the implementation, to avoid obtaining very large input images (for scaling ratio 2 for example), we use the following scheme: we resize the reference image for scaling ratios below 1, keeping the aspect ratio fixed and the query image unchanged. For ratios above 1, we instead resize the query image by one divided by the ratio, while keeping the reference image unchanged. This ensures that the resized images are never larger than the original image dimensions. The resulting image pairs are then passed through the network and we fit a homography for each pair, using our predicted flow and uncertainty map. In particular, as in our two-stage inference strategy, we select matches with a corresponding confidence probability P R=1 superior to 0.1, for R = 1, at the estimated flow resolution, i.e. at a quarter of the input image resolution. To estimate the homography, we use OpenCV's findHomography with RANSAC and an inlier threshold of 1 pixel. From all image pairs with their corresponding scaling ratios, we then select the homography with the highest percentage of inliers, and scale it to the images original resolutions. The original image pair is then coarsely aligned using this homography and from there we follow the same procedure, as explained in Sec. 3.5.|",no,,,,no,,,,
1360,https://github.com/mruffini/SpectralMethod,mruffini_SpectralMethod.tar.gz,A New Spectral Method for Latent Variable Models,,RandomGenerator scipy OtherMethods itertools seaborn sktensor sklearn numpy matplotlib time SpectralMethod,stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mruffini_SpectralMethod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mruffini_SpectralMethod.pdf,Linear Discriminant Analysis,,no,,,,no,,,,
1361,https://github.com/phibenz/double-targeted-uap.pytorch,phibenz_double-targeted-uap.pytorch.tar.gz,Double Targeted Universal Adversarial Perturbations,,"utils csv os, sys, time, random, copy sys glob skimage __future__ torchvision imagenet_utils dataset_utils os, shutil, time json collections os, sys, time, random torch os matplotlib itertools os, shutil urllib networks sklearn argparse numpy config shutil math cv2",cs.LG cs.CR cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/phibenz_double-targeted-uap.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\phibenz_double-targeted-uap.pytorch.pdf,,,no,,,,no,,,,
1362,https://github.com/FMZennaro/Fair-Pooling-Causal-Models,FMZennaro_Fair-Pooling-Causal-Models.tar.gz,Counterfactually Fair Prediction Using Multiple Causal Models,,scipy tensorflow edward numpy matplotlib,cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/FMZennaro_Fair-Pooling-Causal-Models.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\FMZennaro_Fair-Pooling-Causal-Models.pdf,,,no,,,,no,,,,
1368,https://github.com/ymlasu/PSI-PDE,ymlasu_PSI-PDE.tar.gz,Robust Data-Driven Discovery of Partial Differential Equations under Uncertainties,,scipy pytorchtools torch numpy matplotlib time torchvision,math.NA cs.NA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ymlasu_PSI-PDE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ymlasu_PSI-PDE.pdf,,"IntroductionDespite that many dynamical systems can be well characterized by PDEs derived mathematically/physically from basic principles such as conservation laws, lots of other systems have unclear or elusive underlying mechanisms (e.g., ones in neuroscience, finance, and ecology). Thus, the governing equations are usually empirically formulated [1]. Data-driven physics discovery of dynamical systems gradually became possible in recent years due to the rapid development and extensive application of sensing technologies and computational power [2]. Over the past years, extensive efforts have been devoted into discovering representative PDEs for complex dynamical systems of which limited prior knowledge are available [1][2][3][4].Among all the methods investigated for PDE identification [1][2][3][4][5][6][7][8], sparse regression gains the most attention in recent studies due to its inherent parsimony-promoting advantage. Considering a nonlinear PDE of the general form u t = N (u, u x , u xx , ..., x), in which the subscripts denote partial differentiation with respect to temporal or spatial coordinate(s), N (•) is an unknown expression on the right hand side of the PDE. It is usually a nonlinear function of the spatial coordinate x, the measured quantity u(x, t), and its spatial derivatives u x ,u xx , etc. Given time series measurements of u at certain spatial locations, the above equation can be approximated as U t = Θ(U)ξ, in which U t is the discretized form of u t , Θ(U) is a library matrix with each column corresponding to a candidate term in N (•). A key assumption in sparse identification is that N (•) consists of only a few term for a real physical system, which requires the solution of regression (i.e., ξ) to be a sparse vector with only a limited number of nonzero elements. This assumption promotes a parsimonious form of the learned PDE instead of overfitting the measured data with a complex model containing redundant nonlinear higher-order terms.As pioneering researchers in sparse PDE learning, Rudy et al. [1,9] modified the ridge regression method by imposing hard thresholding which recursively eliminates certain terms with coefficient values below a learned threshold. As pointed out in Limitations of [1,9] (Section 4 in Supplementary Materials) and following studies [4,10,11], the identification quality is very sensitive to data quantity and quality. For example, the terms of the reaction diffusion equation cannot be correctly identified using the data with only 0.5% random noise. Furthermore, as indicated in [12], the identification results using this method are susceptible to the selection of hyperparameters of the algorithm, including the regularizer λ and the initial tolerance which is also the tolerance increment d tol . The hyperparameter tuning is especially critical for cases with noisy measurements. This limitation most probably comes from the hard thresholding in the modified algorithm (STRidge). A hard thresholding tends to suppress small coefficients that may not correspond to the most trivial terms of the intermediately learned PDEs.To overcome the challenge of numerical differentiation with scarce and noisy data in sparse regression methods, deep learning techniques were incorporated by generating a large quantity of meta-data and adopting the automatic differentiation function in deep learning frameworks (Tensorflow, PyTorch, etc.) [13,14]. The intermediately learned PDE can be treated as a physics loss term in physics-informed deep learning [15][16][17], and constrained neural networks were developed to improve the performance of PDE identification recursively [7,10,11]. Long et al. [2,18] used a convolutional architecture and symbolic regression to replace the numerical differentiation and sparse regression procedures, respectively. A comprehensive review of the state of the art of PDE learning can be found in [10]. Despite improved performance of PDE identification using these methods, the identification results (both PDE forms and coefficients) are lacking robustness in most studies mentioned above. For example, approaches using constrained neural networks introduced more hyperparameters into the algorithms in addition to those in the used STRidge algorithm, which further increases the challenge of identifying the correct PDE forms since PDE learning problems are sensitive to the hyperparameter tuning. This issue is amplified under noisy data, especially under high noise levels. Complete different identification results may be obtained under different noise levels using same hyperparameter settings. Thus, a sound robust PDE learning needs to produce stable identification results with respect to different noise levels.Considering the gaps of existing studies in discovering PDEs from complex dynamical systems, a robust method for correctly identifying PDEs is needed to discover the underlying physics of the measured systems that lack prior knowledge of the governing principles. Thus, this study attempts to develop a robust method of PDE identification within the framework of sparse regression. The key idea is to address both sparsity and accuracy of the learned PDE. Special focus is on the automatic and progressive selection of learned PDE forms without complex algorithms with hard-to-tune hyperparameters [3]. The proposed scheme automatically promotes sparsity in addition to simplicity of the learned model. Finally, the representativeness of each model will be further evaluated by solving its corresponding PDE with given/extracted initial and/or boundary conditions. The coefficients of each term are optimized by minimizing the error of model prediction with the measured data taken as the ground truth. In this way, the PDE that is most likely to represent the intrinsic mechanisms underlying the observed system will be determined. Since the proposed methodology progressively yields a sparse identification of the governing PDE(s) of a given system, it is named the progressive and sparse identification method of PDEs (PSI-PDE or ψ-PDE method).The remaining part of this paper is structured as follows. Section 2 establishes the framework of the ψ-PDE method; section 3 presents and discusses the results of discovering govern equations for a variety of dynamical systems using the ψ-PDE method; section 4 concludes this study with remarks and recommendations for future work. Methodology: a robust PDE learning methodFigure 1 illustrates the framework of the proposed ψ-PDE method for discovering the governing equations using measured data from a dynamical system. This framework starts from the red noisy curve on the upper left corner, which denotes the measured signals containing all the information one can directly obtain from the instrumented system. For preprocessing the measured data, a neural network (NN) model is built following the practices in [13,14], setting the independent variables (i.e., t, x, etc.) as inputs and the measured quantity (e.g., u) as the output. The measured data are split into training and validation sets and the early stopping strategy is devised in the model training to prevent the NN model from overfitting the noise components contained in the measured data. A smoothed series of signals is expected from this preprocessing, which will be subsequently used to calculate the numerical derivatives (i.e., U t , U x , U xx , etc.) and then construct the library matrix Θ(U) for sparse regression. Numerical methods such as finite difference method (FMD) and polynomial interpolation are used to calculate the temporal and spatial derivatives. With U t and Θ(U) established, to further reduce the influence of noise in sparse regression, fast Fourier transform (FFT) is applied to transform U t and Θ(U) to their frequency domain counterparts U t and Θ( U), respectively. Following FFT, a frequency cutoff is implemented to preserve only the low frequency components that are expected to be less susceptible to noise. Moreover, this step converts the regression problem from the temporal-spatial domain to the frequency domain, which does not change the form of learned PDEs [19].With U t and Θ( U) from FFT with frequency cutoff, sparse regression is conducted in a progressive manner. Algorithms that automatically yield a sparse solution by imposing 1 norm regularization (such as LASSO) or hard thresholding (such as STRidge) are not adopted to avoid the lemma of hyperparameter tuning in PDE learning. Instead, a least squares regression is implemented via x = A\b in MATLAB to recursively examine the importance of each term in the prescribed library by evaluating the resulting regression error and model complexity. In this way, the most important term(s) are step-by-step identified and added to the PDE model until the effects of adding more terms diminish. The details of this procedure are elaborated in the ψ-algorithm in Algorithm 1. This algorithm probably yields more than one candidate PDE models that are hard to compare from the perspective of regression accuracy and model complexity. Finally, all candidate PDEs are solved numerically given sufficient initial/boundary conditions, and the solutions (the blue smooth curve at the upper left corner of Figure 1) are compared with the measured data. In this step, the importance of each term can be further evaluated by eliminating certain terms and optimizing their coefficients. The final PDE for the given system is determined as the one capturing the most intrinsic mechanism represented by the essential terms. The rest of this section demonstrates each step of the ψ-PDE method using an example of discovering the Burgers equation from simulated data.  Discovering PDEs for 1D systemsTable 1 compares the results of learning the 1D Burgers equation (u t = −uu x + 0.01 π u xx ) using the PDE-FIND method [1] considering various noise levels and different combinations of hyperparameters. The STRidge algorithm in the PDE-FIND method mainly has two hyperparameters, i.e., the regularizer λ and the tolerance increment/ initial tolerance d tol . From the authors' viewpoint, the hyperparameter tuning can be challenging in PDE learning where the ground truths and noise level are not known a priori. Therefore, a method may not be robust enough if the learning outcome is susceptible to the variation of hyperparameters.In cases with 0% or 10% noise, the PDE-FIND method yields identical PDE though with different hyperparameter combinations, as shown in the last column of Table 1. Comparing the results with the ground truth PDE, one can find that correct terms are identified. However, without the PDE solving/optimization procedure, their coefficients cannot be optimized in this method. When the noise level increases to 20%, the results become very sensitive to the variation of hyperparameters. With a different hyperparameter combination, the identified terms on the right hand side of PDE can be very different. Moreover, at this noise level, the terms of the ground truth PDE cannot be correctly identified though exhaustive trials of hyperparameter adjustment. Considering this lemma of hyperparameter tuning, especially in cases with a large level of noise, a robust method for PDE learning is needed when limited prior information is available for an unknown system.Table 1: Results of PDE learning using the PDE-FIND method in [1] (Burgers equation:u t = −uu x + 0.01 π u xx ). Noise level λ d tol identified PDE 0% - - u t = −0.7616uu x + 0.0114 π u xx 10% - - u t = −0.6758uu x + 0.0099 π u xx 20% 10 −5 1.0 u t = −0.0823uu x − 0.000001uu xxx 20% 10 −1 1.0 u t = −0.772u 3 u x + 0.0031u 2 u xx 20% 10 −5 0.1 u t = −0.5069uu x + 0.0034 π u xx + 0.0016u 2 u xx − 0.00003uu xxx + 0.00003u 3 u xxx 20% 10 −1 0.1 u t = −0.772u 3 u x + 0.0031u 2 u xxTable 2 lists the results of learning the Burgers equation (u t = −uu x + 0.01 π u xx ) from noisy data using the proposed ψ-PDE method. Without complex and tricky hyperparameter setting/tuning, the ψ-PDE method yields identically correct PDE form even with data containing up to 50% noise. Moreover, by virtue of the critical PDE solving/optimization step in the ψ-PDE method, the values of coefficients are very close to that of the ground truth PDE. It should be noted that the efficiency of the ψ-PDE method in learning correct PDEs holds beyond the noise levels investigated in this study. Figures 9 (a) to (c) compare the solutions of learned PDEs with the measured noisy data. It can be observed that even in cases with significant noise, the ψ-PDE method never overfits the noise components in the measured data with redundant high-order nonlinear terms. Instead, this method always yields a governing equation that captures the most intrinsic invariants underlying the data.Table 2: Results of PDE learning using the ψ-PDE method (Burgers equation:u t = −uu x + 0.01 π u xx ). noise level identified PDE 0% u t = −0.9887uu x + 0.0088 π u xx 10% u t = −1.0293uu x + 0.0104 π u xx 20% u t = −0.9853uu x + 0.0098 π u xx 50% u t = −0.9758uu x + 0.0098 π u xxThe second example of learning PDE from a 1D dynamical system examines the effectiveness of the ψ-method in correctly identifying governing equations containing higher-order spatial derivatives. This section considers a mathematical model of traveling waves on shallow water surfaces, i.e., the KdV equation with the form u t = λ 1 uu x + λ 2 u xxx . The KdV equation can be used to characterize the evolution of many long 1D waves such as the ion  acoustic waves in a plasma and acoustic waves on a crystal lattice [15]. This study investigates the system described by the following KdV equation: u t = −uu x − 0.0025u xxx with the initial condition u(x, 0) = cos(πx) and periodic boundary conditions.Table 3: Results of PDE learning using the ψ-PDE method (KdV equation:u t = −uu x − 0.0025u xxx ). noise level identified PDE 0% u t = −0.9998uu x − 0.0026u xxx 10% u t = −0.9757uu x − 0.0028u xxx 20% u t = −0.9618uu x − 0.0031u xxx 50% u t = −1.0221.uu x − 0.0029u xxxTable 3 summarizes the results of learning PDEs from the simulated traveling waves containing 0% to 50% noise. It shows that the correct PDE form with accurate coefficients can be identified using the ψ-PDE method in all cases. When implementing the ψ-PDE method, it was noted that the form of the KdV equation is easier to identify than that of the Burgers equation, because the ψ-algorithm for sparse regression does not yield more than one candidate PDE even for very noisy cases. The details can be found by running the code for this example available on the website: https://github.com/ymlasu. Figures 10 and 11 show the measured data and the its snapshot with the solution to the learned PDE at t = 0.8 sec. It can be found that even in case with 50% noise, the ψ-PDE method can discover the intrinsic physics underlying the noisy data. Therefore, this example not only highlights the capability of the ψ-method of learning higher-order spatial derivatives but also further proves its the robustness in extracting the underlying physics from noisy measurements. Summary and Further DiscussionsIn this study, a robust data-driven method (i.e., the ψ-PDE method) is proposed for discovering the underlying physics of a given system from measured data. Investigating and improving its robustness is critical for effectively distilling the intrinsic law underlying a complex novel dynamical system. The ψ-PDE method has been tested on various systems in sufficiently challenging scenarios regarding the model complexity and noise intensity, and the identification results approve its effectiveness and generality. Compared with the state-ofthe-art methods in the literature, the ψ-PDE method is advantageous in its robustness since it requires the least effort in hyperparameter tuning which should be obviated in identifying the governing equation(s) of an unknown system. This work was inspired by the pioneering work presented in [3] which discourages automatic discovery of natural laws. After examining the challenge of automatic identification via sparse regression as presented in [1] and its following works (such as [14] and [10]), this study borrows the ""nonautomatic"" idea in [3] and yields more than one candidate solutions through sparse regression with the ψ-algorithm; finally, the representativeness of all candidate models are evaluated through solving and optimizing their respective PDE forms taking the measured data as the ground truth and objective. It has been demonstrated that the ψ-PDE method always yields equations that capture the most intrinsic physics of the observed system. In comparison, most existing methods yield a unique PDE for a given system and do not allow evaluating its effectiveness in characterizing the system or optimizing its representativeness.While it increases the robustness of PDE learning, the PDE solving/optimization step in the ψ-PDE method considerably increases the computational cost especially for a highdimensional system. However, this challenge can be solved by parallel computing and/or surrogate modeling, which will be investigated in future studies for more complex systems.The future work may also include applying the ψ-PDE method real operating dynamical systems to further examine or improve its effectiveness in knowledge discovery.",no,,,,no,,,,
1371,https://github.com/mostly-ai/paper-fidelity-accuracy,mostly-ai_paper-fidelity-accuracy.tar.gz,HOLDOUT-BASED FIDELITY AND PRIVACY ASSESS-MENT OF MIXED-TYPE SYNTHETIC DATA,,pathlib pandas numpy torch os sdv gretel_synthetics,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mostly-ai_paper-fidelity-accuracy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mostly-ai_paper-fidelity-accuracy.pdf,,,no,,,,no,,,,
1374,https://github.com/Zzh-tju/ultralytics-YOLOv3-Cluster-NMS,Zzh-tju_ultralytics-YOLOv3-Cluster-NMS.tar.gz,Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression,@staticmethod,utils PIL copy sys glob skimage tqdm onnx torchvision threading pathlib json pretrainedmodels torch os matplotlib apex scipy test argparse numpy time shutil thop math pycocotools random cv2 models,cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Zzh-tju_ultralytics-YOLOv3-Cluster-NMS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Zzh-tju_ultralytics-YOLOv3-Cluster-NMS.pdf,Convolution  DIoU-NMS  Non Maximum Suppression  1x1 Convolution  SSD  Convolution  Non Maximum Suppression  1x1 Convolution  SSD  Fast-YOLOv3  Focal Loss  Feature Pyramid Network  Residual Connection  Convolution  Average Pooling  RetinaNet  Logistic Regression  Non Maximum Suppression  Global Average Pooling  1x1 Convolution  Batch Normalization  k-Means Clustering  SSD  Softmax  YOLOv3  Darknet-53,,no,,,,no,,,,
1384,https://github.com/thanosvlo/Deep-Segmentation-and-Registration-in-X-Ray-Angiography-Video,thanosvlo_Deep-Segmentation-and-Registration-in-X-Ray-Angiography-Video.tar.gz,Deep Segmentation and Registration in X-Ray Angiography Video,@staticmethod,PIL scipy keras tensorflow pfm re sys glob cv2 lib natsort os ofToolkit png argparse numpy pyflow matplotlib,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thanosvlo_Deep-Segmentation-and-Registration-in-X-Ray-Angiography-Video.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thanosvlo_Deep-Segmentation-and-Registration-in-X-Ray-Angiography-Video.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,"Siamese U-NetU-Net as proposed in [19] and discussed in Section 3.1 is designed to segment static images. We extend this approach to make use of the temporal information captured by dense motion maps. Inspired by the success of Siamese architecture in vision related tasks such as match- ing two images, we train a Siamese network that analyses pairs of frames from different time instances and enforces temporal consistency between their segmentations.The architecture is illustrated in Figure 3 and consist of two U-Nets that share their weights. At each training iteration the Siamese U-Net takes two frames, f t and f t+∆t , producing segmentations s t and s t+∆t . The dense motion map generated by optical flow between the two frames is used to transform s t+∆t into s t+∆t . In case of errorless optical flow and spatial segmentation, s t and s t+∆t should be identical. The multi-class cross entropy loss is( f t , f t+∆t , y t ) = − ∑ c y t,c [log(s t,c ) + log(s t+∆t,c )](1)where y t,c are automatically generated segmentation labels as discussed in Section 3.2 and index c corresponds to one of the three classes in the segmentation and ground truth masks i.e. background, catheter, and vessels.",no,,,,no,,,,
1385,https://github.com/mit-acl/dc2g,mit-acl_dc2g.tar.gz,Planning Beyond The Sensing Horizon Using a Learned Context,,"PIL imageio copy csv multiprocessing sys glob skimage __future__ base64 pprint gym util gym_airsim tensorflow datetime json collections dc2g dijkstra torch os sys, signal trainer matplotlib scipy itertools optparse policy pickle sklearn argparse tester tensorboardX numpy gym_minigrid time shutil run_episode math House3D setuptools cv2 random misc logging",cs.RO cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mit-acl_dc2g.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mit-acl_dc2g.pdf,,,no,,,"epochs, batch size",no,,,,
1393,https://github.com/HuikaiShao/JPFA,HuikaiShao_JPFA.tar.gz,no,,data_loader loss tensorflow yaml sys collections glob random logging tqdm os numpy models inspect time,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/HuikaiShao_JPFA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\HuikaiShao_JPFA.pdf,,,no,,,,no,,,,
1398,https://github.com/vipyoung/kharita,vipyoung_kharita.tar.gz,Kharita: Robust Map Inference using Graph Spanners,,"getopt math scipy sys datetime collections networkx geopy methods operator methods_kharita sklearn matplotlib numpy time, datetime geojson",cs.OH,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/vipyoung_kharita.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\vipyoung_kharita.pdf,,,no,,,,no,,,,
1401,https://github.com/tufts-ml/graph-generation-vi,tufts-ml_graph-generation-vi.tar.gz,Order Matters: Probabilistic Modeling of Node Sequence for Graph Generation,,"utils pandas multiprocessing tqdm pynauty model datetime collections models tempfile torch os dgl functools itertools bisect pickle argparse numpy time shutil train math datasets networkx os, json random dfscode subprocess args",stat.ML cs.LG cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tufts-ml_graph-generation-vi.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tufts-ml_graph-generation-vi.pdf,,,no,,,"hyperparameter from original paper, ",no,,,,
1407,https://github.com/Kyan-Ge/Semi-supervised-Gan,Kyan-Ge_Semi-supervised-Gan.tar.gz,Improved Techniques for Training GANs,,math Nets sys Datasets pdb functional torch argparse os tensorboardX numpy torchvision,cs.LG cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Kyan-Ge_Semi-supervised-Gan.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Kyan-Ge_Semi-supervised-Gan.pdf,Batch Normalization  Virtual Batch Normalization  GAN Feature Matching  Weight Normalization  Label Smoothing  Minibatch Discrimination  Convolution  Generative Adversarial Network,"SVHNFor the SVHN data set, we used the same architecture and experimental setup as for CIFAR-10.  HA, sample quality is considerably reduced (see ""-L""). ""-LS"" removes label smoothing and incurs a noticeable drop in performance relative to ""our methods."" ""-MBF"" removes the minibatch features and incurs a very large drop in performance, greater even than the drop resulting from removing the labels. Adding HA cannot prevent this problem.",no,,,hyperparameter can be found at supplementary website,no,,,,
1416,https://github.com/EmergentSystemLabStudent/Prosodic-DAA,EmergentSystemLabStudent_Prosodic-DAA.tar.gz,Double Articulation Analyzer with Prosody for Unsupervised Word and Phoneme Discovery,@letter_stateseq.setter @property @stateseq.setter,warnings sys glob pybasicbayes tqdm distutils util tensorflow re Cython os matplotlib scipy urllib sklearn argparse speech_feature_extraction joblib numpy time shutil tarfile pyhlm setuptools traceback DSAE_PBHL pyreaper pyhsmm python_speech_features,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/EmergentSystemLabStudent_Prosodic-DAA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\EmergentSystemLabStudent_Prosodic-DAA.pdf,,,no,,yes,,no,,,,
1418,https://github.com/Ma-Lab-Berkeley/ReduNet,Ma-Lab-Berkeley_ReduNet.tar.gz,DEEP NETWORKS FROM THE PRINCIPLE OF RATE REDUCTION,@ (test_features - mean).T,utils PIL loss pandas functional tqdm torchvision load evaluate redunet plot json torch os matplotlib scipy itertools sklearn argparse numpy opt_einsum time datasets architectures,cs.LG cs.IT math.IT math.OC stat.ML cs.LG cs.CV cs.IT math.IT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Ma-Lab-Berkeley_ReduNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Ma-Lab-Berkeley_ReduNet.pdf,Convolution,,no,,,,no,,,,
1419,https://github.com/q-optimize/c3,q-optimize_c3.tar.gz,"Integrated tool-set for control, calibration, and characterization of quantum devices applied to superconducting qubits","@dev_reg_deco @task_deco @pytest.mark.tensorflow @fid_reg_deco @pytest.mark.qiskit @pytest.mark.unit @pytest.mark.parametrize( @pytest.mark.optimizers @algo_reg_deco @hamiltonian_reg_deco @pytest.mark.slow @pytest.mark.parametrize(""backend"", [""c3_qasm_perfect_simulator""]) @comp_reg_deco @abstractmethod @pytest.mark.skip(reason=""takes way too long"") @pytest.mark.heavy @classmethod @pytest.mark.integration @sampling_reg_deco @pytest.mark.skip(reason=""experimental: to be tested"") @tf.function @pytest.fixture() @estimator_reg_deco @abstractclassmethod",copy warnings qiskit sys types ast re tensorflow cma uuid datetime collections rich json tempfile os pytest c3 scipy itertools hjson adaptive pickle typing argparse numpy time shutil abc math tensorboard setuptools random logging tensorflow_probability inspect,quant-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/q-optimize_c3.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\q-optimize_c3.pdf,,"|D. Characterization, C3In C 3 , we use the data-set D obtained during ORBIT calibration to improve the model of the system. For each measurement result m j,k we compute the equivalent simulation result m j,k (β) by calculating the dynamics of the sequence S k (α j ) given a set of model parameter values β = (ω i , δ i , ...). Since simulating the whole data-set is computationally costly, for the purpose of model learning we make a selection of eight pulse parameter sets j per qubit from the full data-set. Each parameter set includes k = 1, .., 25 sequences, meaning that we learn from a total of N = 400 measurement results, relabeled as m n . We then construct a goal functionf 3 (β) = f LL (D||β) = 1 2N N n=1 m n − m n σ n 2 − 1 (11)that captures how well the model prediction m n , with standard deviation σ n , agrees with the recorded values m n . Because of the finite number of measurements, the averaged m n are noisy estimates of the population, with a mean µ n and standard deviation σ n . Thus, they cannot be matched perfectly even when all model parameters are exact. However, we can determine the expectation value of the goal function f LL in the scenario where all m n are exactly a given number of standard deviations away from the underlying true value µ n . A detailed mathematical discussion is presented in Sec. IV C. To provide a more intuitive measure, we express the match f LL in terms of the number of standard deviations that would result in the same score.To minimize f 3 (β), we use a combination of two algorithms: Gradient-free (CMA-ES) to avoid local minima and gradient-based (L-BFGS) to converge quickly once the right minimum has been identified. Fig. 3 shows the convergence of the C 3 optimization for different models. The simple model is not able to reproduce the device's results, as the optimization ends at approximately 8 standard deviations away. This demonstrates that the experiment on the device includes behavior not captured by the simple model. Spectator effects might be significant even when performing only single qubit experiments, making the completely uncoupled model insufficient. Another source of this inconsistency might be SPAM errors not accounted for in the model, that might play a large role in actual measurement results. The parameter values resulting from this C 3 process and all following ones are shown in Table I.Going forward an informed decision has to be made about how to enhance the model. Since the true values of the parameters are not known in an experimental setting, we require a tool to determine the precision to which they are learned. We estimate the sensitivity to changes of model parameters around the optimal values β by performing one-dimensional scans and observing the degradation in model match score, f LL (D||β + δβ). Fig. 4(a) shows that sweeping the value of frequency of qubit B produces a highly irregular landscape of the match score f LL . The simple model is then extended by adding the static coupling g of unknown exact value, resulting in the intermediate model. When repeating C 3 , we initialize model parameters from the initial, rough values. We do not carry over the learned parameters from the simple model to the intermediate model because, by introducing a coupling, we expect slightly shifted frequencies compared to the bare frequencies of the uncoupled qubits. Nonetheless, convergence of the match score shows no improvement from the simple model, still only reaching within approximately 8 standard deviations from experiment results (Fig. 3) and resulting in a similar sensitivity landscape in Fig. 4(a). This suggests that the simple model is a close dispersive approximation of the intermediate model. Indeed, we observe a dispersive shift [43] of 593 KHz, consistent with the expected g 2 /(ω B − ω A ) 666 KHz, given the coupling of g 20 MHz and the frequency difference ω B − ω A 600 MHz.Finally, model complexity is increased by adding three relevant features: Markovian noise simulated by Lindblad master equation, initialization errors due to finite operating temperature and measurement errors in the form of misclassification. The system model is now of the same structure as the ""real"" model of the device. Starting from the best intermediate model parameters, the C 3 procedure converges satisfactorily, approaching the 0 standard deviations mark (Fig. 3) In Fig. 5(a) we show the value of each parameter of the full model during optimization, as we introduce different learning data (in the next sections), and compare with their true value (dashed lines). By learning the model parameters with the ORBIT data (white left section of each plot) the model frequencies ω A/B , anharmonicities δ A/B , coupling g and line transfer function ϕ 0 converge to their true value. The temperature and misclassification parameters are not recovered, and we believe this is due to an extra degree of freedom that is not resolved by the experiments we have performed, as the effects of misclassification, Eq. ( 4), and initial thermal distribution, Eq. ( 3), are similar and can be partially exchanged. Dephasing and relaxation times (not shown) are also not recovered. Indeed, in Fig. 4(c) we show that the sensitivity of the data to dephasing time T * 2 of qubit B is minimal. RB sequences perform an effective random dynamical decoupling [44], providing a possible explanation to this result.  Closed-loop Model-free Calibration: C2In calibration, a given pulse is optimized to improve a figure of merit f 2 (α), computed from experimental measurement results. In addition to gradient-free optimization algorithms, there are methods to approximate the gradients (e.g. [59]), however, such approaches are generally less efficient than gradient-free algorithms [10? ] as they require a high number of evaluations [60]. If the initial point of the optimization is given by C 1 , this implements the already established Ad-HOC [18] method.During calibration, sets of control parameters α j are sent to the experimental setup, alongside instructions of how to evaluate the current controls. For evaluating gate-sets, we suggest the ORBIT figure of merit, as it naturally performs a twirling of all sources of error, providing a single number to optimize. However, protocols tailored to specific needs can also be used, e.g. to obtain a desired conditional phase [5]. C 2 then optimizes the control parameters α j to minimize a figure of merit.While specialized measurements provide a straightforward way to fine-tune controls related to specific device properties, they do not generally account for interdependency. For more complex setups with many parameters, such calibrations cannot be done without extraordinary effort [61]. In contrast, C 3 employs modern gradient-free optimization algorithms, such as CMA-ES (see App. B for further discussion), capable of optimizing dozens of parameters simultaneously, automating the task. D. Model analysisBoth during and after the learning process, it is beneficial to interrogate the model to estimate its properties and their impact on the system behavior. As part of the C 3 tool-set we perform sensitivity analysis for system parameters: Sweeping a single parameter, e.g. qubit frequency, across the range of interest, while keeping other parameters at their current best value, evaluating the model match score at each point, as seen in the example (Fig. 4(a)). The result is a 1-D cut through the optimization landscape that may exhibit a well-defined minimum, multiple extrema indicating a difficult optimization, or even appear flat in the case when a parameter does not affect the behavior of the current experiment. This landscape depends on both the selected model and data it is compared to. Depending on the ruggedness of the sensitivity, one might choose to utilize a gradient-based algorithm from the start or to first perform a gradient-free exploratory search to avoid local minima. In the case of a flat sensitivity, there are two courses of action: If the parameter is of little interest for successive experiments, it may be removed or set to a convenient value within the flat range; otherwise, one needs to design an experiment producing additional data that is sensitive to the parameter. The physicists' knowledge of common experimental practices (e.g. Rabi, Ramsey, Hahn echo sequences) and intuition guides the decision for the experiment design. When suspecting correlations between parameters, cuts in single dimensions are not enough and higher dimensional sweeps are necessary. After a successful learning process, the sensitivity analysis gives an estimate of the precision to which each parameter has been determined. Furthermore, the simulation allows insight into the behavior of the system. Using well established methods such as time-resolved state and process tomography, it is possible to identify coherent errors, such as leakage out of the computational subspace, over-rotations, and the effects of noise. A Good Model also provides the basis for an error budget, as it contains the same limitations as the experiment it accurately predicts. The model can be used for extrapolation by idealizing certain aspects suspected as causes of infidelity (e.g. T 1 setting to infinity), and re-deriving control pulses using a C 1 optimization. The respective gain in fidelity gives an estimate of the error that this aspect is responsible for, suggesting areas of improvement for future devices. FIG. 4 .4FIG. 4. Sensitivity analysis of selected parameters for different models and data sets. In (a) we sweep the qubit frequency ωB and evaluate the goal function fLL on the learning data at each value. The star represents the optimal value returned by C3. Intermediate (blue, dot-dashed) and Full (red, solid) models, show the same frequency value, while the value of the Simple model (green, dashed) is dispersively shifted, as expected. In (b) and (c) we perform the same sweep for the chip temperature and T * 2 of qubit B respectively, evaluated on the full model for different learning data-sets. The ORBIT data is the same used in the full model in (a). Introducing the Quantum Process Tomography (QPT) data (purple, dotted), allows a more precise definition of the temperature. To determine T * 2 we use relaxation and dephasing data (orange, dot-dashed). Match values below 0 can occur because of noisy data, finite sampling and deviation from the assumption of Gaussian distribution of the data. More sensitivity plots are shown in the Supplemental Material [35].|",no,,,,no,,,,
1427,https://github.com/psc-g/ganterpretation,psc-g_ganterpretation.tar.gz,GANterpretations,,PIL scipy categories IPython tensorflow cStringIO absl cv2 os __future__ numpy tensorflow_hub matplotlib ganterpreter,cs.SD cs.AI cs.LG eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/psc-g_ganterpretation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\psc-g_ganterpretation.pdf,,,no,,,,no,,,,
1431,https://github.com/IssamLaradji/CBStyling,IssamLaradji_CBStyling.tar.gz,Class-Based Styling: Real-time Localized Style Transfer with Semantic Segmentation,@torch.no_grad(),"utils dabnet PIL transformer_net pandas cv2, glob, re cv2 tqdm argparse torch os numpy matplotlib torchvision pylab",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IssamLaradji_CBStyling.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IssamLaradji_CBStyling.pdf,,,no,,,,no,,,,
1435,https://github.com/jonathanventura/cylindricalsfmlearner,jonathanventura_cylindricalsfmlearner.tar.gz,Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video,,utils SfMLearner sys glob tqdm __future__ pprint tensorflow headcam os matplotlib nets scipy tf_cylindrical argparse joblib numpy time math data_loader cv2 random,cs.CV cs.LG cs.RO cs.CV cs.LG cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jonathanventura_cylindricalsfmlearner.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jonathanventura_cylindricalsfmlearner.pdf,Max Pooling  CARLA: An Open Urban Driving Simulator  Max Pooling,,no,,,,no,,,,
1438,https://github.com/Daniil-Osokin/lightweight-human-pose-estimation-3d-demo.pytorch,Daniil-Osokin_lightweight-human-pose-estimation-3d-demo.pytorch.tar.gz,XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera,@staticmethod,math torch2trt pose_extractor setuptools json collections sys openvino cv2 modules operator argparse os torch subprocess numpy models platform,cs.CV cs.CV cs.GR cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Daniil-Osokin_lightweight-human-pose-estimation-3d-demo.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Daniil-Osokin_lightweight-human-pose-estimation-3d-demo.pytorch.pdf,,"INTRODUCTIONOptical human motion capture is a key enabling technology in visual computing and related fields [Chai and Hodgins 2005; Menache 2010; Starck and Hilton 2007]. For instance, it is widely used to animate virtual avatars and humans in VFX. It is a key component of many man-machine interfaces and is central to biomedical motion analysis. In recent years, computer graphics and computer vision researchers have developed new motion capture algorithms that operate on ever simpler hardware and under far less restrictive constraints than before. These algorithms do not require special body suits, dense camera arrays, in-studio recording, or markers. Instead, they only need a few calibrated cameras to capture people wearing everyday clothes outdoors, e.g. Elhayek et al. [2016]; Fang et al. [2018]; Huang et al. [2017a]; Kanazawa et al. [2018]; Mehta et al. [2017b]; Omran et al. [2018]; Pavlakos et al. [2019]; Rhodin et al. [2016b]; Stoll et al. [2011]; Xiang et al. [2019]. The latest approaches leverage the power of deep neural networks to capture 3D human pose from a single color image, opening the door to many exciting applications in virtual and augmented reality. Unfortunately, the problem remains extremely challenging due to depth ambiguities, occlusions, and the large variety of appearances and scenes.More importantly, most methods fail under occlusions and focus on a single person. Some recent methods instead focus on the egocentric setting [Rhodin et al. 2016a;Tome et al. 2019;Xu et al. 2019]. Single person tracking in the outside-in setting (non-egocentric) is already hard and starkly under-constrained; multi-person tracking is incomparably harder due to mutliple occlusions, challenging body part to person assignment, and is computationally more demanding. This presents a practical barrier for many applications such as gaming and social VR/AR, which require tracking multiple people from low cost sensors, and in real time.Prior work on multi-person pose estimation runs at best at interactive frame rates (10-15 fps) [Dabral et al. 2019;Rogez et al. 2019] or offline [Moon et al. 2019], and produces per-frame joint position estimates which cannot be directly employed in many end applications requiring joint angle based avatar animations.We introduce a real-time algorithm for motion capture of multiple people in common interaction scenarios using a single color camera. Our full system produces the skeletal joint angles of multiple people in the scene, along with estimates of 3D localization of the subjects in the scene relative to the camera. Our method operates at more than 30 frames-per-second and delivers state-of-the-art accuracy and temporal stability. Our results are of a similar quality as commercial depth sensing based mocap systems.To this end, we propose a new pose formulation and a novel neural network architecture, which jointly enable real-time performance, while handling inter-person and person-object occlusions. A subsequent model-based pose fitting stage produces temporally stable 3D skeletal motions. Our pose formulation uses two deep neural network stages that perform local (per body joint) and global (all body joints) reasoning, respectively. Stage I is fully convolutional and jointly reasons about the 2D and 3D pose for all the subjects in the scene at once, which ensures that the computational cost does not increase with the number of individuals. Since Stage I handles the already complex task of parsing the image for body parts, as well as associating the body parts to identities, our key insight with regards to the pose formulation is to have Stage I only consider body joints for which direct image evidence is available, i.e., joints that are themselves visible or their kinematic parents are visible. This way Stage I does not have to spend representational capacity in hallucinating poses for joints that have no supporting image evidence. For each visible body joint, we predict the 2D part confidence maps, information for associating parts to an individual, and an intermediate 3D pose encoding for the bones that connect at the joint. Thus, the 3D pose encoding is only cognizant of the joint's immediate neighbours (local) in the kinematic chain. A compact fully-connected network forms Stage II, which relies on the intermediate pose encoding and other evidence extracted in the preceding stage, to decode the complete 3D pose. The Stage II network is able to reason about occluded joints using the full body context (global) for each detected subject, and leverages learned pose priors, and the subject 2D and 3D pose evidence. This stage is compact, highly efficient, and acts in parallel for all detected subjects.Stage I is the most computationally expensive part of our pipeline, and the main bottleneck in achieving real-time performance.We achieve real-time performance by contributing a new convolutional neural network (CNN) architecture in Stage I to speed up the most computationally expensive part of our pipeline. We refer to the new architecture as SelecSLS Net. Our proposed architecture depends on far fewer features than competing ones, such as ResNet-50 [He et al. 2016], without any accuracy loss thanks to our insights on selective use of short and long range concatenation-skip connections. This enables fast inference on the complete input frame, without the added pre-or post-processing complexity of a separate bounding box tracker for each subject. Further, the compactness of our Stage II network, which reconciles the partially incomplete 2D pose and 3D pose encoding to a full body pose estimate, enables it to simultaneously handle many people with minimal overhead on top of Stage I. Additionally, we fit a model-based skeleton to the 3D and 2D predictions in order to satisfy kinematic constraints and reconcile the 2D and 3D predictions across time. This produces temporally stable predictions, with skeletal angle estimates, which can readily drive virtual characters.In summary, our technical innovations and new design insights at the individual stages, as well as our insights guiding the proposed multi-stage design enable our final contribution: a complete algorithm for multi-people 3D motion capture from a single camera that achieves real-time performance without sacrificing reliability or accuracy. The run time of our system only mildly depends on the number of subjects in the scene, and even crowded scenes can be tracked at high frame rates. We demonstrate our system's performance on a variety of challenging multi-person scenes. SELECSLSNET ON IMAGE CLASSIFICATIONTo demonstrate the efficacy of our proposed architecture on tasks beyond 2D and 3D body pose estimation, we train a variant of Selec-SLSNet on ImageNet [Russakovsky et al. 2015], a large scale image classification dataset. The network architecture is shown in Figure 2, and the results are shown in Table 3.",no,,,,no,,,,
1449,https://github.com/ailab-pku/rl-framework,ailab-pku_rl-framework.tar.gz,Trust Region Policy Optimization,,utils memory Actor_network evaluator gym tensorflow Memory_buffer model torch os Critic_network argparse tensorboardX numpy time normalized_env train ddpg math DDPG random,cs.LG cs.LG stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ailab-pku_rl-framework.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ailab-pku_rl-framework.pdf,Trust Region Policy Optimization  Experience Replay  Dense Connections  Weight Decay  Rectified Linear Units  Adam  Convolution  Batch Normalization  Deep Deterministic Policy Gradient  Q-Learning  Entropy Regularization  Proximal Policy Optimization,"Simulated Robotic LocomotionWe conducted the robotic locomotion experiments using the MuJoCo simulator (Todorov et al., 2012). The three simulated robots are shown in Figure 2. The states of the robots are their generalized positions and velocities, and the controls are joint torques. Underactuation, high dimensionality, and non-smooth dynamics due to contacts make these tasks very challenging. The following models are included in our evaluation:1. Swimmer. 10-dimensional state space, linear reward for forward progress and a quadratic penalty on joint effort to produce the reward r(x, u) = v x −10 −5 u 2 . The swimmer can propel itself forward by making an undulating motion.2. Hopper. 12-dimensional state space, same reward as the swimmer, with a bonus of +1 for being in a nonterminal state. We ended the episodes when the hopper fell over, which was defined by thresholds on the torso height and angle.3. Walker. 18-dimensional state space. For the walker, we added a penalty for strong impacts of the feet against the ground to encourage a smooth walk rather than a hopping gait.We used δ = 0.01 for all experiments. See Table 2 in the Appendix for more details on the experimental setup and parameters used. We used neural networks to represent the policy, with the architecture shown in Figure 3, and further details provided in Appendix D. To establish a standard baseline, we also included the classic cart-pole balancing problem, based on the formulation from Barto et al. (1983), using a linear policy with six parameters that is easy to optimize with derivative-free black-box optimization methods.The following algorithms were considered in the comparison: single path TRPO; vine TRPO; cross-entropy method (CEM), a gradient-free method (Szita & Lörincz, 2006); covariance matrix adaption (CMA), another gradient-free method (Hansen & Ostermeier, 1996); natural gradient, the classic natural policy gradient algorithm (Kakade, 2002), which differs from single path by the use of a fixed penalty coefficient (Lagrange multiplier) instead of the KL divergence constraint; empirical FIM, identical to single path, except that the FIM is estimated using the covariance matrix of the gradients rather than the analytic estimate; max KL, which was only tractable on the cart-pole problem, and uses the maximum KL divergence in Equation ( 11), rather than the average divergence, allowing us to evaluate the quality of this approximation. The parameters used in the experiments are provided in Appendix E. For the natural gradient method, we swept through the possible values of the stepsize in factors of three, and took the best value according to the final performance.Learning curves showing the total reward averaged across five runs of each algorithm are shown in Figure 4. Single path and vine TRPO solved all of the problems, yielding the best solutions. Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress. These results provide empirical evidence that constraining the KL divergence is a more robust way to choose step sizes and make fast, consistent progress, compared to using a fixed penalty. CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems. The max KL method learned somewhat more slowly than our final method, due to the more restrictive form of the constraint, but overall the result suggests that the average KL divergence constraint has a similar effect as the theorecally justified maximum KL divergence. Videos of the policies learned by TRPO may be viewed on the project website: http://sites.google.com/ site/trpopaper/.Note that TRPO learned all of the gaits with generalpurpose policies and simple reward functions, using minimal prior knowledge. This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake et al., 2004;Geng et al., 2006;Wampler & Popović, 2009).",no,,,,no,,,,
1452,https://github.com/yuzhimanhua/HIMECat,yuzhimanhua_HIMECat.tar.gz,Hierarchical Metadata-Aware Document Categorization under Weak Supervision,,utils gen csv multiprocessing gensim keras re collections json tree load_data nltk os string scipy itertools pickle sklearn argparse numpy time spherecluster models,cs.CL cs.IR cs.AI cs.LG cs.SE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yuzhimanhua_HIMECat.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yuzhimanhua_HIMECat.pdf,,"INTRODUCTIONDocument categorization is an important task in text mining, with many real applications such as sentiment analysis [39], location prediction [4] and scientific paper tagging [21]. Given a large text corpus, automatically inferring the category of each document not only enables effective organization of the data, but also benefits downstream tasks like document retrieval [47]. Hierarchical document categorization [7,12] further considers relationships among categories and classifies documents into a given label hierarchy. Leveraging such hierarchical structures is shown to be effective and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  necessary due to their pervasiveness in web directories 1 , business category lists 2 and many other domains (Figure 1).Although recent studies on hierarchy-aware neural models [24,44,55] and pre-trained language models [5,38] greatly improve the performance of (hierarchical) text categorization, they are less concerned with two challenges in real applications:Limited Training Data. Neural classifiers are data hungry and require a significant amount of manually labeled training documents to achieve good performance. In some scientific domains (e.g., arXiv papers and GitHub repositories), to obtain a large training set is expensive because annotations have to be acquired from domain experts. In these scenarios, it would be favorable to perform classification without much annotation effort. To be specific, one may only expect very few (e.g., 5) training examples for each category.Heterogeneous Signals. Documents on the Web are often accompanied by metadata information. We raise three examples in Figure 1: each GitHub repository has its creator and several topic tags; each arXiv paper has its author(s); each Amazon review is associated with a user and a product. These signals, as potential category indicators, should be considered together with the plain text sequence. Moreover, the label hierarchy, which reflects category dependencies and correlation, should also be viewed as one type of signal. In recognition of these heterogeneous signals, how can we jointly leverage them during category inference?In this paper, to tackle the two challenges, we develop HiMe-Cat, an embedding-based generative framework for Hierarchical Metadata-aware document Categorization under weak supervision. HiMeCat features a hierarchical generative process in the embedding space to simultaneously model (1) category dependencies via a top-down generative assumption along the label hierarchy, (2) metadata information via a joint generative assumption that documents are dependent on both their categories and metadata, and (3) textual semantics via a document-word generative assumption. This hierarchical generative process guides the two important steps of our method: (1) representation learning: Maximizing the likelihood of the hierarchical generative process yields an embedding learning objective which jointly optimizes the representations of the label hierarchy, metadata and texts to effectively exploit the heterogeneous signals;(2) data augmentation: Following the hierarchical generative process allows us to synthesize training documents that augment the original, small-scale training set, which overcomes the limited training data challenge. Combining real and synthesized training data, we train a neural classifier for hierarchical text categorization by taking word representations learned from the previous step as embedding initialization.Inspired by related studies on spherical hierarchical clustering [13] and spherical text embedding [29] which better capture directional similarities and outperform Euclidean space models, we propose to define our hierarchical generative process in the spherical space, where conditional probabilities are described by the von Mises-Fisher (vMF) distribution [8] and Riemannian optimization [2] is adopted for embedding learning.To summarize, our contributions are as follows:• We explore the task of jointly leveraging limited supervision and metadata information for hierarchical text classification. • We develop HiMeCat with a representation learning module and a data augmentation module guided by a novel hierarchical generative process in the spherical space. Our representation learning module jointly learns embeddings from heterogeneous signals, and our data augmentation module synthesizes training documents to address the supervision scarcity bottleneck. • We conduct comprehensive experiments on three datasets from different domains and observe a consistent improvement of HiMe-Cat over competitive baselines. We also show that leveraging the hierarchy, leveraging metadata, and generating training samples are all beneficial to the categorization performance.",no,,,"embedding dimension, margin parameter, vocabulary size, filter widths",no,,,,
1453,https://github.com/aliborji/resolution,aliborji_resolution.tar.gz,Enhancing sensor resolution improves CNN accuracy given the same number of parameters or FLOPS,,thop keras google torchsummary random torch __future__ numpy matplotlib torchvision,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/aliborji_resolution.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\aliborji_resolution.pdf,Depthwise Convolution  Pointwise Convolution  Batch Normalization  Depthwise Separable Convolution  Convolution  Average Pooling  Sigmoid Activation  Dense Connections  Swish  Inverted Residual Block  RMSProp  Rectified Linear Units  Dropout  Squeeze-and-Excitation Block  1x1 Convolution  EfficientNet,,no,,,,no,,,,
1457,https://github.com/sniklaus/softmax-splatting,sniklaus_softmax-splatting.tar.gz,Softmax Splatting for Video Frame Interpolation,,re run sys cupy glob cv2 skimage softsplat torch os numpy,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sniklaus_softmax-splatting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sniklaus_softmax-splatting.pdf,Softmax,,no,,,,no,,,,
1462,https://github.com/thuiar/textoir-demo,thuiar_textoir-demo.tar.gz,TEXTOIR: An Integrated and Visualized Platform for Text Open Intent Recognition,@csrf_exempt @classmethod @xframe_options_exempt,"utils copy pandas os ,sys csv sys pipeline thedataset methods pymysql tqdm easydict backbones platform distutils os, sys, platform, shlex, subprocess, shutil, json os, sys pathlib keras sys, os tensorflow datetime json Cython collections keybert open_intent_detection nltk torch os libmr importlib scipy pickle umap sklearn argparse numpy json , csv , shlex , subprocess , logging , os , sys , platform , shutil , stat , base64 math open_intent_discovery shutil , stat pytorch_pretrained_bert logging random losses dataloaders operator django configs",cs.NE cs.CV cs.LG cs.CL cs.NE cs.CL cs.AI cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thuiar_textoir-demo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thuiar_textoir-demo.pdf,Average Pooling  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Softmax  Multilingual Universal Sentence Encoder  Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,,no,,,,no,,,,
1463,https://github.com/umanlp/SemScale,umanlp_SemScale.tar.gz,Political Text Scaling Meets Computational Semantics,,copy sys nlp __future__ graphs tensorflow re datetime collections nltk os helpers embeddings sts convolution scipy itertools codecs pickle argparse numpy time math ml wfcode random evaluation,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/umanlp_SemScale.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\umanlp_SemScale.pdf,,,no,,,,no,,,,
1465,https://github.com/fmfn/BayesianOptimization,fmfn_BayesianOptimization.tar.gz,"A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",@classmethod @property @staticmethod @verbose.setter,requests threading scipy asyncio warnings datetime json tornado setuptools random sklearn os pytest __future__ numpy bayes_opt time,cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fmfn_BayesianOptimization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fmfn_BayesianOptimization.pdf,,"Active Value Function LearningThe Navigate task learns path finding from any intersection in the topological map to any of the destinations. Although this task operates on a discrete set of waypoints, the underlying map coordinates are continuous, and we can again apply active exploration with GPs. Unlike the previous algorithm that searches for a set of optimal parameters, Algorithm 3 learns the value function at a finite set of states, by actively generating exploratory actions; it is designed to fit within a MAXQ task hierarchy. choose adjacent intersection WP using -greedy or Active exploration.  s = s 29: end while 30: return trajectory in the data matrix D; it is initialized with V (x T , y T , x T , y T ) = 0 for all target destinations T , which enables the GP to create a useful response surface without actually having observed anything yet.V s = V (T axiLoc s , T arget i ) { guaranteed <= 0} 13: V s = V (T axiLoc s , T arget i ) { guaranteed <= 0} 14: if T erminated i (s ) is true then 15: V s ← (1 − α) × V s + α × R 16: for all j = 1 to length(intersections) do 17: {s , N , R } = intersections(j) 18: R ← R + γ N × R 19: V s ← V (T axiLoc s , T arget i ) 20: V s ← (1 − α) × V s + α × RIn the −greedy experiments, a random intersection is chosen with chance 0.1, and the greedy one with chance 0.9. For the active exploration case, we fit a GP over the data matrix D, and pick the adjacent intersection that maximizes the expected improvement function. We parameterize this function with an annealing parameter that decays over time such that initially we place more importance on exploring.The true value will not be known until the N avigate task reaches its desti-nation and terminates, but we still need to mark visited intersections to avoid indefinite looping. Lines 23-26 compute an estimated value for V (s) by summing the immediate discounted reward of executing F ollow(WP , s) with the discounted, previously recorded value of the new state V (s ), and a heuristic penalty factor to avoid looping. Once we reach the destination of this task, T arget i , we have the necessary information to propagate the discounted reward to all the intersections along the trajectory, in lines 15-21.",no,,,,no,,,,
1466,https://github.com/ratschlab/boosting-bbvi,ratschlab_boosting-bbvi.tar.gz,Boosting Black Box Variational Inference,,blr_utils utils copy sys __future__ tensorflow absl os matplotlib six scipy boosting_bbvi plot_utils pickle sklearn edward numpy time setuptools random logging,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ratschlab_boosting-bbvi.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ratschlab_boosting-bbvi.pdf,,,no,,,,no,,,,
1472,https://github.com/yizhou-wang/RODNet,yizhou-wang_RODNet.tar.gz,RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization,,multiprocessing sys tqdm cruw __future__ ctypes re collections json rodnet torch os matplotlib importlib pickle argparse numpy time shutil math setuptools random,cs.CV eess.SP cs.CV eess.SP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yizhou-wang_RODNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yizhou-wang_RODNet.pdf,AutoEncoder,,no,,,"kernel, stride, channels, ",no,,,,
1476,https://github.com/omni-us/research-GANwriting,omni-us_research-GANwriting.tar.gz,GANwriting: Content-Conditioned Generation of Styled Handwritten Word Images,,utils PIL blocks glob recognizer tqdm modules_tro network_tro torchvision pathlib pairs_idx_wid_iam load_data torch os main_torch_latest string scipy marcal_augmentor_v4 loss_tro doc_augment_lib inception argparse tensorboardX numpy time vgg_tro_channel3_modi math loadData4_vgg cv2 random subprocess models Levenshtein,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/omni-us_research-GANwriting.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\omni-us_research-GANwriting.pdf,,,no,,,,no,,,,
1478,https://github.com/cdancette/rubi.bootstrap.pytorch,cdancette_rubi.bootstrap.pytorch.tar.gz,RUBi: Reducing Unimodal Biases for Visual Question Answering,@staticmethod,copy block scipy itertools bootstrap csv codecs sys json collections setuptools h5py random tqdm argparse os torch numpy,cs.CV cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cdancette_rubi.bootstrap.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cdancette_rubi.bootstrap.pytorch.pdf,,,no,,,,no,,,,
1479,https://github.com/nvlabs/tiny-cuda-nn,nvlabs_tiny-cuda-nn.tar.gz,Real-time Neural Radiance Caching for Path Tracing,@staticmethod,getopt copy sys glob tensorflow re os string codecs imp sre_compile textwrap argparse numpy time commentjson pyexr math unicodedata tensorflow_probability,cs.GR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nvlabs_tiny-cuda-nn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nvlabs_tiny-cuda-nn.pdf,,,no,,,,no,,,,
1482,https://github.com/qub-blesson/Scission,qub-blesson_Scission.tar.gz,Scission: Performance-driven and Context-aware Cloud-Edge Distribution of Deep Neural Networks,,pathlib tensorflow csv collections pickle fnmatch enum argparse os typing numpy matplotlib time,cs.DC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/qub-blesson_Scission.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\qub-blesson_Scission.pdf,,,no,,,,no,,,,
1484,https://github.com/Lawhy/Searchive,Lawhy_Searchive.tar.gz,Impact of Ground Truth Annotation Quality on Performance of Semantic Image Segmentation of Traffic Conditions,"@app.route('/') @app.route('/help') @app.route('/about') @classmethod @app.route('/result', methods=['GET'])",sys spellchecker pymongo re firebase_admin datetime json collections nltk os readindex matplotlib scipy ranking pickle indexing sklearn preprocess numpy flask time search scrapy math normalise search_file operator subprocess,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Lawhy_Searchive.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Lawhy_Searchive.pdf,Rectified Linear Units  Batch Normalization  Convolution  Average Pooling  Pyramid Pooling Module  Auxiliary Classifier  Dilated Convolution  PSPNet,,no,,,,no,,,,
1488,https://github.com/kobybibas/pnml_linear_regression,kobybibas_pnml_linear_regression.tar.gz,A New Look at an Old Problem: A Universal Learning Approach to Linear Regression,"@abstractmethod @ray.remote @hydra.main(config_path='../configs', config_name=""pnml_polynomial"") @staticmethod @hydra.main(config_path='../configs', config_name=""real_data"")",copy data_utils pandas yaml glob hydra psutil os ray ray_utils matplotlib scipy typing sklearn numpy time abc logging experimnet_utils learner_utils,cs.LG cs.IT math.IT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kobybibas_pnml_linear_regression.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kobybibas_pnml_linear_regression.pdf,Linear Regression,,no,,,,no,,,,
1490,https://github.com/arafin-lab/model_inversion_experiments,arafin-lab_model_inversion_experiments.tar.gz,ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models,@staticmethod,"requests PIL copy pandas yaml sys skimage tqdm graphviz io __future__ pybrain torchvision tensorflow soundfile fnmatch torch os matplotlib model_inversion visualize_util codecs os, shutil urllib pickle sklearn model_fooling argparse numpy time shutil tarfile unicodedata cyphercat librosa setuptools subprocess metrics SVC_Utils zipfile",cs.CR cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/arafin-lab_model_inversion_experiments.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\arafin-lab_model_inversion_experiments.pdf,,,no,,,,no,,,,
1509,https://github.com/jiacheng-xu/text-sum-uncertainty,jiacheng-xu_text-sum-uncertainty.tar.gz,Understanding Neural Abstractive Summarization Models via Uncertainty,,"pandas json, math, seaborn multiprocessing util re os, random, pickle collections json transformers torch os data_collection matplotlib string os, random scipy itertools os, pickle, random analyze_entropy seaborn typing pickle argparse attention_y_entropy plot_figures numpy math datasets operator logging statistics random analyze_prob_attn",cs.CL cs.CL cs.CL cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jiacheng-xu_text-sum-uncertainty.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jiacheng-xu_text-sum-uncertainty.pdf,Adam  Byte Pair Encoding  Softmax  Layer Normalization  Dense Connections  Multi-Head Attention  Tanh Activation  Dropout  Sigmoid Activation  Gaussian Error Linear Units  Residual Connection  Scaled Dot-Product Attention  Long Short-Term Memory  Sequence to Sequence  BART  Denoising Autoencoder  AutoEncoder  Residual Connection  BART,,no,,no,,no,,,,
1511,https://github.com/eliahuhorwitz/DeepSIM,eliahuhorwitz_DeepSIM.tar.gz,Image Shape Manipulation from a Single Augmented Training Sample,,"PIL imageio sys skimage __future__ torchvision util tensorflow collections data torch os tensorrt fractions matplotlib StringIO dominate apex functools scipy run_engine sets pycuda argparse os, fnmatch numpy time ntpath random subprocess options models io",cs.CV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eliahuhorwitz_DeepSIM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eliahuhorwitz_DeepSIM.pdf,DeepSIM,,no,,no,,no,,,,
1523,https://github.com/locuslab/projected_sinkhorn,locuslab_projected_sinkhorn.tar.gz,SINKHORN DISTANCES: LIGHTSPEED COMPUTATION OF OPTIMAL TRANSPORTATION DISTANCES,,utils math scipy lambertw pgd sys warnings setuptools torch os argparse projected_sinkhorn __future__ numpy models time torchvision,stat.ML cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/locuslab_projected_sinkhorn.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\locuslab_projected_sinkhorn.pdf,,,no,,no,,no,,,,
1525,https://github.com/ansonang3/colorneuralfield,ansonang3_colorneuralfield.tar.gz,A neural field model for color perception unifying assimilation and contrast,@staticmethod,scipy pytorch_argmax settings visualize HSL_data colorsys initialization torch numpy matplotlib time,q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ansonang3_colorneuralfield.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ansonang3_colorneuralfield.pdf,,,no,,no,,no,,,,
1526,https://github.com/583748495/psift,583748495_psift.tar.gz,PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation,@tf.RegisterGradient('ThreeInterpolate') @tf.RegisterGradient('GroupPoint') @staticmethod @tf.RegisterGradient('GatherPoint'),tensorflow sys tf_utils tf_interpolate h5py show3d_balls pickle argparse os __future__ numpy models scannet_dataset time tf_grouping,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/583748495_psift.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\583748495_psift.pdf,,,no,,no,,no,,,,
1531,https://github.com/ofirpress/shortformer,ofirpress_shortformer.tar.gz,Shortformer: Better Language Modeling Using Shorter Inputs,"@register_optimizer('adagrad') @register_model_architecture(""asr_vggtransformer_encoder"", ""vggtransformer_enc_1"") @register_optimizer('sgd') @register_model_architecture(""transformer"", ""transformer"") @register_optimizer('adadelta') @register_model_architecture('dummy_model', 'dummy_model') @register_model('dummy_model') @register_lr_scheduler('reduce_lr_on_plateau') @lru_cache() @register_bpe('byte_bpe') @register_model_architecture('hf_gpt2', 'hf_gpt2') @register_model(""gru_transformer"") @register_model_architecture(""wav2vec"", ""wav2vec"") @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_2"") @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('roberta', 'roberta_base') @register_model(""asr_w2l_conv_glu_encoder"") @register_model_architecture('roberta', 'roberta') @register_model('xlmr') @register_optimizer('adamax') @register_criterion('masked_lm') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_criterion(""cross_entropy_acc"") @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_bpe('hf_byte_bpe') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron_11b') @register_model('camembert') @register_model_architecture('masked_lm', 'masked_lm') @register_task('sentence_ranking') @register_model('transformer_monotonic') @register_model(""asr_vggtransformer_encoder"") @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta_large') @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_fr_big"") @register_model_architecture('transformer_lm', 'transformer_lm') @include_dirs.setter @register_tokenizer('space') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_monotonic_attention(""hard_aligned"") @optimizer.setter @register_task('multilingual_masked_lm') @torch.jit.unused @register_model('model_parallel_transformer_lm') @register_optimizer('lamb') @torch.jit.export @register_model('model_parallel_roberta') @register_task('denoising') @register_task('translation_moe') @register_model('transformer_lm') @register_model_architecture('pipeline_parallel_transformer', @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_model_architecture(""transformer_align"", ""transformer_align"") @register_bpe('bert') @register_model(""asr_vggtransformer"") @register_model_architecture('masked_lm', 'bert_large') @register_model(""iterative_nonautoregressive_transformer"") @register_monotonic_attention(""waitk"") @register_task(""language_modeling"") @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @metrics.aggregate(""train"") @register_model_architecture(""transformer_align"", ""transformer_wmt_en_de_big_align"") @register_model_architecture(""gru_transformer"", ""gru_transformer"") @register_task('multilingual_denoising') @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_1"") @register_monotonic_attention(""infinite_lookback"") @register_model('lstm_lm') @register_model(""wav2vec_seq2seq"") @register_tokenizer('moses') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_criterion('sentence_prediction') @register_bpe('subword_nmt') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_agent(""simul_trans_text"") @register_model_architecture('transformer_lm', 'transformer_lm_big') @metrics.aggregate(""valid"") @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @register_task('commonsense_qa') @register_model(""wav2vec2"") @register_model(""wav2vec_ctc"") @register_model('model_parallel_transformer') @register_criterion('label_smoothed_cross_entropy_with_alignment') @s3_request @register_model_architecture('bart', 'bart_base') @ensemble_encoder @register_task('cross_lingual_lm') @register_model('lightconv') @register_criterion('latency_augmented_label_smoothed_cross_entropy') @register_criterion(""ctc"") @register_bpe('fastbpe') @register_model_architecture(""levenshtein_transformer"", ""levenshtein_transformer"") @register_bpe('gpt2') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big_t2t"") @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @register_task('multilingual_translation') @unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"") @lru_cache(maxsize=1) @register_task('winogrande') @register_model_architecture('roberta', 'roberta_large') @register_model('fconv_lm') @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"") @register_model_architecture('hf_gpt2', 'hf_gpt2_large') @register_criterion(""asg_loss"") @register_bpe('characters') @register_model_architecture('lstm_lm', 'lstm_lm') @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer"") @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_de_big"") @register_model_architecture(""asr_vggtransformer"", ""vggtransformer_base"") @register_model_architecture(""transformer"", ""transformer_iwslt_de_en"") @register_model(""insertion_transformer"") @register_model('pipeline_parallel_transformer') @register_model_architecture('bart', 'bart_large') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model('hf_gpt2') @register_model(""transformer_align"") @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer_wmt_en_de"") @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture(""insertion_transformer"", ""insertion_transformer"") @register_task(""speech_recognition"") @dataclass @register_criterion('wav2vec') @register_criterion('sentence_ranking') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_model_architecture(""gru_transformer"", ""gru_transformer_big"") @property @register_optimizer('nag') @register_model('roberta') @register_criterion('adaptive_loss') @ensemble_decoder @register_model_architecture('roberta', 'xlm') @register_task('translation_from_pretrained_bart') @register_criterion('cross_entropy') @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU') @register_bpe('bytes') @register_model_architecture('bart', 'mbart_base') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @register_task('dummy_masked_lm') @register_task('translation_multi_simple_epoch') @register_task(""audio_pretraining"") @register_model_architecture(""nacrf_transformer"", ""nacrf_transformer"") @wraps(func) @register_criterion('legacy_masked_lm_loss') @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @register_model('masked_lm') @register_model_architecture(""wav2vec_ctc"", ""wav2vec_ctc"") @register_task('wsc') @register_bpe('sentencepiece') @contextlib.contextmanager @register_model_architecture('bart', 'mbart_large') @register_model_architecture('fconv', 'fconv') @register_model_architecture(""transformer"", ""transformer_wmt_en_de"") @register_task('legacy_masked_lm') @register_task('sentence_prediction') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_task('translation_lev') @register_model_architecture('lightconv', 'lightconv') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_model('lightconv_lm') @register_model('lstm') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @torch.jit.script @register_task('masked_lm') @register_model_architecture(""asr_w2l_conv_glu_encoder"", ""w2l_conv_glu_enc"") @register_criterion('wsc') @register_model_architecture('hf_gpt2', 'hf_gpt2_xl') @with_incremental_state @register_criterion('vocab_parallel_cross_entropy') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_model('transformer_unidirectional') @register_optimizer('adam') @register_lr_scheduler('triangular') @register_scoring(""wer"") @register_tokenizer('nltk') @register_task('dummy_mt') @register_model(""nacrf_transformer"") @register_lr_scheduler('tri_stage') @lru_cache(maxsize=8) @register_model(""nonautoregressive_transformer"") @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta') @unittest.skipIf( @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @register_model_architecture(""wav2vec2"", ""wav2vec2"") @register_model(""wav2vec"") @staticmethod @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_criterion('winogrande') @register_model(""transformer"") @register_model_architecture(""wav2vec_seq2seq"", ""wav2vec_seq2seq"") @register_model_architecture('bart', 'mbart_base_wmt20') @register_model(""levenshtein_transformer"") @register_model('bart') @register_model('fconv') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big"") @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @contextmanager @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_scorer(""text"") @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_criterion(""nat_loss"") @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta_base') @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_task('dummy_lm') @register_scoring(""bleu"") @register_criterion('label_smoothed_cross_entropy') @register_model(""cmlm_transformer"") @register_scoring(""sacrebleu"") @register_model_architecture('hf_gpt2', 'hf_gpt2_medium') @register_task('translation') @register_model_architecture('fconv', 'fconv_iwslt_de_en')","copy warnings multiprocessing types hydra ctypes threading en_core_web_lg re h5py dataclasses amp_C torch importlib vizseq spacy fairseq_cli xentropy_cuda pickle boto3 numbers tensorboardX numpy sentencepiece tarfile traceback random scorers inspect requests yaml fairscale pdb __future__ pprint examples pathlib unittest queue botocore tests transformers fnmatch nltk os socket gzip functools hashlib client time shutil gossip tornado operator subprocess gc sys regex tqdm tokenizers atexit datetime cython enum tempfile lightconv_cuda struct fastBPE itertools typing subword_nmt urlparse setuptools sacrebleu io fvcore scripts contextlib sacremoses glob fairseq fileinput soundfile uuid wav2letter glob, os, argparse pyarrow collections json concurrent apex agents bisect urllib palaas argparse editdistance math torchaudio logging torch_xla dynamicconv_cuda",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ofirpress_shortformer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ofirpress_shortformer.pdf,Gaussian Error Linear Units  Layer Normalization,"ExperimentsOur experimental setup is described in §2. We do not change any hyperparameters other than reducing subsequence length while correspondingly increasing batch size to keep the number of tokens per batch constant. As in the baseline, all models are trained for 205 epochs.All models are trained in two stages; the second stage always uses a subsequence length of 3,072, 6 Curriculum learning (Bengio et al., 2009) trains on easier inputs before progressing to harder ones. Our approach does not change the order in which the training examples are given to the model, but instead modifies their lengths.  100 18.14 17.67 17.62 18.00 18.10 18.00 18.51 125 18.61 17.88 17.70 18.00 18.13 17.98 18.49 150 19.45 18.37 17.98 18.01 18.15 18.00 18.49 175 21.16 19.51 18.57 18.23 18.20 18.08 18.57 200 35.38 28.03 23.80 21.45 19.63 18.56 18.84 Table 2: Each model's perplexity at the end of training (dev. set, nonoverlapping eval.). All models have a subsequence length of 3,072 tokens at the end of training. The B&A baseline achieves 18.65 ± 0.24 perplexity.since that lead to the best performance (discussed at end of this subsection).Appendix Table 6 shows the time each training routine takes to match the baseline model's performance on the validation set of  Many configurations match this performance in less than half the time it takes to train the baseline itself; some reach baseline performance in only 37% of the time needed to train the baseline.Although all models take less time to train than the baseline, Table 2 shows that many outperform it. For example, the best model-trained with subsequence length L = 128 until epoch 50outperforms the baseline by 1.1 perplexity despite completing training in 87% of the time the baseline takes to do so. The model that trains with L = 128 until epoch 100 achieves similarly strong results (17.62 perplexity) and finishes training in 74% of the time it takes the baseline. 8 These results are very robust to the choice of initial stage subsequence length and number of epochs. Table 2 shows that all models with an initial stage of L = 1,024 tokens or less that switch to the second stage at epoch 125 or before beat the baseline by a large margin at the end of training. Additionally, Appendix Table 6 shows that those models match the baseline's perplexity in at most 71% of the time it takes to train the baseline.When we use nonoverlapping evaluation, the B&A baseline obtains 18.65 perplexity on the development set; our best model obtains 17.52. When we use sliding window evaluation (following Baevski & Auli, we use stride S = 512), our best 7 Table 7 in the appendix shows the epoch at which every model matched the baseline's performance.8 Table 8 in the appendix shows the total time it took to train each model. model obtains 16.89 perplexity, a large improvement on the 17.92 B&A result in that setting. On the test set, using the same sliding window evaluation, our model obtains 17.56 perplexity, a substantial gain over the baseline's 18.70 test-set perplexity. Appendix Table 10 shows that our best model uses almost five times less memory during the first stage than the baseline.We also found that setting L to less than 3,072 tokens in the second stage degraded performance. (Appendix Table 9 shows staged training results with an initial stage length of 128 for 50 epochs (as in the best model) and varying lengths for the second stage. We found this to also be true for other initial stage lengths and epochs.) Unlike results in Table 1, where we show that models with L larger than 1,024 do not substantially improve token-bytoken generation perplexity, models trained using staged training improve when given longer inputs (Appendix Table 9). Further, we explored using more than two stages (up to six), but this did not outperform our two-stage curriculum.Finally, Appendix A.5 shows that staged training substantially improves on the Toronto Book Corpus (Zhu et al., 2015). ExperimentsWe use the experimental setup described in §2.The B&A baseline achieves 18.65 on the development set. We train two additional baselines, the first uses PIA without caching and the second uses caching but no PIA. If just PIA is used (without caching), performance degrades to 19.35 perplexity, but the model's speed and memory usage do not change. Using caching without PIA severely hurts performance, obtaining 41.59 perplexity. Disabling data shuffling in the PIA-only model achieves similar performance to that model when it does use data shuffling, at 19.44 perplexity. Not shuffling the data is necessary for recurrent-style training that caches previously computed subsequence representations.Our next experiments use the recurrent-style training of Dai et al. (2019), where we receive L new tokens at every training iteration and attend to L cached representations (of the subsequence of tokens that came immediately prior to the L new tokens). As before, we output L predictions at every training iteration. This means that the maximal and minimal effective context window sizes are L + L and L + 1, respectively.In all our models with PIA and caching, we set L = L because a manual exploration of different models where L = L did not yield better results.Table 3 compares the results of our models that use PIA and caching to the baseline on the WikiText-103 dev. set. Evaluation and generation speeds are shown in the nonoverlapping (N.o.) and sliding window (S.W., with stride S = 1) speed columns, respectively. 10 Unlike in the baseline, token-by-token evaluation in our model achieves the same perplexity as nonoverlapping evaluation since in both cases, the predictions for each input subsequence are conditioned not only on the current input, but also on the previous input, making the context window the same in both inference modes (in both cases, at every timestep, the context window is all tokens up to that timestep). Table 3 shows that as we increase subsequence length, perplexity improves, peaking at 512 before starting to degrade. Our best model obtains 17.85 perplexity, which is multiple standard deviations better than the baseline (18.65, N.o.). Table 5 in §6 shows a similar gain on the test set. The best model runs 1% slower than the baseline during N.o. eval. (since caching reduces the speed gain from smaller attention matrices in this mode). Table 10 (appendix) shows that it uses less than half of the memory the baseline does during training. Our best model trains 55% faster than the baseline.Our best model, with subsequence length 512, has attention matrices of size 512 • 1,024 (since we have 512 queries-one per every new token-and 1,024 keys and 1,024 values-one per every new token and every cached token). In the baseline, all attention matrices are of size 3,072 • 3,072.Caching previously computed representations lets us do token-by-token generation efficiently when generating more than L tokens. Our model is nine times faster than the baseline at token-bytoken generation even as it achieves better perplexity and uses much less memory (Tab. 3, col. 5). PIA and caching also greatly improve perplexity on the Toronto Book Corpus; see A.5 in the appendix.",no,,no,,no,,,,
1546,https://github.com/ecrows/l2-reddit-experiment,ecrows_l2-reddit-experiment.tar.gz,TOWARDS ETHICAL CONTENT-BASED DETECTION OF ONLINE INFLUENCE CAMPAIGNS,@classmethod,math scipy bert tensorflow pandas csv sys collections json glob errno statistics sklearn os __future__ numpy pprint,cs.CY cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ecrows_l2-reddit-experiment.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ecrows_l2-reddit-experiment.pdf,,,no,,no,,no,,,,
1549,https://github.com/AsahiLiu/PointDetectron,AsahiLiu_PointDetectron.tar.gz,Group Contextual Encoding for 3D Point Clouds,@staticmethod,"PIL csv sys multiprocessing glob nms model_util_scannet pdb sunrgbd_detection_dataset __future__ eval_det plyfile scannet_detection_dataset backbone_module model_util_sunrgbd encoding tensorflow backbone_module_SA2_denseaspp3_6 enc_layer datetime json torch os dump_helper matplotlib StringIO metric_util importlib scannet_utils tf_visualizer voting_module scipy pointnet2_modules IPython backbone_module_enc_complex_FP2_K8_G12_C3 gzip builtins loss_helper_boxnet typing pc_util pickle argparse numpy os, sys, argparse time sunrgbd_utils load_scannet_data pointnet2 ap_helper math tf_logger backbone_module_enc_FP2_K8_G12_C3 loss_helper setuptools box_util proposal_module cv2 nn_distance pytorch_utils trimesh pointnet2_utils backbone_module_SA2_denseaspp3_6_12 inspect __builtin__ io",cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AsahiLiu_PointDetectron.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AsahiLiu_PointDetectron.pdf,PointNet,,no,,no,,no,,,,
1559,https://github.com/JasonPSmith/TriDy,JasonPSmith_TriDy.tar.gz,AN APPLICATION OF NEIGHBOURHOODS IN DIGRAPHS TO THE CLASSIFICATION OF BINARY DYNAMICS,,scipy pyflagser pandas sys json pyflagsercontain networkx pickle sklearn subprocess os concurrent numpy time,math.AT,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/JasonPSmith_TriDy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\JasonPSmith_TriDy.pdf,,,no,,no,,no,,,,
1560,https://github.com/codestar12/pruning-distilation-bias,codestar12_pruning-distilation-bias.tar.gz,Simon Says: Evaluating and Mitigating Bias in Pruned Neural Networks with Knowledge Distillation,@staticmethod,PIL copy pandas sys glob crd __future__ tensorboard_logger torchvision distiller_zoo datetime src torch os helper socket functools itertools seaborn dataset typing sklearn argparse numpy geomloss time math random models,cs.LG cs.AI cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/codestar12_pruning-distilation-bias.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\codestar12_pruning-distilation-bias.pdf,Knowledge Distillation,,no,,no,,no,,,,
1571,https://github.com/cyrilzakka/GANLocalEditing,cyrilzakka_GANLocalEditing.tar.gz,Editing in Style: Uncovering the Local Semantics of GANs,@property @staticmethod,math spherical_kmeans scipy copy re functools warnings collections ptutils random operator sklearn torch os op numpy matplotlib,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cyrilzakka_GANLocalEditing.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cyrilzakka_GANLocalEditing.pdf,Adaptive Instance Normalization  R1 Regularization  Leaky ReLU  Dense Connections  Feedforward Network  StyleGAN  Convolution  Generative Adversarial Network,,no,,no,,no,,,,
1572,https://github.com/jiaozizhao/Two-in-One-ActionDetection,jiaozizhao_Two-in-One-ActionDetection.tar.gz,Dance with Flow: Two-in-One Stream Action Detection,,"utils cv2, pickle types layers Modulssd_CONV2 os, time torchvision data visdom torch os scipy itertools pickle argparse numpy time math cv2",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jiaozizhao_Two-in-One-ActionDetection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jiaozizhao_Two-in-One-ActionDetection.pdf,,,no,,no,,no,,,,
1574,https://github.com/wilburOne/ZeroShotEvent,wilburOne_ZeroShotEvent.tar.gz,Zero-Shot Transfer Learning for Event Extraction,,utils sys tokenizer layers re nltk os string itertools xml codecs jieba sklearn argparse numpy time unicodedata theano random cPickle,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wilburOne_ZeroShotEvent.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wilburOne_ZeroShotEvent.pdf,,,no,,yes,,no,,,,
1577,https://github.com/moews/dredge,moews_dredge.tar.gz,Filaments of Crime: Informing Policing via Thresholded Ridge Estimation,,numpy sys sklearn scipy,stat.AP cs.CY stat.CO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/moews_dredge.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\moews_dredge.pdf,,"Open-Source Tool for PractitionersSince we want to provide the research community and practitioners with an easy-to-handle implementation that includes all modifications and extensions described in Section 3.4, as well as to enable the reproduction of our work, we introduce a pure-Python software tool for density ridge estimation describing geospatial evidence (DREDGE), written for Python 3. The tool itself is available on, and can be installed via, the Python Package Index (PyPI). cOne of the primary aims in developing a user-friendly tool is to combine customizability with minimal requirements, allowing for the automated use of default parameters and adaptive code behavior while enabling users to fine-tune in accordance with their needs. For this reason, the only mandatory input of DREDGE is an array of coordinates, with one row per data point and latitudes and longitudes in the first and second column, respectively. Since larger datasets lead to smaller bandwidths via the automatic bandwidth calculation described in Section 3.4, we recommend a set of coordinates numbering between 1,000 and 10,000 samples. In case of larger sample sizes, we advise users to manually set a bandwidth, as explained below. In addition to the input of coordinates, four further parameters can be manually set by the user.The first optional parameter concerns the number of neighbors used to calculate an optimal bandwidth as described in Eq. 7, which has to be an integer value larger than zero. Smaller values for this parameters result in smaller bandwidths, leading to a more fine-grained map of ridge estimates. The second optional parameter enables users to set the bandwidth directly, with the requirement that the value is a positive real number. Providing this parameter disables the automatic calculation of an optimal bandwidth, forcing the code to use the given value. The third optional parameter is the convergence threshold that controls the termination behavior of the algorithm, with the same requirement of a positive real number. The lower this parameter is set, the stricter the assessment of convergence to smooth filaments becomes, at the cost of an increased runtime.Lastly, the fourth optional parameter is disabled by default and can be set to any real number from the interval [0, 100], indicating the desired percentage to cut the outputted density ridge estimate points to a subset as described in Eq. 9. If, for example, a value of 10.0 is provided, only ridge estimates above the 90 th percentile of the dataset's approximated probability density function are returned. For a better overview, required and optional input parameters for DREDGE are listed in Tab. II, with optional parameters indicated by an asterisk. All userprovided inputs are checked for compliance with format and data type requirements when calling the tool's primary function, offering custom error messages in case of an incompatibility for one or more input parameters.Software tools geared toward both research and their use by practitioners benefit from being well-documented and freely available, allowing users to easily understand and, if necessary, adapt the source code to their needs. Therefore, the complete code for DREDGE is available as open-source software in a public repository d , accompanied by a quickstart tutorial and a use case featuring example code in the form of a downloadable notebook.",no,,no,,no,,,,
1582,https://github.com/omron-sinicx/multipolar,omron-sinicx_multipolar.tar.gz,MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics,"@pytest.mark.parametrize(""trained_model"", trained_models.keys()) @pytest.mark.parametrize(""experiment"", experiments.keys())",utils pandas stable_baselines yaml warnings pybullet_envs glob pprint pytablewriter ast gym tensorflow re collections statsmodels os pytest roboschool matplotlib importlib xml seaborn tkinter argparse numpy time shutil difflib random subprocess inspect,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/omron-sinicx_multipolar.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\omron-sinicx_multipolar.pdf,,,no,,no,,no,,,,
1600,https://github.com/IIT-PAVIS/acoustic-images-self-supervision,IIT-PAVIS_acoustic-images-self-supervision.tar.gz,Leveraging Acoustic Images for Effective Self-Supervised Audio Representation Learning,@slim.add_arg_scope @add_arg_scope @property,sys glob __future__ tensorflow re datetime collections os trainer matplotlib scipy itertools sklearn argparse logger numpy math librosa traceback cv2 subprocess dataloader models base,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IIT-PAVIS_acoustic-images-self-supervision.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IIT-PAVIS_acoustic-images-self-supervision.pdf,,,no,,no,,no,,,,
1602,https://github.com/shruti-jadon/Brain-Lesions-Segmentation,shruti-jadon_Brain-Lesions-Segmentation.tar.gz,A Comparative Study of 2D Image Segmentation Algorithms for Traumatic Brain Lesions Using CT Data from the ProTECTIII Multicenter Clinical Trial,,PIL keras tensorflow glob nibabel nilearn sklearn argparse os loss_functions numpy models matplotlib,eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shruti-jadon_Brain-Lesions-Segmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shruti-jadon_Brain-Lesions-Segmentation.pdf,UNet++,,no,,no,,no,,,,
1603,https://github.com/openskynetwork/aircraft-localization,openskynetwork_aircraft-localization.tar.gz,LocaRDS: A Localization Reference Data Set,@__check_index @jit(nopython=True) @__check_station @property @staticmethod,gc copy pandas sensorsparams warnings sys multiprocessing pdb tqdm lightgbm ast distutils libs Cython collections json src pwlf datetime pytorch_lightning splineaircraftpos pathos torch os csaps optimize matplotlib datautils common scipy itertools IPython numba aircraftpos pickle typing sklearn argparse joblib numpy time graph_tool round2_mlat math networkx random learnfilter pyproj bayes_opt,cs.NI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/openskynetwork_aircraft-localization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\openskynetwork_aircraft-localization.pdf,,,no,,no,,no,,,,
1607,https://github.com/histocartography/histocartography,histocartography_histocartography.tar.gz,HistoCartography: A Toolkit for Graph Analytics in Digital Pathology,@classmethod @abstractmethod @property @staticmethod,requests PIL copy pandas mlflow csv yaml warnings sys multiprocessing glob skimage tqdm better_apidoc torchvision distutils pathlib re collections json unittest histocartography h5py torch os matplotlib importlib dgl scipy functools itertools hashlib bisect typing pickle sklearn numpy time abc shutil math setuptools traceback networkx cv2 logging random subprocess io,cs.CV eess.IV cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/histocartography_histocartography.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\histocartography_histocartography.pdf,,,no,,no,,no,,,,
1611,https://github.com/ggsDing/ASLNet,ggsDing_ASLNet.tar.gz,Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images,@staticmethod,"utils PIL skimage torchvision osgeo sys, os datetime torch os matplotlib pydensecrf dice_loss scipy functools tensorboardX numpy time math unicodedata datasets random cv2 models",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ggsDing_ASLNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ggsDing_ASLNet.pdf,,,no,,no,,no,,,,
1612,https://github.com/dflemin3/approxposterior,dflemin3_approxposterior.tar.gz,Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions,,scipy re corner warnings sys approxposterior setuptools george emcee tqdm sklearn os __future__ numpy matplotlib time distutils io,stat.CO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dflemin3_approxposterior.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dflemin3_approxposterior.pdf,Gaussian Process,,no,,no,,no,,,,
1616,https://github.com/hyren/acl,hyren_acl.tar.gz,Adversarial Constraint Learning for Structured Prediction,@memoized,utils imageio scipy PIL tensorflow functools generate_gif sys json collections ops random cv2 argparse os numpy matplotlib time,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hyren_acl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hyren_acl.pdf,,,no,,no,,no,,,,
1625,https://github.com/meelgroup/mlic,meelgroup_mlic.tar.gz,MLIC: A MaxSAT-Based framework for learning interpretable classification rules,,math pandas warnings setuptools Orange random cplex tqdm sklearn pyrulelearn subprocess os numpy time,cs.AI cs.LO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/meelgroup_mlic.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\meelgroup_mlic.pdf,,"IntroductionThe last decade has witnessed an unprecedented adoption of machine learning techniques to make sense of available data and make predictions to support decision making for a wide variety of applications ranging from health-care analytics to customer churn predictions, movie recommendations and macro-economic policy. The focus in the machine learning literature has been on increasingly sophisticated systems with the paramount goal of improving the accuracy of their predictions at the cost of making such systems essentially black-box. While in certain tasks such as ad predictions, accuracy is the main objective, in other domains, e.g., in legal, medical, and government, it is essential that the human decision makers who may not have been trained in machine learning can interpret and validate the predictions [20,33].The most popular interpretable techniques that tend to be adopted and trusted by decision makers include classification rules, decision trees, and decision lists [10,29,8,30]. In particular, decision rules with a small number of Boolean clauses tend to be the most interpretable. Such models can be used both to learn interpretable models from the start, and also as proxies that provide post-hoc explanations to pre-trained black-box models [12,1].On the theoretical front, the problem of rule learning was shown to be computationally intractable [32]. Consequently, the earliest practical efforts such as decision list and decision tree approaches relied on a combination of heuristically chosen optimization objectives and greedy algorithmic techniques, and the size of the rule was controlled by either early stopping or ad-hoc rule pruning. Only recently there have been some formulations that attempt to balance the accuracy and the size of the rule in a principled optimization objective either through combinatorial optimization, linear programming (LP) relaxations, submodular optimization, or Bayesian methods [4,25,24] [7,34] as we review in Section 5.Motivated by the significant progress in the development of combinatorial solvers (in particular, MaxSAT), we ask: can we design a combinatorial framework to efficiently construct interpretable classification rules that takes advantage of these recent advances? The primary contribution of this paper is to present a combinatorial framework that enables a precise control of accuracy vs. interpretability, and to verify that the computational advances in the MaxSATcommunity can make it practical to solve large-scale classification problems.In particular, this paper makes following contributions:1. A MaxSAT-based framework, MLIC, that provably trades off accuracy vs. interpretability of the rules 2. A prototype implementation of MLIC based on MaxSAT that is capable of finding optimal (or high-quality near-optimal) classification rules from modern large-scale data-sets 3. We show that in many classification problems interpretability can be achieved at only a minor loss of accuracy, and furthermore, MLIC, which specifically looks for interpretable rules, can learn from much fewer samples than blackbox ML techniques.Furthermore, we hope to share our excitement with applications of constraint programming/MaxSAT in Machine Learning, and to encourage researchers in both interpretable classification and in the CSP/SAT communities to consider this topic further: both in developing new SAT-based formulations for interpretable ML, and in designing bespoke solvers attuned to the problem of interpretable ML.The rest of the paper is organized as follows: We discuss notations and preliminaries in Section 2. We then present MLIC, which is the primary contribution of this paper, in Section 3 and follow up with experimental setup and results over a large set of benchmarks in Section 4. We then discuss related work in Section 5 and finally conclude in Section 7.",no,,no,,no,,,,
1629,https://github.com/GiulioRossetti/ndlib,GiulioRossetti_ndlib.tar.gz,NDlib: a Python Library to Model and Analyze Diffusion Processes Over Complex Networks,@abc.abstractmethod @classmethod @six.add_metaclass(abc.ABCMeta) @staticmethod,PIL copy contextlib warnings sys ndlib multiprocessing past tqdm __future__ future re SALib collections json queue unittest enum os matplotlib sphinx_rtd_theme StringIO six igraph netdispatch scipy codecs sklearn argparse numpy dynetx bokeh abc pyintergraph setuptools mock networkx operator random io,cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/GiulioRossetti_ndlib.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\GiulioRossetti_ndlib.pdf,,,no,,no,,no,,,,
1637,https://github.com/julienraffaud/GMM,julienraffaud_GMM.tar.gz,Greedy Gaussian Segmentation of Multivariate Time Series,,itertools pandas seaborn warnings datetime sklearn numpy matplotlib pylab,math.OC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/julienraffaud_GMM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\julienraffaud_GMM.pdf,,,no,,no,,no,,,,
1638,https://github.com/thaines/helit,thaines_helit.tar.gz,Dirichlet Process Gaussian-mixture model: An application to localizing coalescing binary neutron stars with gravitational-wave observations,@staticmethod,gi copy spacing loo render_word multiprocessing types backsub_dp_c document pruners generators line_layer gamma_cpp ctypes read_cv ms curses pygame re graph_cuts h5py line_graph_c kmeans manager remap helpers view_cv seq_make maxflow line_overlay_layer ds_cpp pydoc pickle conc_cpp tps extract_text record_average lda numpy heapq deinterlace_ev iris solve_weave params_sets dp_utils backsub_dp_cl isotropic marshal chunk_db composite composite_c viewer random threshold render_flow svm frame_crop combine_grid inspect reticle_overlay read_cv_cam utils infer_alpha gaussian_inc generate linear start_cpp render_difference kmeans1 sampling_cpp solvers distutils mask_flow stats_cd bic model kmeans0 unittest opticalflow_lk line_feat make tests hst rule_layer auto_tag os kmeans3 ms_c maxflow_c gen_median string tile_mask gzip goals df dhdp hashlib gcp setProcName dataset colour_bias_cl clip_mask ddp_c time shutil gaussian_prior hg tile_value dec_tree dpgmm homography reflect bz2 gbp operator play write_frame_cv subprocess binary_label render_mask rlda numpy_help_cpp linked_list_cpp block lda_nmp mp_map solve_weave_mp black sys half transform_c record test_p2 params viewport_layer ddp lock_file matrix_cpp smp manager_cl ddhdp texture_cache kde_inc datetime tempfile dp_al line_graph matplotlib light_correct_ms nodes five_word prob_cat scipy wishart utils_gui python_obj_cpp exceptions swood prog_bar let mask_from_colour read_cv_is colour_bias sklearn concentration_dp student_t blur costs video_node corpus ds_link_cpp test_model setuptools errno gmm tile_image misc frf_c gen_random write_frames_cv seq cPickle weave cairo smo solve_shared dir_est_cpp mask_stats mixture exemplars view_pygame write_cv threshold_line kmeans_shared collections json dp_conc smo_aux cv video multiclass pool solve_python_mp mask_sabs kmeans2 gen_classify backsub_dp optparse step_scale blur_c topic doc_gen argparse loo_cov flag_index_array math gbp_c frf p_cat ply2 cvarray gaussian transform smp_cpp skeleton solve_python glyph_db,astro-ph.IM gr-qc,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thaines_helit.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thaines_helit.pdf,,,no,,no,,no,,,,
1641,https://github.com/laonahongchen/Bilevel-Optimization-in-Coordination-Game,laonahongchen_Bilevel-Optimization-in-Coordination-Game.tar.gz,Bi-level Actor-Critic for Multi-agent Coordination,@h_size.setter @decay.deleter @ERMs.deleter @lower_bound.setter @beta.setter @intervals.deleter @l.setter @abc.abstractmethod @l.deleter @type.setter @id.setter @normalise.setter @use_conv.setter @snapshot_gap.setter @maxRe.setter @property @staticmethod @snapshot_dir.setter @snapshot_mode.setter @sequence_len.setter @w_init.setter @ERMs.setter @size.setter @intervals.setter @abstractmethod @layers.setter @maxRe.deleter @episodeCounter.deleter @tau.setter @decay.setter @madrl.setter @eps.deleter @upper_bound.setter @classmethod @eps.setter @gpu.setter @episodeCounter.setter @threshold.setter @contextlib.contextmanager @inc_sync.setter @train_steps.setter @drl.setter,copy tabulate pandas csv contextlib warnings sys malib xxhash critic stats tqdm __future__ highway_env discrete dqn distutils gym net pygame tensorflow bilevel_pg datetime collections fifo_shdqn environment tempfile os actor drl matplotlib exploration six importlib scipy functools itertools cnn2d tflearn saliency maddpg pydoc pickle shlex madrl sets tensorboardX numpy erm time abc tensorflow_probability config shutil math perm setuptools networkx dateutil errno random logging operator bully_q subprocess ops multiagent agent cv2 inspect,cs.MA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/laonahongchen_Bilevel-Optimization-in-Coordination-Game.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\laonahongchen_Bilevel-Optimization-in-Coordination-Game.pdf,,,no,,no,,no,,,,
1651,https://github.com/xrenaa/Human-Motion-Analysis-with-Deep-Metric-Learning,xrenaa_Human-Motion-Analysis-with-Deep-Metric-Learning.tar.gz,Let's Dance: Learning From Online Dance Videos,@staticmethod,math json human_motion_analysis_with_gru torch numpy,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xrenaa_Human-Motion-Analysis-with-Deep-Metric-Learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xrenaa_Human-Motion-Analysis-with-Deep-Metric-Learning.pdf,,,no,,no,,no,,,,
1656,https://github.com/ruotianluo/self-critical.pytorch,ruotianluo_self-critical.pytorch.tar.gz,Controlling Length in Image Captioning,@property @staticmethod,"PIL copy pyciderevalcap csv yaml multiprocessing sys skimage lmdbdict tqdm mmap zlib __future__ base64 torchvision os, sys captioning json collections h5py transformers pytorch_lightning tempfile torch os string six scipy functools itertools hashlib codecs typing subword_nmt pickle argparse numpy eval_utils time math m2transformer pycocotools setuptools traceback random logging detectron2 models yacs pycocoevalcap io",cs.LG cs.AI cs.CV cs.CV cs.CL cs.CV cs.CV cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ruotianluo_self-critical.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ruotianluo_self-critical.pytorch.pdf,Self-critical Sequence Training  REINFORCE  REINFORCE,,no,,no,,no,,,,
1664,https://github.com/shifwang/paper-debiased-feature-importance,shifwang_paper-debiased-feature-importance.tar.gz,A Debiased MDI Feature Importance Measure for Random Forests,,scipy pandas sys UFI treeinterpreter shap sklearn numpy matplotlib irf,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shifwang_paper-debiased-feature-importance.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shifwang_paper-debiased-feature-importance.pdf,,,no,,no,,no,,,,
1672,https://github.com/dongkwonjin/Semantic-Line-MWCS,dongkwonjin_Semantic-Line-MWCS.tar.gz,Harmonious Semantic Line Detection via Maximal Weight Clique Selection,@property,PIL torchvision libs tests torch os evaluation_DHT matplotlib visualizes post_processes pickle networks sklearn numbers numpy trains shutil math datasets cv2 random options evaluation,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dongkwonjin_Semantic-Line-MWCS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dongkwonjin_Semantic-Line-MWCS.pdf,,,no,,no,,no,,,,
1677,https://github.com/id9502/Option-GAIL,id9502_Option-GAIL.tar.gz,Adversarial Option-Aware Hierarchical Imitation Learning,@staticmethod,utils copy sys default_config ast model torch os matplotlib envir typing argparse numpy time pre_train math rlbench random inspect,cs.LG cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/id9502_Option-GAIL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\id9502_Option-GAIL.pdf,,,no,,yes,"batch size, learning rate",no,,,,
1678,https://github.com/WenboGong/MetaSGMCMC,WenboGong_MetaSGMCMC.tar.gz,Meta-Learning for Stochastic Gradient MCMC,,copy BNN_Dataloader pdb Cifar_Dataloader CNN_Sampler Test_Module BNN_Q_D Util torchvision json collections NNSGHMC_toy SGHMC_toy torch os Stein matplotlib IPython gpustat Training_func BNN_Util argparse numpy BNN_Sampler time BNN_Model_def BNN_training_func CNN_training_func CNN_Module operator CNN_Q_D,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/WenboGong_MetaSGMCMC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\WenboGong_MetaSGMCMC.pdf,,,no,,no,,no,,,,
1680,https://github.com/zhuzhicai/SQuAD2.0-Baseline-Test-with-BiDAF-No-Answer,zhuzhicai_SQuAD2.0-Baseline-Test-with-BiDAF-No-Answer.tar.gz,Zero-Shot Relation Extraction via Reading Comprehension,"@app.route('/post_query', method='post') @app.route('/query', method='post') @app.route('/') @property @staticmethod",requests bottle csv sys my ujson cnn_dm jinja2 tqdm __future__ pprint atexit re tensorflow json collections squad tree basic tempfile basic_cnn nltk os socket bna matplotlib string gzip functools itertools hashlib codecs pickle http argparse numpy time shutil math matplotlib_venn socketserver networkx errno operator random logging subprocess docopt zipfile,cs.CL cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zhuzhicai_SQuAD2.0-Baseline-Test-with-BiDAF-No-Answer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zhuzhicai_SQuAD2.0-Baseline-Test-with-BiDAF-No-Answer.pdf,,,no,,no,,no,,,,
1681,https://github.com/rkrohn/redditmodel,rkrohn_redditmodel.tar.gz,Modelling structure and predicting dynamics of discussion threads in online boards,,utils gc copy data_utils ttensor pandas csv warnings sys multiprocessing fit_weibull glob fit_partial_cascade tqdm CascadeTree fit_cascade_gen_model MHP pymongo gensim re tensorflow functions_gen_cascade_model dtensor Node functions_paper_model datetime collections json node2vec cascade_manip tensorly nltk functions_hybrid_model os ktensor socsim_data_functions_gen_cascade_model matplotlib string cascade_analysis scipy functools itertools IPython functions_comparative_hawkes gzip fit_lognormal file_utils bisect plot_utils zss pickle sim_tree sklearn argparse functions_prebuild_model numpy reddit_mongo_data_utils time shutil fit_cascade load_model_data math tarfile ParamGraph networkx operator random logging statistics subprocess tree_edit_distance functions_baseline_model PostParamGraph,cs.SI math.PR cs.SI physics.soc-ph stat.AP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rkrohn_redditmodel.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rkrohn_redditmodel.pdf,,,no,,no,,no,,,,
1682,https://github.com/jagatabhay/TSAI,jagatabhay_TSAI.tar.gz,Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates,@property,utils copy packaging tqdm __future__ torchvision datetime tempfile torch os google matplotlib main3 apex transforms2 numpy time shutil torchsummary albumentations cv2 statistics resnetmodel zipfile,cs.LG cs.CV cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jagatabhay_TSAI.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jagatabhay_TSAI.pdf,,,no,,no,,no,,,,
1683,https://github.com/abdkhanstd/COVID-19,abdkhanstd_COVID-19.tar.gz,Blockchain-Federated-Learning and Deep Learning Models for COVID-19 detection using CT Imaging,@threadsafe_generator @staticmethod,"utils copy csv sys multiprocessing glob nibabel skimage tqdm __future__ processor torchvision threading capsuels pathlib keras tensorflow collections h5py data torch os scipy functools sklearn argparse tensorboardX numpy time numpy, scipy extractor runstats random operator models model_caps_iv3",eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/abdkhanstd_COVID-19.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\abdkhanstd_COVID-19.pdf,Concatenated Skip Connection  Dense Block  1x1 Convolution  Dense Connections  Dropout  Batch Normalization  Residual Connection  Softmax  Rectified Linear Units  Bottleneck Residual Block  DenseNet  Max Pooling  Average Pooling  Convolution  Residual Block  Global Average Pooling  Kaiming Initialization  Residual Network,,no,,no,,no,,,,
1687,https://github.com/RyanXLi/OneshotDet,RyanXLi_OneshotDet.tar.gz,One-Shot Object Detection without Fine-Tuning,"@C2_FORMAT_LOADER.register(""R-50-FPN-RETINANET"") @registry.BACKBONES.register(""R-101-FPN"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""FBNet.roi_head_keypoints"") @C2_FORMAT_LOADER.register(""R-50-C5"") @C2_FORMAT_LOADER.register(""R-101-FPN"") @registry.ROI_BOX_PREDICTOR.register(""FastRCNNPredictor"") @registry.BACKBONES.register(""R-50-C5"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPNXconv1fcFeatureExtractor"") @registry.ROI_BOX_PREDICTOR.register(""FPNPredictor"") @registry.ROI_KEYPOINT_PREDICTOR.register(""KeypointRCNNPredictor"") @registry.BACKBONES.register(""R-50-FPN"") @registry.BACKBONES.register(""MNV2-FPN-RETINANET"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPN2MLPFeatureExtractor"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""FBNet.roi_head_mask"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNC4Predictor"") @C2_FORMAT_LOADER.register(""R-101-C4"") @registry.BACKBONES.register(""R-101-C5"") @property @once_differentiable @staticmethod @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FBNet.roi_head"") @registry.BACKBONES.register(""R-101-C4"") @C2_FORMAT_LOADER.register(""R-50-C4"") @registry.RPN_HEADS.register(""FBNet.rpn_head"") @registry.BACKBONES.register(""R-101-FPN-RETINANET"") @registry.ROI_KEYPOINT_FEATURE_EXTRACTORS.register(""KeypointRCNNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-101-FPN-RETINANET"") @registry.BACKBONES.register(""R-152-FPN"") @registry.BACKBONES.register(""R-50-C4"") @registry.BACKBONES.register(""FBNet"") @registry.BACKBONES.register(""R-50-FPN-RETINANET"") @registry.ROI_MASK_FEATURE_EXTRACTORS.register(""MaskRCNNFPNFeatureExtractor"") @C2_FORMAT_LOADER.register(""R-152-FPN"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""FPN2ROIFeatureExtractor"") @registry.RPN_HEADS.register(""SingleConvRPNHead"") @C2_FORMAT_LOADER.register(""R-101-C5"") @C2_FORMAT_LOADER.register(""R-50-FPN"") @unittest.skipIf(not TEST_CUDA, ""no CUDA detected"") @registry.ROI_MASK_PREDICTOR.register(""MaskRCNNConv1x1Predictor"") @registry.ROI_BOX_FEATURE_EXTRACTORS.register(""ResNet50Conv5ROIFeatureExtractor"")","utils PIL copy cv2, os multiprocessing sys glob tqdm __future__ torchvision pylab os, sys csHelpers datetime json collections unittest h5py tempfile env_tests torch os matplotlib importlib scipy IPython xml itertools bisect imp pickle argparse cityscapesscripts tensorboardX numpy time shutil math pycocotools setuptools predictor maskrcnn_benchmark errno cv2 random logging yacs",cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/RyanXLi_OneshotDet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\RyanXLi_OneshotDet.pdf,,,no,,no,,no,,,,
1688,https://github.com/pmcgrath249/DeepCVLab,pmcgrath249_DeepCVLab.tar.gz,Scalability in Perception for Autonomous Driving: Waymo Open Dataset,,copy warnings sys tqdm easydict torchvision pathlib re tensorflow datetime collections json torch os waymo_open_dataset six pickle numpy setuptools logging,cs.CV cs.LG cs.CV cs.LG stat.ML cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pmcgrath249_DeepCVLab.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pmcgrath249_DeepCVLab.pdf,Concatenated Skip Connection  Convolution  Average Pooling  Global Average Pooling  Kaiming Initialization  1x1 Convolution  Batch Normalization  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  Nesterov Accelerated Gradient  Weight Decay  Step Decay  DenseNet  Dense Block  Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,no,,no,,no,,,,
1689,https://github.com/weihaosky/MFuseNet,weihaosky_MFuseNet.tar.gz,Active Perception with A Monocular Camera for Multiscopic Vision,,"PIL os, argparse, time, cv2 dataloader random os, cv2 torch, torchvision torch os network preprocess numpy matplotlib fuse_utils torchvision",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/weihaosky_MFuseNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\weihaosky_MFuseNet.pdf,,,no,,no,,no,,,,
1691,https://github.com/dermatologist/nlp-qrmine,dermatologist_nlp-qrmine.tar.gz,"QRMine: A python package for triangulation in Grounded Theory Preprint, compiled March 31, 2020","@click.option('--verbose', '-v', is_flag=True, help=""Will print verbose messages."") @click.option('--sentence', is_flag=True, default=False, @pytest.fixture @click.option('--num', '-n', multiple=False, default=3, @click.option('--cart', is_flag=True, @click.option('--knn', is_flag=True, @click.option('--rec', '-r', multiple=False, default=3, @click.option('--svm', is_flag=True, @csvfile.setter @click.option('--inp', '-i', multiple=True, @click.option('--titles', '-t', multiple=True, @click.option('--cat', is_flag=True, @epochs.setter @content.setter @click.option('--nnet', is_flag=True, @property @staticmethod @click.option('--summary', is_flag=True, @click.option('--assign', is_flag=True, @seed.setter @titles.setter @click.option('--out', '-o', multiple=False, default='', @click.option('--topics', is_flag=True, @click.option('--csv', multiple=False, default='', @click.option('--kmeans', is_flag=True, @click.option('--sentiment', is_flag=True, @click.option('--pca', is_flag=True, @click.option('--codedict', is_flag=True, @click.option('--nlp', is_flag=True, @click.command() @documents.setter @click.option('--filters', '-f', multiple=True,",pandas warnings sys imblearn __future__ tensorflow re vaderSentiment click src os qrmine en_core_web_sm pytest sphinx pkg_resources importlib_metadata importlib textacy mlxtend sklearn numpy xgboost shutil setuptools operator random recommonmark subprocess inspect,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dermatologist_nlp-qrmine.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dermatologist_nlp-qrmine.pdf,,,no,,no,,no,,,,
1697,https://github.com/oaraque/moral-foundations,oaraque_moral-foundations.tar.gz,MoralStrength: Exploiting a Moral Lexicon and Embedding Similarity for Moral Foundations Prediction,@pytest.fixture,math moralstrength pandas spacy gsitk setuptools glob pickle data sklearn os pytest numpy io,cs.CL cs.CY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oaraque_moral-foundations.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oaraque_moral-foundations.pdf,Logistic Regression,"Experimental DesignTo evaluate the potentials of the MoralStrength lexicon, we postulate the problem as a classification task. In particular, we employ the three approaches previously described, namely Moral Freq, Moral Stats, and SIMON, to predict the moral rhetoric in each of the aforementioned datasets (see section 3.4).In our experimental design, we include a basic Bag-of-Words (unigram) model providing a standardized way of obtaining a baseline in the computational linguistics field. We also report as a baseline the frequency counts employing the original MFD. We built a series of logistic regression models; firstly, we assess 6 The original corpus is available at https://github.com/t-davidson/ hate-speech-and-offensive-language/tree/master/data.  the predictive power of the unigrams, Moral Freq, Moral Stats, and SIMON lexicon methods alone. Then, we train logistic regression models concatenating the features extracted by the above approaches 8 . In this way, we examine the effective performance of both engineered and word embedding features in analyzing user-generated text. We also combine the unigrams to the proposed lexicon approaches described above. Hence, for each dataset and moral dimension, we train a series of logistic regression models following a 10-fold cross-validation scheme.We then report the F1-score as the evaluation metric per moral dimension since this is the once employed in the majority of the related studies.To directly compare our proposed framework with the current state-of-theart approach of Lin et al. [40], we replicated their same configuration. Namely, we perform over-sampling on the original dataset to overcome the highly imbalanced nature of the benchmark data (see Section 3.4). After over-sampling on the Hurricane Sandy data, we resulted with an average number of training examples, N = 6, 128, instead of the original dataset size, N = 3, 478 (see Table 2).Since over-sampling implies ""artificial"" data samples, we propose an alternative methodology; more specifically, we performed under-sampling, which also deals with the issue of unbalanced classes, however, in doing so, it randomly excludes data points of the most populated class. In this way, for the Hurricane Sandy we had N = 824 data points (see Table 2. By reporting the score for both methods, we ensure the results are not biased by the technique used to address the class imbalance.For all experiments, we report the performance in terms of F1-score, which is the metric also employed by Lin et al. [40], as well as the average F1-score over all moral dimensions. Moreover, to compare the improvement of the simplest model, which for this study we consider being the Moral Freq model, we employ the Friedman statistical test [47], which yields a ranking of the proposed method ordered by their performance. To obtain further insights on the statistical significance of our obtained results for the baseline model the Bonferroni-Dunn [47] post-hoc statistical test is performed with α = 0.05.",no,,no,,no,,,,
1698,https://github.com/ictnlp/TLAT-NMT,ictnlp_TLAT-NMT.tar.gz,Token-level Adaptive Training for Neural Machine Translation,"@register_optimizer('adagrad') @register_optimizer('sgd') @register_task('language_modeling') @register_optimizer('adadelta') @register_lr_scheduler('reduce_lr_on_plateau') @register_model_architecture('transformer', 'transformer_iwslt_de_en') @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_de_big') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_model_architecture('masked_lm', 'masked_lm') @register_model_architecture('transformer_lm', 'transformer_lm') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_optimizer('lamb') @register_model_architecture('transformer', 'transformer_wmt_en_de') @register_model('transformer_lm') @register_task('translation_moe') @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_model_architecture('masked_lm', 'bert_large') @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @register_model_architecture('transformer', 'transformer_wmt_en_de_big_t2t') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_model_architecture('transformer_lm', 'transformer_lm_big') @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @s3_request @register_task('cross_lingual_lm') @register_model('lightconv') @register_model('transformer') @register_task('multilingual_translation') @register_model_architecture('transformer', 'transformer') @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @register_model('fconv_lm') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture('transformer', 'transformer_wmt_en_de_big') @register_criterion('masked_lm_loss') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_optimizer('nag') @property @register_criterion('adaptive_loss') @register_criterion('cross_entropy') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @wraps(func) @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @register_model('masked_lm') @contextlib.contextmanager @register_model_architecture('fconv', 'fconv') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_model_architecture('lightconv', 'lightconv') @register_model('lightconv_lm') @register_model('lstm') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @register_task('masked_lm') @register_lr_scheduler('triangular') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_optimizer('adam') @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @staticmethod @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_model('fconv') @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_fr_big') @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_criterion('label_smoothed_cross_entropy') @register_task('translation') @register_model_architecture('fconv', 'fconv_iwslt_de_en')",requests copy scripts contextlib warnings sys multiprocessing types sacremoses generate fairseq pdb tqdm __future__ ctypes fileinput generator pathlib re collections json unittest botocore tests fnmatch tempfile torch os socket eval_lm html importlib struct interactive apex functools itertools gzip hashlib bisect builtins urllib typing pickle boto3 argparse subword_nmt urlparse numbers tensorboardX numpy shlex preprocess time shutil sentencepiece train math tarfile setuptools traceback sacrebleu random logging operator subprocess inspect io,cs.CL cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ictnlp_TLAT-NMT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ictnlp_TLAT-NMT.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,,no,,no,,no,,,,
1699,https://github.com/yoojy31/mdod,yoojy31_mdod.tar.gz,Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image,@roidb_handler.setter @ abc.abstractmethod @abc.abstractmethod @property @staticmethod @ staticmethod,PIL copy timm pandas yaml sys pdb lib tqdm __future__ torchvision util tensorflow uuid json option torch os matplotlib scipy xml seaborn pickle argparse tensorboardX numpy time abc shutil math pycocotools random cv2,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yoojy31_mdod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yoojy31_mdod.pdf,,,no,,no,,no,,,,
1709,https://github.com/mclaughlin6464/ThirdPlace,mclaughlin6464_ThirdPlace.tar.gz,Detection of satellite remnants in the Galactic Halo with Gaia -III. Detection limits for Ultra Faint Dwarf Galaxies,,pandas pytorch_wavelets glob skimage imutils healpy h5py torch os matplotlib scipy tables gaia_query sklearn argparse ugali numpy math networkx cv2 pywt,astro-ph.GA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mclaughlin6464_ThirdPlace.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mclaughlin6464_ThirdPlace.pdf,,,no,,no,,no,,,,
1711,https://github.com/context-mover/HypEval,context-mover_HypEval.tar.gz,Experiments with Three Approaches to Recognizing Lexical Entailment,,hypeval sys collections setuptools glob random logging sklearn os __future__ numpy pprint time io,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/context-mover_HypEval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\context-mover_HypEval.pdf,,"|Precision, recall, F-measure, and accuracyLike AP, precision and recall were originally designed as performance measures for information retrieval systems. The precision of a system is an estimate of the conditional probability that a document is truly relevant to a query, if the system says it is relevant. The recall of a system is an estimate of the conditional probability that the system will say that a document is relevant to a query, if it truly is relevant.There is a tradeoff between precision and recall; one may be optimized at the cost of the other. The F-measure is the harmonic mean of precision and recall. It is designed to reward a balance of precision and recall.Accuracy is a natural and intuitive performance measure, but it is sensitive to the relative sizes of the classes. It is easy to interpret accuracy when we have two equal-sized classes, but it is difficult to interpret when one class is much larger than the other. The F-measure is a better measure when the classes are not balanced.As with AP, there are two variations of precision, recall, and F-measure, depending on whether we focus on class 0 or class 1. Let C be a 2 × 2 confusion matrix, where c ij is the number of word pairs that are actually in class i and the algorithm has predicted that they are in class j (here i, j ∈ {0, 1}). We define precision, recall, and F-measure as follows:Pre 0 = c 00 /(c 00 + c 10 ) (4) Pre 1 = c 11 /(c 11 + c 01 ) (5) Rec 0 = c 00 /(c 00 + c 01 ) (6) Rec 1 = c 11 /(c 11 + c 10 ) (7) F 0 = 2 • Pre 0 • Rec 0 /(Pre 0 + Rec 0 ) (8) F 1 = 2 • Pre 1 • Rec 1 /(Pre 1 + Rec 1 )(9)Following standard practice (Witten, Frank, and Hall 2011), we merge the two variations of each measure by taking their weighted averages, where the weights are determined by the class sizes:w 0 = (c 00 + c 01 )/(c 00 + c 01 + c 10 + c 11 ) (10) w 1 = (c 11 + c 10 )/(c 00 + c 01 + c 10 + c 11 ) (11) Pre = w 0 • Pre 0 + w 1 • Pre 1 (12) Rec = w 0 • Rec 0 + w 1 • Rec 1 (13) F = w 0 • F 0 + w 1 • F 1 (14)Finally, we define accuracy as usual:Acc = 100 • (c 00 + c 11 )/(c 00 + c 01 + c 10 + c 11 ) (15)The factor of 100 converts the accuracy from a fraction to a percentage score.6 Three approaches to lexical entailmentIn this section, we discuss the three approaches to RLE and describe the algorithms for each approach in detail. All three approaches are based on word-context matrices. For an introduction to the concepts behind word-context matrices, see the survey paper by Turney and Pantel (2010).In preliminary experiments with our development datasets, Dev1 and Dev2, we tuned the three approaches to optimize their performance. We describe how Dev1 and Dev2 were generated in Section 8.1.1. For each algorithm, we selected the matrix or matrices that were most accurate with the development data. For both balAPinc and ConVecs, we chose the word-context matrix from Turney, Neuman, Assaf, and Cohen (2011). For SimDiffs, we chose two word-context matrices from Turney (2012). 3 ConVecs and SimDiffs use support vector machines (SVMs) for supervised learning. We used the development datasets to select the best kernels for the SVMs. The best kernel for ConVecs was a second-degree polynomial kernel and the best kernel for SimDiffs was a radial basis function (RBF) kernel. Experimental setupFor the experiments, we split the dataset into three (approximately) equal parts, two development sets (Dev1 and Dev2) and one test set (Test). The splits were random, except the balance of the class sizes was maintained in all three subsets. Dev1 and Dev2 both contain 768 pairs and Test contains 772 pairs.Table 5 shows the number of word pairs in the Test set for each of the ten highlevel categories. In Tables 2 and 3, we see that a ||= b is 0 for all subcategories of  and 3, b ||= a is 1 for all subcategories of the category class-inclusion, so it is not surprising to see that there are 55 pairs for b ||= a in the row for class-inclusion in Table 5 and 0 pairs for b ||= a. The number of pairs labeled entails is 261+125 = 386 and the number labeled does not entail is 176 + 210 = 386. balAPinc measure has two parameters to tune, max F for the maximum number of features and T as a threshold for classification. On Dev1, we calculated balAPinc five times, using five different values for max F , 1000, 2000, 3000, 4000, and 5000. For each given value of max F , we set T to the value that optimized the F-measure on Dev1. This gave us five pairs of values for max F and T . We tested each of these five settings on Dev2 and chose the setting that maximized the F-measure, which was max F = 1000. The balAPinc measure is robust with respect to the parameter settings. The accuracy on Dev2 ranged from 56.5% with max F = 1000 to 52.5% with max F = 5000. We kept the best max F setting, but we tuned T again on the union of Dev1 and Dev2. With these parameter settings, we then applied balAPinc to the Test set.ConVecs has two parameters to tune, k and p for U k Σ p k . For k, we tried 100, 200, 300, 400, and 500. For p, we tried ten values, from 0.1 to 1.0 in increments of 0.1. For each of the fifty pairs of values for k and p, we ran Weka, using Dev1 as training data and Dev2 as testing data. The maximum F-measure on Dev2 was achieved with k = 100 and p = 0.4. ConVecs is robust with respect to the parameter settings. The accuracy on Dev2 ranged from a high of 70.1% to a low of 64.6%. We then ran Weka one more time, using k = 100 and p = 0.4, with the union of Dev1 and Dev2 as training data and Test as testing data.SimDiffs has four parameters to tune, k d and p d for domain space and k f and  Experimental setupWe experimented with four different ways of splitting the dataset. The Evaluation column in Table 10 indicates the experimental setup (dataset splitting).The standard evaluation is ten-fold cross-validation in which the folds are random. This evaluation yields relatively high scores, because, although every pair in the KDSZ dataset is unique, many pairs share a common term. This makes supervised learning easier, because a pair in the testing fold will often share a term with several pairs in the training folds.The clustered evaluation is designed to be more challenging than the standard evaluation. The clustered evaluation is ten-fold cross-validation with non-random folds. We put pairs that share a common term into the same fold. Due to the large number of pairs with shared terms, it is not possible to construct ten folds such that there are absolutely no terms that are shared by any two folds. Therefore we gave a high priority to isolating the most common shared words to single folds, but we allowed a few less common shared words to appear in more than one fold. Thus a pair in the testing fold will only rarely share a term with pairs in the training folds.The standard and clustered evaluations have more examples in class 0 (does not entail) than in class 1 (entails). The balanced dataset takes the clustered evaluation a step further, by first clustering folds and then randomly removing pairs labeled as class 0, until the folds all have an equal number of pairs in both classes.For the different evaluation, instead of cross-validation, the algorithms are trained on the JMTH dataset and tested on the KDSZ dataset, after the KDSZ dataset has been balanced by randomly removing pairs labeled as class 0.The balAPinc measure has two parameters, max F for the maximum number of features and T as a threshold for classification. In all four experimental setups, we used the setting max F = 1000, based on the tuning experiments with the JMTH dataset (Section 8.1). For T , we used the training split in each of the four experimental setups. For the standard, clustered, and balanced setups, the training split is the nine folds used for training in each step of the ten-fold cross-validation. For the different setup, the training split is the whole JMTH dataset. For all four setups, we set T to the value that optimized the F-measure on the training split.ConVecs has two parameters to tune, k and p for U k Σ p k . In all four experimental setups, we used k = 100 and p = 0.4, based on the experiments with the JMTH dataset. The training splits were used to teach the supervised learning algorithm (the polynomial kernel SMO SVM in Weka).SimDiffs has four parameters to tune. We used k d = k f = 200 and p d = p f = 0.6, based on the experiments with the JMTH dataset. The training splits were used to teach the supervised learning algorithm (the RBF kernel SMO SVM in Weka). ResultsIn Table 10, the four experimental setups (standard, clustered, balanced, and different) are given in order of increasing challenge and increasing realism. Of the four experimental setups, we believe that the different evaluation is the most challenging and most realistic. If an RLE module is part of a commercial RTE system, the module will inevitably encounter word pairs in the field that are quite different from the pairs it saw during training. The different evaluation comes closest to approximating field usage.On the different evaluations, balAPinc achieves an accuracy of 58.2%, ConVecs has an accuracy of 56.1%, and SimDiffs reaches 57.4%. There is no statistically significant difference between any of these accuracies, according to Fisher's Exact Test at the 95% confidence level.With ConVecs and SimDiffs, compared to balAPinc, there is a relatively large gap between the standard performance and the different performance. This is be- Note that the gap between the standard performance and the different performance is not simply a question of the quantity of data. In the different setup, there is a qualitative difference between the training data and the testing data. Increasing the size of the training dataset with more data of the same type will not be helpful. The goal of the different setup is to test the ability of the algorithms to bridge the qualitative gap between the training and testing data. This qualitative gap is more challenging for supervised learning than a quantitative gap. It is a gap that learning algorithms inevitably face in real applications (Pan and Yang 2010).The KDSZ dataset has been used in previous research, but the past results are not comparable with our results. Kotlerman et al. (2010) reported AP 1 without AP 0 , but there is a trade-off between AP 1 and AP 0 . Kotlerman et al. (2010) did not attempt to evaluate balAPinc as a classifier, so they did not report precision, recall, F-measure, or accuracy. Discussion of resultsTable 12 summarizes the accuracy results from the experiments. For the KDSZ and BBDS experiments, only the different evaluation is shown. Bold font is used to mark the cases where the accuracy is significantly less than the accuracy of SimDiffs. In no case is the accuracy significantly greater than the accuracy of SimDiffs.The JMTH dataset is based on seventy-nine types of semantic relations. The pairs in this dataset were labeled in accordance with the relational definition of lexical entailment (see Section 2). This explains why balAPinc, which was designed with the substitutional definition in mind, performs poorly on the JMTH dataset. ConVecs and SimDiffs were designed for semantic relation classification, so it is not surprising that they perform much better than balAPinc.The KDSZ dataset was labeled using the substitutional definition of lexical entailment (see Section 2). On this dataset, there is no statistically significant difference between any of the algorithms. This is the ideal dataset for balAPinc, the dataset for which it was designed, so it is natural that balAPinc has the highest accuracy. On the other hand, we see that the two learning algorithms handle this dataset well, although they were trained on the JMTH dataset (recall that this is the different setup), which is quite different from the KDSZ dataset. It is good that they are both able to cope with the qualitative difference between the training data and the testing data.All of the positive pairs in the BBDS dataset are instances of the hyponymhypernym semantic relation. Instances of this relation are substitutable, so bal-APinc is designed to handle them. ConVecs was also designed specifically for this dataset, and we see from Table 11 that ConVecs reaches an accuracy of 87.6% when the training data is similar to the testing data. However, ConVecs has trouble bridging the qualitative gap between the training data (the JMTH dataset) and the testing data with the different setup. On the other hand, SimDiffs is able to bridge this gap.We have argued that the different evaluation is the most realistic scenario, but it could be argued that the entails class is more important than the does not entail this puts the emphasis on the entails class. For the KDSZ and BBDS datasets, we report the clustered setup. This is closer to the evaluation setup of Kotlerman et al. (2010). In this table, we do not use bold font to mark significant differences, because there is no agreement on the appropriate statistical test for AP 1 .Although Tables 12 and 13 are based on different scores and experimental setups, both support SimDiffs and the similarity differences hypothesis. More generally, they suggest that second-order features are useful for modeling lexical entailment. They also suggest that it is beneficial to use two different spaces when constructing features for lexical entailment.Manually designing an asymmetric similarity measure is a difficult task, as we can see from the equations in Section 6.1. We believe that lexical entailment is more tractable when it is approached as a supervised learning problem. The effort involved in manually designing feature vectors is less than that required for designing similarity measures. The performance of SimDiffs indicates that supervised learning can yield better results than manually designing measures.|",no,,no,,no,,,,
1712,https://github.com/eserie/wax-ml,eserie_wax-ml.tar.gz,WAX-ML: A Python library for machine learning and feedback loops on streaming data,"@eager_function @pytest.mark.parametrize(""use_jit"", [False, True]) @final @pytest.fixture @pytest.mark.parametrize(""dtype"", [""float32"", ""float64""]) @functools.wraps(f) @pytest.mark.parametrize(""p"", [0, 1, 2, 3, 4, ep.inf]) @partial(HaikuEnv, iter(raw_observations_array()), name=""Env"") @jax.jit @overload @jax.value_and_grad @pytest.mark.parametrize(""tensor_type"", [""numpy"", ""jax"", ""tensorflow"", ""torch""]) @compare_allclose @pytest.mark.parametrize( @functools.wraps(func) @overload  # noqa: F811 (waiting for pyflakes > 2.1.1) @compare_allclose(rtol=1e-6) @hk.transform @pytest.fixture(scope=""session"", params=[""t1"", ""t2""]) @overload  # noqa: F811 @common_dtype @partial(HaikuEnv, data_generator) @partial(HaikuEnv, iter(raw_observations_dataarray()), name=""Env"") @pytest.mark.parametrize(""f"", [ep.logical_and, ep.logical_or]) @dataclass @partial(unroll_transform_with_state, dynamic=False) @pytest.mark.parametrize(""astensor"", [False, True]) @unroll_transform_with_state @property @unroll @partial(HaikuAgent, name=""Agent"") @pytest.mark.parametrize(""i"", [0, 1]) @pytest.mark.parametrize(""format"", [""dataframe""]) @pytest.mark.parametrize(""assume_centered"", [False, True]) @staticmethod @partial(jax.jit, device=gpus[0]) @partial(HaikuEnv, iter(raw_observations_dataframe()), name=""Env"") @pytest.mark.parametrize(""format"", [""dataarray"", ""dataframe"", ""series""]) @samedevice @jit_init_apply @pytest.mark.parametrize(""t1, t2"", TEST_INT_LIST) @pytest.mark.parametrize(""dtype"", [""<M8[us]"", ""<M8[ns]""]) @pytest.mark.parametrize(""value"", [1, -1, 2]) @abstractmethod @pytest.mark.parametrize(""seed"", [onp.int32(10), onp.int64(10), 10]) @contextmanager @pytest.mark.parametrize(""fill_value"", [0.0, 0, ""0""]) @partial(HaikuEnv, iter(raw_observations()), name=""Env"") @add_batch @pytest.mark.parametrize(""axis"", [None, 0, 1, (0, 1)]) @hk.transform_with_state @pytest.mark.parametrize(""seed"", [10.0]) @pytest.mark.parametrize(""keepdims"", [False, True]) @HaikuAgent @classmethod @abstractmethod  # noqa: F811 (waiting for pyflakes > 2.1.1) @pytest.mark.parametrize(""static"", [True, False]) @compare_all @pytest.fixture(scope=""session"") @pytest.mark.parametrize(""axis"", [0, 1, -1]) @partial(unroll_transform_with_state, skip_first=skip_first) @partial(add_batch, take_mean=False) @pytest.mark.parametrize(""adjust"", [False, True, ""linear""]) @pytest.mark.parametrize(""indexing"", [""ij"", ""xy""]) @compare_allclose(rtol=1e-5) @pytest.mark.parametrize(""others_astensors"", [False, True]) @pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf]) @tf.function @tf.autograph.experimental.do_not_convert @compare_equal @dataclass(frozen=True)",requests packaging pandas warnings sys types glob plotnine optax tqdm pathlib tensorflow haiku datetime collections dataclasses torch os pytest matplotlib jax importlib functools itertools seaborn typing sklearn numbers numpy abc setuptools typing_extensions logging xarray wax inspect io,cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eserie_wax-ml.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eserie_wax-ml.pdf,,,no,,no,,no,,,,
1715,https://github.com/NeuroCSUT/intentions,NeuroCSUT_intentions.tar.gz,Do deep reinforcement learning agents model intentions?,@classmethod,PIL copy gym tensorflow collections os matplotlib Video maddpg policy pickle sklearn argparse skvideo pyglet numpy time train math setuptools random multiagent ensemble,cs.LG cs.AI cs.NE cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/NeuroCSUT_intentions.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\NeuroCSUT_intentions.pdf,Experience Replay  Dense Connections  Weight Decay  Rectified Linear Units  Adam  Convolution  Batch Normalization  MADDPG  Q-Learning  Experience Replay  Dense Connections  Weight Decay  Rectified Linear Units  Adam  Convolution  Batch Normalization  MADDPG,"METHODSOur experiments are based on a cooperative navigation task described in [9]. In this task three agents try to cover three landmarks and have to coordinate which agent covers which landmark (see Figure 1). The reward at every time step isr = i min j (d i j ) − cwhere d i j is the distance from landmark i to agent j and c is the number of collisions. These rewards incentivize that there is exactly one agent close to each landmark and there are as few collisions as possible. The observation of each agent consists of 14 real values: velocity of the agent (2), position of the agent (2), egocentric coordinates of all landmarks (6) and egocentric coordinates of all other agents (4). Action of an agent consists of 5 real values: acceleration in four possible directions (only positive values) and a dummy value for no action.We follow the basic training scheme from [9] and we use the MADDPG algorithm with default settings. The network has two hidden layers with 128 nodes each. We train the agents for 100,000 episodes, when approximately the convergence happens. Thereafter we evaluate the agents for 4,000 episodes and record the observations, hidden states (from both layers) and actions of each agent.To decode other agents' goals we train a linear readout neuron to predict the final landmark (the landmark covered at the final timestep of the episode) of other agents from each agent's hidden state. This is treated as a classification task with three landmarks as the three classes (the readout model has three outputs). For comparison we also try to predict the final landmark from the observation (network's input) and the actions (network's output). We separate the 4,000 episodes randomly into training and test set (test set size was 25%) and report classification accuracy on the test set. The training is done separately for each of the 25 time steps (all episodes are of the same length). Use of a linear model guarantees that the information is present explicitly in the model's input.To test the generalization ability of the agents we put them together with two ""Sheldon"" agents -agents that go straight to a fixed landmark (""their spot""). This leaves one landmark free for the trained agent to cover. We run an evaluation for 9 possible combinations of 3 agents and 3 free landmarks. One evaluation lasts 4000 episodes and we report the percentage of episodes where all landmarks were covered. We compare this with the same measure from previous evaluations where all agents were trained agents. All results are averaged over 5 random seeds.Finally we tried three possible modifications to MADDPG algorithm to alleviate the generalization issues:• MADDPG + shuffle: randomize the order of agents for each episode. The order determines the position of other agents' data in agent's observation. Randomizing makes it impossible to have fixed assumptions about other agents' behavior.• MADDPG + shared: use a shared model for all agents, making them basically equivalent and eliminating the option for landmark biases. • MADDPG + ensemble: use an ensemble of agents for each position as suggested in [9]. The agents in an ensemble develop different policies because of different random initialization and different training samples they see from replay. This increases the diversity of partners any agent experiences, which forces it to develop more general strategies.",no,,no,,no,,,,
1720,https://github.com/fredshone/citychef,fredshone_citychef.tar.gz,Scalable Population Synthesis with Deep Generative Modeling,@vectorize @property @staticmethod,pandas shapely geopandas datetime collections enum os matplotlib scipy gzip itertools sklearn lxml numpy halo setuptools networkx logging pyproj,stat.ML cs.LG cs.MA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fredshone_citychef.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fredshone_citychef.pdf,AutoEncoder  Variational Autoencoder,,no,"grid search, RMSProp",yes,"hidden layers, learning rate, batch size, epochs",no,,,,
1721,https://github.com/otiliastr/coper,otiliastr_coper.tar.gz,Contextual Parameter Generation for Knowledge Graph Link Prediction,@abc.abstractmethod @property @staticmethod,"requests copy yaml warnings glob tqdm __future__ os, sys tensorflow qa_cpg json collections src torch os matplotlib six functools itertools pickle argparse numbers numpy abc shutil tarfile math operator logging random",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/otiliastr_coper.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\otiliastr_coper.pdf,,,no,,no,,no,,,,
1722,https://github.com/jiahaoLjh/PlaneSweepPose,jiahaoLjh_PlaneSweepPose.tar.gz,Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo,@staticmethod,utils copy yaml sys glob tqdm easydict pprint pathlib _init_paths core json collections torch os scipy dataset pickle argparse numpy time random logging models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jiahaoLjh_PlaneSweepPose.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jiahaoLjh_PlaneSweepPose.pdf,,"Back Projection ProjectionReference View 1 Reference View 2 counterpart, the fusion of information from multi-view images with multiple persons is more challenging since the identity of the 2D poses from each camera view is unknown. Previous works such as [5,7,11] address this problem in three steps. The 2D poses are first estimated for each camera view independently. Subsequently, the 2D poses from different views that correspond to the same person are identified and grouped together. Finally, the 3D pose of each person is estimated with triangulation or optimization-based pictorial structure models using the set of grouped 2D pose detections from multiple views.The establishment of cross-view correspondences is critical for multi-view multi-person 3D pose estimation. Traditional methods use either greedy matching approach [11] for fast inference speed, or optimization-based approach [5,7,8] for better global consistency. Recently, Voxel-Pose [25] is proposed to jointly solve the challenging crossview matching and 3D pose estimation problems in an object-detection paradigm. Instead of explicitly searching for 2D pose correspondences, VoxelPose projects the 2D pose heatmaps from multiple views to a common 3D space, and performs both 3D pose detection and estimation in the 3D volumetric space. The 3D object detection formulation avoids the explicit cross-view matching step, thus effectively reduces the impact from incorrectly established cross-view correspondences. Despite its effectiveness, several limitations exist for the object-detection-based pipeline: 1) Prior knowledge of the common 3D space dimension according to the multi-camera settings is needed to define the volumetric space for 3D object detection. 2) The back-projection of the 2D pose detections on each 3D voxel is not scalable to larger scenes. 3) 3D convolution that is applied to all voxel locations incurs unnecessary heavy computations, especially for large sparse scenes.In this work, we present our plane-sweep-based approach for multi-view multi-person 3D pose estimation. Our approach avoids explicit cross-view matching and aggregates multiple views for 3D pose estimation in a single shot. Specifically, we build our framework upon the concept of plane sweep stereo [6] to estimate the depth for each joint of each person in a target camera view. As illustrated in Figure 1, 2D poses are first back-projected to successive virtual depth planes, and then warped to the respective reference camera views. We measure the cross-view consistency at each depth level, which is then used to regress the depths from standard convolutional neural networks. Our depth regression adopts a two-stage coarse-to-fine scheme. Personlevel depth is first estimated for each 2D pose. Joint-level relative depth with respect to the person-level depth within a much smaller depth range is then regressed for each joint. The two stages can be trained together in an end-to-end manner. During inference, we obtain the 3D poses by backprojecting the 2D poses with the estimated depths. Multiple 3D poses of the same person from different views can be easily merged via a simple distance-based clustering.We evaluate our plane-sweep-based framework on three benchmark datasets, i.e., the Campus and the Shelf datasets, and CMU Panoptic dataset, where we outperform existing state-of-the-arts. In addition to the removal of explicit cross-view matching and triangulation compared to the traditional three-step approaches, our method is also more efficient compared to VoxelPose in two aspects: 1) In contrast to VoxelPose that builds voxels in the 3D space, we leverage on the plane sweep algorithm that is proportional to only the number of virtual depth planes. 2) Instead of performing 3D convolution on all voxel locations, we utilize the much faster 1D convolutions for each 2D pose. Furthermore, our method is more generablizable to scenarios with no prior knowledge of the multi-camera settings since only the range of virtual depth planes needs to be pre-defined for each camera view.Our contributions in this work are:• We present a plane-sweep-based approach to perform multi-view multi-person 3D pose estimation without the need for explicit cross-view matching.• Our approach outperforms existing state-of-the-arts on benchmark datasets, while being much more efficient compared to existing works. ! 𝑑Joint Depth Regressor ! 𝑑 (""#$)+ Score Matrix ∈ ℝ !×# Score Matrix ∈ ℝ ! (""#$) ×# (a) (b)Figure 2: Overview of our approach. 2D pose estimation is first performed for each camera view. We then use the plane sweep algorithm to aggregate the cross-view consistency score for the target person highlighted with jet colormap. Personlevel depth is regressed first in (a). Joint-level relative depth is then estimated in (b) and combined with the person-level depth to reconstruct the 3D pose.person. Kadkhodamohammadi et al. [18] propose to compute a distance between each pair of 2D poses from different views based on the epipolar constraints, and then find the cross-view correspondences with the lowest distance.Instead of directly performing triangulation, the matched 2D poses from all camera views are stacked together and passed into a regression neural network to estimate the 3D pose. Dong et al. [7] enhance the cross-view consistency with appearance features. They utilize a person Re-ID network [27] to get the appearance features for each person. These features are then used to compute the appearancebased distance. They also formulate a convex optimization problem to solve for the optimal correspondence matrix, and use a rank constraint to enforce cycle-consistency. Chen et al. [5] propose to match cross-view 2D poses by applying the epipolar constraints on feet joints instead of the entire 2D pose. They perform bipartite matching for each pair of views on the pairwise affinities defined on feet joints. A maximum a posteriori (MAP) estimator is adopted for the 3D pose reconstruction. Huang et al. [11] propose a greedy bottom-up matching approach for 2D pose grouping. Candidate 3D poses are first obtained from triangulation of each pair of 2D poses. These candidate 3D poses form a 3D pose subspace that are then used with a distance-based greedy clustering approach to group the cross-view poses.Triangulation with learnable weights inspired by [16] is applied for each group to obtain the 3D pose estimates.The aforementioned methods are multi-stage pipelines, where incorrect correspondences can cause large errors in the subsequent 3D pose estimation step. A recent work, VoxelPose [25], presents a novel pipeline that avoids the explicit cross-view matching and performs 3D pose estimation directly from the multi-view input. This work is inspired by the volumetric approach presented in [16] that generates 3D volumes from 2D detections. To identify multiple persons in the common 3D volumetric space, VoxelPose utilizes a 3D object detection formulation to localize each 3D pose, followed by a per-person 3D pose estimation. VoxelPose shows promising results since cross-view consistency is implicitly enforced in the 3D pose estimation. However, the 3D convolution used on the volumetric space is computationally expensive, thus not scalable for larger scenes.In this work, we present our multi-view 3D pose estimation approach. Inspired by plane sweep stereo [6,12] for dense depth regression, our approach utilizes a pose-aware geometric consistency metric to aggregate multi-view information and performs depth regression for 2D poses without explicitly establishing correspondences. Our approach demonstrates higher 3D pose estimation precision, while being much more efficient compared to previous works. Our MethodOur task is to estimate the 3D poses for all persons in a common 3D space from multi-view images captured by a set of synchronized and calibrated cameras. The overview of our framework is shown in Figure 2. We first perform 2D pose estimation for each camera view independently using a top-down multi-person pose estimation approach, e.g., HR-Net [23]. Subsequently, we perform depth regression for each candidate 2D pose with J joints under a target camera view by utilizing 2D pose detections from multiple reference views. Finally, the 3D poses can be reconstructed from back-projections of the candidate 2D poses with the estimated depths. In this section, we present our multi-view depth regression approach based on plane sweep stereo. A coarse person-level depth regression module is introduced first in Section 3.1, followed by a per-person joint-level relative depth regression module in Section 3.2. Person-Level Depth RegressionOur framework is inspired by the plane sweep stereo for dense depth estimation. The basic idea of plane sweep stereo is to back-project the target view image to a set of successive virtual depth planes, and then warp these projections to the reference view images so that photometric consistency can be measured to determine the depth of each target view pixel. We adopt the concept of plane sweep in our framework for the person-and joint-level depth regression. In contrast to the dense depth estimation in the standard plane sweep stereo that relies on photometric consistency, we measure a pose-aware geometric consistency for the depth regression of 2D human poses instead. In this section, we present a person-level depth regression module to coarsely localize the depth for each candidate 2D pose. The person-level depth is defined to be the depth of the center hip joint of each person in our implementation. ConclusionIn this work, we present our plane-sweep-based approach to regress 2D pose depths for the task of multiview multi-person 3D pose estimation. Our method uses the plane sweep algorithm to aggregate multi-view information based on a pose-aware geometric consistency and effectively estimates the depths for each 2D pose in a target camera view without explicitly establishing cross-view correspondences. Depth regression is performed in a coarse-tofine scheme, where we first regress the person-level depth followed by the joint-level relative depth estimation. Our framework is computationally more efficient and shows superior performance compared to previous state-of-the-arts.Figure 1 :1Figure 1: Our method is based on plane sweep stereo to regress depths for 2D pose detections. 2D poses are backprojected to successive depth planes and warped to reference views for consistency measurement which is utilized for depth regression.",no,,no,,no,,,,
1723,https://github.com/dfki-nlp/emp-exp,dfki-nlp_emp-exp.tar.gz,Efficient Explanations from Empirical Explainers,"@trainer.on(Events.ITERATION_COMPLETED(every=logging_training_loss_every)) @trainer.on(Events.STARTED) @reinit__is_reduced @sync_all_reduce(""_y_pred"", ""_y_true"") @staticmethod @trainer.on(Events.EPOCH_COMPLETED) @classmethod",copy pandas warnings captum tqdm utils_and_base_types re evaluate datetime json _jsonnet transformers data torch os overrides explain visualize ignite typing sklearn argparse numpy train math datasets linecache logging random subprocess,cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dfki-nlp_emp-exp.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dfki-nlp_emp-exp.pdf,,,no,,no,,no,,,,
1725,https://github.com/williamleif/histwords,williamleif_histwords.tar.gz,Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,@classmethod,requests copy pandas multiprocessing sys explicit zlib base64 SocketServer statutils threading re ioutils stat datetime collections json webhandle nltk statsmodels os mimetypes helpers lru BaseHTTPServer urllib2 matplotlib string cooccurrence common scipy SimpleHTTPServer ssl sklearn argparse pyximport googlengrams urlparse numpy pylru googlengram Queue coha time heapq representations math embedding vecanalysis setuptools webserve random sparsesvd googlegngram subprocess cPickle docopt __builtin__,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/williamleif_histwords.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\williamleif_histwords.pdf,,,no,,no,,no,,,,
1726,https://github.com/jostmey/dkm,jostmey_dkm.tar.gz,Dynamic Kernel Matching for Non-conforming Data: A Case Study of T-cell Receptor Datasets,,Length pandas csv Abundance KMer tensorflow Alignment model h5py os dataplumbing FullFlatten scipy NormalizeInitialization GlobalPool pyteomics dataset pickle sklearn argparse MaskCopy Aggregate numpy math alignment alignment_score random tcrdist BatchExpand metrics,q-bio.QM cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jostmey_dkm.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jostmey_dkm.pdf,,,no,,no,,no,,,,
1733,https://github.com/eminorhan/robust-vision,eminorhan_robust-vision.tar.gz,Improving the robustness of ImageNet classifiers using elements of human visual cognition,@property,utils scipy foolbox keras collections NewKerasModel cv2 logging sklearn argparse pkg_resources os torch __future__ numpy torchvision classification_models,cs.CV cs.LG cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eminorhan_robust-vision.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eminorhan_robust-vision.pdf,,,no,,no,,no,,,,
1737,https://github.com/zerenyenice/spsa-fsr,zerenyenice_spsa-fsr.tar.gz,SPSA-FSR: Simultaneous Perturbation Stochastic Approximation for Feature Selection and Ranking An Introduction and Accuracy Performance Comparison with Other Wrapper Methods,@staticmethod,spfsr_log pandas spfsr_engine random logging sklearn spfsr_kernel numpy,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zerenyenice_spsa-fsr.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zerenyenice_spsa-fsr.pdf,,,no,,no,,no,,,,
1738,https://github.com/Factotum8/test_task_street_view_house_numbers,Factotum8_test_task_street_view_house_numbers.tar.gz,Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks,,PIL tensorflow argparse os numpy six,cs.CV cs.CV cs.LG cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Factotum8_test_task_street_view_house_numbers.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Factotum8_test_task_street_view_house_numbers.pdf,,,no,,no,,no,,,,
1740,https://github.com/calclavia/tal-asrd,calclavia_tal-asrd.tar.gz,Speech Recognition and Multi-Speaker Diarization of Long Conversations,@pl.data_loader @use_named_args(space) @abstractmethod @rank_zero_only @property @torch.jit.ignore,"requests gc pandas wave contextlib warnings sys multiprocessing glob wget skopt fairseq tqdm bs4 aeneas edit_distance rezero hdbscan re pydub datetime collections json wildspeech pyannote pytorch_lightning transformers nltk torch os unidecode string apex functools itertools numba hashlib scipy mutagen typing pickle sklearn argparse lxml joblib numpy time shutil abc sentencepiece difflib math editdistance torchaudio librosa webrtcvad traceback dateutil random logging torch, os subprocess wandb inspect",eess.AS cs.LG cs.SD,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/calclavia_tal-asrd.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\calclavia_tal-asrd.pdf,,"|Separate ASR and DiarizationTypically, ASR and SD on multi-speaker audio are conducted through independent pipelines. The ASR model is trained to predict the spoken words Y in the audio. The SD model is trained to produce speaker embeddings which are then clustered during inference to determine who spoke when [11] and when the speakers change. A reconciliation step is then required to assign the SD model's time-position speaker labels to the ASR model's output Y to produce word-level speaker labels S.For our baselines, we train a sequence transduction model to perform ASR. We seek to learn hidden representations hi = dec(enc(X), y<i) for each token output, where enc(. . .) and dec(. . .) refer to an encoder and decoder neural network respectively. This representation is used to predict probabilities for each token P (yi||X, y<i) = softmax(W hi) of the output vocabulary, where W is a learned weight matrix. We minimize the cross entropy loss of P (yi||X, y<i) against true tokens in a conventional sequence-to-sequence manner. We train a diarization-only acoustic model to classify speakers [12] from the TAL training set. We learn features for each audio frame, and calculate cross-entropy loss of predicted speaker against the true speaker for that frame. During evaluation, we compute the weighted average features for each word using the attention focus (Section 4.6) and then cluster with HDBScan [13] to assign word-level speaker labels.|",no,,no,,no,,,,
1743,https://github.com/BFTrainer/BFTrainer,BFTrainer_BFTrainer.tar.gz,BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes,@hvd.elastic.run,utils prog pandas psutil managerOperations torchvision threading stat collections manager enum torch os socket gurobipy horovod scipy itertools jobInfo argparse persistqueue numpy time DBOperations timeit subprocess msgOperations sys_admin,cs.DC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BFTrainer_BFTrainer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BFTrainer_BFTrainer.pdf,,"INTRODUCTIONSupercomputers typically service requests for computational resources with policies that adhere roughly to first come first serve (FCFS) semantics. This approach inevitably results in idle nodes as larger tasks block subsequent tasks from executing. Backfilling [38], a strategy by which later tasks are promoted to run sooner if doing so does not delay other tasks, can improve efficiency, but substantial idle resources inevitably remain. For example, if tasks T1, T2, and T3 request 20%, 90%, and 50% of all nodes, respectively, FCFS will run these tasks sequentially, in that order, with the result that 80% and 10% of the machine is idle as T1 and T2 execute, respectively. If T3 requires less time than T1, then backfilling can promote it to run concurrently with T1, but substantial idle resources remain.Such unfillable nodes are common on leadership-class supercomputers that operate under policies that prioritize large applications that cannot run on small clusters or would take so long as to be impractical: what is known as capability computing [3,23,28]. We refer to a node that the main scheduler does not use for (regular or backfilled) jobs as an idle node, and denote the set of all idle nodes at a particular time as N .We describe here how N can be used effectively for DNN training, an activity that consumes growing numbers of compute resources in the cloud [30] and at supercomputer centers [45]. DNN training is malleable due to the data parallelism mechanism used to process a batch of data for gradient calculation, and can easily be rescaled as one needs to checkpoint only the model weights and (in the case of stateful optimizers) optimizer state. Indeed, deep learning frameworks such as AdaptDL [30], TorchElastic of Py-Torch [27], and Elastic Horovod [34] enable scaling up and down the number of workers dynamically at runtime with slight cost, without requiring a restart or resuming from checkpoints saved to durable storage.Our proposed BFTrainer leverages this malleability of DNN training to make optimal use of non-backfilled idle supercomputer nodes. Basically, BFTrainer collects idle nodes into a resource pool, N , from which it reallocates them for DNN training jobs (referred to as Trainers in the rest of the paper). The core idea is that because nodes in N can come and leave without any commitment, the use of BFTrainer does not affect jobs submitted to the main scheduler. That same characteristic makes it difficult for conventional HPC applications that are not malleable, and are expensive to migrate or checkpoint and restart, to use N effectively.BFTrainer may be used in two ways. In the first, a single user runs multiple Trainers: for example, for hyperparameter optimization (HPO) or neural architecture search (NAS). Here each user has an isolated BFTrainer instance and specifies a metric (e.g., aggregated throughput) for BFTrainer to allocate nodes optimally for their Trainers. To support multiple users or if a single user cannot consume all idle nodes, we can divide the entire supercomputer into several small clusters logically and have one BFTrainer for each small cluster to manage idle nodes for that cluster. In the second scenario, all users submit to a single BFTrainer instance; in this case, an administrator needs to propose a priority score (e.g., efficiency) for BFTrainer to plan resource allocation optimally.Specific benefits of BFTrainer include the following:• Zero cost to jobs in the main scheduler: All Trainers running on idle nodes are fully preemptable at all times; thus, BFTrainer has no impact on other jobs.• Optimal elastic scheduling: A mathematical optimization model is used to identify node allocations for Trainers that are optimal given available information. • Customizable objective: Administrators or users can assign a metric, for example, throughput, scaling efficiency, or priority score, and BFTrainer will allocate resources to different Trainers to optimize the metric.The use of BFTrainer can deliver benefits to both supercomputer centers and supercomputer users. For a supercomputer center, BFTrainer can increase the amount of computing performed; for users, assuming that their supercomputer center incentivizes the use of BFTrainer by reduced charges, it can reduce their costs if they are prepared to accept a less certain scheduling approach (e.g., AWS spot instances provide a similar tradeoff). Note that depending on relative demand, BFTrainer may run slower or faster than the main queue.We next use Summit supercomputer logs to obtain quantitative data on the frequency of unfillable nodes ( §2); define an MILP model for allocating nodes in N to Trainers ( §3); introduce our experimental setup and evaluation metrics ( §4); evaluate our resource allocation algorithm by replaying real traces with real DNN models ( §5); review related work ( §6); and conclude ( §7).",no,,no,,no,,,,
1746,https://github.com/learningmatter-mit/T-NFF,learningmatter-mit_T-NFF.tar.gz,Temperature-transferable coarse-graining of ionic liquids with dual graph convolutional neural networks,@state_dict.setter @classmethod @property @staticmethod,gc copy nff pandas sys pdb mdtraj ase collections unittest json torch os matplotlib functools itertools pickle sklearn argparse numbers tensorboardX numpy time networkx logging,physics.comp-ph cond-mat.mtrl-sci,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/learningmatter-mit_T-NFF.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\learningmatter-mit_T-NFF.pdf,,,no,,no,,no,,,,
1754,https://github.com/shridharrhegde/vgg-age,shridharrhegde_vgg-age.tar.gz,Deep Convolutional Neural Network for Age Estimation based on VGG-Face Model,,shutil torch os numpy time torchvision,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shridharrhegde_vgg-age.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shridharrhegde_vgg-age.pdf,,,no,,no,,no,,,,
1756,https://github.com/beomjoonkim/MetaLearnBO,beomjoonkim_MetaLearnBO.tar.gz,Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior,,functions csv multiprocessing sys pdb plot_tools __future__ fancyimpute active_learners threading keras tensorflow gp_sample prior_est collections os helper cvxpy socket matplotlib six importlib scipy knnimpute functools itertools seaborn plotters rand_search plot_utils pickle sklearn kitchen2d argparse GPy numpy Queue time bo gp logging mpl_toolkits cPickle,cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/beomjoonkim_MetaLearnBO.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\beomjoonkim_MetaLearnBO.pdf,Gaussian Process,"IntroductionBayesian optimization (BO) is a popular approach to optimizing black-box functions that are expensive to evaluate. Because of expensive evaluations, BO aims to approximately locate the function maximizer without evaluating the function too many times. This requires a good strategy to adaptively choose where to evaluate based on the current observations. BO adopts a Bayesian perspective and assumes that there is a prior on the function; typically, we use a Gaussian process (GP) prior. Then, the information collection strategy can rely on the prior to focus on good inputs, where the goodness is determined by an acquisition function derived from the GP prior and current observations. In past literature, it has been shown both theoretically and empirically that if the function is indeed drawn from the given prior, there are many acquisition functions that BO can use to locate the function maximizer quickly [51,5,53].However, in reality, the prior we choose to use in BO often does not reflect the distribution from which the function is drawn. Hence, we sometimes have to estimate the hyper-parameters of a chosen form of the prior on the fly as we collect more data [50]. One popular choice is to estimate the prior parameters using empirical Bayes with, e.g., the maximum likelihood estimator [44] .Despite the vast literature that shows many empirical Bayes approaches have well-founded theoretical guarantees such as consistency [40] and admissibility [26], it is difficult to analyze a version of BO that uses empirical Bayes because of the circular dependencies between the estimated parameters and the data acquisition strategies. The requirement to select the prior model and estimate its parameters leads to a BO version of the chicken-and-egg dilemma: the prior model selection depends on the data collected and the data collection strategy depends on having a ""correct"" prior. Theoretically, there is little evidence that BO with unknown parameters in the prior can work well. Empirically, there is evidence showing it works well in some situations, but not others [33,23], which is not surprising in light of no free lunch results [56,22].In this paper, we propose a simple yet effective strategy for learning a prior in a meta-learning setting where training data on functions from the same Gaussian process prior are available. We use a variant of empirical Bayes that gives unbiased estimates for both the parameters in the prior and the posterior given observations of the function we wish to optimize. We analyze the regret bounds in two settings: (1) finite input space, and (2) compact input space in R d . We clarify additional assumptions on the training data and form of Gaussian processes of both settings in Sec. 4.1 and Sec. 4.2. We prove theorems that show a near-zero regret bound for variants of GP-UCB [2,51] and probability of improvement (PI) [29,53]. The regret bound decreases to a constant proportional to the observational noise as online evaluations and offline data size increase.From a more pragmatic perspective on Bayesian optimization for important areas such as robotics, we further explore how our approach works for problems in task and motion planning domains [27], and we explain why the assumptions in our theorems make sense for these problems in Sec. 5. Indeed, assuming a common kernel, such as squared exponential or Matérn, is very limiting for robotic problems that involve discontinuity and non-stationarity. However, with our approach of setting the prior and posterior parameters, BO outperforms all other methods in the task and motion planning benchmark problems.The contributions of this paper are (1) a stand-alone BO module that takes in only a multi-task training data set as input and then actively selects inputs to efficiently optimize a new function and (2) analysis of the regret of this module. The analysis is constructive, and determines appropriate hyperparameter settings for the GP-UCB acquisition function. Thus, we make a step forward to resolving the problem that, despite being used for hyperparameter tuning, BO algorithms themselves have hyperparameters.2 Background and related work BO optimizes a black-box objective function through sequential queries. We usually assume knowledge of a Gaussian process [44] prior on the function, though other priors such as Bayesian neural networks and their variants [17,30] are applicable too. Then, given possibly noisy observations and the prior distribution, we can do Bayesian posterior inference and construct acquisition functions [29,38,2] to search for the function optimizer.However, in practice, we do not know the prior and it must be estimated. One of the most popular methods of prior estimation in BO is to optimize mean/kernel hyper-parameters by maximizing data-likelihood of the current observations [44,19]. Another popular approach is to put a prior on the mean/kernel hyper-parameters and obtain a distribution of such hyper-parameters to adapt the model given observations [20,50]. These methods require a predetermined form of the mean function and the kernel function. In the existing literature, mean functions are usually set to be 0 or linear and the popular kernel functions include Matérn kernels, Gaussian kernels, linear kernels [44] or additive/product combinations of the above [11,24].Meta BO aims to improve the optimization of a given objective function by learning from past experiences with other similar functions. Meta BO can be viewed as a special case of transfer learning or multi-task learning. One well-studied instance of meta BO is the machine learning (ML) hyper-parameter tuning problem on a dataset, where, typically, the validation errors are the functions to optimize [14]. The key question is how to transfer the knowledge from previous experiments on other datasets to the selection of ML hyper-parameters for the current dataset.To determine the similarity between validation error functions on different datasets, meta-features of datasets are often used [6]. With those meta-features of datasets, one can use contextual Bayesian optimization approaches [28] that operate with a probabilistic functional model on both the dataset meta-features and ML hyper-parameters [3]. Feurer et al. [16], on the other hand, used meta-features of datasets to construct a distance metric, and to sort hyper-parameters that are known to work for similar datasets according to their distances to the current dataset. The best k hyper-parameters are then used to initialize a vanilla BO algorithm. If the function meta-features are not given, one can estimate the meta-features, such as the mean and variance of all observations, using Monte Carlo methods [52], maximum likelihood estimates [57] or maximum a posteriori estimates [43,42].As an alternative to using meta-features of functions, one can construct a kernel between functions. For functions that are represented by GPs, Malkomes et al. [36] studied a ""kernel kernel"", a kernel for kernels, such that one can use BO with a ""kernel kernel"" to select which kernel to use to model or optimize an objective function [35] in a Bayesian way. However, [36] requires an initial set of kernels to select from. Instead, Golovin et al. [18] introduced a setting where the functions come in sequence and the posterior of the former function becomes the prior of the current function. Removing the assumption that functions come sequentially, Feurer et al. [15] proposed a method to learn an additive ensemble of GPs that are known to fit all of those past ""training functions"".Theoretically, it has been shown that meta BO methods that use information from similar functions may result in an improvement for the cumulative regret bound [28,47] or the simple regret bound [42] with the assumptions that the GP priors are given. If the form of the GP kernel is given and the prior mean function is 0 but the kernel hyper-parameters are unknown, it is possible to obtain a regret bound given a range of these hyper-parameters [54]. In this paper, we prove a regret bound for meta BO where the GP prior is unknown; this means, neither the range of GP hyper-parameters nor the form of the kernel or mean function is given.A more ambitious approach to solving meta BO is to train an end-to-end system, such as a recurrent neural network [21], that takes the history of observations as an input and outputs the next point to evaluate [8]. Though it has been demonstrated that the method in [8] can learn to trade-off exploration and exploitation for a short horizon, it is unclear how many ""training instances"", in the form of observations of BO performed on similar functions, are necessary to learn the optimization strategies for any given horizon of optimization. In this paper, we show both theoretically and empirically how the number of ""training instances"" in our method affects the performance of BO.Our methods are most similar to the BOX algorithm [27], which uses evaluations of previous functions to make point estimates of a mean and covariance matrix on the values over a discrete domain. Our methods for the discrete setting (described in Sec. 4.1) directly improve on BOX by choosing the exploration parameters in GP-UCB more effectively. This general strategy is extended to the continuous-domain setting in Sec. 4.2, in which we extend a method for learning the GP prior [41] and the use the learned prior in GP-UCB and PI.Learning how to learn, or ""meta learning"", has a long history in machine learning [46]. It was argued that learning how to learn is ""learning the prior"" [4] with ""point sets"" [37], a set of iid sets of potentially non-iid points. We follow this simple intuition and present a meta BO approach that learns its GP prior from the data collected on functions that are assumed to have been drawn from the same prior distribution.Empirical Bayes [45,26] is a standard methodology for estimating unknown parameters of a Bayesian model. Our approach is a variant of empirical Bayes. We can view our computations as the construction of a sequence of estimators for a Bayesian model. The key difference from traditional empirical Bayes methods is that we are able to prove a regret bound for a BO method that uses estimated parameters to construct priors and posteriors. In particular, we use frequentist concentration bounds to analyze Bayesian procedures, which is one way to certify empirical Bayes in statistics [49,13].",no,,no,,no,,,,
1761,https://github.com/eth-cscs/abcpy,eth-cscs_abcpy.tar.gz,ABCpy: An High-Performance Computing Perspective to Approximate Bayesian Computation *,@wraps(func) @abstractmethod @staticmethod @classmethod @abstractproperty,copy warnings glmnet cloudpickle sys tqdm abcpy examples unittest tests torch os matplotlib ot sphinx_rtd_theme rpy2 scipy functools pyspark pickle sklearn numbers numpy time abc pip setuptools operator logging mpi4py,stat.CO stat.ME stat.CO stat.ML stat.AP physics.soc-ph q-bio.PE stat.ME econ.EM math.ST stat.TH stat.ME stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eth-cscs_abcpy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eth-cscs_abcpy.pdf,Approximate Bayesian Computation,,no,multiple pilot runs,no,,no,,,,
1779,https://github.com/sccfnad/Sparse-convolutional-coding-for-neuronal-assembly-detection,sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.tar.gz,Sparse convolutional coding for neuronal assembly detection,,"Plotting copy sys __future__ sys, os json h5py os matplotlib Sorting sys, re, os scipy itertools sklearn argparse numpy time shutil math Convensembles",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.pdf,,,no,,no,,no,,,,
1786,https://github.com/isarth/SemEval9_MIDAS,isarth_SemEval9_MIDAS.tar.gz,Suggestion Mining from Online Reviews using ULMFiT,,ekphrasis re pandas text_preprocessing csv modules fastai sklearn numpy,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/isarth_SemEval9_MIDAS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\isarth_SemEval9_MIDAS.pdf,,"Conclusion and Future WorkIn this work we showed how transfer learning could be used for the task of classifying sentences extracted from unstructured text as suggestion and non-suggestions. Towards this end we train a ULMFiT model on the dataset (only Sub Task A) provided by the organizers of the SemEval 2019 Task 9 and rank 10th in the competition out of 34 participating teams.In the future we would like to experiment and show the effectiveness of our trained model in Sub Task B where the training dataset remains the same, but the test dataset consists of suggestions from a different domain. It would be interesting to see how our model performs in predicting outof-domain suggestions and show the ability of the ULMFiT model to fine-tune itself to a completely new domain with the already existing pre-trained model. Another interesting area would be to explore Multi Task Learning models and see how the domain of suggestion mining could get benefited by borrowing weights from models trained on other related tasks and similar tasks across different domains.Figure 1 :1Figure 1: Confusion matrix training data",no,,no,,no,,,,
1790,https://github.com/seba-1511/shapechanger,seba-1511_shapechanger.tar.gz,Shapechanger: Environments for Transfer Learning,,tty PIL csv sys torchvision gym mj_transfer threading distutils RPi torch os socket struct msvcrt scipy picamera termios numpy time train setuptools random io,cs.LG cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/seba-1511_shapechanger.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\seba-1511_shapechanger.pdf,,,no,,no,,no,,,,
1796,https://github.com/dahliau/DPDist,dahliau_DPDist.tar.gz,DPDist : Comparing Point Clouds Using Deep Point Cloud Distance,@property @staticmethod,csv sys tf_util nibabel generate_poses_ours plyfile dpdist_util tensorflow datetime json h5py os helper socket eulerangles provider matplotlib transforms3d importlib scipy functools itertools google_drive_downloader argparse numpy tf_util_loss time math pc_distance trimesh mpl_toolkits modelnet_dataset,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dahliau_DPDist.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dahliau_DPDist.pdf,PointNet,,no,,no,,no,,,,
1798,https://github.com/petrovskaia/maxvol-for-soil,petrovskaia_maxvol-for-soil.tar.gz,Optimal soil sampling design based on the maxvol algorithm,,scipy maxvol_cut maxvolpy sklearn numpy matplotlib,cs.CY stat.AP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/petrovskaia_maxvol-for-soil.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\petrovskaia_maxvol-for-soil.pdf,,,no,,no,,no,,,,
1800,https://github.com/INK-USC/deceive-KG-models,INK-USC_deceive-KG-models.tar.gz,,@classmethod @property @staticmethod @register_task('plain_qa'),utils rn copy pandas newprocess csv multiprocessing utils_time graph_utils sys new_val_score fairseq tqdm deep_triple_classifier val_score_reco dqn keras tensorflow re gn model modeling json collections test_script transformers env nltk torch os matplotlib string torch_geometric torch_scatter create_new_vocab dgl scipy itertools spacy construct_cpnet train_RN Replay_Memory pickle typing sklearn argparse numpy time train math data_loader test_fact_validation newer_val_score networkx random logging conceptnet model_RN predict,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/INK-USC_deceive-KG-models.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\INK-USC_deceive-KG-models.pdf,,"|RL-BASED KG PERTURBATIONWe introduce an RL-based approach for perturbing the KG. Given a KG, G, we train a policy to output a perturbed KG, G , such that ATS, fATS(G, G ), is minimized, while downstream performance, g(G ), is maximized. Specifically, the RL agent is trained to perturb G via relation replacement, so we call our algorithm RL-RR. Because the agent is limited to applying N = ||T || perturbations to G, our RL problem is framed as a finite horizon Markov decision process. In the rest of this section, we define the actions, states, and reward in our RL problem, then explain how RL-RR is implemented. Actions The action space consists of all possible relation replacements in G, i.e., replacing (e1, r, e2) ∈ T with (e1, r , e2). Since having such a large action space poses computational issues, we decouple each action into a sequence of three subactions and operate instead in this smaller subaction space. Hence, a perturbation action at time step t would be at = (a (0)t , a(1)t , a(2)t ). Namely, a (0) t is sampling entity e1 ∈ E; a (1)t is selecting edge (e1, r, e2) ∈ T ; and a (2) t is selecting relation r ∈ R to replace r in (e1, r, e2). To make the policy choose low-ATS perturbations, we further restrict the a (2) t subaction space to be the K subactions resulting in the lowest ATS. Note that each a (i) t is represented by its corresponding pre-trained TransE (Bordes et al., 2013) entity, relation, or edge embedding in G. Since these TransE embeddings are not updated by the perturbation policy, we use a (i) t to refer to both the subaction and subaction embedding. Meanwhile, at does not have any representation besides its constituent subaction embeddings.States The state space is the set of all G with the same entities and connectivity structure as G.Here, we make a distinction between state and state embedding. The state at t is the actual KG after t perturbation steps and is denoted as Gt. The state embedding at t is a vector representation of Gt and is denoted as st. To match at, we also decouple st into substate embeddings: st = (s (0)t , s (1) t , s (2) t ).Reward The reward function pushes the policy to maximize downstream performance. For commonsense QA, higher reward corresponds to lower KL divergence between the predicted and true answer distributions. For item recommendation, we use validation AUC as the reward function.|",no,,no,,no,,,,
1801,https://github.com/lstruski/Processing-of-missing-data-by-neural-networks,lstruski_Processing-of-missing-data-by-neural-networks.tar.gz,Processing of missing data by neural networks,,tensorflow datetime sys sklearn tqdm os argparse numpy time,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lstruski_Processing-of-missing-data-by-neural-networks.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lstruski_Processing-of-missing-data-by-neural-networks.pdf,,"ExperimentsWe evaluated our model on three types of architectures. First, as a proof of concept, we verified the proposed approach in the context of autoencoder (AE). Next we applied multilayer perceptron (MLP) to multiclass classification problem and finally we used shallow radial basis function network (RBFN) in binary classification. For a comparison we only considered methods with publicly available codes and thus many methods described in the related work section have not been taken into account.The code implementing the proposed method is available at https://github.com/lstruski/ Processing-of-missing-data-by-neural-networks.Autoencoder. Autoencoder (AE) is usually used for generating compressed representation of data. However, in this experiment, we were interested in restoring corrupted images, where part of data was hidden. As a data set, we used grayscale handwritten digits retrieved from MNIST database. For each image of the size 28 × 28 = 784 pixels, we removed a square patch of the size 5 13 × 13. The location of the patch was uniformly sampled for each image. AE used in the experiments consists of 5 hidden layers with 256, 128, 64, 128, 256 neurons in subsequent layers. The first layer was parametrized by ReLU activation functions, while the remaining units used sigmoids 6 .As describe in Section 1, our model assumes that there is no complete data in training phase. Therefore, the loss function was computed based only on pixels from outside the mask.As a baseline, we considered combination of analogical architecture with popular imputation techniques:k-nn: Missing features were replaced with mean values of those features computed from the K nearest training samples (we used K = 5). Neighborhood was measured using Euclidean distance in the subspace of observed features.mean: Missing features were replaced with mean values of those features computed for all (incomplete) training samples.dropout: Input neurons with missing values were dropped 7 .Additionally, we used a type of context encoder (CE), where missing features were replaced with mean values, however in contrast to mean imputation, the complete data were used as an output of the network in training phase. This model was expected to perform better, because it used complete data in computing the network loss function.Incomplete inputs and their reconstructions obtained with various approaches are presented in Figure 4 (more examples are included in Supplementary Material, section 3). It can be observed that our method gives sharper images then the competitive methods. In order to support the qualitative results, we calculated mean square error of reconstruction (see Table 1). Quantitative results confirm that our method has lower error than imputation methods, both inside and outside the mask. Moreover, it overcomes CE in case of the whole area and the area outside the mask. In case of the area inside the mask, CE error is only slightly better than ours, however CE requires complete data in training.Multilayer perceptron. In this experiment, we considered a typical MLP architecture with 3 ReLU hidden layers. It was applied to multiclass classification problem on Epileptic Seizure Recognition data set (ESR) taken from [35]. Each 178-dimensional vector (out of 11500 samples) is EEG recording of a given person for 1 second, categorized into one of 5 classes. To generate missing attributes, we randomly removed 25%, 50%, 75% and 90% of values.In addition to the imputation methods described in the previous experiment, we also used iterative filling of missing attributes using Multiple Imputation by Chained Equation (mice), where several imputations are drawing from the conditional distribution of data by Markov chain Monte Carlo techniques [10,11]. Moreover, we considered the mixture of Gaussians (gmm), where missing features were replaced with values sampled from GMM estimated from incomplete data using EM algorithm 8 .We applied double 5-fold cross-validation procedure to report classification results and we tuned required hyper-parameters. The number of the mixture components for our method was selected in the inner cross-validation from the possible values {2, 3, 5}. Initial mixture of Gaussians was selected using classical GMM with diagonal matrices. The results were assessed using classical accuracy measure.The results presented in Table 2 show the advantage of our model over classical imputation methods, which give reasonable results only for low number of missing values. It is also slightly better than dropout, which is more robust to the number of absent attributes than typical imputations. It can be seen that our method gives comparable scores to CE, even though CE had access to complete training data. We also ran MLP on complete ESR data (with no missing attributes), which gave 0.836 of accuracy.Radial basis function network. RBFN can be considered as a minimal architecture implementing our model, which contains only one hidden layer. We used cross-entropy function applied on a softmax in the output layer. This network suits well for small low-dimensional data.For the evaluation, we considered two-class data sets retrieved from UCI repository [36] with internally missing attributes, see Table 3 (more data sets are included in Supplementary Materials, section 4). Since the classification is binary, we extended baseline with two additional SVM kernel models which work directly with incomplete data without performing any imputations:geom: Its objective function is based on the geometric interpretation of the margin and aims to maximize the margin of each sample in its own relevant subspace [26].karma: This algorithm iteratively tunes kernel classifier under low-rank assumptions [29].The above SVM methods were combined with RBF kernel function.We applied analogical cross-validation procedure as before. The number of RBF units was selected in the inner cross-validation from the range {25, 50, 75, 100}. Initial centers of RBFNs were randomly selected from training data while variances were samples from N (0, 1). For SVM methods, the margin parameter C and kernel radius γ were selected from {2 k : k = −5, −3, . . . , 9} for both parameters. For karma, additional parameter γ karma was selected from the set {1, 2}. The results, presented in Table 4, indicate that our model outperformed imputation techniques in almost all cases. It partially confirms that the use of raw incomplete data in neural networks is usually better approach than filling missing attributes before learning process. Moreover, it obtained more accurate results than modified kernel methods, which directly work on incomplete data.",no,cross-validation,no,,no,,,,
1802,https://github.com/therooler/pennylane-qllh,therooler_pennylane-qllh.tar.gz,Implementing perceptron models with qubits,@staticmethod,tensorflow itertools setuptools pennylane rockyraccoon typing os numpy matplotlib,quant-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/therooler_pennylane-qllh.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\therooler_pennylane-qllh.pdf,,,no,,no,,no,,,,
1803,https://github.com/caelan/SS-Replan,caelan_SS-Replan.tar.gz,Online Replanning in Belief Space for Partially Observable Task and Motion Problems,@property,copy pstats sys multiprocessing psutil __future__ examples run_pybullet datetime pybullet_tools collections src resource cProfile os run_experiment string trac_ik_python scipy itertools sklearn argparse numpy time math pddlstream traceback random,cs.RO cs.AI cs.AI cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/caelan_SS-Replan.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\caelan_SS-Replan.pdf,,,no,,no,,no,,,,
1811,https://github.com/alexandertornede/online_as,alexandertornede_online_as.tar.gz,Machine Learning for Online Algorithm Selection under Censored Feedback,@staticmethod,evaluation_of_train_test_split copy pandas ax par_10_regret_metric warnings sys multiprocessing learner_runtime_metric simple_runtime_metric sqlalchemy re analysis_utility collections aslib_scenario os par_10_metric matplotlib database_utils configparser sksurv scipy itertools seaborn forestci mlxtend sklearn numpy time approaches math number_unsolved_instances random logging operator evaluation mysql,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alexandertornede_online_as.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alexandertornede_online_as.pdf,,,no,,yes,"Thompson variants, LinUCB variants",no,,,,
1814,https://github.com/LLNL/csld,LLNL_csld.tar.gz,Compressive sensing lattice dynamics. I. General formalism,"@unitized(""ang"") @abstractmethod @cached_class @unitized(""amu"") @singleton @wraps(klass, assigned=(""__name__"", ""__module__""), updated=()) @total_ordering @property @staticmethod @classmethod @abstractproperty","copy pymatgen warnings sys glob f_util bcs_driver prettyplotlib io csld __future__ ctypes curses re Cython collections json doctest fnmatch cssolve os fractions matplotlib string six scipy functools itertools bregman cmath _c_util pyhull scipy, scipy sklearn f_phonon numbers numpy monty abc time math setuptools errno random logging io, re subprocess mpl_toolkits spglib",physics.comp-ph cond-mat.mtrl-sci cond-mat.mtrl-sci,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/LLNL_csld.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\LLNL_csld.pdf,,,no,,no,,no,,,,
1816,https://github.com/kocurvik/retinanet_traffic_3D,kocurvik_retinanet_traffic_3D.tar.gz,Detection of 3D Bounding Boxes of Vehicles Using Perspective Transformation for Accurate Speed Measurement,"@pytest.mark.parametrize(""backbone"", parameters) @pytest.mark.parametrize(""backbone, alpha"", parameters) @pytest.fixture(autouse=True)",PIL copy csv warnings sys keras_resnet __future__ warper threading keras tensorflow dataset_utils model json queue stringio os pytest six scipy xml keras_retinanet pickle argparse numpy time math pycocotools setuptools cv2 geometry random coco io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kocurvik_retinanet_traffic_3D.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kocurvik_retinanet_traffic_3D.pdf,Convolution  1x1 Convolution  Focal Loss  Feature Pyramid Network  RetinaNet,,no,,no,,no,,,,
1818,https://github.com/xuxy09/texformer,xuxy09_texformer.tar.gz,3D Human Texture Estimation from a Single Image with Transformers,@staticmethod @torch.no_grad(),utils imageio PIL loss warnings sys tqdm easydict __future__ torchvision dataset_pytorch collections json transformers torch os reid_resnet matplotlib scipy functools RSC_net neural_renderer pickle argparse tensorboardX numpy config shutil math smplx cv2 random NMR lpips options layer,cs.CV cs.AI cs.GR cs.LG cs.MM cs.CV cs.AI cs.CV cs.AI cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xuxy09_texformer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xuxy09_texformer.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Scaled Dot-Product Attention  Byte Pair Encoding  Layer Normalization  Residual Connection  Adam  Dropout  Softmax  Multi-Head Attention  Label Smoothing  Dense Connections  Transformer  Low-resolution input  High-resolution input,,no,,no,,no,,,,
1819,https://github.com/birdortyedi/image-retrieval-with-capsules,birdortyedi_image-retrieval-with-capsules.tar.gz,Fashion Image Retrieval with Capsule Networks,@staticmethod,utils TripletDirectoryIterator keras re tensorflow csv blocks models random layers tqdm argparse os config numpy colorama time shutil,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/birdortyedi_image-retrieval-with-capsules.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\birdortyedi_image-retrieval-with-capsules.pdf,,,no,,no,,no,,,,
1821,https://github.com/liyzcj/leakgan-py3,liyzcj_leakgan-py3.tar.gz,Long Text Generation via Adversarial Training with Leaked Information,,scipy tensorflow LeakGANModel cPickle target_lstm target_lstm20 split_sentence random pickle nltk os dataloader numpy Discriminator,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/liyzcj_leakgan-py3.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\liyzcj_leakgan-py3.pdf,,,no,,no,"learning rate, window size, kernel numbers, smootheness of rescale activation",no,,,,
1830,https://github.com/stanford-futuredata/selection-via-proxy,stanford-futuredata_selection-via-proxy.tar.gz,SELECTION VIA PROXY: EFFICIENT DATA SELECTION FOR DEEP LEARNING,"@dataset_options @click.option('--subset', type=int, default=512_466, show_default=True, @click.option('--dataset', '-d', type=click.Choice(DATASETS), @click.option('rounds', '--round', '-r', multiple=True, type=int, @click.option('--min-count', default=1, show_default=True, @abc.abstractmethod @miscellaneous_options @click.option('--eval-target-at', multiple=True, type=int, @click.option('--track-test-acc/--no-track-test-acc', @property @click.option('--precomputed-selection', @click.option('--selection-method', type=click.Choice(coreset_methods), @click.option('--learning-rate', default=0.05, show_default=True, @click.option('--selection-method', type=click.Choice(active_learning_methods), @click.group() @click.option('--ngrams', '-n', default=2, show_default=True, @click.option('--train-target/--no-train-target', @click.option('--initial-subset', type=int, default=1_000, show_default=True, @cli.command() @click.option('--dim', default=10, show_default=True, @click.option('--bucket', default=10_000_000, show_default=True, @click.option('--initial-subset', type=int, default=25_623, show_default=True, @computing_options @click.option('sizes', '--size', multiple=True, type=int, @click.option('--initial-subset', type=int, default=72_000, show_default=True, @click.option('--datasets-dir', default='./data', show_default=True, @click.option('--threads', default=4, show_default=True, @click.argument('executable') @training_options @click.option('--seed', '-s', type=int, @click.option('--run-dir', default='./run', show_default=True, @click.option('--epochs', default=5, show_default=True, @click.option('--subset', type=int, default=10_000, show_default=True, @click.option('--subset', type=int, default=1_800_000, @proxy_training_overrides @click.option('--selection-method',",pandas contextlib glob tqdm torchvision re click datetime collections json torch os svp gzip functools apex scipy typing numpy time abc math setuptools typing_extensions subprocess,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stanford-futuredata_selection-via-proxy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stanford-futuredata_selection-via-proxy.pdf,,,no,,yes,"batch size, learning rate, epochs",no,,,,
1831,https://github.com/midas-research/DECA,midas-research_DECA.tar.gz,Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition,,utils loss_datagen PIL sys glob pprint torchvision dataloader_utils collections torch os scipy pickle imresize argparse data_model_loader numpy time shutil config math random cv2 models,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/midas-research_DECA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\midas-research_DECA.pdf,,,no,,no,,no,,,,
1835,https://github.com/anonymseal/MLCheck,anonymseal_MLCheck.tar.gz,MLCheck-Property-Driven Testing of Machine Learning Models,@abstractmethod @staticmethod,"utils copy pandas csv adf_tutorial warnings sys multiprocessing mlCheck tqdm __future__ z3 pydot os, time torchvision fileinput AEQUITAS_TestCases tensorflow re os,sys collections json queue torch os loss_funcs matplotlib string StringIO scipy import_ipynb functools AEQUITAS_MainFiles itertools PytorchDNNStruct ART_MainFiles sklearn joblib numpy Queue time abc adf_utils parsimonious math re, sys traceback FairAwareTestCases random statistics MainFiles multi_utils adf_baseline adf_data io",cs.SE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/anonymseal_MLCheck.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\anonymseal_MLCheck.pdf,,"INTRODUCTIONThe importance of quality assurance for applications developed using machine learning (ML) increases steadily as they are being deployed in a growing number of domains and sites. Supervised ML algorithms ""learn"" their behaviour as generalizations of training data using sophisticated statistical or mathematical methods. Still, developers need to make sure that their software-whether learned or programmed-satisfies certain specified requirements. Currently, two orthogonal approaches can be followed to achieve this goal: (A) employing an ML algorithm guaranteeing some requirement per design, or (B) validating the requirement on the model generated by the ML algorithm.Both approaches have their individual shortcomings: Approach A is only available for a handful of requirements (e.g., fairness, monotonicity, robustness) [23,33,41]. Moreover, such algorithms cannot ensure the complete fulfillment of the requirement. For example, Galhotra et al. [15] have found fairness-aware ML algorithms to generate unfair predictions, and Sharma et al. [35] detected non-monotonic predictions in supposedly monotone classifiers. For robustness to adversarial attacks, the algorithms can only reduce the attack surface. Approach B, on the other hand, is only possible if a validation technique exists which is applicable to (1) the specific type of machine learning classifier under consideration (i.e., neural network, SVM, etc.) and (2) the specific property to be checked. Current validation techniques are restricted to either a single model type or a single property (or even both).In this paper, we propose property-driven testing as a validation technique for machine learning models overcoming the shortcomings of approach B. Our technique allows developers to specify the property under interest and-based on the property-performs a targeted generation of test cases. The target is to find test cases violating the property. The approach is applicable to arbitrary types of non-stochastic properties and arbitrary types of supervised machine learning models. We consider the model under test (MUT) as a black-box of which we just observe the input-output behaviour. To achieve a systematic generation of test cases, specific to both MUT and property, we train a second white-box model approximating the MUT by using its predictions as training data. Knowing the white-box's internal structure, we can apply state-of-the-art verification technology to verify the property on it. A verification result of ""failure"" (property not satisfied) is then accompanied by (one or more) counterexamples, which we subsequently store as test inputs whenever they are failures for the MUT as well.We currently employ two types of ML models as approximating white-boxes: decision trees and neural networks. While no prior knowledge is required pertaining to the internal structure of the model under test, the internals of the white-box model are accessible to verification. Test generation proceeds by (1) encoding both property and white-box model as logical formulae and (2) using an SMT (Satisfiability Modulo Theories) solver to check their satisfiability. Counterexamples in this case directly come in the form of satisfying assignments to logical variables which encode feature and class values. Due to the usage of an approximating white-box model, test generation is an iterative procedure: whenever a counterexample on the white-box model is found which is not valid for the MUT, the white-box model gets retrained. This way the approximation quality of the white-box model is successively improved.We have implemented our approach in a tool called MLCheck and evaluated it on requirements of three different application areas:• Software discrimination studies whether ML models give predictions which are (un)biased with respect to some attributes. Different definitions of such fairness requirements exist (see [40]); we exemplarily use individual discrimination [15]. • Knowledge graphs are a family of knowledge representation techniques. We consider learning classifiers for entities based on knowledge graphs embeddings [12] and exemplarily consider the properties of class disjointness and subsumption. • Security of machine learning applications investigates if ML models are vulnerable to attacks, i.e., can be manipulated as to give specific predictions. We exemplarily study vulnerability to trojan attacks [17].In all three areas, we compare our approach to either other tools specifically testing such properties (if they exist) or to a baseline employing a property-based tester [9] for test case generation. The evaluation shows that MLCheck can outperform other tools with respect to effectiveness in generating property falsifying test cases. MLCheck in particular excels at hard tasks where other tools cannot find any test case. This increased performance does moreover not come at the prize of a much higher runtime.Summarizing, this paper makes the following contributions:• we present a language for specifying properties on machine learning models, • we propose a method for systematic test case generation, driven by the property and ML model under consideration, and • we systematically evaluate our approach in three different application areas employing 56 models under test generated from 24 data sets.The tool and all data to replicate the results mentioned in this paper can be found at https://github.com/anonymseal/MLCheck. Test Data GenerationFor test data generation, we employ a technique called verificationbased testing, first proposed in [35]. Verification-based testing performs formal verification of the property to be checked via SMT (satisfiability modulo theories) solving. Since we treat the model under test (MUT) as black-box (and since we aim at a testing technique applicable to any kind of machine learning model), verification first of all requires the existence of a verifiable white-box model. To this end, we train a white-box model approximating the MUT using predictions of the MUT as training data. On the white-box model, we verify the property, and use counterexamples to the property as test inputs. As the white-box model is only an approximation of the MUT, not all such counterexamples must be valid counterexamples in the MUT. In case of counterexamples being invalid for the MUT, we do not include them in the test suite and instead retrain the white-box model to enhance its approximation quality.The overall workflow of test data generation is depicted in Fig- ure 4. Inputs are the model under test (MUT) and the property specification, the output is a test suite. We briefly discuss all steps in the sequel.White-Box Model Training. The white-box model on which we verify the property is generated from predictions of the blackbox model (the MUT). To this end, we generate training data for the white-box model from randomly chosen data instances together with the MUT's predictions on these instances.Currently, our approach employs two types of white-box models which the user can choose from: decision trees and neural networks. During the evaluation, we compare them with respect to efficiency and effectiveness in generating test inputs (see Section 6). For training, we take ML algorithms from the scikit-learn and PyTorch library to create white-box models.Formula Generation. Property and white-box model are translated to logical formulae. The construction guarantees that these formulae in conjunction are satisfiable if and only if the property does not hold for the white-box model. Our aim is the generation of test inputs violating the property. The translation employs the SMT-LIB format 3 to leverage state-of-the-art SMT (Satisfiability Modulo Theories) solvers for satisfiability checking. The translation itself is detailed in Section 4.SMT Solving and Augmentation. Next, the SMT solver Z3 [11] is used to check satisfiability. When the formula is satisfiable, we extract the logical model of the formula. This logical model is a counterexample to the property, i.e., gives us values of data instances and predicted classes violating the property. Such a counterexample serves as a test input and thus becomes part of the test suite (unless it is no counterexample for the MUT). To generate several test inputs, we furthermore use an augmentation phase and let the SMT solver construct further logical models. This is done by adding more constraints to the logical formula ruling out previously returned counterexamples.Retraining. As verification takes place on the white-box model, not every thus computed counterexample is also a valid counterexample for the MUT (which is only approximated by the white-box model). Therefore, we compare the prediction of the white-box model on generated test inputs with that of the black-box model. In case of differences, the test input plus MUT prediction is added to the training set for the white-box model. After having collected several such invalid counterexamples, the white-box model is retrained as to improve its approximation quality.These steps are repeated until a user-definable maximum number of samples has been reached. SetupEvaluation requires to have (1) models under test (obtained by training on some data sets), (2) properties to be checked (already given) and (3) tools to compare MLCheck to.Datasets. We use different data sets to construct MUTs in the different application areas. Some statistics about the data sets can be found in Table 3.• For the fairness experiments, we have taken the Adult and German credit datasets from the UCI machine learning repository. 8 We used ""gender"" as sensitive feature (for checking individual discrimination) as this feature has also been used in previous works of fairness testing [1,15]. • For testing concept relationships, we first employed the Pyke embedding approach [12] to map entities from the DBpedia knowledge graph (version 3.6) 9 to real vectors in 50 dimensions. Such embedding approaches compute the features of entities. Pyke achieves the best results in the class prediction task and yields features which are well suited to the classification of entities. Our experimental results suggest that finding counterexamples for classifiers trained on these embeddings is a difficult task, thus such classifiers provide good benchmarks for testing tools. We then generated 6 datasets, which each contained embeddings from 3 classes (our labels).In three of the datasets, 2 of the classes were known to be disjoint (e.g., persons and places). The other three datasets contained two classes of which one subsumed the other (e.g., persons and actors). • For testing on trojan attacks, we employ the same datasets as used by Baluta et al. [3] for quantitatively verifying neural networks wrt. trojan attacks. They use the MNIST 10 dataset containing images of hand-written digits and resize the images to 10×10. To obtain models which are vulnerable to trojan attacks, we further extended this training set with additional ""poisened"" data instances 11 , i.e., instances in which some trigger t is present and the specific target prediction z is given. This way, we train a model which is vulnerable towards such an attack. Figure 5 shows two instances (images) with triggers set (2 out of 4 triggers). The trigger features are the first 2 and 7 pixels, respectively, in the upper left corner of the image, set to some randomly chosen color occuring in these images. The target prediction of these two attacks in this case is class 4, i.e., we steer the MUT into falsely predicting an image containing the triggers to show the digit 4. The more such ""poisened"" data instances we add to the training set, the more likely it is that attacks succeed and the more difficult it gets to generate test cases violating the attack property. ML algorithms. Out of the training sets, we generate ML models using scikit-learn and PyTorch, the latter for all neural networks as it provides more sophisticated configuration options for training NNs. For fairness testing, we train a random forest, a logistic regression classifier, a naive Bayes classifier and a decision tree. Moreover, we employ two fair-aware classifiers [5,41], i.e., classifiers which are supposed to generate non-discriminating models. For concept relationship testing, we also train a neural network and a random forest. Finally, for trojan attacks we just train a neural network since this is the main classifier used on images. We use two different architectures for the neural network: one with 1 hidden layer of 100 neurons (called NN1 in Tables 7 and 8) and one with 2 hidden layers with 64 neurons (NN2). Note that there is no need to employ networks with several hidden layers as long as the network is able to approximate the MUT well enough.Baselines. For fairness testing, there are specialized tools for testing for individual discrimination. We compared our tools with the Symbolic Generation (SG) algorithm of Aggarwal et al. [1] 12 and with AEQUITAS [38]. We do not consider THEMIS [15] for our comparison as this has already been shown to be less effective in comparison to SG and AEQUITAS as stated by Zhang et al. [44]. We configured our tool to generate multiple counterexamples since SG and AEQUITAS also construct several failing test inputs (in order to compute some unfairness score).For concept relationships and trojan attacks, there are no specialized testing tools available. Here, we have used the Python implementation (Hypothesis) of the property-based testing tool qickCheck as our baseline approach to compare against. In this case, we configured MLCheck to generate a single counterexample. Parameter max_samples was set to 1000 in all cases and bound_cex to false.Note that the ground truth about the models under test is unknown in all the experiments, i.e., we do not a priori know whether the trained classifiers do or do not satisfy the property.All experiments were run on a machine with 2 cores Intel(R) Core(TM) i5-7300U CPU with 2.60GHz and 16GB memory using Python version 3.6 with GPU as Intel(R) HD Graphics 620.",no,,no,,no,,,,
1837,https://github.com/nikhil1024/Vision-A-deep-learning-approach-to-provide-walking-assistance-to-the-visually-impaired,nikhil1024_Vision-A-deep-learning-approach-to-provide-walking-assistance-to-the-visually-impaired.tar.gz,Vision: A Deep Learning Approach to provide walking assistance to the visually impaired,@once_differentiable,get_data tqdm torchvision threading load_data torch os matplotlib Text_to_Speech synthesize_right_image_300x300 scipy spatial_correlation_sampler_backend Stereo_Vision pyglet numpy time random cv2 Object_Detection,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nikhil1024_Vision-A-deep-learning-approach-to-provide-walking-assistance-to-the-visually-impaired.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nikhil1024_Vision-A-deep-learning-approach-to-provide-walking-assistance-to-the-visually-impaired.pdf,,,no,,no,,no,,,,
1839,https://github.com/fregu856/ebms_3dod,fregu856_ebms_3dod.tar.gz,Accurate 3D Object Detection using Energy-Based Models,"@cuda.jit('(float32[:], float32[:])', device=True, inline=True) @lr.setter @beta.setter @cuda.jit('(float32[:], float32[:], int32)', device=True, inline=True) @mom.setter @property @staticmethod @numba.jit @cuda.jit( @cuda.jit('(float32, float32, float32[:])', device=True, inline=True) @cuda.jit('(int64, float32, float32[:, :], uint64[:])') @abstractmethod @wd.setter @cuda.jit('(float32[:], float32[:], float32[:])', device=True, inline=True) @classmethod @numba.jit(nopython=True, parallel=True) @cuda.jit('(int64, float32, float32[:], uint64[:])') @numba.jit(nopython=True) @cuda.jit('(int64, int64, float32[:], float32[:], float32[:])', fastmath=False) @cuda.jit('(float32[:], int32)', device=True, inline=True) @numba.njit @numba.jit(nopython=False)",utils copy warnings sys glob tools skimage tqdm __future__ mmcv pathlib mayavi re open3d collections resource iou3d_cuda torch os concurrent matplotlib six functools numba xml terminaltables pickle typing argparse mmdet numpy time shutil abc math pycocotools setuptools cv2 logging spconv pcdet inspect io,cs.CV cs.LG cs.RO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fregu856_ebms_3dod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fregu856_ebms_3dod.pdf,,,no,manual experiments using val split,no,,no,,,,
1842,https://github.com/julian-risch/TRAC-COLING2018,julian-risch_TRAC-COLING2018.tar.gz,Aggression Identification Using Deep Learning and Data Augmentation,,math keras re pandas vaderSentiment xgboost tensorflow warnings lightgbm sys sklearn nltk fastText os numpy,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/julian-risch_TRAC-COLING2018.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\julian-risch_TRAC-COLING2018.pdf,Logistic Regression,"EnsemblingWord embeddings, word n-grams, character n-grams, and hand-picked features capture different properties of user posts and therefore have different strengths and weaknesses. For example, word n-grams suffer from out-of-vocabulary problems, which makes them sensitive to obfuscated words. The dataset contains posts that make extensive use of obfuscation, such as ""Son of a B****"", ""***k them!!!!"". Word embeddings and word n-grams cannot capture the meaning of these obfuscated posts, but character ngrams or the number of asterisks and exclamation marks as hand-picked features can.For the four models, we analyze the pairwise Pearson correlation of their predictions as listed in Table 1. The word n-gram and the character n-gram models have the highest correlation. In contrast, the recurrent neural network and the word n-gram model have a rather low correlation. Their low correlation motivates to combine their predictions, because we can assume that they each other well. If they both have a similarly high F1-score, their combination outperforms the single models. The different strengths and weaknesses of the proposed four models motivate their combination in an ensemble. For each of the four models, we run 10-fold cross-validation and create out-of-fold predictions. For each of the 10 runs, we also make predictions for the test set and average all 10 predictions per model. We use the out-of-fold predictions to learn, what combination of the single models performs best. Instead of a simple weighted average of the different models, we propose a stacking approach: Given a comment, we extract features and based on these features, decide how to weight the different models' predictions for this particular comment.For each comment, we extract: (1) the length (number of characters), (2) the relative number of uppercase characters (number of uppercase characters divided by the total number of characters), (3) the relative number of non-alpha characters (number of non-alpha characters divided by the total number of characters), and (4) the relative number of exclamation marks (number of exclamation marks divided by the total number of characters). For english-language comments, we also extract the relative number of how many words in the comment have a GloVe embedding (Pennington et al., 2014). Although we use fasttext embeddings, which do not suffer from out-of-vocabulary problems, we include this feature to measure how many uncommon words are used in the comment. Based on the extracted features, we train a stacker on the out-of-fold predictions and combine all approaches in an ensemble. The stacker uses gradient boosting trees. More precisely, we use 75 trees with a depth of 3, a bagging fraction of 0.8 and a feature fraction of 0.45.",no,,no,,no,,,,
1843,https://github.com/jbesty/PINN_system_identification,jbesty_PINN_system_identification.tar.gz,Physics-Informed Neural Networks for Non-linear System Identification for Power System Dynamics,,scipy tensorflow numpy PINNs time,eess.SY cs.LG cs.SY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jbesty_PINN_system_identification.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jbesty_PINN_system_identification.pdf,,,no,,no,,no,,,,
1845,https://github.com/markcutajar/raw-music-tagging-cnns,markcutajar_raw-music-tagging-cnns.tar.gz,FMA: A DATASET FOR MUSIC ANALYSIS,@RateLimited(5)  # calls per second @RateLimited(5) @staticmethod @RateLimited(2)  # call per second,csv multiprocessing sys threading tensorflow pylast pydub json os sqlite3 cloud pydst array argparse numpy time setuptools logging python_speech_features io,cs.SD cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/markcutajar_raw-music-tagging-cnns.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\markcutajar_raw-music-tagging-cnns.pdf,,,no,,no,,no,,,,
1851,https://github.com/zju3dv/iMoCap,zju3dv_iMoCap.tar.gz,Motion Capture from Internet Videos,,tabulate warnings glob json collections torch os scipy hashlib argparse joblib numpy config shutil smplx smplmodel random subprocess yacs,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zju3dv_iMoCap.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zju3dv_iMoCap.pdf,,,no,,no,,no,,,,
1854,https://github.com/qianguih/RSNet,qianguih_RSNet.tar.gz,Recurrent Slice Networks for 3D Segmentation of Point Clouds,@property @staticmethod,utils _ext sys glob nibabel layers plyfile net torchvision tensorflow h5py load_data torch os eulerangles matplotlib functools itertools argparse numpy time shutil math cv2 random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/qianguih_RSNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\qianguih_RSNet.pdf,,"B. More Results and Discussions on the Scan-Net datasetThe ScanNet dataset contain 1,513 scenes captured by the Matterport 3D sensor. We follow the official training/testing split [3] in this paper. The points are annotated in 20 categories and one background class. As shown in Table.11, the ScanNet dataset is also highly unbalanced. Thus, we use the mean IOU and mean accuracy as evaluation met-rics in the main text to better measure the performances for this dataset. To process the ScanNet dataset, out RSNet takes points with 3 dimensional features (xyz coordinates) as inputs as in [24].In order to further improve the performances on the ScanNet dataset, we train a RSNet taking not only xyz coordinates but also RGB intensities as inputs. The results are reported in Table .12. It shows that RGB information can slightly improve the performances of our baseline model. The mean IOU and mean accuracy are improved by 1.81 and 1.97. Moreover, detailed per-class IOUs show that the RGB information is particularly helpful for categories like door, window, and picture. These classes can be easily confused with walls when only geometric information (xyz coordinate) is present. However, RGB information helps the network distinguish them from each other. ceiling floor wall beam column window door chair table bookcase sofa board clutter Data Per-centage (%) 25. 3  Figure 2 :2Figure 2: Diagram of an RSNet. The three parallel branches denote the slicing direction along x, y, and z axis.",no,ablation studies,no,"slicing resolution, sliding block size, sliding stride",no,,,,
1872,https://github.com/quanvuong/handful-of-trials-pytorch,quanvuong_handful-of-trials-pytorch.tar.gz,Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,@staticmethod @torch.no_grad(),dotmap tqdm __future__ pprint gym tensorflow Agent env torch os optimizers importlib MPC scipy DotmapUtils MBExperiment argparse numpy time config random,cs.LG cs.AI cs.RO stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/quanvuong_handful-of-trials-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\quanvuong_handful-of-trials-pytorch.pdf,Experience Replay  Dense Connections  Rectified Linear Units  Adam  Soft Actor Critic,,no,,no,,no,,,,
1884,https://github.com/CVPRUSFTampa/EventSegmentation,CVPRUSFTampa_EventSegmentation.tar.gz,A Perceptual Prediction Framework for Self Supervised Event Segmentation,,"PIL scipy math tensorflow warnings sys json collections collections, cv2 random sklearn statsmodels os cv2, os, sys, numpy __future__ numpy matplotlib",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CVPRUSFTampa_EventSegmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CVPRUSFTampa_EventSegmentation.pdf,,,no,,no,,no,,,,
1885,https://github.com/amilsted/unitree,amilsted_unitree.tar.gz,TensorNetwork on TensorFlow: A Spin Chain Application Using Tree Tensor Networks,"@pytest.fixture(params=[""tensorflow"", ""jax"", ""numpy""]) @pytest.fixture(",math tree_tensor_network copy tensorflow scipy time contextlib sys tensornetwork pytest __future__ numpy jax,cond-mat.str-el cs.LG hep-th physics.comp-ph stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amilsted_unitree.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amilsted_unitree.pdf,,"IV. BENCHMARK RESULTSWe consider a 2D lattice made of L x × L y = 128 × 5 quantum spins or, equivalently, a 1D lattice made of L x = 128 effective spins, each of dimension 2 5 = 32. We choose the value h = 2.9, which is seen to lead to a scaling of ground state entanglement entropy compatible with being near a quantum critical point. This value is slightly below ≈ 3.04, which corresponds to the critical point in a fully 2D lattice, that is for L x , L y → ∞. We approximate the ground state using a TTN for increasing values of χ in the range 32 ≤ χ ≤ 256. For each value of χ we minimize the expectation value of the energy per site by iterating the isometry update scheme outlined above, until the energy per site changes by less than 10 −10 after a whole sweep of updates.A. Ground state energy Fig. 5 shows the converged value of the ground state energy per site e(χ) as a function of the bond dimension χ. The energy per site only changes by about 3 × 10 −6 as we increase the bond dimension from 203 to 256, suggesting that the error in the energy due to using a finite value of χ might be on that order of magnitude. In Fig. 6 we then see that the energy converges to its extrapolated value e(∞) ≈ −3.11229 roughly as χ −2 . Variational ground state energy per site e(χ) as a function of 1/χ 2 . At large bond dimension χ, the energy seems to be scaling to some χ = ∞ limit value e(∞) ≈ −3.11229 as 1/χ 2 . V. COMPUTATIONAL TIMEA highlight of TensorNetwork is that, thanks to running on top of TensorFlow, the same tensor network code [1] can be used on different computational resources. We used TensorFlow v1.13.1 built with the Intel math kernel library (MKL). The computations described above were carried out using real numbers at 64 bit floating-point precision. We employed Google's cloud compute engine. For CPU computations we used Xeon Skylake with 1, 8, 16, and 32 cores. For GPU computations we used NVIDIA Tesla V100. For further reference, we also run equivalent numpy code using a single CPU.The computational cost of the TTN algorithm scales as χ 4 for sufficiently large χ. This is indeed the scaling of both tensor-tensor contractions and of the SVD of the environment E required to update an isometry w. There are also other steps, including permuting indices of rank-3 tensors, that scale as χ 3 .Fig. 9 shows the computational time required in order to update all the isometries in the TTN once (wall time per sweep). We see that for large bond dimension χ, single CPU computations with code using either the numpy library or TensorNetwork both scale as χ 4 , as expected. However, using TensorNetwork is twice as fast. We also observe that for large bond dimension, using TensorNetwork with a GPU is about 100 times faster than with a CPU. Moreover, with the range of tested bond dimension χ ≤ 256, the cost still scales roughly as χ 3 on the GPU (larger values of χ will be tested in the near future). Finally, further optimizations are still required to fully take advantage of TPU architecture (work in progress), but early experiments suggest that the performance will likely exceed that of the GPU when those optimizations are completed. For large χ, the computational time on a single CPU (both numpy and TensorNetwork) scales as O(χ 4 ), as anticipated, with TensorNetwork being twice as fast as numpy for large χ. On the GPU, the computational time does not yet reach the large χ scaling O(χ 4 ) but scales instead roughly as O(χ 3 ) for the largest bond dimensions we tested. Using clusters of 8, 16, 32 CPUs reduces the gap with the GPU, although a GPU is still a factor ×6 faster than 32 CPUs.walltime per sweep10 1 10 2 10 33 × 10 7 × 4 CPU, 1 core (numpy) CPU, 1 core CPU, 8 cores CPU, 16 cores CPU, 32 cores GPU10 03264128256FIG. 9. Computational time as a function of the bond di-mension χ.",no,,no,,no,,,,
1910,https://github.com/overshiki/unet-pytorch,overshiki_unet-pytorch.tar.gz,U-Net: Convolutional Networks for Biomedical Image Segmentation,,"timeit utils math model cupy dataset torch, os torch numpy model_convention",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/overshiki_unet-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\overshiki_unet-pytorch.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,no,,no,,no,,,,
1918,https://github.com/ellendmk/DL_and_RG,ellendmk_DL_and_RG.tar.gz,Is Deep Learning a Renormalization Group Flow?,@staticmethod,math itertools tensorflow datetime random os matplotlib __future__ numpy rbm time,cs.LG cond-mat.stat-mech physics.comp-ph stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ellendmk_DL_and_RG.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ellendmk_DL_and_RG.pdf,,,no,,,,no,,,,
1921,https://github.com/malteschilling/ddrl_hexapod,malteschilling_ddrl_hexapod.tar.gz,Decentralized Deep Reinforcement Learning for a Distributed and Adaptive Locomotion Controller of a Hexapod Robot,,importlib threading tensorflow baselines pandas seaborn sys multiprocessing statistics os numpy gym_gazebo2 matplotlib time gym,cs.RO cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/malteschilling_ddrl_hexapod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\malteschilling_ddrl_hexapod.pdf,,"Lower LevelMotor Control Environment a t < l a t e x i t s h a 1 _ b a s e 6 4 = "" y a V m I D 0 / V N h / v q a n S / e Q x O 4 4 W R w = "" > A A A C C n i c b V B L T g J B F H y D P 8 Q f 6 t J N R 2 L i i s w g i S 5 J 3 L j E K J 8 E J q S n p 4 E O P Z 9 0 v y G S C T f Q r Fig. 1: Visualization of influences for the biological inspired approach: On the left (a) the standard view of interaction with the environment in reinforcement learning [13] is extended to a hierarchical perspective [10] as advocated, for example in [6]. For higher level control (shown in blue) this is in agreement with what we know on the structure of motor control in mammals [14] about descending pathways and modulation of lower level control centers (shown in green) in the spinal cord (see schematic b) in blue and green). Such structures are shared not only in mammals, but also in invertebrates and insects [15], see c). Work in such simpler model systems allows more detailed analysis of interaction with the environment which has stressed the importance of very fast and local reflex activity controlled directly on the lowest level or that are even realized by passive properties as muscle elasticities or preflexes (shown in orange). One important characteristic emphasized by this work is the emergence of behavior as a result of decentralized and locally interacting concurrent control structures (bottom part of c), one example is given by the decentralized control structure found in stick insects [16], but this concurrency is as well assumed in primates [17]. In the presented approach, this decentralized architecture (bottom part of c) is used on a hexapod and learning of the six local control modules (realized as deep neural networks) is driven by a reward signal as in reinforcement learning.d 7 D n X H r J b y G J 7 C B W Q h Y S S e V q v f y q s u L p d B o 2 9 9 W b m N z a 3 s n v 1 v Y 2 z 8 4 P C o e n z R 1 l C j G G y y S k W p 7 V H M p Q t 5 A g Z K 3 Y 8 V p 4 E n e 8 k a 3 M 7 8 1 5 k q L K H z E S c z d g A 5 C 0 R e M o p E e a A 9 7 x Z J d t u c g 6 8 T J S A k y 1 H v F n 6 4 f s S T g I T J J t e 4 4 d o x u S h U K J v m 0 0 E 0 0 j y k b 0 Q H v G B r S g G s 3 n U e d k g u j + K Q f K f N C J H P 1 7 0 Z K A 6 0 n g W c m A 4 p D v e r N x P + 8 j j 8 W s c 5 u P S 2 O L S f B / o 2 b i j B O k I d s E a S f S I I R m f V C f K E 4 Q z k x h D I l z F 8 I G 1 J F G Z r 2 C q Y k Z 7 W S d d K s l J 2 r c u W + W q p V s 7 r y c A b n c A k O X E M N 7 q A O D W A w g B d 4 h T f r 2 X q 3 P q z P x W j O y n Z O Y Q n W 1 y + Z 0 5 u C < / l a t e x i t > S t < l a t e x i t s h a 1 _ b a s e 6 4 = "" 6 g X k k R n P r g 3 2 M S 5 l 8 q 0 c f h o L 4 a 8 = "" > A A A C C n i c b V B L T g J B F H z j F / G H u n T T k Z i 4 I j N I o k s S N y 4 x y C e B C e n p a a B D z y f d b 4 h k w g 1 0 q / d w Z 9 x 6 C a / h C W x g F g J W 0 k m l 6 r 2 8 6 v J i K T T a 9 r e 1 s b m 1 v b O b 2 8 v v H x w e H R d O T p s 6 S h T j D R b J S L U 9 q r k U I W + g Q M n b s e I 0 8 C R v e a O 7 m d 8 a c 6 V F F D 7 i J O Z u Q A e h 6 A t G 0 U j 1 e g 9 7 h a J d s u c g 6 8 T J S B E y 1 H q F n 6 4 f s S T g I T J J t e 4 4 d o x u S h U K J v k 0 3 0 0 0 j y k b 0 Q H v G B r S g G s 3 n U e d k k u j + K Q f K f N C J H P 1 7 0 Z K A 6 0 n g W c m A 4 p D v e r N x P + 8 j j 8 W s c 5 u P S 2 O L S f B / q 2 b i j B O k I d s E a S f S I I R m f V C f K E 4 Q z k x h D I l z F 8 I G 1 J F G Z r 2 8 q Y k Z 7 W S d d I s l 5 z r U v m h U q x W s r p y c A 4 X c A U O 3 E A V 7 q E G D W A w g B d 4 h T f r 2 X q 3 P q z P x e i G l e 2 c w R K s r 1 + C v 5 t 0 < / l a t e x i t > state R t < l a t e x i t s h a 1 _ b a s e 6 4 = "" c T e d c t B a I s Y C W 6 q a W 0 F l E / d E 4 V 8 = "" > A A A C C n i c b V D J T g J B F H z j i r i h H r 1 0 J C a e y A y S 6 J H E i 0 d c W B K Y k J 6 e B j r 0 L O l + Q y Q T / k C v + h / e j F d / w t / w C 2 x g D g J W 0 k m l 6 r 2 8 6 v J i K T T a 9 r e 1 t r 6 x u b W d 2 8 n v 7 u 0 f H B a O j h s 6 S h T j d R b J S L U 8 q r k U I a + j Q M l b s e I 0 8 C R v e s O b q d 8 c c a V F F D 7 i O O Z u Q P u h 6 A l G 0 U g P 9 1 3 s F o p 2 y Z 6 B r B I n I 0 X I U O s W f j p + x J K A h 8 g k 1 b r t 2 D G 6 K V U o m O S T f C f R P K Z s S P u 8 b W h I A 6 7 d d B Z 1 Q s 6 N 4 p N e p M w L k c z U v x s p D b Q e B 5 6 Z D C g O 9 L I 3 F f / z 2 v 5 I x D q 7 9 T Q / t p g E e 9 d u K s I 4 Q R 6 y e Z B e I g l G Z N o L 8 Y X i D O X Y E M q U M H 8 h b E A V Z W j a y 5 u S n O V K V k m j X H I u S + W 7 S r F a y e r K w S m c w Q U 4 c A V V u I U a 1 I F B H 1 7 g F d 6 s Z + v d + r A + 5 6 N r V r Z z A g u w v n 4 B g R m b c w = = < / l a t e x i t > reward action Rt+1 < l a t e x i t s h a 1 _ b a s e 6 4 = "" t f o S X 8 X n a W S 0 W + b a 3 1 M k 9 8 c R Y 9 Q = "" > A A A C D n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k U Q h J J U Q Z c F N y 6 r 2 A e 0 o U w m 0 3 b o Z B J m b o o l 9 B 9 0 q / / h T t z 6 C / 6 G X + C 0 z c K 2 H h g 4 n H M v 9 8 z x Y 8 E 1 O s 6 3 l V t b 3 9 j c y m 8 X d n b 3 9 g + K h 0 c N H S W K s j q N R K R a P t F M c M n q y F G w V q w Y C X 3 B m v 7 w d u o 3 R 0 x p H s l H H M f M C 0 l f 8 h 6 n B I 3 U f O i m e O F O u s W S U 3 Z m s F e J m 5 E S Z K h 1 i z + d I K J J y C R S Q b R u u 0 6 M X k o U c i r Y p N B J N I s J H Z I + a x s q S c i 0 l 8 7 i T u w z o w R 2 L 1 L m S b R n 6 t + N l I R a j 0 P f T I Y E B 3 r Z m 4 r / e e 1 g x G O d 3 X q a H 1 t M g r 0 b L + U y T p B J O g / S S 4 S N k T 3 t x g 6 4 Y h T F 2 B B C F T d / s e m A K E L R N F g w J b n L l a y S R q X s X p Y r 9 1 e l q p P V l Y c T O I V z c O E a q n A H N a g D h S G 8 w C u 8 W c / W u / V h f c 5 H c 1 a 2 c w w L s L 5 + A U i h n O s = < / l a t e x i t > St+1 < l a t e x i t s h a 1 _ b a s e 6 4 = "" u v k b b Y W g B T P y G V q 2 u I 8 6 E j 7 G p n w = "" > A A A C D n i c b V D L S s N A F L 2 p r 1 p f V Z d u g k U Q h J J U Q Z c F N y 4 r 2 g e 0 o U w m 0 3 b o Z B J m b o o l 9 B 9 0 q / / h T t z 6 C / 6 G X + C 0 z c K 2 H h g 4 n H M v 9 8 z x Y 8 E 1 O s 6 3 l V t b 3 9 j c y m 8 X d n b 3 9 g + K h 0 c N H S W K s j q N R K R a P t F M c M n q y F G w V q w Y C X 3 B m v 7 w d u o 3 R 0 x p H s l H H M f M C 0 l f 8 h 6 n B I 3 U f O i m e O F O u s W S U 3 Z m s F e J m 5 E S Z K h 1 i z + d I K J J y C R S Q b R u u 0 6 M X k o U c i r Y p N B J N I s J H Z I + a x s q S c i 0 l 8 7 i T u w z o w R 2 L 1 L m S b R n 6 t + N l I R a j 0 P f T I Y E B 3 r Z m 4 r / e e 1 g x G O d 3 X q a H 1 t M g r 0 b L + U y T p B J O g / S S 4 S N k T 3 t x g 6 4 Y h T F 2 B B C F T d / s e m A K E L R N F g w J b n L l a y S R q X s X p Y r 9 1 e l q p P V l Y c T O I V z c O E a q n A H N a g D h S G 8 w C u 8 W c / W u / V h f c 5 H c 1 a 2 c w w L s L 5 + A U p L n O w = < / l a tcontrol principle into a control architecture for six-legged walking and want to analyze how learning in general and DRL -as a contribution of this publication -benefits from this structure and how in the end control performance is affected by such a structure. Therefore, we train and compare a decentralized control architecture and compare this to learning of a baseline centralized control approach. As a result, the decentralized controller learns faster and, importantly, is not only able to produce comparable walking behavior, but even results in significantly better performance and is able to generalize as well towards novel environments. Decentralization appears as a viable principle for DRL and an important aspect for adaptive behavior [6] as demonstrated here in our simulation on a six-legged robot. B. Deep Reinforcement Learning FrameworkReinforcement Learning is characterized by an agent that is interacting in an environment (Fig. 1 a). While the agent produces actions, it gets as a response an updated state of the environment and a reward signal. The goal of the agent is to learn a policy π(S) for sequential decision making that maximizes the accumulated long-term return. During learning the agent has to explore possible alternatives and while getting more confident should come up with a policy it can exploit (for an introduction see [1], [13]). Sequential decision making can be formalized as a Markov Decision Process which provides and is defined as a tuple of:• observable states S, • possible actions A, • a reward signal R providing the immediate reward after choosing an action from the current state,• transition probabilities P that describe the probability distribution over states after following an action when being in the current specific state, and• a discount factor γ that describes how to decrease the weights of future rewards. We are dealing with a continuous control task: the state space as given by the sensory signals is continuous as well as the action space which corresponds to the joint motor signals.Overall, these spaces are high dimensional and continuous which requires to rely on function approximation for learning As a framework for DRL we employed OpenAI's gym. The connection to the simulator through ROS was wrapped as a new environment through a connector using ROS2Learn [31]. This allowed to use the standard baseline implementations of DRL algorithms [32]. In this approach Proximal Policy Optimization (PPO) [33] was used as it has shown to work well on continuous tasks without the need of intensive hyperparameter tuning. As a further advantage, it has a comparably low sampling complexity (amount of training samples required to find usable policies).The reward function was defined as the mean velocity over an episode (1024 iteration steps, equals nearly 41 seconds). The agent uses the received reward R t+1 after executing an action A t in state S t in order to directly optimize its' policy π. A neural network was used as a function approximator for the learned policy. The policy networks consisted of two hidden layers of each 64 units with tanh activation functions. Updating the neural networks' weights is called an epoch. After training, the policy network should generalize to novel sensed states and modulate motor output in order to achieve a maximal external reward. The next section will describe in detail the two compared architectures that employ such policy networks.",no,,,,no,,,,
1926,https://github.com/serenabooth/Bayes-TrEx,serenabooth_Bayes-TrEx.tar.gz,BAYES-TREX: a Bayesian Sampling Approach to Model Transparency by Example,,"utils PIL nn_models csv multiprocessing sys torch, torchvision, pyro, argparse, os tqdm torch, torchvision os, csv __future__ torchvision argparse, os, uuid, json re random, argparse, torch, os, pyro, math, uuid, json, tqdm datetime collections json h5py sys, random, os cProfile math, sys, random, argparse, json, os, tempfile torch iep os argparse, os, json matplotlib nets json, os, math argparse, json, os scipy mathutils itertools dataset question_engine run_model sklearn argparse numpy bpy, bpy_extras time random, argparse, torch, os, pyro, math, uuid, json shutil math argparse, json, os, itertools, random, shutil random cv2 torch, pyro, argparse, os models pyro",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/serenabooth_Bayes-TrEx.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\serenabooth_Bayes-TrEx.pdf,,"The classifier rarely makes ambiguous predictions onsharp and realistic images. To experimentally check the second explanation, we train a classifier to be consistently ambiguous between class i and i + 1 for an image of digit i (wrapping around at 10 = 0) using the following KL-divergence loss:l(y, f (x)) = KL(p y , f (x)),(13)p y,i = 0.5 i = y or i = (y + 1) mod 10, 0 otherwise. ( 14)Using this classifier, we sample ambiguous examples for 0v1, 1v2, ..., 9v0. Sampling succeeds for all ten pairs, even when using the same GAN model that rarely succeeded in the prior experiment. Fig. 7 presents the 0v1 samples and predicted confidence by this modified classifier, and the remaining pairs are visualized in Appx. F. Given this sampling success, we conclude that the second explanation is correct. In addition, BAYES-TREX is also unable to generate ambiguous examples for CLEVR with the manually defined data distribution. Given that the pre-trained classifier only achieves ≈60% accuracy, the result suggests that the model is likely overconfident. Indeed, this has previously been observed in similar settings (Kim, Ricci, and Serre 2018).",no,,,,no,,,,
1929,https://github.com/thak123/bert-twitter-sentiment,thak123_bert-twitter-sentiment.tar.gz,Pretraining and Fine-Tuning Strategies for Sentiment Analysis of Latvian Tweets,"@app.route(""/predict"", methods=['POST']) @ app.route(""/models"") @ app.route(""/demo"") @dataclass @ app.route(""/"")",utils requests pandas sys glob engine tokenizer tqdm re model datetime absl transformers dataclasses torch os matplotlib scipy apex seaborn dataset ptvsd pickle typing sklearn argparse tensorboardX numpy flask config shutil math loguru random logging,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thak123_bert-twitter-sentiment.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thak123_bert-twitter-sentiment.pdf,,"ExperimentsIn this section, we describe the experimental setup for sentiment analysis. Our experimental setup consists of pre-training and fine-tuning steps. We perform the following pre-processing steps on the text: 1. Tokenization. 2. Removal of URLs. 3. Replacement of consecutive user mentions with a single mention. 4. User mention replacement with a placeholder ('mention i' where the i stands for the i th mention in the tweet). 5. Lower-casing of the whole tweet.",no,,no,,no,,,,
1931,https://github.com/sandeepsoni/MHP,sandeepsoni_MHP.tar.gz,DETECTING SOCIAL INFLUENCE IN EVENT CASCADES BY COMPARING DISCRIMINATIVE RANKERS,,HPUtils scipy sklearn cvxpy __future__ numpy,cs.SI cs.LG physics.soc-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sandeepsoni_MHP.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sandeepsoni_MHP.pdf,,,no,,no,,no,,,,
1934,https://github.com/nrdg/fracridge,nrdg_fracridge.tar.gz,"A reparameterization of Ridge Regression Fractional ridge regression: a fast, interpretable reparameterization of ridge regression","@pytest.mark.parametrize(""bb"", [(1), (2), (1000)]) @pytest.mark.parametrize(""fit_intercept"", [False]) @njit(fastmath=True, nogil=True, cache=True) @pytest.mark.parametrize(""fit_intercept"", [(False), (True)]) @pytest.mark.parametrize(""jit"", [True, False]) @pytest.mark.parametrize(""fit_intercept"", [True, False]) @pytest.mark.parametrize(""nn"", [(1000), (10), (284)]) @pytest.mark.parametrize(""nn, pp"", [(1000, 10), (10, 100), (284, 50)]) @pytest.mark.parametrize(""frac"", [0.1, 0.23, 1])",fuzzywuzzy warnings sys fracridge distutils pathlib json collections sphinx_gallery setuptools_scm os pytest sphinx matplotlib sphinx_rtd_theme string scipy functools numba sklearn numpy shutil setuptools subprocess,stat.ME cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nrdg_fracridge.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nrdg_fracridge.pdf,,"PythonThe Python implementation of FRR depends on Scipy (Virtanen et al., 2020) and Numpy (van der Walt et al., 2011). The object-oriented interface provided conforms with the API of the popular Scikit-Learn library (Pedregosa et al., 2011;Buitinck et al., 2013), including automated tests that verify compliance with this API. Unit tests are implemented using pytest (Krekel et al., 2004-). Documentation is automatically compiled using sphinx, with sphinx-gallery examples ( Òscar Nájera et al., 2020). The Python implementation also optionally uses Numba (Lam et al., 2015) for just-in-time compilation of a few of the underlying numerical routines used in the implementation. This functionality relies on an implementation provided in the hyperlearn library (Han-Chen, 2020) and copied into the fracridge source-code, together with its license, which is compatible with the fracridge license. In addition to its release on GitHub, the software is available to install through the Python Package Index (PyPI) through the standard Python Package Installer (pip install fracridge). For Python, we did not implement standardization procedures, as those are implemented as a part of Scikit-Learn. ); coef3 = inv(X'*X + alphaI)*X'*y; norm(coef) norm(coef2) norm(coef2) ./ norm(coef) norm(coef2-coef3)Python import numpy as np from numpy.linalg import inv, norm from fracridge import fracridge from fracridge import FracRidge y = np.random.randn(100) X = np.random.randn(100, 10) # Calculate coefficients with naive OLS coef = inv(X.T @ X) @ X.T @ y # Call fracridge function: coef2, alpha = fracridge(X, y, 0.3) # Calculate coefficients with naive RR alphaI = alpha * np.eye(X.shape[1]) coef3 = inv(X.T @ X + alphaI) @ X.T @ y print(norm(coef)) print(norm(coef2)) print(norm(coef2) / norm(coef)) print(norm(coef2 -coef3))# sklearn-compatible object-oriented API: fr = FracRidge(fracs=0.3) fr.fit(X, y) coef_oo = fr.coef_ alpha_oo = fr.alpha_ print(norm(coef_oo) / norm(coef))",no,,no,,no,,,,
1936,https://github.com/amoliu/CayleyNet,amoliu_CayleyNet.tar.gz,CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters,,"gensim scipy sklearn sklearn, sklearn numpy matplotlib time, re",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amoliu_CayleyNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amoliu_CayleyNet.pdf,CayleyNet,,no,,no,,no,,,,
1938,https://github.com/gunnxx/rove,gunnxx_rove.tar.gz,Robust-to-Noise Models in Natural Language Processing Tasks,@property,utils torchtext model json random typing logging tqdm argparse os dill torch,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gunnxx_rove.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gunnxx_rove.pdf,fastText,"MethodologyWe do experiments on two corpora: Airline Twitter Sentiment 2 and Movie Review (Maas et al., 2011), which are marked up for sentiment analysis task.We conduct three types of experiments: (a) the train-and testsets are spell-checked and artificial noise in inserted; (b) the train-and testsets are not changed (with the above mentioned exception for Russian corpus) and no artificial noise is added; and (c) the trainset is spell-checked and noised, the testset is unchanged.These experimental setups are meant to demonstrate the robustness of tested architectures to artificial and natural noise.As baselines we use architectures based on fasttext word embedding model (Bojanowski et al., 2016) and an architecture which follows (Kim et al., 2016). Another baseline, which is purely character-level, will be adopted from the work (Kim, 2014). MethodologyWe conduct three types of experiments: (a) the trainset and testset are not changed and no artificial noise is induced; (b) the artificial noise is inserted into trainset and testset simultaneously; and (c) the trainset is being noised, the testset is unchanged.These experimental setups are meant to demonstrate the robustness of tested architectures to artificial and natural noise (i.e. typos).The proposed corpora to use are: English and Russian news corpora, CoNLL'03 (Tjong Kim Sang and De Meulder, 2003) and Persons-1000 (Mozharova and Loukachevitch, 2016) respectively, and French social media corpus CAp'2017 (Lopez et al., 2017).We investigate variations of the state of the art architecture for Russian (Anh et al., 2017) and English (Lample et al., 2016) languages and apply the same architecture to the French language corpus.",no,,no,,no,,,,
1940,https://github.com/afarahi/kllr,afarahi_kllr.tar.gz,Stellar Property Statistics of Massive Halos from Cosmological Hydrodynamics Simulations: Common Kernel Shapes,,scipy pandas warnings setuptools tqdm sklearn os numpy matplotlib kllr,astro-ph.GA astro-ph.CO astro-ph.CO astro-ph.GA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/afarahi_kllr.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\afarahi_kllr.pdf,,,no,,no,,no,,,,
1944,https://github.com/pnieves2019/MAELAS,pnieves2019_MAELAS.tar.gz,MAELAS: MAgneto-ELAStic properties calculation via computational high-throughput approach,,math scipy re stat pymatgen setuptools sys pyfiglet shlex sklearn argparse os subprocess maelas numpy matplotlib,cond-mat.mtrl-sci cond-mat.mtrl-sci physics.comp-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pnieves2019_MAELAS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pnieves2019_MAELAS.pdf,,,no,,,,no,,,,
1952,https://github.com/Hermina/GOT,Hermina_GOT.tar.gz,GOT: An Optimal Transport framework for Graph comparison,,scipy copy warnings networkx random sklearn torch numpy matplotlib,cs.LG cs.SI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Hermina_GOT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Hermina_GOT.pdf,,,no,,no,,no,,,,
1966,https://github.com/lingchunkai/ACNet,lingchunkai_ACNet.tar.gz,Deep Archimedean Copulas,@ex.capture @ex.automain @ex.config @staticmethod,main train copy dirac_phi scipy phi_listing pickle logging sklearn argparse torch sacred numpy os gen_data matplotlib,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lingchunkai_ACNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lingchunkai_ACNet.pdf,,"IntroductionModeling dependencies between random variables is a central problem in machine learning and statistics. Copulas are a special class of cumulative density functions which specify the dependencies between random variables without any restriction on their marginals. This has led to long lines of research in modeling and learning copulas [15,7], as well as their applications in fields such as finance and healthcare [5,3]. Amongst the most common class of copulas are Archimedean Copulas, which are defined by a one-dimensional function ϕ, known as the generator, and often favored for their simplicity and ability to model extreme distributions. A key problem in the application of Archimedean Copulas is the selection of the parametric form of ϕ as well as the limitations on the expressiveness of commonly used copula. Present workarounds include the selection of the best model between a fixed set of commonly used copulas, the use of methods based on information criterion such as Akaike and Bayesian Information Criterion (AIC, BIC), as well as more modern nonparametric methods.In this paper, we propose ACNet, a novel network architecture which models the generator of an Archimedean copula using a deep neural network, allowing for network parameters to be learnt using backpropagation and gradient descent. The core idea behind ACNet is to model the generator as a sum of convex combination of a finite set of exponential functions with varying rates of decay, while exploiting their invariance to convex combinations and multiplications with other exponentials. ACNet is built from simple, differentiable building blocks, ensuring that log-likelihood is a differentiable function of ϕ, ensuring ease of training via backpropagation. By possessing a larger set of parameters, ACNet is able to approximate all copulas with completely monotone generators, a large class which encompasses most of the commonly used copulas, but also other Archimedean 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:2012.03137v1 [cs.LG] 5 Dec 2020 copulas which have no straightforward closed forms. To our knowledge, ACNet is the first method to utilize deep representations to model generators for Archimedean copulas directly.ACNet enjoys several theoretical properties, such as a simple interpretation of network weights in terms of a Markov reward process, resulting in a numerically stable, dimension independent method of sampling from the copula. Using this interpretation, we show that deep variants of ACNet are theoretically able to represent generators which shallow nets may not. By modeling the cumulative density directly, ACNet is able to provide a wide range of probabilistic quantities such as conditional densities and distributions using a single trained model. This flexibility in expression extends to both inference and training and is not possible with other deep methods such as Generative Adversarial Networks (GANs) or Normalizing Flows, which at best allow for the evaluation of densities.Empirical results show that ACNet is able to learn standard copula with little to no hyperparameter tuning. When tested on real-world data, we observed that ACNet was able to learn new generators which are a better qualitative description of observed data compared to commonly used Archimedean copulas. Lastly, we demonstrate the effectiveness of ACNet in situations where measurements are uncertain within known boundaries. This task is challenging for methods which learn densities as evaluating probabilities would then involve the costly numerical integration of densities.We (i) propose ACNet, the first network to learn completely monotone generating functions for the purpose of learning copulas, (ii) study the theoretical properties of ACNet, including a simple interpretation of network weights and an efficient sampling process, (iii) show how ACNet may be used to compute probabilistic quantities beyond log-likelihood and cumulative densities, and (iv) evaluate ACNet on both synthetic and real-world data, demonstrating that ACNet combines the ease of use enjoyed by commonly used copulas and the representational capacity of Archimedean copulas. The source code for this paper may be found at https://github.com/lingchunkai/ACNet. ExperimentsHere, we first empirically demonstrate the efficacy of ACNet in fitting both synthetic and real-world data. We then end off by applying ACNet to Scenario 2 of Section 3.5, and show that ACNet can be used to fit data even when the data exhibits uncertainty in measurements. The goal of these experiments is not to serve as comparison against neural density estimators (which typically model joint densities and not joint distribution functions), but rather as an alternative to frequently used parametric copula. Experiments are conducted on a 3.1 GHz Intel Core i5 with 16 GB of RAM. We utilize the PyTorch [28] framework for automatic differentiation. We use double precision arithmetic as the inversion of ϕ requires numerical precision. When using Newton's method to compute ϕ −1 , we terminate when the error is ≤ 1e − 10. For all our experiments we use ACNet with L = 2 and H 1 = H 2 = 10, i.e., 2 hidden layers each of width 10. The network is small but sufficient for our purpose since {ϕ nn } is only 1-dimensional. Φ A and Φ B were initialized in the range [0, 1] and (0, 2) uniformly at random. We use stochastic gradient descent with a learning rate of 1e − 5, momentum of 0.9, and a batch size of 200. No hyperparameter tuning was performed. Practical considerations and limitations of ACNetExperiments when d > 2. Here, we show that ACNet is capable of fitting distributions with more than 2 dimensions. We use the GAS dataset [34], which comprises readings from chemical sensors  (iv)-(vi) joint distributions, log-densities and samples drawn from the trained network. used in simulations for drift compensation. To simplify the situation, we use features 0, 4 and 7 from a single sensor during the second month (see [34] for details) and perform normalization for each feature in a similar fashion Section 4.2, yielding a dataset comprising 445 readings. The network architecture and train/test split are identical to Section 4.2.As before, we train ACNet by minimizing log-loss and compare our results against the Clayton, Frank, and Gumbel copulas. The results are in Figure 8. We observe that ACNet is able to fit the data reasonably despite the data not being entirely symmetric over the 3 dimensions. ACNet achieves a test/train loss of -1.389 and -1.456, outperforming the Frank copula (the best performing parametric copula), which obtained a test/train loss of -1.356 and -1.357. Similar to the Boston housing dataset, ACNet overfits. This is unsurprising since the dataset is fairly small. Generally, we do not recommend using ACNet with high dimensions. First, this often results in numerical issues since training ACNet by minimizing the log-loss requires differentiating the copula d times. Generally, we observe that ACNet faces numerical problems for d ≥ 5 even when employing double precision. Second, high dimensional data is rarely symmetric unless there is some underlying structure supporting this belief.Failure cases. Not all datasets are well modelled by ACNet. Consider the POWER dataset [12] (Figure 9), which contains measurements for electric power consumption in a single household. For simplicity, we focus on the joint distribution of the power consumption between the kitchen and laundry room. Clearly, the POWER dataset is unlike the previous distributions, as it posesses a high level of 'discreteness'. Since there are few appliances in each room and each active appliance consumes a fixed amount of power, we would expect that each combination of active appliances would lead to a distinct profile in power consumption. As seen from Figure 9, ACNet is unable to accurately fit this distribution. It is worth noting however, that despite learning a distribution that appears qualitatively different, ACNet still achieves a test loss of -0.221, which is significantly better than the uniform distribution and slightly superior to the Clayton copula, the best fit among the copula we compared with. Running times. ACNet's generator is represented by a neural network and is thus slower to train compared to single-parameter copulas. However, performing training is still feasible in practice. With our experimental setup, we are able to train 15 minibatches each of size 200 in 1 second without utilizing a GPU. Furthermore, in all our experiments, the network converges within 10 • 4 iterations. For a training set with 2000 points, ACNet converges in 3-5 hours. Computational costs are split roughly evenly between the forward and backward passes-the former involves solving for the inverse while the latter involves taking 2 (or more) rounds of differentiation.",no,,no,,no,,,,
1987,https://github.com/DetionDX/GP-GAN-Gender-Preserving-GAN-for-Synthesizing-Faces-from-Landmarks,DetionDX_GP-GAN-Gender-Preserving-GAN-for-Synthesizing-Faces-from-Landmarks.tar.gz,GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks,@staticmethod,"requests PIL pandas csv warnings pdb bs4 zlib __future__ torchvision util collections data visdom torch os inspect, re dominate struct functools argparse numpy time tarfile ntpath random options models zipfile",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DetionDX_GP-GAN-Gender-Preserving-GAN-for-Synthesizing-Faces-from-Landmarks.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DetionDX_GP-GAN-Gender-Preserving-GAN-for-Synthesizing-Faces-from-Landmarks.pdf,U-Net  Rectified Linear Units  Batch Normalization  Convolution  Average Pooling  Concatenated Skip Connection  Global Average Pooling  Dense Block  Kaiming Initialization  1x1 Convolution  Dropout  Dense Connections  Max Pooling  Softmax  DenseNet,,no,,no,,no,,,,
1996,https://github.com/Quiff789/Mapper-instability,Quiff789_Mapper-instability.tar.gz,A NUMERICAL MEASURE OF THE INSTABILITY OF MAPPER-TYPE ALGORITHMS,,math copy itertools re sys random sklearn kmapper os mpl_toolkits numpy matplotlib,math.AT cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Quiff789_Mapper-instability.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Quiff789_Mapper-instability.pdf,,"Table 11For the values of epsilon of 0.06 and 0.09, the instability decreases due to the disappearance of noise represented by spurious small connected components in the Mapper graph. The major part of the structure of the Mapper graph remains the same, revealing both the inner and outer circle. Above the epsilon values of 0.1, there is a spike in the instability value corresponding to a loss of detail in the inner circle within the Mapper graph. A similar spike occurs around the 0.32 value of epsilon, corresponding to the loss of the inner circle from the Mapper graph. The final large increase in instability occurs around the 0.45 value of epsilon, and it corresponds to the gradual merging of the two circles in the Mapper graph.We now pass to experiments that explore the dependence of the Mapper graph on the values of resolution and gain. Figure2presents a contour plot of the instability of Mapper on another dataset consisting of noisy concentric circles created by varying the percentage overlap between bins (gain) and the number of bins (resolution).considers a dataset with two noisy concentric circles. We produce a family of Mapper graphs using the -neighbourhood clustering with varying values of epsilon. The specific clustering procedure used was DBSCAN from the sklearn python package.Similarly to the discussion on Table1, it is possible to identify a number of global features within the plot with structural changes in the Mapper graph.",no,,no,,no,,,,
1999,https://github.com/amanpreet692/Open4Business,amanpreet692_Open4Business.tar.gz,Open4Business (O4B): An Open Access Dataset for Summarizing Business Documents,@abstractmethod @staticmethod,requests crossref csv sys glob tqdm parse_doc2xml pathlib re json dataset_generation nltk os unidecode matplotlib seaborn unpywall pickle sklearn argparse lxml numpy time abc ntpath random logging,cs.IR cs.AI cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amanpreet692_Open4Business.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amanpreet692_Open4Business.pdf,,,no,,no,,no,,,,
