{
    "dataset.py": {
        "torch": {
            "C2SDataSet_6": {
                "base_class_0": {
                    "value": "torch.utils.data.Dataset",
                    "possible_values": []
                },
                "self.f": {
                    "value": "filedata",
                    "possible_values": []
                },
                "self.size": {
                    "value": "data_size",
                    "possible_values": []
                },
                "self.target_dict": {
                    "value": "target_dict",
                    "possible_values": []
                },
                "self.path_dict": {
                    "value": "path_dict",
                    "possible_values": []
                },
                "self.terminal_dict": {
                    "value": "terminal_dict",
                    "possible_values": []
                },
                "self.device": {
                    "value": "device",
                    "possible_values": []
                }
            },
            "tensor_114": {
                "data": {
                    "value": "starts",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "starts + [[self.terminal_dict['<pad>'] for i in range(self.max_terminal_length)] for j in range(pad_length)]",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.long",
                    "possible_values": []
                }
            },
            "tensor_115": {
                "data": {
                    "value": "paths",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "paths + [[self.path_dict['<pad>'] for i in range(self.max_path_length)] for j in range(pad_length)]",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.long",
                    "possible_values": []
                }
            },
            "tensor_116": {
                "data": {
                    "value": "ends",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "ends + [[self.terminal_dict['<pad>'] for i in range(self.max_terminal_length)] for j in range(pad_length)]",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.long",
                    "possible_values": []
                }
            },
            "tensor_117": {
                "data": {
                    "value": "target",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "target + [self.target_dict['<pad>']] * (self.max_target_length - len(target))",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.long",
                    "possible_values": []
                }
            },
            "tensor_118": {
                "data": {
                    "value": "context_mask",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "context_mask + [0] * pad_length",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            },
            "tensor_120": {
                "data": {
                    "value": "start_mask",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "start_mask + [[0 for i in range(self.max_terminal_length)] for j in range(pad_length)]",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            },
            "tensor_121": {
                "data": {
                    "value": "path_length",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "path_length + [1] * pad_length",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.int64",
                    "possible_values": []
                }
            },
            "tensor_122": {
                "data": {
                    "value": "end_mask",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "end_mask + [[0 for i in range(self.max_terminal_length)] for j in range(pad_length)]",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            },
            "tensor_123": {
                "data": {
                    "value": "target_mask",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ],
                        [
                            "[1] * (len(target) - 1)",
                            "BinOp"
                        ],
                        [
                            "target_mask + [0] * (self.max_target_length - len(target))",
                            "BinOp"
                        ]
                    ]
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            }
        }
    },
    "main.py": {
        "torch": {
            "device_65": {
                "variable": {
                    "value": "device",
                    "possible_values": []
                },
                "type": {
                    "value": "args.gpu if args.gpu != -1 and torch.cuda.is_available() else 'cpu'",
                    "possible_values": []
                }
            },
            "DataLoader_108": {
                "variable": {
                    "value": "trainloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "C2SDataSet(args, train_h5, args.trainnum, terminal_dict, path_dict, target_dict, device)",
                    "possible_values": []
                },
                "batch_size": {
                    "value": "args.batchsize",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "args.num_worker",
                    "possible_values": []
                }
            },
            "DataLoader_114": {
                "variable": {
                    "value": "validloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "C2SDataSet(args, test_h5, args.validnum, terminal_dict, path_dict, target_dict, device)",
                    "possible_values": []
                },
                "batch_size": {
                    "value": "args.batchsize",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "args.num_worker",
                    "possible_values": []
                }
            },
            "SGD_120": {
                "variable": {
                    "value": "optimizer",
                    "possible_values": []
                },
                "params": {
                    "value": "c2s.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "0.01",
                    "possible_values": []
                },
                "momentum": {
                    "value": "0.95",
                    "possible_values": []
                }
            },
            "StepLR_122": {
                "variable": {
                    "value": "scheduler",
                    "possible_values": []
                },
                "optimizer": {
                    "value": "optimizer",
                    "possible_values": [
                        [
                            "optim.SGD(c2s.parameters(), lr=0.01, momentum=0.95)",
                            "Call"
                        ]
                    ]
                },
                "step_size": {
                    "value": "1",
                    "possible_values": []
                },
                "gamma": {
                    "value": "0.95",
                    "possible_values": []
                },
                "last_epoch": {
                    "value": "-1",
                    "possible_values": []
                }
            },
            "DataLoader_127": {
                "variable": {
                    "value": "trainloader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "C2SDataSet(args, train_h5, args.trainnum, terminal_dict, path_dict, target_dict, device)",
                    "possible_values": []
                },
                "batch_size": {
                    "value": "args.batchsize",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "args.num_worker",
                    "possible_values": []
                }
            },
            "save_166": {
                "obj": {
                    "value": "c2s.state_dict()",
                    "possible_values": []
                },
                "f": {
                    "value": "args.savename + '.model'",
                    "possible_values": []
                }
            },
            "load_106": {
                "f": {
                    "value": "args.resume",
                    "possible_values": []
                }
            },
            "save_164": {
                "obj": {
                    "value": "c2s.state_dict()",
                    "possible_values": []
                },
                "f": {
                    "value": "args.savename + str(epoch) + '.model'",
                    "possible_values": []
                }
            },
            "is_available_66": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "no_grad_150": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "model.py": {
        "torch": {
            "Code2Seq_7": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "self.decode_size": {
                    "value": "decode_size",
                    "possible_values": [
                        [
                            "320",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.terminal_embed_size": {
                    "value": "terminal_embed_size",
                    "possible_values": [
                        [
                            "128",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.path_embed_size": {
                    "value": "path_embed_size",
                    "possible_values": [
                        [
                            "128",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.path_rnn_size": {
                    "value": "path_rnn_size",
                    "possible_values": [
                        [
                            "128 * 2",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.target_dict": {
                    "value": "target_dict",
                    "possible_values": []
                },
                "self.device": {
                    "value": "device",
                    "possible_values": []
                },
                "Embedding_27": {
                    "variable": {
                        "value": "self.terminal_element_embedding",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "terminal_vocab_size",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "terminal_embed_size",
                        "possible_values": [
                            [
                                "128",
                                "MethodArgument"
                            ]
                        ]
                    }
                },
                "Embedding_29": {
                    "variable": {
                        "value": "self.path_element_embedding",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "path_element_vocab_size",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "path_embed_size",
                        "possible_values": [
                            [
                                "128",
                                "MethodArgument"
                            ]
                        ]
                    }
                },
                "Embedding_31": {
                    "variable": {
                        "value": "self.target_element_embedding",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "self.target_vocab_size",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "target_embed_size",
                        "possible_values": [
                            [
                                "128",
                                "MethodArgument"
                            ]
                        ]
                    }
                },
                "Parameter_36": {
                    "variable": {
                        "value": "self.terminal_element_embedding.weight",
                        "possible_values": []
                    },
                    "data": {
                        "value": "torch.rand(terminal_vocab_size, terminal_embed_size) * math.sqrt(1 / terminal_embed_size)",
                        "possible_values": []
                    }
                },
                "Parameter_39": {
                    "variable": {
                        "value": "self.path_element_embedding.weight",
                        "possible_values": []
                    },
                    "data": {
                        "value": "torch.rand(path_element_vocab_size, path_embed_size) * math.sqrt(1 / path_embed_size)",
                        "possible_values": []
                    }
                },
                "Parameter_42": {
                    "variable": {
                        "value": "self.target_element_embedding.weight",
                        "possible_values": []
                    },
                    "data": {
                        "value": "torch.rand(self.target_vocab_size, target_embed_size) * math.sqrt(1 / target_embed_size)",
                        "possible_values": []
                    }
                },
                "LSTM_47": {
                    "variable": {
                        "value": "self.path_rnn",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "path_embed_size",
                        "possible_values": []
                    },
                    "num_layers": {
                        "value": "1",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    },
                    "dropout": {
                        "value": "self.path_rnn_drop",
                        "possible_values": []
                    },
                    "bidirectional": {
                        "value": "True",
                        "possible_values": []
                    }
                },
                "Linear_53": {
                    "variable": {
                        "value": "self.input_linear",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "terminal_embed_size * 2 + path_rnn_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "self.decode_size",
                        "possible_values": []
                    },
                    "bias": {
                        "value": "False",
                        "possible_values": []
                    }
                },
                "Dropout_56": {
                    "variable": {
                        "value": "self.input_dropout",
                        "possible_values": []
                    },
                    "p": {
                        "value": "self.embed_drop",
                        "possible_values": []
                    }
                },
                "LayerNorm_57": {
                    "variable": {
                        "value": "self.input_layer_norm",
                        "possible_values": []
                    },
                    "normalized_shape": {
                        "value": "self.decode_size",
                        "possible_values": []
                    }
                },
                "LSTMCell_60": {
                    "variable": {
                        "value": "self.decoder_rnn",
                        "possible_values": []
                    },
                    "input_size": {
                        "value": "target_embed_size",
                        "possible_values": [
                            [
                                "128",
                                "MethodArgument"
                            ]
                        ]
                    },
                    "hidden_size": {
                        "value": "decode_size",
                        "possible_values": [
                            [
                                "320",
                                "MethodArgument"
                            ]
                        ]
                    }
                },
                "Linear_62": {
                    "variable": {
                        "value": "self.Whc",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "self.decode_size + self.decode_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "decode_size",
                        "possible_values": [
                            [
                                "320",
                                "MethodArgument"
                            ]
                        ]
                    }
                },
                "LayerNorm_63": {
                    "variable": {
                        "value": "self.decoder_layer_norm",
                        "possible_values": []
                    },
                    "normalized_shape": {
                        "value": "self.decode_size",
                        "possible_values": []
                    }
                },
                "Linear_64": {
                    "variable": {
                        "value": "self.output_linear",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "decode_size",
                        "possible_values": [
                            [
                                "320",
                                "MethodArgument"
                            ]
                        ]
                    },
                    "out_features": {
                        "value": "self.target_vocab_size",
                        "possible_values": []
                    },
                    "bias": {
                        "value": "False",
                        "possible_values": []
                    }
                },
                "CrossEntropyLoss_67": {
                    "variable": {
                        "value": "self.loss",
                        "possible_values": []
                    },
                    "reduction": {
                        "value": "none",
                        "possible_values": []
                    }
                }
            },
            "sum_79": {
                "variable": {
                    "value": "embed_fasttext_start",
                    "possible_values": []
                },
                "input": {
                    "value": "embed_element_start * start_mask.unsqueeze(-1)",
                    "possible_values": []
                },
                "dtype": {
                    "value": "2",
                    "possible_values": []
                }
            },
            "sum_87": {
                "variable": {
                    "value": "embed_fasttext_end",
                    "possible_values": []
                },
                "input": {
                    "value": "embed_element_end * end_mask.unsqueeze(-1)",
                    "possible_values": []
                },
                "dtype": {
                    "value": "2",
                    "possible_values": []
                }
            },
            "pack_padded_sequence_99": {
                "variable": {
                    "value": "embed_element_path_packed",
                    "possible_values": []
                },
                "input": {
                    "value": "embed_element_path",
                    "possible_values": [
                        [
                            "self.path_element_embedding(paths.view(batch, -1)).view(batch * max_e, path_size, self.path_embed_size)",
                            "Call"
                        ]
                    ]
                },
                "lengths": {
                    "value": "ordered_len",
                    "possible_values": []
                },
                "batch_first": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "index_select_107": {
                "variable": {
                    "value": "rnn_embed_path",
                    "possible_values": []
                },
                "input": {
                    "value": "hn.view(batch * max_e, self.path_rnn_size)",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                },
                "index": {
                    "value": "ordered_idx",
                    "possible_values": []
                }
            },
            "cat_112": {
                "variable": {
                    "value": "combined_context_vectors",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(embed_fasttext_start, rnn_embed_path, embed_fasttext_end)",
                    "possible_values": []
                },
                "dim": {
                    "value": "2",
                    "possible_values": []
                }
            },
            "tanh_123": {
                "variable": {
                    "value": "combined_context_vectors",
                    "possible_values": []
                },
                "input": {
                    "value": "combined_context_vectors",
                    "possible_values": [
                        [
                            "torch.cat((embed_fasttext_start, rnn_embed_path, embed_fasttext_end), dim=2)",
                            "Call"
                        ],
                        [
                            "self.input_linear(combined_context_vectors.view(batch * max_e, -1)).view(batch, max_e, -1)",
                            "Call"
                        ],
                        [
                            "self.input_layer_norm(combined_context_vectors.view(-1, self.decode_size)).view(ccv_size)",
                            "Call"
                        ],
                        [
                            "torch.tanh(combined_context_vectors)",
                            "Call"
                        ],
                        [
                            "self.input_dropout(combined_context_vectors)",
                            "Call"
                        ]
                    ]
                }
            },
            "argmax_142": {
                "variable": {
                    "value": "predict",
                    "possible_values": []
                },
                "input": {
                    "value": "output",
                    "possible_values": [
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "self.output_linear(output).unsqueeze(1)",
                            "Call"
                        ],
                        [
                            "targets[:, 0].clone()",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ],
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "torch.argmax(output_, dim=1)",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ]
                    ]
                }
            },
            "sum_164": {
                "variable": {
                    "value": "context_length",
                    "possible_values": []
                },
                "input": {
                    "value": "context_mask > 0",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                },
                "keepdim": {
                    "value": "True",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            },
            "zeros_172": {
                "variable": {
                    "value": "all_output",
                    "possible_values": []
                },
                "*size": {
                    "value": "batch",
                    "possible_values": []
                },
                "out": {
                    "value": "1",
                    "possible_values": []
                },
                "dtype": {
                    "value": "self.target_vocab_size",
                    "possible_values": []
                }
            },
            "sum_209": {
                "variable": {
                    "value": "context_length",
                    "possible_values": []
                },
                "input": {
                    "value": "context_mask > 0",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                },
                "keepdim": {
                    "value": "True",
                    "possible_values": []
                },
                "dtype": {
                    "value": "torch.float",
                    "possible_values": []
                }
            },
            "zeros_218": {
                "variable": {
                    "value": "all_output",
                    "possible_values": []
                },
                "*size": {
                    "value": "batch",
                    "possible_values": []
                },
                "out": {
                    "value": "1",
                    "possible_values": []
                },
                "dtype": {
                    "value": "self.target_vocab_size",
                    "possible_values": []
                }
            },
            "bmm_182": {
                "variable": {
                    "value": "attn",
                    "possible_values": []
                },
                "input": {
                    "value": "encode_context",
                    "possible_values": []
                },
                "mat2": {
                    "value": "h_t.unsqueeze(-1)",
                    "possible_values": []
                }
            },
            "softmax_190": {
                "variable": {
                    "value": "attn_weight",
                    "possible_values": []
                },
                "input": {
                    "value": "attn",
                    "possible_values": [
                        [
                            "torch.bmm(encode_context, h_t.unsqueeze(-1))",
                            "Call"
                        ],
                        [
                            "attn.squeeze(-1)",
                            "Call"
                        ],
                        [
                            "attn + n_context_mask",
                            "BinOp"
                        ],
                        [
                            "torch.bmm(encode_context, h_t.unsqueeze(-1))",
                            "Call"
                        ],
                        [
                            "attn.squeeze(-1) + n_context_mask",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "bmm_192": {
                "variable": {
                    "value": "context",
                    "possible_values": []
                },
                "input": {
                    "value": "attn_weight.unsqueeze(1)",
                    "possible_values": []
                },
                "mat2": {
                    "value": "encode_context",
                    "possible_values": []
                }
            },
            "squeeze_192": {
                "variable": {
                    "value": "context",
                    "possible_values": []
                },
                "input": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_194": {
                "variable": {
                    "value": "h_tc",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[h_t, context]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "tanh_198": {
                "variable": {
                    "value": "output",
                    "possible_values": []
                },
                "input": {
                    "value": "output",
                    "possible_values": [
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "self.output_linear(output).unsqueeze(1)",
                            "Call"
                        ],
                        [
                            "targets[:, 0].clone()",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ],
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "torch.argmax(output_, dim=1)",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ]
                    ]
                }
            },
            "cat_201": {
                "variable": {
                    "value": "all_output",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[all_output, output]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "bmm_227": {
                "variable": {
                    "value": "attn",
                    "possible_values": []
                },
                "input": {
                    "value": "encode_context",
                    "possible_values": []
                },
                "mat2": {
                    "value": "h_t.unsqueeze(-1)",
                    "possible_values": []
                }
            },
            "softmax_234": {
                "variable": {
                    "value": "attn_weight",
                    "possible_values": []
                },
                "input": {
                    "value": "attn",
                    "possible_values": [
                        [
                            "torch.bmm(encode_context, h_t.unsqueeze(-1))",
                            "Call"
                        ],
                        [
                            "attn.squeeze(-1)",
                            "Call"
                        ],
                        [
                            "attn + n_context_mask",
                            "BinOp"
                        ],
                        [
                            "torch.bmm(encode_context, h_t.unsqueeze(-1))",
                            "Call"
                        ],
                        [
                            "attn.squeeze(-1) + n_context_mask",
                            "BinOp"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "bmm_236": {
                "variable": {
                    "value": "context",
                    "possible_values": []
                },
                "input": {
                    "value": "attn_weight.unsqueeze(1)",
                    "possible_values": []
                },
                "mat2": {
                    "value": "encode_context",
                    "possible_values": []
                }
            },
            "squeeze_236": {
                "variable": {
                    "value": "context",
                    "possible_values": []
                },
                "input": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_238": {
                "variable": {
                    "value": "h_tc",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[h_t, context]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "tanh_241": {
                "variable": {
                    "value": "output",
                    "possible_values": []
                },
                "input": {
                    "value": "output",
                    "possible_values": [
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "self.output_linear(output).unsqueeze(1)",
                            "Call"
                        ],
                        [
                            "targets[:, 0].clone()",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ],
                        [
                            "self.Whc(h_tc)",
                            "Call"
                        ],
                        [
                            "self.decoder_layer_norm(output)",
                            "Call"
                        ],
                        [
                            "torch.tanh(output)",
                            "Call"
                        ],
                        [
                            "torch.argmax(output_, dim=1)",
                            "Call"
                        ],
                        [
                            "self.target_element_embedding(output)",
                            "Call"
                        ]
                    ]
                }
            },
            "softmax_244": {
                "variable": {
                    "value": "output_",
                    "possible_values": []
                },
                "input": {
                    "value": "self.output_linear(output)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "argmax_246": {
                "variable": {
                    "value": "output",
                    "possible_values": []
                },
                "input": {
                    "value": "output_",
                    "possible_values": [
                        [
                            "F.softmax(self.output_linear(output), dim=1)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_249": {
                "variable": {
                    "value": "all_output",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[all_output, output_.unsqueeze(1)]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "sum_167": {
                "input": {
                    "value": "encode_context",
                    "possible_values": []
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "sum_213": {
                "input": {
                    "value": "encode_context",
                    "possible_values": []
                },
                "dtype": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "rand_36": {
                "*size": {
                    "value": "terminal_vocab_size",
                    "possible_values": []
                },
                "out": {
                    "value": "terminal_embed_size",
                    "possible_values": [
                        [
                            "128",
                            "MethodArgument"
                        ]
                    ]
                }
            },
            "rand_39": {
                "*size": {
                    "value": "path_element_vocab_size",
                    "possible_values": []
                },
                "out": {
                    "value": "path_embed_size",
                    "possible_values": [
                        [
                            "128",
                            "MethodArgument"
                        ]
                    ]
                }
            },
            "rand_42": {
                "*size": {
                    "value": "self.target_vocab_size",
                    "possible_values": []
                },
                "out": {
                    "value": "target_embed_size",
                    "possible_values": [
                        [
                            "128",
                            "MethodArgument"
                        ]
                    ]
                }
            },
            "sum_133": {
                "input": {
                    "value": "result_loss",
                    "possible_values": [
                        [
                            "self.loss(outputs[:, 1:].permute(0, 2, 1), targets[:, 1:]) * target_mask",
                            "BinOp"
                        ]
                    ]
                }
            }
        }
    }
}