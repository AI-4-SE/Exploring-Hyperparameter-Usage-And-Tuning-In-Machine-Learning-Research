{
    "data_loader.py": {
        "torch": {
            "CoCoDataset_87": {
                "base_class_0": {
                    "value": "torch.utils.data.Dataset",
                    "possible_values": []
                },
                "self.transform": {
                    "value": "transform",
                    "possible_values": []
                },
                "self.mode": {
                    "value": "mode",
                    "possible_values": [
                        [
                            "'train'",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.batch_size": {
                    "value": "batch_size",
                    "possible_values": [
                        [
                            "1",
                            "MethodArgument"
                        ]
                    ]
                },
                "self.img_folder": {
                    "value": "img_folder",
                    "possible_values": [
                        [
                            "os.path.join(cocoapi_loc, 'cocoapi/images/train2014/')",
                            "Call"
                        ],
                        [
                            "os.path.join(cocoapi_loc, 'cocoapi/images/test2014/')",
                            "Call"
                        ]
                    ]
                }
            },
            "SubsetRandomSampler_72": {
                "variable": {
                    "value": "initial_sampler",
                    "possible_values": []
                },
                "indices": {
                    "value": "indices",
                    "possible_values": [
                        [
                            "dataset.get_train_indices()",
                            "Call"
                        ],
                        [
                            "list(np.random.choice(all_indices, size=self.batch_size))",
                            "Call"
                        ]
                    ]
                }
            },
            "DataLoader_74": {
                "variable": {
                    "value": "data_loader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "dataset",
                    "possible_values": [
                        [
                            "CoCoDataset(transform=transform, mode=mode, batch_size=batch_size, vocab_threshold=vocab_threshold, vocab_file=vocab_file, start_word=start_word, end_word=end_word, unk_word=unk_word, annotations_file=annotations_file, vocab_from_file=vocab_from_file, img_folder=img_folder)",
                            "Call"
                        ]
                    ]
                },
                "num_workers": {
                    "value": "num_workers",
                    "possible_values": [
                        [
                            "0",
                            "MethodArgument"
                        ]
                    ]
                },
                "batch_sampler": {
                    "value": "data.sampler.BatchSampler(sampler=initial_sampler, batch_size=dataset.batch_size, drop_last=False)",
                    "possible_values": []
                }
            },
            "DataLoader_80": {
                "variable": {
                    "value": "data_loader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "dataset",
                    "possible_values": [
                        [
                            "CoCoDataset(transform=transform, mode=mode, batch_size=batch_size, vocab_threshold=vocab_threshold, vocab_file=vocab_file, start_word=start_word, end_word=end_word, unk_word=unk_word, annotations_file=annotations_file, vocab_from_file=vocab_from_file, img_folder=img_folder)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "dataset.batch_size",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "True",
                    "possible_values": []
                },
                "num_workers": {
                    "value": "num_workers",
                    "possible_values": [
                        [
                            "0",
                            "MethodArgument"
                        ]
                    ]
                }
            },
            "Tensor_125": {
                "variable": {
                    "value": "caption",
                    "possible_values": []
                }
            },
            "BatchSampler_76": {
                "sampler": {
                    "value": "initial_sampler",
                    "possible_values": [
                        [
                            "data.sampler.SubsetRandomSampler(indices=indices)",
                            "Call"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "dataset.batch_size",
                    "possible_values": []
                },
                "drop_last": {
                    "value": "False",
                    "possible_values": []
                }
            }
        }
    },
    "model.py": {
        "torch": {
            "EncoderCNN_5": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Sequential_13": {
                    "variable": {
                        "value": "self.resnet",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "*modules",
                        "possible_values": []
                    }
                },
                "Linear_14": {
                    "variable": {
                        "value": "self.embed",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "resnet.fc.in_features",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "embed_size",
                        "possible_values": []
                    }
                },
                "BatchNorm1d_15": {
                    "variable": {
                        "value": "self.batch_norm",
                        "possible_values": []
                    },
                    "num_features": {
                        "value": "embed_size",
                        "possible_values": []
                    },
                    "momentum": {
                        "value": "0.01",
                        "possible_values": []
                    }
                }
            },
            "DecoderRNN_24": {
                "base_class_0": {
                    "value": "torch.nn.Module",
                    "possible_values": []
                },
                "Embedding_28": {
                    "variable": {
                        "value": "self.embed",
                        "possible_values": []
                    },
                    "num_embeddings": {
                        "value": "vocab_size",
                        "possible_values": []
                    },
                    "embedding_dim": {
                        "value": "embed_size",
                        "possible_values": []
                    }
                },
                "LSTM_29": {
                    "variable": {
                        "value": "self.lstm",
                        "possible_values": []
                    },
                    "*args": {
                        "value": "embed_size",
                        "possible_values": []
                    },
                    "batch_first": {
                        "value": "True",
                        "possible_values": []
                    }
                },
                "Linear_30": {
                    "variable": {
                        "value": "self.linear",
                        "possible_values": []
                    },
                    "in_features": {
                        "value": "hidden_size",
                        "possible_values": []
                    },
                    "out_features": {
                        "value": "vocab_size",
                        "possible_values": []
                    }
                }
            },
            "cat_35": {
                "variable": {
                    "value": "embeddings",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(features.unsqueeze(1), embeddings)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            }
        }
    }
}