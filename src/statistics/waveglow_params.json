{
    "convert_model.py": {
        "torch": {
            "load_72": {
                "variable": {
                    "value": "model",
                    "possible_values": []
                },
                "f": {
                    "value": "old_model_path",
                    "possible_values": [
                        [
                            "sys.argv[1]",
                            "Subscript"
                        ]
                    ]
                },
                "map_location": {
                    "value": "cpu",
                    "possible_values": []
                }
            },
            "ModuleList_17": {
                "variable": {
                    "value": "wavenet.res_skip_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Conv1d_43": {
                "variable": {
                    "value": "cond_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_mel_channels",
                    "possible_values": [
                        [
                            "wavenet.cond_layers[0].weight.shape[1]",
                            "Subscript"
                        ]
                    ]
                },
                "out_channels": {
                    "value": "2 * n_channels * n_layers",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "Parameter_50": {
                "variable": {
                    "value": "cond_layer.weight",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.cat(cond_layer_weight)",
                    "possible_values": []
                }
            },
            "Parameter_51": {
                "variable": {
                    "value": "cond_layer.bias",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.cat(cond_layer_bias)",
                    "possible_values": []
                }
            },
            "weight_norm_52": {
                "variable": {
                    "value": "cond_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "cond_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_mel_channels, 2 * n_channels * n_layers, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(cond_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "save_74": {
                "obj": {
                    "value": "model",
                    "possible_values": [
                        [
                            "torch.load(old_model_path, map_location='cpu')",
                            "Call"
                        ]
                    ]
                },
                "f": {
                    "value": "new_model_path",
                    "possible_values": [
                        [
                            "sys.argv[2]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "Conv1d_23": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": [
                        [
                            "wavenet.n_channels",
                            "Attribute"
                        ],
                        [
                            "wavenet.n_channels",
                            "Attribute"
                        ]
                    ]
                },
                "out_channels": {
                    "value": "res_skip_channels",
                    "possible_values": [
                        [
                            "2 * n_channels",
                            "BinOp"
                        ],
                        [
                            "n_channels",
                            "Name"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "remove_weight_norm_24": {
                "variable": {
                    "value": "skip_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "wavenet.skip_layers[i]",
                    "possible_values": []
                }
            },
            "weight_norm_32": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "res_skip_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_channels, res_skip_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(res_skip_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "remove_weight_norm_47": {
                "variable": {
                    "value": "_cond_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "wavenet.cond_layers[i]",
                    "possible_values": []
                }
            },
            "remove_weight_norm_26": {
                "variable": {
                    "value": "res_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "wavenet.res_layers[i]",
                    "possible_values": []
                }
            },
            "Parameter_27": {
                "variable": {
                    "value": "res_skip_layer.weight",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.cat([res_layer.weight, skip_layer.weight])",
                    "possible_values": []
                }
            },
            "Parameter_28": {
                "variable": {
                    "value": "res_skip_layer.bias",
                    "possible_values": []
                },
                "data": {
                    "value": "torch.cat([res_layer.bias, skip_layer.bias])",
                    "possible_values": []
                }
            },
            "Parameter_30": {
                "variable": {
                    "value": "res_skip_layer.weight",
                    "possible_values": []
                },
                "data": {
                    "value": "skip_layer.weight",
                    "possible_values": []
                }
            },
            "Parameter_31": {
                "variable": {
                    "value": "res_skip_layer.bias",
                    "possible_values": []
                },
                "data": {
                    "value": "skip_layer.bias",
                    "possible_values": []
                }
            },
            "cat_50": {
                "tensors": {
                    "value": "cond_layer_weight",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                }
            },
            "cat_51": {
                "tensors": {
                    "value": "cond_layer_bias",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                }
            },
            "cat_27": {
                "tensors": {
                    "value": "[res_layer.weight, skip_layer.weight]",
                    "possible_values": []
                }
            },
            "cat_28": {
                "tensors": {
                    "value": "[res_layer.bias, skip_layer.bias]",
                    "possible_values": []
                }
            }
        }
    },
    "denoiser.py": {
        "torch": {
            "clamp_38": {
                "variable": {
                    "value": "audio_spec_denoised",
                    "possible_values": []
                },
                "input": {
                    "value": "audio_spec_denoised",
                    "possible_values": [
                        [
                            "audio_spec - self.bias_spec * strength",
                            "BinOp"
                        ],
                        [
                            "torch.clamp(audio_spec_denoised, 0.0)",
                            "Call"
                        ]
                    ]
                },
                "min": {
                    "value": "0.0",
                    "possible_values": []
                }
            },
            "zeros_17": {
                "variable": {
                    "value": "mel_input",
                    "possible_values": []
                },
                "*size": {
                    "value": "(1, 80, 88)",
                    "possible_values": []
                },
                "dtype": {
                    "value": "waveglow.upsample.weight.dtype",
                    "possible_values": []
                },
                "device": {
                    "value": "waveglow.upsample.weight.device",
                    "possible_values": []
                }
            },
            "randn_22": {
                "variable": {
                    "value": "mel_input",
                    "possible_values": []
                },
                "*size": {
                    "value": "(1, 80, 88)",
                    "possible_values": []
                },
                "dtype": {
                    "value": "waveglow.upsample.weight.dtype",
                    "possible_values": []
                },
                "device": {
                    "value": "waveglow.upsample.weight.device",
                    "possible_values": []
                }
            },
            "no_grad_29": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "distributed.py": {
        "torch": {
            "cat_68": {
                "variable": {
                    "value": "flat",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[t.contiguous().view(-1) for t in tensors]",
                    "possible_values": []
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "device_count_151": {
                "variable": {
                    "value": "num_gpus",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "is_available_44": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "set_device_48": {
                "device": {
                    "value": "rank % torch.cuda.device_count()",
                    "possible_values": []
                }
            },
            "broadcast_103": {
                "tensor": {
                    "value": "p",
                    "possible_values": [
                        [
                            "module.state_dict().values()",
                            "Call"
                        ],
                        [
                            "subprocess.Popen([str(sys.executable)] + args_list, stdout=stdout)",
                            "Call"
                        ],
                        [
                            "workers",
                            "Name"
                        ]
                    ]
                },
                "devices": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "device_count_48": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "is_tensor_101": {
                "obj": {
                    "value": "p",
                    "possible_values": [
                        [
                            "module.state_dict().values()",
                            "Call"
                        ],
                        [
                            "subprocess.Popen([str(sys.executable)] + args_list, stdout=stdout)",
                            "Call"
                        ],
                        [
                            "workers",
                            "Name"
                        ]
                    ]
                }
            }
        }
    },
    "glow.py": {
        "torch": {
            "tanh_37": {
                "variable": {
                    "value": "t_act",
                    "possible_values": []
                },
                "input": {
                    "value": "in_act[:, :n_channels_int, :]",
                    "possible_values": []
                }
            },
            "sigmoid_38": {
                "variable": {
                    "value": "s_act",
                    "possible_values": []
                },
                "input": {
                    "value": "in_act[:, n_channels_int:, :]",
                    "possible_values": []
                }
            },
            "ModuleList_307": {
                "variable": {
                    "value": "new_conv_list",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Conv1d_70": {
                "variable": {
                    "value": "self.conv",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "c",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "c",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                },
                "stride": {
                    "value": "1",
                    "possible_values": []
                },
                "padding": {
                    "value": "0",
                    "possible_values": []
                },
                "bias": {
                    "value": "False",
                    "possible_values": []
                }
            },
            "ModuleList_118": {
                "variable": {
                    "value": "self.in_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "ModuleList_119": {
                "variable": {
                    "value": "self.res_skip_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Conv1d_121": {
                "variable": {
                    "value": "start",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_in_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_122": {
                "variable": {
                    "value": "start",
                    "possible_values": []
                },
                "module": {
                    "value": "start",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_in_channels, n_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(start, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "Conv1d_127": {
                "variable": {
                    "value": "end",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_in_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "Conv1d_132": {
                "variable": {
                    "value": "cond_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_channels * n_layers",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_133": {
                "variable": {
                    "value": "self.cond_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "cond_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_mel_channels, 2 * n_channels * n_layers, 1)",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "zeros_like_156": {
                "variable": {
                    "value": "output",
                    "possible_values": []
                },
                "input": {
                    "value": "audio",
                    "possible_values": [
                        [
                            "self.start(audio)",
                            "Call"
                        ],
                        [
                            "audio + res_skip_acts[:, :self.n_channels, :]",
                            "BinOp"
                        ],
                        [
                            "audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)",
                            "Call"
                        ],
                        [
                            "audio[:, self.n_early_size:, :]",
                            "Subscript"
                        ],
                        [
                            "torch.cat([audio_0, audio_1], 1)",
                            "Call"
                        ],
                        [
                            "torch.cuda.HalfTensor(spect.size(0), self.n_remaining_channels, spect.size(2)).normal_()",
                            "Call"
                        ],
                        [
                            "torch.cuda.FloatTensor(spect.size(0), self.n_remaining_channels, spect.size(2)).normal_()",
                            "Call"
                        ],
                        [
                            "torch.autograd.Variable(sigma * audio)",
                            "Call"
                        ],
                        [
                            "torch.cat([audio_0, audio_1], 1)",
                            "Call"
                        ],
                        [
                            "self.convinv[k](audio, reverse=True)",
                            "Call"
                        ],
                        [
                            "audio.permute(0, 2, 1).contiguous().view(audio.size(0), -1).data",
                            "Attribute"
                        ],
                        [
                            "torch.cat((sigma * z, audio), 1)",
                            "Call"
                        ]
                    ]
                }
            },
            "ConvTranspose1d_183": {
                "variable": {
                    "value": "self.upsample",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1024",
                    "possible_values": []
                },
                "stride": {
                    "value": "256",
                    "possible_values": []
                }
            },
            "ModuleList_191": {
                "variable": {
                    "value": "self.WN",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "ModuleList_192": {
                "variable": {
                    "value": "self.convinv",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "remove_weight_norm_309": {
                "variable": {
                    "value": "old_conv",
                    "possible_values": []
                },
                "module": {
                    "value": "old_conv",
                    "possible_values": [
                        [
                            "conv_list",
                            "Name"
                        ],
                        [
                            "torch.nn.utils.remove_weight_norm(old_conv)",
                            "Call"
                        ]
                    ]
                }
            },
            "conv1d_96": {
                "variable": {
                    "value": "z",
                    "possible_values": []
                },
                "input": {
                    "value": "z",
                    "possible_values": [
                        [
                            "self.conv(z)",
                            "Call"
                        ],
                        [
                            "F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)",
                            "Call"
                        ],
                        [
                            "torch.cuda.HalfTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()",
                            "Call"
                        ],
                        [
                            "torch.cuda.FloatTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()",
                            "Call"
                        ]
                    ]
                },
                "weight": {
                    "value": "self.W_inverse",
                    "possible_values": []
                },
                "bias": {
                    "value": "None",
                    "possible_values": []
                },
                "stride": {
                    "value": "1",
                    "possible_values": []
                },
                "padding": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "Conv1d_138": {
                "variable": {
                    "value": "in_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "kernel_size",
                    "possible_values": []
                },
                "dilation": {
                    "value": "dilation",
                    "possible_values": [
                        [
                            "2 ** i",
                            "BinOp"
                        ]
                    ]
                },
                "padding": {
                    "value": "padding",
                    "possible_values": [
                        [
                            "int((kernel_size * dilation - dilation) / 2)",
                            "Call"
                        ]
                    ]
                }
            },
            "weight_norm_140": {
                "variable": {
                    "value": "in_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "in_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_channels, 2 * n_channels, kernel_size, dilation=dilation, padding=padding)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(in_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "Conv1d_149": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "res_skip_channels",
                    "possible_values": [
                        [
                            "2 * n_channels",
                            "BinOp"
                        ],
                        [
                            "n_channels",
                            "Name"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_150": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "res_skip_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_channels, res_skip_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(res_skip_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "cat_246": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[audio_0, audio_1]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_281": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[audio_0, audio_1]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "remove_weight_norm_299": {
                "variable": {
                    "value": "WN.start",
                    "possible_values": []
                },
                "module": {
                    "value": "WN.start",
                    "possible_values": []
                }
            },
            "remove_weight_norm_301": {
                "variable": {
                    "value": "WN.cond_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "WN.cond_layer",
                    "possible_values": []
                }
            },
            "sum_52": {
                "variable": {
                    "value": "log_s_total",
                    "possible_values": []
                },
                "input": {
                    "value": "log_s",
                    "possible_values": [
                        [
                            "output[:, n_half:, :]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "qr_74": {
                "input": {
                    "value": "torch.FloatTensor(c, c).normal_()",
                    "possible_values": []
                }
            },
            "det_77": {
                "input": {
                    "value": "W",
                    "possible_values": [
                        [
                            "torch.qr(torch.FloatTensor(c, c).normal_())[0]",
                            "Subscript"
                        ],
                        [
                            "W.view(c, c, 1)",
                            "Call"
                        ],
                        [
                            "self.conv.weight.squeeze()",
                            "Call"
                        ]
                    ]
                }
            },
            "cat_249": {
                "tensors": {
                    "value": "output_audio",
                    "possible_values": [
                        [
                            "[]",
                            "List"
                        ]
                    ]
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_290": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(sigma * z, audio)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "logdet_100": {
                "input": {
                    "value": "W",
                    "possible_values": [
                        [
                            "torch.qr(torch.FloatTensor(c, c).normal_())[0]",
                            "Subscript"
                        ],
                        [
                            "W.view(c, c, 1)",
                            "Call"
                        ],
                        [
                            "self.conv.weight.squeeze()",
                            "Call"
                        ]
                    ]
                }
            },
            "exp_280": {
                "input": {
                    "value": "s",
                    "possible_values": [
                        [
                            "output[:, n_half:, :]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "sum_55": {
                "input": {
                    "value": "log_s",
                    "possible_values": [
                        [
                            "output[:, n_half:, :]",
                            "Subscript"
                        ]
                    ]
                }
            },
            "sum_58": {
                "input": {
                    "value": "z * z",
                    "possible_values": []
                }
            },
            "exp_243": {
                "input": {
                    "value": "log_s",
                    "possible_values": [
                        [
                            "output[:, n_half:, :]",
                            "Subscript"
                        ]
                    ]
                }
            }
        }
    },
    "glow_old.py": {
        "torch": {
            "tanh_10": {
                "variable": {
                    "value": "t_act",
                    "possible_values": []
                },
                "input": {
                    "value": "in_act[:, :n_channels_int, :]",
                    "possible_values": []
                }
            },
            "sigmoid_11": {
                "variable": {
                    "value": "s_act",
                    "possible_values": []
                },
                "input": {
                    "value": "in_act[:, n_channels_int:, :]",
                    "possible_values": []
                }
            },
            "ModuleList_29": {
                "variable": {
                    "value": "self.in_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "ModuleList_30": {
                "variable": {
                    "value": "self.res_skip_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "ModuleList_31": {
                "variable": {
                    "value": "self.cond_layers",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Conv1d_33": {
                "variable": {
                    "value": "start",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_in_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_34": {
                "variable": {
                    "value": "start",
                    "possible_values": []
                },
                "module": {
                    "value": "start",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_in_channels, n_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(start, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "Conv1d_39": {
                "variable": {
                    "value": "end",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_in_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "ConvTranspose1d_94": {
                "variable": {
                    "value": "self.upsample",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1024",
                    "possible_values": []
                },
                "stride": {
                    "value": "256",
                    "possible_values": []
                }
            },
            "ModuleList_102": {
                "variable": {
                    "value": "self.WN",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "ModuleList_103": {
                "variable": {
                    "value": "self.convinv",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "Conv1d_47": {
                "variable": {
                    "value": "in_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "kernel_size",
                    "possible_values": []
                },
                "dilation": {
                    "value": "dilation",
                    "possible_values": [
                        [
                            "2 ** i",
                            "BinOp"
                        ]
                    ]
                },
                "padding": {
                    "value": "padding",
                    "possible_values": [
                        [
                            "int((kernel_size * dilation - dilation) / 2)",
                            "Call"
                        ]
                    ]
                }
            },
            "weight_norm_49": {
                "variable": {
                    "value": "in_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "in_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_channels, 2 * n_channels, kernel_size, dilation=dilation, padding=padding)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(in_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "Conv1d_52": {
                "variable": {
                    "value": "cond_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_mel_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "2 * n_channels",
                    "possible_values": []
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_53": {
                "variable": {
                    "value": "cond_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "cond_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_mel_channels, 2 * n_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(cond_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "Conv1d_61": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "in_channels": {
                    "value": "n_channels",
                    "possible_values": []
                },
                "out_channels": {
                    "value": "res_skip_channels",
                    "possible_values": [
                        [
                            "2 * n_channels",
                            "BinOp"
                        ],
                        [
                            "n_channels",
                            "Name"
                        ]
                    ]
                },
                "kernel_size": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "weight_norm_62": {
                "variable": {
                    "value": "res_skip_layer",
                    "possible_values": []
                },
                "module": {
                    "value": "res_skip_layer",
                    "possible_values": [
                        [
                            "torch.nn.Conv1d(n_channels, res_skip_channels, 1)",
                            "Call"
                        ],
                        [
                            "torch.nn.utils.weight_norm(res_skip_layer, name='weight')",
                            "Call"
                        ]
                    ]
                },
                "name": {
                    "value": "weight",
                    "possible_values": []
                }
            },
            "remove_weight_norm_229": {
                "variable": {
                    "value": "WN.start",
                    "possible_values": []
                },
                "module": {
                    "value": "WN.start",
                    "possible_values": []
                }
            },
            "cat_206": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[audio[:, :n_half, :], audio_1]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_208": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "[audio_1, audio[:, n_half:, :]]",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "cat_221": {
                "variable": {
                    "value": "audio",
                    "possible_values": []
                },
                "tensors": {
                    "value": "(sigma * z, audio)",
                    "possible_values": []
                },
                "dim": {
                    "value": "1",
                    "possible_values": []
                }
            },
            "exp_204": {
                "input": {
                    "value": "s",
                    "possible_values": [
                        [
                            "output[:, n_half:, :]",
                            "Subscript"
                        ]
                    ]
                }
            }
        }
    },
    "inference.py": {
        "torch": {
            "load_49": {
                "variable": {
                    "value": "mel",
                    "possible_values": []
                },
                "f": {
                    "value": "file_path",
                    "possible_values": []
                }
            },
            "unsqueeze_51": {
                "variable": {
                    "value": "mel",
                    "possible_values": []
                },
                "input": {
                    "value": "mel",
                    "possible_values": [
                        [
                            "torch.load(file_path)",
                            "Call"
                        ],
                        [
                            "torch.autograd.Variable(mel.cuda())",
                            "Call"
                        ],
                        [
                            "torch.unsqueeze(mel, 0)",
                            "Call"
                        ],
                        [
                            "mel.half() if is_fp16 else mel",
                            "IfExp"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "load_37": {
                "f": {
                    "value": "waveglow_path",
                    "possible_values": []
                }
            },
            "no_grad_53": {
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            }
        }
    },
    "mel2samp.py": {
        "torch": {
            "squeeze_83": {
                "variable": {
                    "value": "melspec",
                    "possible_values": []
                },
                "input": {
                    "value": "melspec",
                    "possible_values": [
                        [
                            "self.stft.mel_spectrogram(audio_norm)",
                            "Call"
                        ],
                        [
                            "torch.squeeze(melspec, 0)",
                            "Call"
                        ]
                    ]
                },
                "dim": {
                    "value": "0",
                    "possible_values": []
                }
            },
            "from_numpy_57": {
                "ndarray": {
                    "value": "data",
                    "possible_values": [
                        [
                            "f.read()",
                            "Call"
                        ]
                    ]
                }
            },
            "save_142": {
                "obj": {
                    "value": "melspectrogram",
                    "possible_values": [
                        [
                            "mel2samp.get_mel(audio)",
                            "Call"
                        ]
                    ]
                },
                "f": {
                    "value": "new_filepath",
                    "possible_values": [
                        [
                            "args.output_dir + '/' + filename + '.pt'",
                            "BinOp"
                        ]
                    ]
                }
            },
            "pad_100": {
                "input": {
                    "value": "audio",
                    "possible_values": [
                        [
                            "audio[audio_start:audio_start + self.segment_length]",
                            "Subscript"
                        ],
                        [
                            "torch.nn.functional.pad(audio, (0, self.segment_length - audio.size(0)), 'constant').data",
                            "Attribute"
                        ],
                        [
                            "audio / MAX_WAV_VALUE",
                            "BinOp"
                        ]
                    ]
                },
                "pad": {
                    "value": "(0, self.segment_length - audio.size(0))",
                    "possible_values": []
                },
                "mode": {
                    "value": "constant",
                    "possible_values": []
                }
            }
        }
    },
    "train.py": {
        "torch": {
            "load_43": {
                "variable": {
                    "value": "checkpoint_dict",
                    "possible_values": []
                },
                "f": {
                    "value": "checkpoint_path",
                    "possible_values": [
                        [
                            "'{}/waveglow_{}'.format(output_directory, iteration)",
                            "Call"
                        ]
                    ]
                },
                "map_location": {
                    "value": "cpu",
                    "possible_values": []
                }
            },
            "Adam_80": {
                "variable": {
                    "value": "optimizer",
                    "possible_values": []
                },
                "params": {
                    "value": "model.parameters()",
                    "possible_values": []
                },
                "lr": {
                    "value": "learning_rate",
                    "possible_values": []
                }
            },
            "DataLoader_97": {
                "variable": {
                    "value": "train_loader",
                    "possible_values": []
                },
                "dataset": {
                    "value": "trainset",
                    "possible_values": [
                        [
                            "Mel2Samp(**data_config)",
                            "Call"
                        ]
                    ]
                },
                "num_workers": {
                    "value": "1",
                    "possible_values": []
                },
                "shuffle": {
                    "value": "False",
                    "possible_values": []
                },
                "sampler": {
                    "value": "train_sampler",
                    "possible_values": [
                        [
                            "DistributedSampler(trainset) if num_gpus > 1 else None",
                            "IfExp"
                        ]
                    ]
                },
                "batch_size": {
                    "value": "batch_size",
                    "possible_values": []
                },
                "pin_memory": {
                    "value": "False",
                    "possible_values": []
                },
                "drop_last": {
                    "value": "True",
                    "possible_values": []
                }
            },
            "device_count_176": {
                "variable": {
                    "value": "num_gpus",
                    "possible_values": []
                },
                "params": {
                    "value": "default",
                    "possible_values": []
                }
            },
            "save_57": {
                "obj": {
                    "value": "{'model': model_for_saving, 'iteration': iteration, 'optimizer': optimizer.state_dict(), 'learning_rate': learning_rate}",
                    "possible_values": []
                },
                "f": {
                    "value": "filepath",
                    "possible_values": []
                }
            },
            "manual_seed_65": {
                "seed": {
                    "value": "seed",
                    "possible_values": []
                }
            },
            "manual_seed_66": {
                "seed": {
                    "value": "seed",
                    "possible_values": []
                }
            },
            "DistributedSampler_95": {
                "dataset": {
                    "value": "trainset",
                    "possible_values": [
                        [
                            "Mel2Samp(**data_config)",
                            "Call"
                        ]
                    ]
                }
            }
        }
    }
}