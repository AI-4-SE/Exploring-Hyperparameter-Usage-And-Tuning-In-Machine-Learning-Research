{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number repos:  982\n",
      "Number files:  982\n"
     ]
    }
   ],
   "source": [
    "with open(\"repos.json\", \"r\", encoding=\"utf-8\") as src:\n",
    "    repos = json.load(src)\n",
    "    print(\"Number repos: \", len(repos))\n",
    "\n",
    "repo_files = set()\n",
    "\n",
    "for x in glob.glob(\"../data/statistics/*\"):\n",
    "    file_name = x.split(\"\\\\\")[-1]\n",
    "    if file_name in repos:\n",
    "        repo_files.add(x)\n",
    "\n",
    "print(\"Number files: \", len(repo_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(df: pd.DataFrame) -> None:\n",
    "    print(df.to_latex(index=False))\n",
    "\n",
    "def get_module(name, data):\n",
    "    module = next(filter(lambda x: name == x[\"name\"], data))\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(library_name: str, library_dir: str, files: List) -> Dict:\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in files:\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name_parts = key.split(\"_\")\n",
    "                                if len(class_name_parts) > 2:\n",
    "                                    class_name = \"_\".join(class_name_parts[:-1])\n",
    "                                else:\n",
    "                                    class_name = class_name_parts[0]\n",
    "                                classes.append(class_name)\n",
    "\n",
    "    return Counter(classes).most_common()\n",
    "\n",
    "classes = get_classes(\"sklearn\", \"../modules/sklearn_estimators.json\", repo_files)\n",
    "top_classes = [x[0] for x in classes][:30]\n",
    "top_classes_count = [x[1] for x in classes][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(library_name: str, library_dir: str, files, classes) -> List:\n",
    "\n",
    "    param_data = []\n",
    "    \n",
    "    for class_name in classes:\n",
    "        params_set = []\n",
    "\n",
    "        for project in list(files):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for key, data in module_data.items():\n",
    "                                if key[0].isupper():\n",
    "                                    module_name_parts = key.split(\"_\")\n",
    "                                    if len(module_name_parts) > 2:\n",
    "                                        module_name = \"_\".join(module_name_parts[:-1])\n",
    "                                    else:\n",
    "                                        module_name = module_name_parts[0]\n",
    "\n",
    "                                    if class_name == module_name:\n",
    "\n",
    "                                        for name, _ in data.items():\n",
    "                                            if name in (\"variable\", \"params\"):\n",
    "                                                continue\n",
    "                                            else:\n",
    "                                                params_set.append(name)\n",
    "                                                        \n",
    "                                            \n",
    "        \n",
    "        param_counter = Counter(params_set).most_common(3)\n",
    "        #print(param_counter)\n",
    "        param_data.append(param_counter)\n",
    "        \n",
    "    return param_data\n",
    "\n",
    "\n",
    "params = get_params(\"sklearn\", \"../modules/sklearn_estimators.json\", repo_files, top_classes)\n",
    "\n",
    "top_params = []  \n",
    "top_params_count = []  \n",
    "for item in params:\n",
    "    _params = [x[0] for x in item]\n",
    "    _count = [x[1] for x in item]\n",
    "    top_params.append(_params)\n",
    "    top_params_count.append(_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrll}\n",
      "\\toprule\n",
      "               Algorithm &  Algorithm Count &                             Top Hyperparameters & Top Hyperparameter Count \\\\\n",
      "\\midrule\n",
      "                    TSNE &               54 &                [n\\_components, init, perplexity] &             [49, 25, 25] \\\\\n",
      "                  KMeans &               49 &                [n\\_clusters, init, random\\_state] &             [48, 21, 17] \\\\\n",
      "          StandardScaler &               47 &                 [with\\_mean, with\\_std, **kwargs] &               [10, 3, 1] \\\\\n",
      "                     PCA &               45 &              [n\\_components, whiten, svd\\_solver] &               [37, 3, 2] \\\\\n",
      "                   KFold &               35 &               [n\\_splits, shuffle, random\\_state] &             [35, 35, 27] \\\\\n",
      "      LogisticRegression &               30 &                       [C, random\\_state, solver] &              [16, 12, 7] \\\\\n",
      "            LabelEncoder &               29 &                                              [] &                       [] \\\\\n",
      "            GridSearchCV &               22 &                     [estimator, param\\_grid, cv] &             [22, 22, 22] \\\\\n",
      "                   Bunch &               21 &                                      [X, y, sd] &              [18, 17, 4] \\\\\n",
      "  RandomForestClassifier &               20 &               [n\\_estimators, max\\_depth, n\\_jobs] &               [17, 8, 8] \\\\\n",
      "         StratifiedKFold &               18 &               [n\\_splits, shuffle, random\\_state] &             [18, 15, 13] \\\\\n",
      "         TfidfVectorizer &               18 &          [strip\\_accents, stop\\_words, lowercase] &                [6, 5, 4] \\\\\n",
      "         CountVectorizer &               17 &               [token\\_pattern, min\\_df, analyzer] &                [5, 5, 4] \\\\\n",
      "        LinearRegression &               17 &                         [fit\\_intercept, n\\_jobs] &                   [5, 1] \\\\\n",
      "        NearestNeighbors &               15 &                [n\\_neighbors, algorithm, metric] &              [15, 12, 4] \\\\\n",
      "               LinearSVC &               13 &                      [verbose, C, random\\_state] &                [2, 2, 1] \\\\\n",
      "                     RBF &               12 &                                  [length\\_scale] &                     [12] \\\\\n",
      "                Pipeline &               11 &                                         [steps] &                     [11] \\\\\n",
      "            MinMaxScaler &               10 &                                 [feature\\_range] &                      [6] \\\\\n",
      "         GaussianMixture &               10 &      [n\\_components, covariance\\_type, reg\\_covar] &               [10, 8, 2] \\\\\n",
      "         RANSACRegressor &                9 & [random\\_state, min\\_samples, residual\\_threshold] &                [8, 6, 6] \\\\\n",
      "              DotProduct &                9 &                                       [sigma\\_0] &                      [9] \\\\\n",
      "             WhiteKernel &                9 &               [noise\\_level, noise\\_level\\_bounds] &                   [9, 9] \\\\\n",
      "GaussianProcessRegressor &                9 &     [kernel, n\\_restarts\\_optimizer, normalize\\_y] &                [9, 9, 5] \\\\\n",
      "     MultiLabelBinarizer &                8 &                        [sparse\\_output, classes] &                   [5, 1] \\\\\n",
      " AgglomerativeClustering &                7 &                 [n\\_clusters, affinity, linkage] &                [5, 5, 5] \\\\\n",
      "                   Ridge &                7 &            [alpha, fit\\_intercept, random\\_state] &                [6, 5, 5] \\\\\n",
      "            TruncatedSVD &                6 &                    [n\\_components, random\\_state] &                   [6, 4] \\\\\n",
      "          LabelBinarizer &                6 &                                     [neg\\_label] &                      [1] \\\\\n",
      "                     SVC &                6 &                        [C, kernel, probability] &                [4, 2, 2] \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssimon\\AppData\\Local\\Temp\\ipykernel_34720\\2510577943.py:7: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(df.to_latex(index=False))\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"Algorithm\"] = top_classes\n",
    "df[\"Algorithm Count\"] = top_classes_count\n",
    "df[\"Top Hyperparameters\"] = top_params\n",
    "df[\"Top Hyperparameter Count\"] = top_params_count\n",
    "\n",
    "print(df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
