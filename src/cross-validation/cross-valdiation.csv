,repo_url,tar_filename,title,decorators,imports,arxiv_categories,file location 1,file location 2,methods,paper_sections,are parameters tuned?,which technique is used?,are final parameters reported?,notes,"hyperparameter explicitly fine-tuned and mentioned? (manual tuning, experimtental tuning, grid search, etc...)",Unnamed: 15,Unnamed: 16,Unnamed: 17,Unnamed: 18
548,https://github.com/IMLHF/DRUnet-SE,IMLHF_DRUnet-SE.tar.gz,END-TO-END SPEECH ENHANCEMENT BASED ON DISCRETE COSINE TRANSFORM,,bn util math scipy tensorflow soundfile librosa model sys data_prepare pickle tqdm argparse os helper numpy time config,cs.SD eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IMLHF_DRUnet-SE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IMLHF_DRUnet-SE.pdf,,,yes,Adam,yes,"learning rate, betas, epsilon, batch size",no,,,,
764,https://github.com/krpratik/RE-MIMO,krpratik_RE-MIMO.tar.gz,RE-MIMO: Recurrent and Permutation Equivariant Neural MIMO Detection,,copy oampnet warnings sys __future__ iterative_classifier MultiheadAttention TransformerEncoder EncoderDecoderBlock collections torch os cvxpy matplotlib oampnet_base scipy pickle TransformerEncoderLayer numpy NumTransmitterEncoding time math detnet classical_solvers sample_generator detnet_base TransformerDecoderLayer,eess.SP stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/krpratik_RE-MIMO.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\krpratik_RE-MIMO.pdf,,"|V. EXPERIMENTSIn this section, we present numerical results to evaluate and compare the performance of RE-MIMO with other well established MIMO detection schemes. We use the symbol error rate (SER) as the performance evaluation metric in our experiments. We perform a separate experiment to understand the inner working dynamics of RE-MIMO. The detection schemes used in the experiment for comparison purposes are listed below. Then, we discuss the generation of data on which these schemes are trained and/or tested. Below we discuss the implementation details of the detection schemes used in our experiments.• MMSE: Classical linear receiver that inverts the signal by applying the channel-noise regularized pseudoinverse of the channel matrix. • AMP: AMP algorithm as given in [8] and implemented with 50 iterations. It was verified in [19] that increasing the number of iterations does not lead to an increase in performance.• SDR: Semidefinite programming with rank 1 relaxation and implemented using an efficient interior point method [45]. • V-BLAST: Multi-stage successive interference cancellation BLAST algorithm and using ZF detector at the detection stage as presented in [46]. • ML: The optimal ML solver for (2) implemented using Gurobi [47] optimization package. • DetNet: Deep learning architecture with 3N tr layers as introduced in [16]. We experiment with complex MIMO systems, hence, we double the dimensionality of latent variables (z, v) to accommodate the increased complexity. A separate DetNet is trained for each combination of {N r , N tr , QAM-order}. • OAMPNet: The OAMP based iterations unrolled in 10 layers as proposed in [17]. Each layer requires computing a matrix pseudoinverse and has 2 learnable parameters. Like DetNet, a separate OAMPNet is trained for each combination of {N r , N tr , QAM-order}. • OAMPNet-2: The OAMP based iterations unrolled in 10 layers as proposed in [18]. Each layer requires computing a matrix pseudoinverse and has 4 learnable parameters. Similar to OAMPNet, a separate OAMPNet-2 is trained for each combination of {N r , N tr , QAM-order}. • RE-MIMO: Recurrent permutation equivariant neural detector as explained in sec. IV. Unlike DetNet and OAMPNet-2, we train a single detector for a given combination of {N r , QAM-order}, i.e., a single RE-MIMO handles all the values of N tr in a pre-defined range.The implementation code for AMP, SDR, V-BLAST, and ML is taken from the GitHub repository of [19]. Our repository containing PyTorch implementation of learning-based schemes and the experiments presented in the paper is available at https: //github.com/krpratik/RE-MIMO.In our experiments, we consider massive MIMO systems where the number of transmitters is smaller than the number of receivers at the BS. Unless mentioned otherwise, our experiments consider MIMO configurations with N r = 64 and system size ratios ( Ntr Nr ) belonging to the range, Ntr Nr ∈ 1 4 , 1 2 . For system size ratios below the aforementioned range, the performance of traditional detection schemes (such as V-BLAST) is at par with the optimal detection schemes and can effectively be deployed for symbol detection purposes.The performance of detection algorithms highly depends on the type of MIMO channel. Therefore, we evaluate the performance of RE-MIMO under both i.i.d. Gaussian and correlated Rayleigh fading channels with perfect CSI. The channel noise n follows a zero-mean i.i.d. Gaussian distribu-  tion whose variance is related to the SNR as per the formulaSNR = E ||||Hx|||| 2 2 E [||||n|||| 2 2 ](25)To generalize the scope of our findings, we experiment with two different orders of QAM modulation scheme, QAM-16 and QAM-64. 1) Interpolation & Extrapolation properties:To analyze the interpolation and extrapolation capabilities of RE-MIMO, we train it on every alternate value (even values) of N tr in the range [16,32], i.e., N tr ∈ {n tr ∈ [16,32] : (∃k ∈ Z)[n tr = 2k]}. We then test RE-MIMO on the values of N tr that lie in the aforementioned range but were left out during training (odd values), to gauge the interpolation capability of RE-MIMO. For analyzing the extrapolation capability, we test RE-MIMO on values of N tr beyond the training range, i.e., N tr ∈ {36, 40}.Fig. 12 depicts the SER performance of RE-MIMO for MIMO systems with N tr ∈ {23, 36, 40} on QAM-16 modulation and i.i.d. Gaussian channel matrix. We compare the performance of the above version of our detector (RE-MIMO-I) with OAMPNet-2 (trained separately for each N tr value), a full version of our detector (RE-MIMO-II) which is trained on all of the values of N tr in the range [16,44] without skipping, and an exclusive version of our detector (RE-MIMO-III) where we train a separate detector for each value of N tr ∈ {23, 36, 40}.It is evident from Fig. 12 that RE-MIMO-I elegantly interpolates to intermediate (odd) values of N tr without any performance degradation. This suggests that the proposed RE-MIMO is scalable to MIMO systems with higher configurations in the sense that in case of higher values of N r , training the RE-MIMO on every alternate (or possibly more sparse) values of  N tr in the required range suffices for the entire range of users.Another very important and interesting observation we make is that training RE-MIMO to attend to a variable number of users does not lead to any performance degradation as compared to exclusively training of RE-MIMO for a fixed number of users.On the other hand, the performance of RE-MIMO degrades significantly when extrapolated to N tr values far beyond the training range, which we expect to be a rare occurrence for a properly provisioned system.2) Attention Interpretation: It is a known fact that mutual interference between co-transmitters depends on their channel alignment. This suggests that the attention (α ij ) RE-MIMO pays to user j while decoding the symbol for user i, should be related to the alignment between h i and h j . We conduct a controlled experiment to verify that our notion of attention and interference align with each other. We train a simplified version of RE-MIMO with eight EP blocks, where each EP block has only one attention head, for a MIMO system with N r = 32 and a fixed value of N tr = 12. For each training sample, we begin with 12 orthogonal channel vectors. Then we proceed to form three sets of channels (users) containing 4 channels each, i.e., A = {hi } 4 i=1 , B = {h i } 8 i=5 , C = {h i } 12 i=9 .Subsequently, we introduce correlation among each channel set separately by post-multiplying it with a mixing matrix such that the resulting channels belonging to one set are a linear combination of the original orthogonal channels of that set. Henceforth, channels belonging to the same set no longer remain orthogonal to each other but will remain orthogonal with every other channel outside its parent set. This is a controlled experiment in the sense that we control which user will interfere with whom. We train RE-MIMO on such a controlled dataset for QAM-16 modulation. Fig. 13 illustrates the attention heat map obtained from the self-attention layer of the third EP block. Our RE-MIMO successfully figures out the set of users it should pay attention to while decoding a particular user symbol. E. TrainingIn this subsection, we discuss the methodology used for training the learning-based algorithms for our experiments.The learning-based schemes used in our experiments, i.e., RE-MIMO, DetNet, OAMPNet, and OAMPNet-2, were implemented in PyTorch [49]. We frame the MIMO problem to its equivalent real-valued representation for Py- For a given combination of {N r , QAM-order}, we train a single RE-MIMO for all the number of transmitters in the range N tr ∈ [N r /4, N r /2]. We use Adam optimizer [50] with an initial learning rate of 10 −4 for training RE-MIMO. We deploy a learning rate scheduler (PyTorch ReduceLROnPlateau) to dynamically adjust the learning rate based on validation set measurements. The loss function in (11) is used to train the network with an equal weighing of loss at each iteration step, i.e., w t 1/T . We train RE-MIMO for 120 epochs with a mini-batch size of 256 samples, and after each epoch (5K iterations), we compute the loss on the validation set and if the loss hits a plateau, i.e., the performance metric stopped improving, the learning rate is reduced by a factor k = 0.91. Each training iteration of RE-MIMO involves sampling a value of N tr from a triangular probability distribution, N tr ∼ T (a, b, c), with lower limit a = N r /4, upper limit b = N r /2, and the mode c coinciding with the upper limit, i.e., c = b. Interference is more when N tr is high and hence, the detector needs to be trained more often on higher values of N tr . A target SNR range is determined for the sampled value of N tr and subsequently, all the samples in the training mini-batch have the same N tr with their SNR values uniformly distributed on U (SNR min , SNR max ). The validation set contains samples pertaining to two different system sizes, {N tr = 16, N r = 64} and {N tr = 32, N r = 64}. Each system size has a different target SNR range. The validation set contains 5k samples for each SNR value in the target SNR range of the respective system size. The final validation loss is the mean of losses over the entire validation set samples of both the system sizes.After training, the final evaluation of RE-MIMO is done on the test set consisting of 10 7 samples for each SNR value.|",yes,Adam,yes,"learning rate, learning rate scheduler, epochs, mini batch size, iterations",no,,,,
1094,https://github.com/safreita1/malnet-image,safreita1_malnet-image.tar.gz,MalNet: A Large-Scale Cybersecurity Image Database of Malicious Software,,main PIL copy warnings multiprocessing tqdm pprint tensorflow collections os matplotlib classification_models sklearn dill joblib numpy config binascii process losses androguard,cs.CR cs.AI cs.CV cs.LG cs.LG cs.AI cs.CR cs.CV cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/safreita1_malnet-image.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\safreita1_malnet-image.pdf,,,yes,Adam,no,epochs,no,,,,
299,https://github.com/genforce/sefa,genforce_sefa.tar.gz,Closed-Form Factorization of Latent Semantics in GANs,"@st.cache(allow_output_mutation=True, show_spinner=False) @staticmethod",utils SessionState cv2 tqdm torch os argparse subprocess base64 numpy models streamlit,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/genforce_sefa.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\genforce_sefa.pdf,,"Comparison with Supervised ApproachWe compare our closed-form algorithm with the stateof-the-art supervised method, InterFaceGAN [24]. We conduct experiments on face synthesis models due to the well definition of facial attributes. In particular, we make comparison between SeFa and InterFaceGAN on both the conventional generator (i.e., PGGAN [16]) and the stylebased generator (i.e., StyleGAN [17]). Qualitative Results. Fig. 5 visualizes some manipulation results by using the identified semantics. We can tell that SeFa achieves similar performance as InterFaceGAN from the perspective of editing pose, gender, eyeglasses, and expression (smile), suggesting its effectiveness. More importantly, InterFaceGAN requires sampling numerous data and pre-training attribute predictors. By contrast, SeFa is completely independent of data sampling and model training, which is more efficient and generalizable.Re-scoring Analysis. For quantitative analysis, we train an attribute predictor on CelebA dataset [19] with ResNet-50 structure [11], following [24]. With this predictor, we are able to perform re-scoring analysis to quantitatively evaluate whether the identified directions can properly represent the corresponding attributes. In particular, we randomly sample 2K images and manipulate them along a certain discovered direction. We then use the prepared predictor to check how the semantic score varies in such manipulation process. Tab. 2 shows the results where we have three observations. (i) SeFa can adequately control some attribute, such as pose and gender, similar to Inter-FaceGAN. (ii) When altering one semantic, InterFaceGAN shows stronger robustness to other attributes, benefiting from its supervised training manner. For example, the age and eyeglasses corresponding to the same latent direction identified by SeFa. That is because the training data is somewhat biased (i.e., older people are more likely to wear eyeglasses), as pointed out by [24]. By contrast, involving labels as the supervision can help learn a more accurate direction to some extent. (iii) SeFa fails to discover the direction corresponding to eyeglasses. The reason is that the presence of eyeglasses is not a large variation and hence does not meet the optimization objective in Eq. ( 4).Diversity Comparison. Supervised approach highly depends on the available attribute predictors. By contrast, our method is more general and can find more diverse semantics in the latent space. As shown in Fig. 6 (a), we successfully identify the directions corresponding to hair color, hair style, and brightness. This surpasses InterFaceGAN since predictors for these attributes are not easy to acquire in practice. Also, supervised methods are usually limited by the training objective. For example, InterFaceGAN is proposed to handle binary attributes [24]. In comparison, our method can identify more complex attributes, like the different hair styles shown in Fig. 6 (b).",no,,,,no,,,,
1396,https://github.com/coocgan/cooc_texture,coocgan_cooc_texture.tar.gz,Co-occurrence Based Texture Synthesis,,utils PIL sys cooc_utils tqdm torchvision datetime network torch os matplotlib scipy functools itertools sklearn argparse numpy time config math random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/coocgan_cooc_texture.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\coocgan_cooc_texture.pdf,,"INTRODUCTIONDeep learning has revolutionized our ability to generate novel images. Most notably, generative adversarial networks (GANs) have shown impressive results in various domains, including textures. Nowadays, GANs can generate a distribution of texture images that is often indistinguishable from the distribution of real ones. The goal of the generator is conceptually simple and boils down to training the network weights to map a latent code, sampled from a random distribution, to realistic samples.However, generative networks are typically non-explainable and hard to control. That is, given a generated sample, it is generally hard to explain its latent representation and to directly modify it to output a new sample with desired properties.Recently, we are witnessing growing interests in interpreting the latent space of GANs, aspiring to better understand its behaviour [Bau et al. 2019;Radford et al. 2015;Shen et al. 2019]. These studies tap into vector arithmetics in latent space, or use more complicated entangled properties to achieve better control of the texture generation process. However, many questions remain open, and local control is still difficult to achieve. In this work, we seek a generative model for textures that is intuitive to understand and easy to edit and manipulate. Our key insight is that we can bypass the need to understand a highly entangled latent space by using a structured latent space, with meaningful interpretable vectors.We use a statistical tool, co-occurrences, to serve as an encoding of texture patches. Co-occurrences, first introduced by Julesz [Julesz 1962], capture the local joint probability of pairs of pixel values to co-occur together. They have long been used to analyze textures [Haralick et al. 1973;Isola et al. 2014]. In our work, we take the opposite direction. Given a co-occurrence matrix, we can generate a variety of texture images that match it.Technically, we train a fully convolutional conditional GAN (cGAN), conditioned on local co-occurrence statistics. The convolutional architecture allows for arbitrarily large generated textures, while local co-occurrence statistics gives us explainable control over local texture appearance. This allows synthesizing a variety of inhomogeneous textures, such as those shown in Figure 1, by conditioning locally on different co-occurrences collected from the input textures.To enable end-to-end training using co-occurrence statistics, we introduce a new differentiable co-occurrence loss, which penalizes inconsistencies between the co-occurrence of the synthesized texture and the input condition. We demonstrate that our proposed training objective allows for a simple and interpretable latent representation, without compromising fidelity on a large variety of inhomogeneous textures. We show that our approach can be used, for example, to interpolate between two texture regions by interpolating between their corresponding co-occurrence matrices, resulting in a dynamic texture morph. We also illustrate how to control the generated texture by editing the co-occurrence input.Our contributions are twofold: First, we introduce a stable, intuitive and interpretable latent representation for texture synthesis, offering parametric control over the synthesized output using co-occurrence statistics. Second, we present a fully convolutional cGAN architecture with a differentiable co-occurrence loss, enabling end-to-end training. Our code is publicly available at https://github.com/coocgan/cooc_texture/.",yes,Adam,not all,"momentum, learning rate, epochs, window size, stride, padding, activation",no,,,,
1030,https://github.com/tum-pbs/racecar,tum-pbs_racecar.tar.gz,Data-driven Regularization via Racecar Training for Generalizing Neural Networks,,"imageio tensorflow os, shutil argparse os numpy time",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tum-pbs_racecar.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tum-pbs_racecar.pdf,,"B.2 Disentangled RepresentationsThe InfoGAN approach [5] demonstrated the possibility to control the output of generative models via maximizing mutual information between outputs and structured latent variables. However, mutual information is very hard to estimate in practice [51]. The previous section and Fig. 4(b) of the main paper demonstrated that models from racecar training (both RR 1 A and RR A ) can increase the mutual information between network inputs and outputs. Intuitively, racecar training explicitly constrains the model to reocver an input given an output, which directly translates into an increase of mutual information between input and output distributions compared to regular training runs. For highlighting how racecar training can yield disentangled representations (as discussed in the later paragraphs of Sec. 4 of the main text), we follow the experimental setup of InfoGAN [5]: the input dimension of our network is 74, containing 1 ten-dimensional category code c 1 , 2 continuous latent codes c 2 , c 3 ∼ U(−1, 1) and 62 noise variables. Here, U denotes a uniform distribution.Training Details: As InfoGAN focuses on structuring latent variables and thus only increases the mutual information between latent variables and the output, we also focus the racecar training on the corresponding latent variables. I.e., the goal is to maximize their mutual information with the output of the generative model. Hence, we train a model RR 1 for which only latent dimensions c 1 , c 2 , c 3 of the input layer are involved in the racecar loss. We still employ a full reverse pass structure in the neural network architecture. c 1 is a ten-dimensional category code, which is used for controlling the output digit category, while c 2 and c 3 are continuous latent codes, to represent (previously unknown) key properties of the digits, such as orientation or thickness. Building relationship between c 1 and outputs is more difficult than for c 2 or c 3 , since the 10 different digit outputs need to be encoded in a sinlge continuous variable c 1 . Thus, for the corresponding racecar loss term for c 1 we use a slightly larger λ factor (by 33%) than for c 2 and c 3 . The forward and reverse pass architectures of the network are given in Tab. 11, and hyperparameters are listed in Tab. 12. Details of our results are shown in Fig. 18. Models are trained using a GAN loss [17] as the loss function for the outputs.Analysis of Results : In Fig. 18 we show additional results for the disentangling test case. It is visible that the racecar training with the RR 1 model yield distinct and meaningful latent space",yes,,yes,"batch size, learning rate, epochs, steps",no,,,,
122,https://github.com/xeTaiz/dvao,xeTaiz_dvao.tar.gz,Deep Volumetric Ambient Occlusion,@pl.data_loader @contextmanager,"utils contextlib torchinterp1d peak_finder tqdm ssim3d_torch ctypes pathlib sys, os uuid pydicom collections pytorch_lightning enum torch os functools itertools xml tf_utils torch, torch pytorch_msssim argparse numpy time ranger train math volume_loader math, os, sys, random os, ctypes, math dicom_numpy time, os",cs.CV cs.GR cs.LG eess.IV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xeTaiz_dvao.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xeTaiz_dvao.pdf,Softplus  Tanh Activation  Mish,"METHODIn this section we introduce DVAO. In contrast to prior approaches that compute ambient occlusion numerically, we frame the problem of volumetric ambient occlusion as a supervised learning problem. Thus, we train a 3D convolutional neural network to predict volumetric ambient occlusion. Our predicted AO volume is at a resolution of 128 3 and our figures are rendered by ray casting the full resolution volume, while sampling the AO at the given 128 3 .Since volumetric ambient occlusion depends highly on the opacity of the volume, our neural net needs to consider opacity, which is usually modulated by means of the transfer function. Unfortunately, conventional CNNs can only operate on structured data, such as images or volumes, and are thus incompatible with the typically unstructured representations of transfer functions. In order to make such unstructured information compatible with our CNN, we investigate a variety of possible representations and injection strategies for providing the information represented by the transfer function to the CNN.In the following we first discuss the data that is necessary to train and validate our neural network, as well as challenges arising from their TrainingWe train our network in a supervised fashion using stochastic gradient descent. As optimizer we use rectified Adam [27] with Lookahead [50] with the default parameters and a learning rate of 0.001. As batch size we use 1, since we cannot fit larger batches in memory with the proposed architecture. Simulating larger batch sizes using gradient accumulation did not improve training performance in our tests, however we did not analyse this in full detail. Our network was implemented in PyTorch [35] and is trained using mixed precision. The training takes around 20 hours on a single RTX 2080 Ti GPU, requiring 10.5 GB of GPU memory.Loss The loss function defines the objective of the neural network and is critical to produce accurate volumetric ambient occlusion. We tested several loss functions and propose to use a combination of mean squared error (MSE) and structural dissimilarity index (DSSIM). The final loss L (p,t) of a prediction p and target t is defined in Equation 3for volumes of shapeW × H × D with N = W • H • D: L (p,t) = 1 − (2µ p µ t + c 1 )(2σ pt + c 2 ) (µ 2 p + µ 2 t + c 1 )(σ 2 p + σ 2 t + c 2 ) + α N N ∑ i (p i − t i ) 2 (3) = DSSIM(p,t) + α • MSE(p,t)Hereby c 1 = 0.01, c 2 = 0.03 are small constants for numerical stability and µ x , σ 2x and σ xy are the means, variances and covariances within a local neighborhood for all voxels of x (and y) respectively. α is a hyperparameter to balance the two losses and we empirically chose α = 5 based on our experiments, however we found that the training is not very sensitive to this parameter.Note that traditional image-based SSIM [48] uses a 2D neighborhood to compute local means, variances and covariances. We use this imagebased method on all slices along the z dimension and use the average over all slices to form the DSSIM-2D loss. Analogously, we define the DSSIM-3D loss, which uses 3D neighborhoods instead, in order to better assess structural similarity along the z dimension.© 2020 IEEE. This is the author's version of the article that has been published in IEEE Transactions on Visualization and Computer Graphics. The final version of this record is available at: 10.1109/TVCG.2020.3030344 Table 1: Performance comparison of the injection strategies (columns) for different loss functions (rows). We report SSIM, MSE and inference time for each method. The Preclassified strategy in general performs the best while being the slowest. While DSSIM-2D results in the best test SSIM and DSSIM-3D + MSE in the best MSE for this strategy, we determine the DSSIM-2D + MSE model to perform the best overall since it nearly matches our best results in both SSIM and MSE. The inference times were measured on an RTX 2070 and mostly show that strategies that can omit execution of the encoder during inference get a slight performance advantage over the Preclassified strategy.  2 , we can drastically reduce the memory consumption and running time of our network. In fact, using this techniques enables us to use a comparably large network and volume resolution [9,22].",yes,SGD,,"SGD default parameters, learning rate, batch size, ",no,,,,
916,https://github.com/MubarizZaffar/VPR-Bench,MubarizZaffar_VPR-Bench.tar.gz,NetVLAD: CNN architecture for weakly supervised place recognition,@jit(nopython=True),perform_vpr_invariance_analysis csv sys glob vpr_system layers skimage execute_evaluation_mode shapely Hog_feature distutils tensorflow create_datasetspecific_VPRperformance_summary Cython datetime os caffe matplotlib scipy itertools numba pickle sklearn argparse numpy evaluate_vpr_techniques time performance_comparison _collections math netvlad_tf cv2 VPR_Techniques,cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MubarizZaffar_VPR-Bench.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MubarizZaffar_VPR-Bench.pdf,,,yes,SGD,yes,"model architecture, margin, learning rate, decay, epochs, momentum, weight decay, batch size, ",no,,,,
985,https://github.com/shuuchen/video_autoencoder,shuuchen_video_autoencoder.tar.gz,Unsupervised Learning of Video Representations using LSTMs,,torch torchvision,cs.LG cs.CV cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/shuuchen_video_autoencoder.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\shuuchen_video_autoencoder.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory,,no,,,,no,,,,
427,https://github.com/jfainberg/sincnet_adapt,jfainberg_sincnet_adapt.tar.gz,ACOUSTIC MODEL ADAPTATION FROM RAW WAVEFORMS WITH SINCNET,@interfaces.legacy_get_updates_support @staticmethod,utils copy sys signal layers __future__ keras tensorflow model collections json h5py os kaldi_io optimizers six learning_to_adapt kaldi itertools data_generator numpy math random,eess.AS cs.CL cs.SD,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jfainberg_sincnet_adapt.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jfainberg_sincnet_adapt.pdf,,"INTRODUCTIONAutomatic speech recognition models have in recent years obtained impressive word error rates (WERs) [1]. A key component to improving performance is to reduce mismatch between the acoustic model and test data, by explicit adaptation or normalisation of acoustic factors (e.g. [2,3,4,5,6]). Methods such as Vocal Tract Length Normalisation (VTLN) [7], which aims to mitigate large variations in individual speakers acoustics, scales the filterbank in standard feature extraction. There has, however, been a growing interest in reducing the amount of hand-crafted feature extraction that is required for acoustic modelling of speech [8,9,10,11]. The motivations to learn part, or all, of the feature extractor range from aiding interpretability [8,10], to obtaining more optimal representations for the task at hand [12]. Jaitly and Hinton [13], for example, argued that low-dimensional, hand-crafted features, such as Mel-frequency cepstral coefficents (MFCCs), may lose relevant information that is otherwise present in the original signals.From raw time-domain waveforms, convolutional neural networks (CNN) have shown promising results [8,9,14,15]. This work was partially supported by a PhD studentship funded by Bloomberg, by the EU H2020 project ELG (grant agreement 825627), and by the EPSRC project SpeechWave (EP/R012180/1).It has even been demonstrated that it is possible to learn bandpass beamformers from multi-channel raw waveforms [15], and a feature extractor learned from raw frequency representations of speech has been shown to outperform conventional methods [16]. Their interpretability, however, is sometimes limited, and it is not always clear how to apply existing adaptation techniques. In a recent approach called SincNet [10], Ravanelli and Bengio propose to constrain the CNN filters learned from raw time-domain signals, by requiring each kernel to model a rectangular band-pass filter. The authors show that this yields improved efficiency, and that the filters are more easily interpretable.In this paper we propose to make use of these characteristics for the adaptation of raw waveform acoustic models: we would like efficient, compact representations that are quick to estimate and cheap to store. We explore whether we can obtain this by adapting the cut-off frequencies, and the gains of the filters in SincNet. This layer may be particularly well suited for speaker adaptation, as the lower layers are known to carry more speaker information than the other layers [17,18]. We will show in Section 2.1 that adapting this parameterisation of the CNN filters has similarities with, and crucial differences from, VTLN, feature-space Maximum Likelihood Linear Regression (fMLLR) [3], and Learning Hidden Unit Contributions (LHUC) [2]. VTLN has been used to mitigate large variations in vocal tract length for the recognition of children's speech [19]. In our experiments we adapt from adults' to children's speech and show that we obtain VTLNlike scaling functions of the filter frequencies.There are related approaches in literature that aim to learn, and update filterbanks on top of e.g. raw spectra [12,9,20,21]. As argued in these papers, fixed filterbanks may not be an optimal choice for a particular task. Sailor and Patil [22] indeed showed that their proposed convolutional restricted Boltzmann machine (RBM) model learns different centre frequencies depending on the task at hand. Our work is perhaps most closely related to Seki et al. [21], who proposed to adapt a filterbank composed of differentiable functions such as Gaussian or Gammatone filters. They demonstrated more than 7% relative reductions in WER when adapting to speakers in a spontaneous Japanese speech transcription task. Our work differs in that we propose to adapt the SincNet layer, which operates on raw waveforms, rather than power spectra.We review SincNet and related methods in Section 2. Sec-arXiv:1909.13759v1 [eess.AS] 30 Sep 2019 tion 3 presents the experimental setup, with results in Section 4. Section 5 concludes the paper.",no,,,,no,,,,
137,https://github.com/zjysteven/bitslice_sparsity,zjysteven_bitslice_sparsity.tar.gz,Exploring Bit-Slice Sparsity in Deep Neural Networks for Efficient ReRAM-Based Deployment,@wraps(func) @six.add_metaclass(FixMeta) @contextmanager @pytest.fixture @staticmethod @pytest.mark.parametrize(,copy contextlib yaml sys __future__ torchvision util_func collections torch os pytest six functools nics_fix_pt argparse numpy time shutil math vgg setuptools ResNet inspect,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zjysteven_bitslice_sparsity.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zjysteven_bitslice_sparsity.pdf,,,no,,,,no,,,,
1082,https://github.com/negotiatorvivian/SAT-Solver,negotiatorvivian_SAT-Solver.tar.gz,PDP: A General Neural Framework for Learning Constraint Satisfaction Solvers,@property @staticmethod,"pandas warnings sys pdp multiprocessing os, sys linecache, json json collections transformer torch os apex scipy itertools argparse numpy time math setuptools subprocess",cs.LG cs.LO cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/negotiatorvivian_SAT-Solver.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\negotiatorvivian_SAT-Solver.pdf,,,yes,Adam,not all,"learning rate, gradient clipping, weight decay, dropout, default setting from other study",no,,,,
1197,https://github.com/proroklab/magat_pathplanning,proroklab_magat_pathplanning.tar.gz,Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path Planning,@staticmethod,"utils gc PIL copy pandas csv yaml sys multiprocessing signal glob pdb easydict graphs pprint torchvision onlineExpert hdf5storage re hashids datetime json fnmatch cycler torch os torchsummaryX matplotlib scipy gzip itertools hashlib seaborn agents urllib pickle drawSvg sklearn argparse tensorboardX numpy time shutil math offlineExpert random logging cv2 sklearn, sklearn subprocess operator dataloader zipfile",cs.RO cs.DC cs.LG cs.MA stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/proroklab_magat_pathplanning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\proroklab_magat_pathplanning.pdf,,"VII. EXPERIMENTSIn this section, we firstly introduce the metrics we use in the evaluation (Sec. VII-A), and then provide details of the experimental setup (Sec. VII-B). We then move on to introduce the map sets we use in the experiments (Sec. VII-C) and the baselines against which we compare our proposed methods (Sec. VII-D). Finally, we present and discuss our experimental results (Sec. VII-E).A. Metrics 1) Success Rate (α) = n success /n, is the proportion of successful cases over the total number of tested cases n. A case is considered successful (complete) when all robots reach their goals prior to exceeding the maximum allowed steps. In our case, the max allowed step (i.e., maximum makespan) is normally set to T max = 3T MP * , where T MP * is the makespan (the time period from the first robot moves to the last robot arrives its goal) of the expert solution.2) Flowtime Increase (δ FT ) = (FT − FT * )/FT * , measures the difference between the sum of the executed path lengths (FT) and that of the expert (target) path (FT * ). We set the length of the predicted path T i = T max = 3T MP * , if the robot i does not reach its goal. This contributes a penalty to the total flowtime if a robot fails to reach its goal.",yes,Adam,not all,"momentum, learning rate, decay, epochs, batch size, l2 regualrization",no,,,,
907,https://github.com/Yuxinya/SG_predict,Yuxinya_SG_predict.tar.gz,COMPOSITION BASED CRYSTAL MATERIALS SYMMETRY PREDICTION USING MACHINE LEARNING WITH ENHANCED DESCRIPTORS A PREPRINT,,matminer scipy keras re pandas pymatgen setuptools bz2 _pickle pickle sklearn argparse os joblib numpy,cond-mat.mtrl-sci,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Yuxinya_SG_predict.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Yuxinya_SG_predict.pdf,,,yes,Adam,yes,"number of trees, max features, max depth, learning rate, alpha, gamma, lambda, learning rate, epochs, batch size",no,,,,
840,https://github.com/eladhoffer/fix_your_classifier,eladhoffer_fix_your_classifier.tar.gz,FIX YOUR CLASSIFIER: THE MARGINAL VALUE OF TRAINING THE LAST WEIGHT LAYER,,math scipy torch,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eladhoffer_fix_your_classifier.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eladhoffer_fix_your_classifier.pdf,,,no,,,"hyperparameter used as in original work, ",no,,,,
627,https://github.com/bnewm0609/eos-decision,bnewm0609_eos-decision.tar.gz,The EOS Decision and Length Extrapolation,@torch.no_grad(),utils copy yaml sys glob regimen tqdm onmt jsonlines re truncation_models collections json data torch os matplotlib string itertools reporter pickle sklearn argparse numpy shutil math sacrebleu truncation_regimen random subprocess models,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bnewm0609_eos-decision.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bnewm0609_eos-decision.pdf,,"ResultsRecall that we train all models to obtain perfect in-domain bracket closing accuracy on a held-out validation set. However, the +EOS models' performance is severely degraded on out-of-domain sequences 10 times longer than in-domain ones, while the -EOS models still perform well across all conditions (Table 2). The perplexity results mirror the results of the bracket closing task (Appendix B).Because the Dyck-(k,m) is relatively simple, the model's hidden states are interpretable. We run the model on the training data, stack the models' hidden states, extract their top two principal components, and note that they form clusters (Figure 3) that encode stack states (Appendix; Figure 5). We observe that as both models process sequences, they hop between the clusters. However, in the +EOS model, the hidden states move from one side of each cluster to the other during processing, while this does not happen in the -EOS model. Because of this, we refer to these elongated clusters as length manifolds.As the +EOS model processes inputs past the maximum length it has seen during training, the representations lie past the boundary of the clusters, which we observe leads to a degradation of the model's ability to transition between states. We hypothesize this is because the recurrent dynamics break near the edges of the training-time length manifolds. We can see this degradation of the model dynamics in the out-of-domain +EOS plot (Figure 3). The hidden states at positions further in the sequences congregate at the tips of the length manifolds, or completely deviate from the training data. Contrast this plot to the in-domain -EOS plot, where there are no length manifolds visible and the out-of-domain -EOS plot, where we can see some length manifolds, but less evidence of the degradation of predictions (Table 2).We hypothesize that these length manifolds form because +EOS models need to predict where each sequence ends in order to confidently put probability mass on the EOS token. 2 Our main takeaway is then that this length tracking has negative effects when the model is used to predict longer sequences.",yes,"Adam, Hyperparameter Search",yes,"batch size, learning rate",yes,,,,
93,https://github.com/bbc/xcnet,bbc_xcnet.tar.gz,Attention-based Stylisation for Exemplar Image Colourisation,@staticmethod,PIL csv glob engine skimage tqdm faiss torchvision re datetime data torch os importlib mkl scipy functools pickle sklearn argparse numpy time shutil tarfile cv2 random models,eess.IV cs.CV cs.LG cs.MM,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bbc_xcnet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bbc_xcnet.pdf,Axial Attention,,yes,Adam,yes,"epochs, learning rate, batch size",no,,,,
1239,https://github.com/owkin/idash-2020,owkin_idash-2020.tar.gz,Differentially Private Federated Learning for Cancer Prediction,,utils pandas opacus warnings sys configargparse pathlib distant src torch os socket struct profiles itertools docker sklearn numpy time shutil random statistics subprocess models,stat.ML cs.CR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/owkin_idash-2020.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\owkin_idash-2020.pdf,,,yes,Grid Search,no,,yes,,,,
870,https://github.com/bhiziroglu/Conditional-Generative-Adversarial-Network,bhiziroglu_Conditional-Generative-Adversarial-Network.tar.gz,Conditional Generative Adversarial Nets,,pickle torch __future__ numpy torchvision,cs.LG cs.AI cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bhiziroglu_Conditional-Generative-Adversarial-Network.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bhiziroglu_Conditional-Generative-Adversarial-Network.pdf,,,yes,"Random Grid Search, Manual Search, SGD",not all,"mini batch size, leearning rate, decay, momentum, dropout",yes,,,,
698,https://github.com/endo-yuki-t/DiversifyingSMIS,endo-yuki-t_DiversifyingSMIS.tar.gz,Diversifying Semantic Image Synthesis and Editing via Class-and Layer-wise VAEs,@staticmethod,PIL sys torchvision util re tensorflow trainers datetime collections data torch os StringIO dominate importlib scipy pickle argparse dill numpy time ntpath random cv2 options models io,cs.CV cs.GR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/endo-yuki-t_DiversifyingSMIS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\endo-yuki-t_DiversifyingSMIS.pdf,AutoEncoder,,yes,Adam,not all ,"same parameters as in other study, momentum, learning rate, batch size, epochs",no,,,,
721,https://github.com/MECLabTUDA/ACS,MECLabTUDA_ACS.tar.gz,Adversarial Continual Learning for Multi-Domain Hippocampal Segmentation,@property @staticmethod,PIL copy pandas csv sys nibabel tqdm torchvision telegram re SimpleITK datetime json mp torch os matplotlib functools seaborn typing pickle torchio argparse numpy time shutil math torchsummary setuptools random args,eess.IV cs.CV cs.LG cs.CV cs.LG eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MECLabTUDA_ACS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MECLabTUDA_ACS.pdf,,,no,,,"batch size, epochs",,,,,
515,https://github.com/yelusaleng/RRU-Net,yelusaleng_RRU-Net.tar.gz,RRU-Net: The Ringed Residual U-Net for Image Splicing Forgery Detection,,utils dice_loss PIL unet random tqdm torch os numpy matplotlib time eval,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yelusaleng_RRU-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yelusaleng_RRU-Net.pdf,,,yes,SGD,yes,"batch size, momentum, weight decay, learning rate",no,,,,
540,https://github.com/lgalke/lifelong-learning,lgalke_lifelong-learning.tar.gz,Lifelong Learning of Graph Neural Networks for Open-World Node Classification,@staticmethod @torch.no_grad() @MEMORY.cache,gc pandas sys tqdm json collections tempfile lifelong_learning torch os matplotlib torch_geometric scipy dgl itertools seaborn pickle sklearn argparse joblib numpy abc sampler_torch math drift_magnitude datasets networkx operator models,cs.LG cs.SI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lgalke_lifelong-learning.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lgalke_lifelong-learning.pdf,,"I. INTRODUCTIONGraph neural networks [1] (GNNs) have emerged as stateof-the-art methods in numerous tasks on graph-structured data such as vertex classification [2]- [5], graph classification [6], link prediction [7], and unsupervised vertex representation learning [8]. An intriguing property of GNNs is that they are capable of inductive learning. An inductive model for graph data only depends on the vertex features and the graph structure given by its edges. In many cases [3], [9], [10], this is a major advantage over models that rely on a static vertex embedding [11], which would need to be retrained [12] as soon as any new vertex appears. In contrast, inductively trained GNNs can be applied to new data -or even a different graph -without any retraining because of the property of only requiring vertex features and edges.However, being able to apply the same model to unseen data also comes with challenges that have not been analyzed so far. Let us assume, we have a vertex classification model and new data streams in over time, i. e., new edges and vertices arrive; then even new classes may arise! This raises several new questions: Do we need to retrain the model? How much ©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. past data should be preserved for retraining? Is it helpful to preserve implicit knowledge within the model parameters or should we retrain from scratch? Fig. 1. Lifelong Open-World Node Classification. At each time t the learner has to classify new vertices of task Tt (red). The learner may use knowledge from previous tasks to adapt to the current task, eventually cut off by a history size (blue). The current task might come with previously unseen classes, e. g., see class ""c"" that emerged only at task t − 2 and was subsequently added to the class set. After evaluating each task Tt, we continue with task T t+1 .To answer these questions, we frame the problem as an instance of lifelong machine learning [13]- [15]. In lifelong learning, the learner has to perform a sequence of tasks T 1 , T 2 , . . . , T t , and may use knowledge K gained in previous tasks to perform task T t (see illustration in Figure 1). In our case, each task consists of classifying vertices given an attributed graph. Knowledge K may be stored explicitly (the training data of past tasks) or implicitly within the model parameters. A particular challenge of lifelong learning in the context of graph data is that vertices cannot be processed independently because models typically take connected vertices into account. We also consider the challenge that the set of classes in task T t differs from classes in previous tasks, which is known as the open-world classification [15] problem.We address these challenges by introducing a new incremental training method for lifelong learning on graph data. This training method can be applied on any dataset by the introduction of a new measure to harmonize temporal variances in the evolution of graph datasets with different characteristics and granularities. The method is capable to integrate various existing GNNs, both isotropic and anisotropic as well as scalable GNN methods. In our experiments, we thoroughly evaluate representative and scalable GNN architectures (and one graph-agnostic multi-layer perceptron) using our incremental training method that retrains the model for each task. We use an artificial history size that limits the amount of past data (called here: explicit knowledge) available for training and compare limited history-size retraining against unlimited full-history retraining.Furthermore, we compare reusing model parameters from previous tasks (warm restart) against retraining from scratch (cold restarts) to analyze the influence of implicit knowledge.In an ablation study, we compare incrementally training models against once-trained models. To facilitate our analyses, we contribute three new datasets for lifelong learning; one co-authorship and two citation graph datasets with different degrees of changes in the class set. In total, we have experimented with 48 different incremental training configurations, namely 6 architectures × 4 history sizes × cold restarts and warm restarts, which we evaluate on three new datasets. To enable a fair comparison, we tune the hyperparameters for each configuration separately. We repeat all experiments 10 times with different random seeds.The results of our 1,440 experiments (48 configurations repeated 10 times on 3 datasets) reveal the following insights:(i) Data from only few past tasks are sufficient for retraining Surprisingly, only the data from very few past tasks are sufficient to maintain a high level of accuracy that matches the accuracy of the same model retrained using all past data. In our experiments, merely 50% of the GNN's receptive field (corresponding to history sizes of 3 or 4 past tasks) are sufficient to match at least 95% of the accuracy of the same model trained on the full history.(ii) Reusing parameters requires less data Tuning the learning rate may compensate for the knowledge lost when randomly re-initializing the model. However, when the history size is small, using warm restarts tends to be beneficial such that more knowledge from past tasks can be preserved implicitly. Our results further suggest that using warm restarts is generally preferable for lifelong learning.(iii) Incremental training is a simple yet effective technique to tackle lifelong learning on graphs By comparing incrementally trained models with once-trained models, we find that the accuracy of once-trained models decreases quickly when new classes appear. We note that even when no new classes appear over time, incremental training still benefits from the increased amount of training data.These insights have direct consequences for using GNNs in practical applications. It allows to decide how much historical data should be to kept to maintain a certain performance versus having memory available in the GPUs. This is an important criterion that influences which GNN methods are applicable [10], [16]. We publicly provide the datasets as well as the evaluation framework to extend our experiments and accelerate research in lifelong learning on graphs.Below, we discuss the related works and provide a problem statement with our training procedure and new measure in Section III. We describe the experimental apparatus and datasets in Section IV. The results of our experiments are reported in Section V as well as in Section VI for our ablation study. We discuss the results in Section VII before we conclude.",yes,"experimental tuning, separately tuned, Adam",no,"experiments 10x repeated with different randon seeds, learning rate, history size, restart configuration, dropout rate epochs",yes,,,,
825,https://github.com/YadiLao/MM-Tag,YadiLao_MM-Tag.tar.gz,A Tree Search algorithm For Sequence Labeling,,gc sys util tensorflow torch_process_data datetime json collections treelib new_value torch os matplotlib cnn_lstm functools pg codecs pickle sklearn argparse numpy time math operator logging random,cs.CL cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YadiLao_MM-Tag.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YadiLao_MM-Tag.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Conditional Random Field,,no,,,"learning rate, tree search trade off, hidden units",no,,,,
405,https://github.com/Forethought-Technologies/ieee-dsmp-2018-paper,Forethought-Technologies_ieee-dsmp-2018-paper.tar.gz,Automated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks,@interfaces.legacy_recurrent_support @classmethod @property,utils pandas csv warnings bs4 scrape_bugzilla __future__ grid_search gensim keras re attention_layer json text_processing textblob nltk os string itertools urllib sklearn ind_rnn numpy time fastText,cs.IR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Forethought-Technologies_ieee-dsmp-2018-paper.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Forethought-Technologies_ieee-dsmp-2018-paper.pdf,,,yes,Grid Search,no,,yes,,,,
1213,https://github.com/cx921003/UPG-GAN,cx921003_UPG-GAN.tar.gz,Unpaired Pose Guided Human Image Generation,@staticmethod,requests PIL warnings bs4 __future__ torchvision util re collections data visdom torch os dominate scipy functools itertools argparse numpy time tarfile ntpath random options models inspect zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cx921003_UPG-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cx921003_UPG-GAN.pdf,,,yes,,not all,"learning rate, loss weights",no,,,,
771,https://github.com/wenqingchu/Semantic-CariGANs,wenqingchu_Semantic-CariGANs.tar.gz,Learning to Caricature via Semantic Shape Transform,@staticmethod,requests PIL loss base_dataset retrieval warnings sys pdb tqdm bs4 function __future__ torchvision net util datetime collections json image_folder sampler data torch os visdom matplotlib dominate importlib scipy functools itertools networks argparse tensorboardX numpy logger parsing time celeba_dataset siamese_dataset math DAENet tarfile optimizer ntpath AdaIn psp random cv2 logging transform options models zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wenqingchu_Semantic-CariGANs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wenqingchu_Semantic-CariGANs.pdf,,,yes,Adam,yes,"batch size, learning rate, epochs, decay, other hyperparameter",no,,,,
1210,https://github.com/Smilels/TeachNet_Teleoperation,Smilels_TeachNet_Teleoperation.tar.gz,Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network,@mlab.animate(delay=200) @jit(nopython=True),utils PIL copy pandas csv sys multiprocessing glob sensor_msgs rospy __future__ torchvision std_msgs mayavi model moveit_commander os torch cv_bridge matplotlib pyquaternion IPython numba seaborn pickle argparse tensorboardX numpy time shutil math sr_robot_commander cv2 ros_numpy mpl_toolkits shadow_teleop,cs.RO,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Smilels_TeachNet_Teleoperation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Smilels_TeachNet_Teleoperation.pdf,,,no,,,,no,,,,
737,https://github.com/thomlake/pytorch-attention,thomlake_pytorch-attention.tar.gz,End-To-End Memory Networks,"@pytest.mark.parametrize('v_mask, v_unmask', [(0, 1), (float('-inf'), 0)]) @pytest.mark.parametrize(",attention pytest torch numpy distutils,cs.NE cs.CL cs.NE cs.CL cs.LG cs.NE stat.ML stat.ML cs.CG cs.LG cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thomlake_pytorch-attention.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thomlake_pytorch-attention.pdf,Softmax  End-To-End Memory Network  Sigmoid Activation  Tanh Activation  Neural Turing Machine  Long Short-Term Memory  Content-based Attention  Tanh Activation  Additive Attention  Sigmoid Activation  Tanh Activation  Additive Attention  Softmax  Long Short-Term Memory  Pointer Network,,yes,Experimental tuning,not all,"learning rate, epochs, batch size, ",yes,,,,
531,https://github.com/ngrayluna/NotCake,ngrayluna_NotCake.tar.gz,"You Only Look Once: Unified, Real-Time Object Detection","@app.route(""/"") @app.route(""/about"") @app.route(""/home"") @app.route(""/howto"") @app.route(""/objectDetect"", methods=[""GET"",""POST""])",utils PIL frontend copy sys keras tensorflow model json imgaug kmeans os matplotlib xml argparse processing numpy flask preprocessing shutil random cv2 backend,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ngrayluna_NotCake.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ngrayluna_NotCake.pdf,Leaky ReLU  Non Maximum Suppression  1x1 Convolution  Color Jitter  Random Resized Crop  RoIPool  Fast R-CNN  Dropout  Rectified Linear Units  Max Pooling  Convolution  Dense Connections  Softmax  VGG  Step Decay  SGD with Momentum  YOLOv1,,yes,,yes,"epochs, batch size, momentum, decay, learning rate, dropout",no,,,,
542,https://github.com/TotalVariation/Flattenet,TotalVariation_Flattenet.tar.gz,FlatteNet: A Simple Versatile Framework for Dense Pixelwise Prediction,@property,utils PIL sys tqdm __future__ pprint pathlib _init_paths core collections detail torch os scipy bisect typing argparse tensorboardX numpy time shutil config timeit datasets pycocotools cv2 logging random models yacs,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/TotalVariation_Flattenet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\TotalVariation_Flattenet.pdf,Residual Connection  Bottleneck Residual Block  Global Average Pooling  Residual Block  Rectified Linear Units  Max Pooling  Average Pooling  1x1 Convolution  Convolution  Batch Normalization  Mixup  Label Smoothing  Cosine Annealing  Weight Decay  Nesterov Accelerated Gradient  Xavier Initialization  Random Horizontal Flip  Random Resized Crop  ResNet-D  Dilated Convolution  Convolution,,yes,SGD,yes,"learning rate, dropout, epochs, mini batch size, momentum, weight decay, horizontal flipping",no,,,,
1377,https://github.com/lauramanduchi/DC-GMM,lauramanduchi_DC-GMM.tar.gz,Deep Conditional Gaussian Mixture Model for Constrained Clustering,,"PIL pandas projector_plugin yaml source sys ast torchvision pathlib tensorflow uuid torch os UTKFace socket os, argparse matplotlib heart_echo scipy gzip seaborn pickle sklearn argparse numpy time timeit math tensorboard cv2 logging random tensorflow_probability",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lauramanduchi_DC-GMM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lauramanduchi_DC-GMM.pdf,,,yes,Grid Search,yes,"penalty weight, learning rate, batch size, epochs, decay, epoch decay, embedding dimension",yes,,,,
189,https://github.com/Vibashan/Image-Fusion-Transformer,Vibashan_Image-Fusion-Transformer.tar.gz,Image Fusion Transformer,,utils warnings sys pdb checkpoint tqdm net tensorflow args_fusion torch os matplotlib scipy pytorch_msssim kornia numpy time math random cv2 vit_model,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Vibashan_Image-Fusion-Transformer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Vibashan_Image-Fusion-Transformer.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Label Smoothing  Residual Connection  Dense Connections  Softmax  Scaled Dot-Product Attention  Layer Normalization  Adam  Dropout  Byte Pair Encoding  Multi-Head Attention  Transformer  Convolution,"A. Implementation detailsFor visible and infrared fusion, we train our model on 80000 pairs of visible and infrared images in the KAIST dataset. We test on 21 pairs of visible and infrared images in the TNO Human Factors dataset during testing. Further, we follow RFN-Nest experimental setup by resizing the images to 256 × 256 and setting hyperparameters w I1 , w I2 , w 1 , α equal to 6, 3, 100, 700. For all experiments, we set the learning rate, epoch, and batch size equal to 10 −4 , 4 and 2, respectively.For the experiment with MRI and PET images, the network is trained on 9981 cropped patches with image pairs obtained from the Harvard MRI and PET datasets. The trained model is evaluated on 20 pairs of MRI and PET images sampled from the Harvard MRI and PET image fusion dataset. During training, we resize the images to 84 × 84 and convert the PET images to IHS scale to fuse the I channel with an MRI image. For all experiments, we set the learning rate, epoch, and batch size equal to 10 −4 , 4 and 2, respectively. Our network is based on PyTorch and implemented on NVIDIA TITAN Xp GPU.",yes,,yes,"learning rate, epochs, batch size",no,,,,
1339,https://github.com/DearCaat/ioplin,DearCaat_ioplin.tar.gz,An Iteratively Optimized Patch Label Inference Network for Automatic Pavement Disease Detection,,ioplin PIL keras tensorflow sys setuptools random cv2 sklearn argparse os efficientnet numpy io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/DearCaat_ioplin.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\DearCaat_ioplin.pdf,,,no,,,,no,,,,
874,https://github.com/chatc/TriageSQL,chatc_TriageSQL.tar.gz,Did You Ask a Good Question? A Cross-Domain Question Intention Classification Benchmark for Text-to-SQL,,utils copy name pandas re model json collections transformers torch os sqlite3 matplotlib itertools seaborn pyecharts sklearn numpy time config random models,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/chatc_TriageSQL.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\chatc_TriageSQL.pdf,Multi-Head Attention  Layer Normalization  WordPiece  Softmax  Adam  Dense Connections  Weight Decay  Dropout  Linear Warmup With Linear Decay  Attention Dropout  Gaussian Error Linear Units  Scaled Dot-Product Attention  Residual Connection  BERT  RoBERTa,,no,,,,no,,,,
90,https://github.com/runsong123/adafit,runsong123_adafit.tar.gz,AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds,@property @staticmethod,utils AdaFit_multi_scale sys nibabel __future__ normal_estimation_utils plyfile pathlib ThreeDmFVNet tensorflow netBase visualization CGAL torch os eulerangles provider matplotlib dataset_single_scale importlib scipy itertools pcpnet_dataset pickle sklearn argparse tensorboardX numpy time shutil math dataset_multi_scale random AdaFit_single_scale mpl_toolkits,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/runsong123_adafit.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\runsong123_adafit.pdf,,"PCPNet datasetWe follow exactly the same experimental setup as [7] including train-test split, training data augmentation and adding noise or density variations on testing data. We use the Adam [37] optimizer and a learning rate of 5 × 10 −4 for the training with a batch size of 256. AdaFit is trained with 600 epochs on a 2080Ti GPU.Baselines. We consider three types of baseline methods: 1) the traditional normal estimation methods PCA [18] and jet [22]; 2) the learning-based surface fitting methods Lenssen et al. [15], DeepFit [13]; 3) the learning-based normal regression methods PCPNet [7] and Nesti-Net [8].Metrics. We use the angle RMSE between the predicted normal and the ground truth as our main metrics for evaluation. We also draw the AUC curve of normal errors to show the error distribution.Quantitative results. The RMSE of AdaFit and baseline methods are shown in Table 1 and the AUC of all methods are shown in Fig. 6. The results show that AdaFit outperforms both the traditional methods and learning-based methods in all settings, which demonstrates the effectiveness of using offsets to adjust the point set. Especially on the point clouds with density variations, baseline methods may fail to find enough points on sparse regions for the surface fitting while AdaFit use the offset to project points to the neighboring regions for more robust surface fitting. In addition, we adding the results of DeepFit [13] with denoising pre-processing [31]. Though denoising reduces the noise and finds smooth surfaces, it does not offset the points Aug. AdaFit DeepFit [13] Denoising+ DeepFit [13] Lenssen et al. [15] Nesti-Net [8] PCPNet [7] PCA [18] Jet [22] No noise 5.19 to the ground truth positions on the surfaces, which still results in a larger RMSE. Qualitative results. Fig. 7 shows the normals estimated by AdaFit and Fig. 8 visualizes the angle errors of AdaFit and baseline methods on different shapes. It can be seen that Lenssen et al. [15] perform better on the flattened regions while DeepFit [13] can better handle curved regions but is not accurate for regions with sharp curves. In contrast, the proposed AdaFit is relatively more robust on all regions and all settings (noise or density variations). Additional detailed analysis on normal estimation of sharp edges can be found in the supplementary materials.",yes,Adam,yes,"learning rate, batch size, epochs",no,,,,
10,https://github.com/imcsq/SMAPGAN,imcsq_SMAPGAN.tar.gz,SM AP GAN : Generative Adversarial Network-Based Semi-Supervised Styled Map Tile Generation Method,@abstractmethod @staticmethod,"requests PIL warnings sys skimage bs4 __future__ torchvision util collections data visdom torch os dominate importlib scipy functools itertools argparse numpy time abc math tarfile ntpath random os, glob, cv2, time subprocess options models zipfile",eess.IV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/imcsq_SMAPGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\imcsq_SMAPGAN.pdf,Convolution  Generative Adversarial Network,,yes,Adam Optimizer,yes,"Adam values, epochs",no,,,,
1405,https://github.com/valeoai/carrada_dataset,valeoai_carrada_dataset.tar.gz,CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler Annotations,"@pytest.mark.parametrize('get_true_data_frame', SEQ_NAMES[1], indirect=True) @pytest.mark.parametrize('get_true_ra', SEQ_NAMES[2], indirect=True) @pytest.mark.parametrize('get_true_data_instance', SEQ_NAMES[2], indirect=True) @pytest.mark.parametrize('get_true_data', SEQ_NAMES[1], indirect=True) @pytest.fixture @jit @pytest.mark.parametrize('get_true_ra', SEQ_NAMES[0], indirect=True) @pytest.mark.parametrize('get_true_data', SEQ_NAMES[0], indirect=True) @pytest.mark.parametrize('get_true_data_instance', SEQ_NAMES[0], indirect=True) @property @pytest.mark.parametrize('get_true_data_frame', SEQ_NAMES[2], indirect=True) @pytest.mark.parametrize('get_true_rd', SEQ_NAMES[0], indirect=True) @pytest.mark.parametrize('get_true_data_instance', SEQ_NAMES[1], indirect=True) @pytest.mark.parametrize('get_true_data', SEQ_NAMES[2], indirect=True) @pytest.mark.parametrize('get_true_rd', SEQ_NAMES[1], indirect=True) @pytest.mark.parametrize('get_true_data_frame', SEQ_NAMES[0], indirect=True) @pytest.mark.parametrize('get_true_ra', SEQ_NAMES[1], indirect=True) @pytest.mark.parametrize('get_true_rd', SEQ_NAMES[2], indirect=True)",filterpy PIL pandas sys glob skimage __future__ torchvision pathlib json os pytest matplotlib configparser scipy itertools numba sklearn argparse xmltodict carrada_dataset numpy time setuptools random cv2,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/valeoai_carrada_dataset.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\valeoai_carrada_dataset.pdf,,"B. Radar sensor and effectsA radar sensor emits electromagnetic waves via one or several transmitter antennas (Tx). The waves are reflected by an object and received by the radar via one or several receiver antennas (Rx). The comparison between the transmitted and the received waveforms infers the distance, the radial velocity, the azimuth angle and the elevation of the reflector regarding the radar position [16]. Most of the automotive radars use Multiple Input Multiple Output (MIMO) systems: each couple of Tx/Rx receives the reflected signal assigned to a specific Tx transmitting a waveform.Frequency-Modulated Continuous Wave (FMCW) radar transmits a signal, called a chirp [5], whose frequency is linearly modulated over the sweeping period T s : At time t s ∈ {0, • • • , T s }, the emitted sinusoidal signal has frequencyf s = f c + B T s t s ,(1)where f c is the carrier frequency and B the bandwidth, and its phase readsφ E (t) = 2πf s t.(2)After reflection on an object at distance r(t) from the emitter, the received signal has phase:φ R (t) = 2πf s (t − τ ) = φ E (t) − φ(t),(3)where τ = 2r(t) c is the time delay of the signal round trip, with c the velocity of the wave through the air considered as constant, and φ(t) is the phase shift:φ(t) = 2πf s τ = 2πf s 2r(t) c .(4)Measuring this phase shift (or equivalently the time delay between the transmitted and the reflected signal) grants access to the distance between the sensor and the reflecting object. Its radial velocity is accessed through the frequency shift between the two signals, a.k.a. the Doppler effect. Indeed, the phase shift varies when the target is moving:f d = 1 2π dφ dt = 2v R c f s ,(5)where v R = dr/dt is the radial velocity of the target object w.r.t. the radar. This yields the frequency Doppler effect whereby frequency change rate between transmitted and received signals,f d fs = 2v R c, depends linearly on the relative speed of the reflector. Measuring this Doppler effect therefore amounts to recovering the radial speedv R = cf d 2f s .(6)The transmitted and received signals, s E and s R are compared with a mixer that generates a so-called Intermediate Frequency (IF) signal. The transmitted signal term is filtered using a low-pass filter and digitized by an Analog-to-Digital Converter (ADC). In this manner, the recorded signal carries the Doppler frequencies and ranges of all reflectors.Using the MIMO system with multiple Rx antennas, the time delay between the received signals of each Rx transmitted by a given Tx carries the orientation information of the object. Depending on the positioning of the antennas, the azimuth angle and the elevation of the object are respectively deduced from the horizontal and vertical pairs of Tx/Rx. The azimuth angle α is deduced from the variation between the phase shift of adjacent pairs of Rx. We have∆φ α = 2πf s 2h sin(α) c, where h is the distance separating the adjacent receivers.Consecutive filtered IF signals are stored in a frame buffer which is a time-domain 3D tensor: the first dimension corresponds to the chirp index; the second one is the chirp sampling defined by the linearly-modulated frequency range; the third tensor dimension indexes Tx/Rx antenna pairs.The Fast Fourier Transform (FFT) algorithm applies a Discrete Fourier Transform (DFT) to the recorded data from the time domain to the frequency domain. The 3D tensor is processed using a 3D-FFT: a Range-FFT along the rows resolving the object range, a Doppler-FFT along the columns resolving the object radial velocity and an Angle-FFT along the depth resolving the angle between two objects. The range, velocity and angle bins in the output tensor correspond to discretized values defined by the resolution of the radar. The range resolution is defined as δd = c 2B . The radial velocity resolution δv R = c 2fcT is inversely proportional to the frame duration time. The angle resolution δα =c fcNRxh cos(α)is the minimum angle separation between two objects to be distinguished, with N Rx the number of Rx antennas and α the azimuth angle between the radar and an object at distance D reflecting the signal.The next section will describe the settings of the radar sensor used and recorded dataset.",yes,Adam,not all,"epochs, betas, epsilon, batch size, learning rate, decay",no,,,,
1180,https://github.com/xiaozai/det,xiaozai_det.tar.gz,DepthTrack: Unveiling the Power of RGBD Tracking,@model_constructor @wraps(f) @staticmethod @functools.wraps(op) @classmethod @tensor_operation,PIL copy pandas csv warnings sys multiprocessing glob ltr prroi_pool tqdm skimage __future__ torchvision pathlib re jactorch collections json unittest tempfile visdom torch os jpeg4py matplotlib importlib scipy functools itertools xml pickle argparse lvis trax tensorboardX numpy time shutil _collections math pytracking pycocotools tikzplotlib traceback cv2 random gdown inspect,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xiaozai_det.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xiaozai_det.pdf,,,no,,,parameter setting from original authors that have tuned the parameters,no,,,,
1217,https://github.com/steven7woo/Accuracy-First-Differential-Privacy,steven7woo_Accuracy-First-Differential-Privacy.tar.gz,Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM,,"output_pert sys noise_reduc run_ridge os, sys ridge sparsevec sys, os naive_covar os covar os, sys, traceback matplotlib theory sklearn run_logist naive_outputpert logist numpy sgd_ridge project math traceback random",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/steven7woo_Accuracy-First-Differential-Privacy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\steven7woo_Accuracy-First-Differential-Privacy.pdf,,,no,,,"number of data points, regularization parameters, failure probability",no,,,,
953,https://github.com/peterparity/symder,peterparity_symder.tar.gz,Discovering Sparse Interpretable Dynamics from Partial Observations,"@partial(rescale_z, scale_vec=scale_vec) @odeint_zero.defjvp @partial(jax.custom_jvp, nondiff_argnums=(0,))",utils pathlib scipy functools haiku encoder pickle data sklearn tqdm os optax argparse typing numpy jax symder,cs.LG physics.comp-ph physics.data-an,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/peterparity_symder.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\peterparity_symder.pdf,,,yes,AdaBelief,not all,"steps, learning rate, alphas, ",no,,,,
624,https://github.com/Sid2697/Word-recognition-and-retrieval,Sid2697_Word-recognition-and-retrieval.tar.gz,AGDC: Automatic Garbage Detection and Collection,,pdb pickle logging tqdm argparse torch sklearn numpy time Levenshtein,cs.RO cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Sid2697_Word-recognition-and-retrieval.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Sid2697_Word-recognition-and-retrieval.pdf,,,no,,,,no,,,,
23,https://github.com/MolecularAI/pysmilesutils,MolecularAI_pysmilesutils.tar.gz,SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules,"@pytest.fixture @pytest.mark.parametrize(""Dataset, kwargs"", datasets) @property @abstractmethod","warnings sys pickle, sys, os tqdm re rdkit collections h5py transformer torch os pytest matplotlib pysmilesutils IPython itertools math, os, time, sys pickle typing numpy time abc math pdb, random setuptools random statistics",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MolecularAI_pysmilesutils.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MolecularAI_pysmilesutils.pdf,,,yes,Bayesian optimization with Gaussian processs (GpyOpt),yes,"number of layers, number of units in layers, dropout, hidden layer size, number of dense hidden layers, weight regularization on dense layers (L1, L2), learning rate",yes,,,,
196,https://github.com/anonymous-author-iccv2019/sanity_checks,anonymous-author-iccv2019_sanity_checks.tar.gz,What Do Adversarially Robust Models Look At?,,PIL cv2 misc google_drive_downloader random torch os options argparse numpy models torchvision,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/anonymous-author-iccv2019_sanity_checks.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\anonymous-author-iccv2019_sanity_checks.pdf,,,yes (not sure),SGD,yes,"epochs, learning rate, momentum, batch size, ",no,,,,
1414,https://github.com/alexklwong/calibrated-backprojection-network,alexklwong_calibrated-backprojection-network.tar.gz,Unsupervised Depth Completion with Calibrated Backprojection Layers,,"PIL data_utils warnings multiprocessing net_utils torch, torchvision os, time global_constants datasets, data_utils, eval_utils kbnet_model os, sys, glob, argparse torch os os, sys, glob, cv2, argparse matplotlib os, sys, glob, argparse, cv2 networks sklearn argparse numpy posenet_model log_utils, losses, networks, net_utils cv2 transforms log_utils kbnet",cs.CV cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/alexklwong_calibrated-backprojection-network.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\alexklwong_calibrated-backprojection-network.pdf,,"C.2. To Various Density LevelsIn Table 8, we consider three different levels of density for the sparse depth inputs, 0.50%, 0.15%, 0.05% of the image space, that are provided by the VOID dataset [46]. To this end, we train a single model on VOID using sparse depth maps of 0.50% density and evaluate it on 0.50%, 0.15%, 0.05% density test sets. We also compare our method against [44,46] under these density levels.  We train a single model on VOID using sparse depth maps of 0.50% density and evaluate it on 0.50%, 0.15%, 0.05% density test sets. As expected, performance degrade as the input become more sparse. Overall, we perform better than [44,46]; however, at 0.05%, [44] performs better on the RMSE metric.As expected, as density decreases, our performance also degrades. However, we still outperform both [44,46] un-Figure 13: Qualitative results on generalization. We trained our model on VOID [46] (captured by Intel RealSense) and tested the model on NYUv2 [39] (captured by Microsoft Kinect). We also trained the baseline [46] on VOID and tested it on NYUv2. Here we show the point clouds in 3D and colored. Because of the mismatch in cameras, [46] predicted a distorted scene; whereas, while ours is not perfect, it is reasonable. der all three levels. We note that at the sparsest setting of 0.05%, [44] does beat us on the RMSE metric. The reason for this is that we selected the kernel sizes for our model based on the sparsity level of 0.5%; therefore, when testing it on 10× sparser point cloud, our depth representation will be more sparse as well, which limits the potential of our calibrated backprojection layers. In contrast, [44] proposed a network to first estimate the dense coarse topology. This phenomenon is also observed in KITTI, shown in Table 3 of the main text, where we removed our sparse-to-dense module and we observed a significant drop in performance.Fig. 12 shows qualitative evaluations on the three density levels. For 0.50%, error is low overall and the shape of the recovered scene resembles that of the ground truth. When we decrease density to 0.15%, we observe slight blurring in object shapes and increased errors in homogeneous regions. At 0.05%, we begin to observe artifact such as the green ""blob"" corresponding to the wall with more exaggerated errors in homogeneous regions. This is because locally the textureless surfaces give little to no information on object shape. Without sparse depth to anchor their values, they can be arbitrary. In this case the ""empty"" region is predicted as far.",yes,Adam,not all,"betas, epochs, batch size, laerning rate",no,,,,
1485,https://github.com/johnchenresearch/NS3L,johnchenresearch_NS3L.tar.gz,Negative sampling in semi-supervised learning,,torch,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/johnchenresearch_NS3L.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\johnchenresearch_NS3L.pdf,,"IntroductionDeep learning has been hugely successful in areas such as image classification (Krizhevsky et al., 2012;He et al., 2016;Zagoruyko & Komodakis, 2016;Huang et al., 2017) and speech recognition (Sak et al., 2014;Sercu et al., 2016), where a large amount of labeled data is available. However, in practice it is often prohibitively expensive to create a large, high quality labeled dataset, due to lack of time, resources, or other factors. For example, the ImageNet dataset-which consists of 3.2 million labeled images in 5247 categoriestook nearly two and half years to complete with the aid of Amazon's Mechanical Turk (Deng et al., 2009). Some medical tasks may require months of preparation, expensive hardware, the collaboration of many experts, and often are limited by the number of participants (Miotto et al., 2016). As a result, it is desirable to exploit unlabeled data to aid the training of deep learning models. This form of learning is semi-supervised learning (Chapelle & Scholkopf, 2006)  (SSL). Unlike supervised learning, the aim of SSL is to leverage unlabeled data, in conjunction with labeled data, to improve performance. SSL is typically evaluated on labeled datasets where a certain proportion of labels have been discarded. There have been a number of instances in which SSL is reported to achieve performance close to purely supervised learning (Laine & Aila, 2017;Miyato et al., 2017;Tarvainen & Valpola, 2017;Berthelot et al., 2019), where the purely supervised learning model is trained on the much larger whole dataset. However, despite significant progress in this field, it is still difficult to quantify when unlabeled data may aid the performance except in a handful of cases (Balcan & Blum, 2005;Ben-David et al., 2008;Kääriäinen, 2005;Niyogi, 2013;Rigollet, 2007;Singh et al., 2009;Wasserman & Lafferty, 2008).In this work, we restrict our attention to SSL algorithms which add a loss term to the neural network loss. These algorithms are the most flexible and practical given the difficulties in hyperparameter tuning in the entire model training process, in addition to achieving the state-of-the-art performance.We introduce Negative Sampling in Semi-Supervised Learning (NS 3 L): a simple, fast, easy to tune SSL algorithm, motivated by negative sampling/contrastive estimation (Mikolov et al., 2013;Smith & Eisner, 2005). In negative sampling/contrastive estimation, in order to train a model on unlabeled data, we exploit implicit negative evidence, originating from the unlabeled samples: Using negative sampling, we seek for good models that discriminate a supervised example from its neighborhood, comprised of unsupervised examples, assigned with a random (and potentially wrong) class. Stated differently, the learner learns that not only the supervised example is good, but that the same example is locally optimal in the space of examples, and that alternative examples are inferior. With negative sampling/contrastive estimation, instead of explaining and exploiting all of the data (that is not available during training), the model implicitly must only explain why the observed, supervised example is better than its unsupervised neighbors.Overall, NS 3 L adds a loss term to the learning objective, and is shown to improve performance simply by doing so to other state-of-the-art SSL objectives. Since modern datasets arXiv:1911.05166v2 [cs.LG] 28 Jun 2020 often have a large number of classes (Russakovsky et al., 2014), we are motivated by the observation that it is often much easier to label a sample with a class or classes it is not, as opposed to the one class it is, exploiting ideas from negative sampling/contrastive estimation (Mikolov et al., 2013;Smith & Eisner, 2005). Key Contributions. Our findings can be summarized as follows:i) We propose a new SSL algorithm, which is easy to tune, and improves SSL performance of other state of the art algorithms across a wide range of reasonable hyperparameters, simply by adding the NS 3 L loss in their objective.ii) Adding the NS 3 L loss to a variety of losses, including Virtual Adversarial Training (VAT) (Miyato et al., 2017), Π model, and MixMatch (Berthelot et al., 2019), we observe improved performance compared to vanilla alternatives as well as the addition of Pseudo-Labeling or Entropy Minimization, for the standard SSL benchmarks of SVHN, CIFAR10, and CIFAR100.iii) Adding the NS 3 L loss to the state-of-the-art SSL algorithm, i.e., the MixMatch procedure (Berthelot et al., 2019), NS 3 L combined with MixMatch produces superior performance for the standard SSL benchmarks of SVHN, CIFAR10 and STL-10.Namely, adding the NS 3 L loss to existing SSL algorithms is an easy way to improve performance, and requires limited extra computational resources for hyperparameter tuning, since it is interpretable, fast, and sufficiently easy to tune. Related WorkIn this paper, we restrict our attention to a subset of SSL algorithms which add a loss to the supervised loss function. These algorithms tend to be more practical in terms of hyperparameter tuning (Berthelot et al., 2019). There are a number of SSL algorithms not discussed in this paper, following and as mentioned in (Berthelot et al., 2019): including ""transductive"" models (Joachims, 1999;2003;Gammerman et al., 1998), graph-based methods (Zhu et al., 2003;Bengio et al., 2006), and generative modeling (Joachims, 2003;Belkin & Niyogi, 2002;Salakhutdinov & Hinton, 2007;Goodfellow et al., 2011;Odena, 2016;Pu et al., 2016;Salimans et al., 2016). For a comprehensive overview of SSL methods, refer to (Chapelle & Scholkopf, 2006), or (Zhu et al., 2003). Baseline MethodsFor baseline methods, we consider Pseudo-Labeling, due to its simplicity on the level of NS 3 L, and MixMatch and VAT for its performance, in addition to VAT + Entropy Minimization and VAT + Psuedo-Labeling. We also include Π model and omit Mean Teacher, although we follow the experiments of (Oliver et al., 2018) and both produce worse performance than VAT. The supervised baseline is trained on the remaining labeled data after some labels have been removed. We generally follow the tuned hyperparameters in the literature and do not observe noticeable gains from further hyperparameter tuning. CIFAR10:We evaluate the accuracy of each method with 4,000 labeled samples and 41,000 unlabeled samples, as is standard practice. The results are given in Table 1. Further results comparing the addition of Entropy Minimization, Pseudo-Labeling and NS 3 L are given in Table 2. MixMatch results are given in Table 3. For NS 3 L, we use a threshold T = 0.04, learning rate of 6e-4, and λ 1 = 1. Identical hyperparameters are used for Π model + NS 3 L. For VAT + NS 3 L, we use a shared learning rate of 6e-4 and reduce λ 1 from 1 to 0.3, which is identical to λ 2 . We perform extensive hyperparameter tuning for VAT + PL. For MixMatch, as in (Berthelot et al., 2019), we use α = 0.75 and λ 3 = 75. For NS 3 L + MixMatch, we use a threshold of T = 0.05 and a coefficient of λ 1 = 5 for 250 labeled samples and λ 1 = 10 for 4,000 labeled samples. All other settings remain as is optimized individually.We created 5 splits of the number of labeled samples, each with a different seed. Each model is trained on a different split and test error is reported with mean and standard deviation. We find that NS 3 L performs reasonably well and significantly better than Pseudo-Labeling, over a 1.5% improvement. A significant gain over all algorithms is attained by adding the NS 3 L loss to the VAT loss. VAT + NS 3 L achieves almost a 1% improvement over VAT, and is about 0.5% better than VAT + EntMin and VAT + PL. We also find that adding NS 3 L immediately improves the performance of MixMatch, with a 2% improvement with 250 labeled samples and a small improvement for 4,000 samples. The 250 labeled samples case may be the more interesting case since it highlights the sample efficiency of the method. This underscores the flexibility of NS 3 L to improve existing methods. SVHN:We evaluate the accuracy of each method with 1,000 labeled samples and 64,932 unlabeled samples, as is standard practice. The results are shown in Table 1. Mix-Match results are shown in Table 4. We use the same hyperparameters for NS 3 L, Π model + NS 3 L and VAT + NS 3 L as in CIFAR10. For MixMatch ollowing the literature, we use α = 0.75 and λ 3 = 250. For NS 3 L + MixMatch, we again use a threshold of T = 0.05 and a coefficient of λ 1 = 2 for both 250 labeled samples and 1,000 labeled samples.Again, 5 splits are created, each with a different seed. Each model is trained on a different split and test error is reported with mean and standard deviation. Here, NS 3 L achieves competitive learning rate with VAT, 6.52% versus 6.20%, and is significantly better than Pseudo-Labeling, at 7.70%. By combining NS 3 L with VAT, test error is further reduced by a notable margin, almost 1% better than VAT alone and more than 0.5% better than VAT + EntMin.By adding NS 3 L to MixMatch, the model achieves almost the same test error with 250 labeled samples than it does using only MixMatch on 1,000 labeled samples. In other words, in this case applying NS 3 L improves performance almost equivalent to having 4x the amount of labeled data. In the cases of 250 labeled samples and 1,000 labeled samples, adding NS 3 L to MixMatch improves performance by 0.4% and 0.15% respectively, achieving state-of-the-art results. We evaluate the accuracy of MixMatch and Mix-Match + NS 3 L with 1,000 labeled samples and 100,000 unlabeled samples. The results are given in Table 5. Following the literature, we use α = 0.75 and λ 3 = 50. For NS 3 L, we again use a threshold of T = 0.05 and λ 1 = 2. We trained the model for a significantly fewer epochs than in (Berthelot et al., 2019), however even in this case NS 3 L can improve upon MixMatch, reducing test error slightly.  1. For NS 3 L, we use a threshold T = 0.04/10 = 0.004, learning rate of 6e-4, and λ 1 = 1, following the settings in CIFAR10 and SVHN. For VAT + NS 3 L in CIFAR100, we use a shared learning rate of 3e-3 and λ 1 = 0.3, λ 2 = 0.6.As before, we created 5 splits of 10,000 labeled samples, each with a different seed, and each model is trained on a different split. Test error is reported with mean and standard deviation. NS 3 L is observed to improve 0.6% test error over Pseudo-Labeling and adding NS 3 L to VAT reduces test error slightly and achieves the best performance. This suggests that EntMin and NS 3 L boosts VAT even with little hyperparameter tuning, and perhaps should be used as default. We note that the performance of SSL methods can be sensitive to hyperparameter tuning, and minor hyperparameter tuning may improve performance greatly. Due to VAT performing additional forward and backwards passes, NS 3 L alone runs more than 2x faster than VAT.",yes,Adam,not all,"batch size, learning rate",yes,,,,
188,https://github.com/rockingdingo/context_recommendation,rockingdingo_context_recommendation.tar.gz,Infer Implicit Contexts in Real-time Online-to-O line Recommendation,@property,"gc pandas sys MultiHeadDenoisingAutoencoder _pickle __future__ os, sys tensorflow absl json collections contextual_dataset_yelp re, io os matplotlib contextual_macdae_yelp six scipy itertools codecs contextual_vae_yelp sklearn numpy shutil tarfile macDAE metric contextual_dae_yelp model_utils MultiHeadVariationalAutoencoder random cPickle",cs.IR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/rockingdingo_context_recommendation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\rockingdingo_context_recommendation.pdf,AutoEncoder,,yes,Adam,yes,"dropout, penalty cost, cosine similarity,  learning rate, epochs",no,,,,
1041,https://github.com/juntaoy/dali-full-anaphora,juntaoy_dali-full-anaphora.tar.gz,A Cluster Ranking Model for Full Anaphora Resolution,@classmethod @property,"copy sys arrau_scorer coref_model tqdm __future__ pyhocon util threading sys,json re tensorflow conll,crac modeling collections json h5py tempfile data os six functools itertools hashlib codecs coref_ops sklearn numpy time tokenization shutil math unicodedata errno operator random subprocess codecs,os metrics eval",cs.CL cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/juntaoy_dali-full-anaphora.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\juntaoy_dali-full-anaphora.pdf,,,yes,Adam,yes,"layer size, dropout, embedding size, max span width, max number clusters, token ratio, optimizer, steps, learning rate",no,,,,
794,https://github.com/wubaoyuan/CNN-FCF-CVPR-2019,wubaoyuan_CNN-FCF-CVPR-2019.tar.gz,Compressing Convolutional Neural Networks via Factorized Convolutional Filters,,functions math scipy datetime sys osqp argparse torch os numpy models time torchvision,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wubaoyuan_CNN-FCF-CVPR-2019.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wubaoyuan_CNN-FCF-CVPR-2019.pdf,,,yes,"ADMM, SGD",yes,"epochs, batch size, momentum, learning rate, weight decay, decay",no,,,,
1493,https://github.com/YaojieBao/An-Information-theoretic-Metric-of-Transferability,YaojieBao_An-Information-theoretic-Metric-of-Transferability.tar.gz,An Information-Theoretic Metric of Transferability for Task Transfer Learning,,utils PIL copy multiprocessing sys pdb lib skimage __future__ threading keras tensorflow datetime h5py data os matplotlib transforms3d importlib scipy itertools task_viz pickle sklearn argparse numpy time init_paths math random subprocess models general_utils,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/YaojieBao_An-Information-theoretic-Metric-of-Transferability.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\YaojieBao_An-Information-theoretic-Metric-of-Transferability.pdf,,,yes,SGD,,"batch size, epochs",no,,,,
425,https://github.com/dataplayer12/Fly-LSH,dataplayer12_Fly-LSH.tar.gz,Improving Similarity Search with High-dimensional Locality-sensitive Hashing,,"scipy functools tensorflow collections lshutils sklearn os numpy bokeh pickle, time",cs.LG cs.AI cs.DS stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dataplayer12_Fly-LSH.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dataplayer12_Fly-LSH.pdf,,,no,,,,no,,,,
1113,https://github.com/AIPHES/Language-Agnostic-Contextualized-Encoders,AIPHES_Language-Agnostic-Contextualized-Encoders.tar.gz,Inducing Language-Agnostic Multilingual Representations,@add_start_docstrings(,utils copy pandas csv sys multiprocessing mosestokenizer __future__ truecase re collections json transformers torch os string scipy functools itertools spacy_udpipe xlm_roberta pyemd argparse numpy math unicodedata logging roberta,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AIPHES_Language-Agnostic-Contextualized-Encoders.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AIPHES_Language-Agnostic-Contextualized-Encoders.pdf,,,no,,,,no,,,,
124,https://github.com/harvardnlp/botnet-detection,harvardnlp_botnet-detection.tar.gz,Automating botnet detection with graph neural networks,@property @staticmethod,"pandas csv multiprocessing sys background tqdm __future__ os, sys pathlib sys, os download_pcap datetime h5py torch os six torch_scatter torch_geometric dgl itertools botdet urllib pickle sklearn argparse synthesize_botnet sys,os numpy deepdish time shutil tarfile math ipaddress setuptools networkx errno random logging subprocess inspect io",cs.CR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/harvardnlp_botnet-detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\harvardnlp_botnet-detection.pdf,,,yes,Adam,yes,"learning rate, weight decay, ",no,,,,
607,https://github.com/Natlem/M-HTCN,Natlem_M-HTCN.tar.gz,Incremental Multi-Target Domain Adaptation for Object Detection with Efficient Domain Transfer,@roidb_handler.setter @dataclass @property @staticmethod @once_differentiable @ex.capture @ex.config @ex.main,PIL init_frcnn_utils copy imageio yaml sys glob pdb easydict dotenv __future__ inc_frcnn_utils ast torchvision pathlib re tensorflow uuid model datetime json distiller collections dataclasses torch os matplotlib StringIO scipy functools IPython xml itertools urllib typing pickle argparse pyximport frcnn_utils numpy time traineval_net_HTCN dtm_util thop math datasets pycocotools setuptools cffi distill_frcnn_utils experiments random cv2 roi_data_layer subprocess sacred io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Natlem_M-HTCN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Natlem_M-HTCN.pdf,,,yes,,yes,"other hyperparameter were chosen by separate hold-out validation process, learning rate , momentum, alpha,iterations, epochs, ",no,,,,
1145,https://github.com/seyonechithrananda/selfies-mirror,seyonechithrananda_selfies-mirror.tar.gz,Self-Referencing Embedded Strings (SELFIES): A 100% robust molecular string representation,"@pytest.mark.skip(reason=""covered by round-trip test"") @pytest.fixture @pytest.mark.skip(reason=""eMolecules dataset not on GitHub"") @pytest.mark.parametrize(""test_name, column_name"", datasets) @pytest.fixture() @staticmethod","faulthandler pandas yaml GrammarVAE_grammar sys pdb __future__ os, sys examples selfies rdkit h5py nltk torch os pytest matplotlib deepsmiles GrammarVAE_codes six GPlus2S itertools adjusted_selfies_fcts typing tensorboardX numpy time math data_loader setuptools random one_hot_converter",cs.LG physics.chem-ph quant-ph stat.ML cs.NE cs.LG physics.chem-ph physics.comp-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/seyonechithrananda_selfies-mirror.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\seyonechithrananda_selfies-mirror.pdf,Interpretability  Genetic Algorithms,,yes,Experimental Tuning,no,,yes,,,,
648,https://github.com/pykao/ISLES2017-mRS-prediction,pykao_ISLES2017-mRS-prediction.tar.gz,Predicting Clinical Outcome of Stroke Patients with Tractographic Feature,,utils csv multiprocessing nibabel skimage medpy SimpleITK os matplotlib scipy itertools sklearn argparse natsort xgboost paths numpy shutil math logging subprocess utils_40,eess.IV cs.LG q-bio.QM stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pykao_ISLES2017-mRS-prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pykao_ISLES2017-mRS-prediction.pdf,,,no,,,,no,,,,
1150,https://github.com/JeroenZegers/Nabu-MSSS,JeroenZegers_Nabu-MSSS.tar.gz,MULTI-SCENARIO DEEP LEARNING FOR MULTI-SPEAKER SOURCE SEPARATION,@property @abstractmethod,utils copy warnings sys skopt pdb __future__ tensorflow model json collections os nabu plotly estimators matplotlib six scipy itertools optimizer_plots sklearn numpy speaker_verification_handler time shutil abc sigproc math random cPickle,cs.LG stat.ML cs.LG stat.ML cs.LG eess.AS stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/JeroenZegers_Nabu-MSSS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\JeroenZegers_Nabu-MSSS.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Random Search  Sigmoid Activation  Tanh Activation  Long Short-Term Memory,,yes,Adam,yes,"learning rate, betas, epsilon, embedding dimension, frrequency dimension, output nodes, standard deviation",no,,,,
1186,https://github.com/SmallNana/GMAN_pytorch,SmallNana_GMAN_pytorch.tar.gz,GMAN: A Graph Multi-Attention Network for Traffic Prediction,,utils train math gensim pandas model datetime test networkx node2vec random tqdm torch argparse numpy matplotlib time,eess.SP cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/SmallNana_GMAN_pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\SmallNana_GMAN_pytorch.pdf,Multi-Attention Network,,yes,"Adam, Tuning on validation set",not all,"follows previous work, learning rate, number of attention blocks, number of attention heads, dimension of attention heads",yes,,,,
952,https://github.com/cdjhz/multigen,cdjhz_multigen.tar.gz,Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph,"@add_start_docstrings(""""""The GPT2 Model transformer with a language modeling and a multiple-choice classification @torch.no_grad() @lru_cache() @property @staticmethod @add_start_docstrings(""The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top."", @classmethod","copy csv meteor sys multiprocessing glob pdb cider fairseq tqdm regex __future__ sys, math, re threading re tensorflow collections json transformers dictionary data nltk bleu os torch configparser rouge torch_scatter scipy apex itertools functools spacy typing pickle tokenization_gpt2 argparse numpy time shutil timeit math optimization networkx random logging seq_generator subprocess modeling_gpt2 io",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cdjhz_multigen.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cdjhz_multigen.pdf,,,yes,"Hyperparameter Search, Adam",not all,"betas, epsilon, decay, model architecture, beam size",yes,,,,
1024,https://github.com/pvjosue/WaveBlocks,pvjosue_WaveBlocks.tar.gz,LEARNING TO MODEL AND CALIBRATE OPTICS VIA A DIFFERENTIABLE WAVE OPTICS SIMULATOR,@staticmethod,math scipy WaveBlocksPytorch PIL tables cmath microscopes torch torchvision numpy matplotlib time abc,eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pvjosue_WaveBlocks.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pvjosue_WaveBlocks.pdf,,,yes,Adam,no,,no,,,,
222,https://github.com/jordipons/musicnn-training,jordipons_musicnn-training.tar.gz,MUSICNN: PRE-TRAINED CONVOLUTIONAL NEURAL,,"pandas config_file, shared csv warnings sys _pickle tqdm models_midend models_frontend pathlib tensorflow pescador datetime json os config_file models_baselines pickle sklearn argparse joblib numpy time models_backend librosa random subprocess config_file, shared, train models",cs.SD cs.CL eess.AS,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jordipons_musicnn-training.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jordipons_musicnn-training.pdf,,"These models are trained with two different datasets: the MagnaTagATune dataset (the MTT of 19k training songs [3]) 1 and the Million Song Dataset (the MSD of 200k training songs [1]) 2 .Which pre-trained models are available? Although the main focus of the library is to release pre-trained musically motivated convolutional neural networks, we also provide several vgg-like models 3 (as baselines for comparison). A high-level depiction of the musicnn architecture 4 is depicted in the following figure: What's the musicnn library for? Out-of-the-box music audio tagging. From within python, one can estimate the top-10 tags by simply running: from musicnn.tagger import top_tags top_tags('music_file.mp3', model='MTT_musicnn', topN=10)From the command-line, one can also print the top-N tags on the screen (top) or save them to a file (bottom): python -m musicnn.tagger music.au --model 'MTT_musicnn' --topN 10 --print python -m musicnn.tagger audio.wav -m 'MTT_vgg' --topN 5 --save out.tags What's the musicnn library for? Music feature extraction. Out of the extractor, see the example below, one gets the output of the model (the taggram and its associated tags) and all the intermediate representations of it (we refer to those as features). The features are packed in a dictionary and, for the musicnn models, you can extract timbral, temporal, cnn1, cnn2, cnn3, mean_pool, max_pool, and penultimate features 4 . For the vgg models, you can extract pool1, pool2, pool3, pool4, and pool5 features 3 . from musicnn.extractor import extractor output = extractor(file, model='MTT_musicnn', extract_features=True) taggram, tags, features = output What's the musicnn library for? Transfer learning. Our pre-trained deep learning models can be finetuned, together with an output neural-network that acts as a classifier, to perform any other music task. To assess the utility of our embeddings, we build SVM classifiers on top of several pre-trained models that act as music feature extractors. The tools used to run this simple transfer learning experiment are accessible online: https://github.com/jordipons/sklearn-audio-transfer-learningWe report accuracy results on the test set of the GTZAN (fault-filtered) dataset, and our processing pipeline consists of ""feature extraction"" + 128 PCA + SVM. The feature extraction can be based on VGGish audioset features (77.58% accuracy), OpenL3 audioset features (74.65% accuracy), MTT_musicnn features (71.37% accuracy), MTT_vgg features (72.75% accuracy), or MSD_musicnn features (77.24% accuracy). Note that our MSD pre-trained models outperform the MTT ones. Besides, the MSD_musicnn achieves similar results than the VGGish audioset features (that are trained with a much larger dataset: 2M audios).How did you train musicnn models? The code we employed to train the models above is also accessible: But the musicnn-training framework also allows to implement other models. For example, a similar architecture than musicnn but with an attention-based output layer (instead of the temporal pooling layer) can achieve 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset -and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset. You can find further details about this new architecture online.Figure 1 :1Figure 1: The musicnn architecture: a musically motivated convolutional neural network [4, 5].",no,,,,no,,,,
1043,https://github.com/thangvubk/SCNet,thangvubk_SCNet.tar.gz,SCNet: Training Inference Sample Consistency for Instance Segmentation,"@contextlib.asynccontextmanager @auto_fp16() @patch('mmdet.datasets.XMLDataset.load_annotations', MagicMock) @force_fp32() @DETECTORS.register_module() @patch('mmdet.datasets.CocoDataset._filter_imgs', MagicMock) @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses')) @weighted_loss @ROI_EXTRACTORS.register_module() @force_fp32(apply_to=('x', 'y')) @force_fp32(apply_to=('cls_scores', 'bbox_preds')) @patch('mmdet.datasets.CityscapesDataset.load_annotations', MagicMock) @force_fp32(apply_to=('pred_maps', )) @functools.wraps(loss_func) @force_fp32(apply_to=('feats', ), out_fp16=True) @pytest.mark.parametrize('arch_name,arch,out_channels', regnet_test_data) @pytest.mark.parametrize( @patch('mmdet.datasets.CityscapesDataset._filter_imgs', MagicMock) @auto_fp16(apply_to=('x', 'y')) @force_fp32(apply_to=('bbox_pred', )) @pytest.mark.parametrize('dataset', @HEADS.register_module() @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'iou_preds')) @patch('mmdet.datasets.CustomDataset._filter_imgs', MagicMock) @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'bbox_preds_refine')) @patch('mmdet.models.utils.MultiheadAttention.forward', @PIPELINES.register_module() @LOSSES.register_module() @property @staticmethod @pytest.mark.skipif( @pytest.mark.parametrize('EvalHookParam', (EvalHook, DistEvalHook)) @BBOX_ASSIGNERS.register_module() @MATCH_COST.register_module() @pytest.mark.parametrize('EvalHookCls', (EvalHook, DistEvalHook)) @pytest.mark.parametrize('cfg_file', [ @HOOKS.register_module() @abstractmethod @patch('mmdet.datasets.CocoDataset.load_annotations', MagicMock) @force_fp32(apply_to=('mask_pred', )) @force_fp32(apply_to=('cls_score', 'bbox_pred')) @POSITIONAL_ENCODING.register_module() @patch('mmdet.apis.single_gpu_test', MagicMock) @SHARED_HEADS.register_module() @auto_fp16(apply_to=('x', )) @force_fp32(apply_to=('pred', )) @force_fp32(apply_to=('x', 'y'), out_fp16=True) @patch('mmdet.datasets.XMLDataset._filter_imgs', MagicMock) @force_fp32( @force_fp32(apply_to=('segm_pred', )) @auto_fp16(apply_to=('img', )) @TRANSFORMER.register_module() @classmethod @pytest.mark.parametrize('classes, expected_length', [(['bus'], 2), @force_fp32(apply_to=('mask_iou_pred', )) @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'coeff_preds')) @BBOX_CODERS.register_module() @IOU_CALCULATORS.register_module() @patch('mmdet.models.utils.TransformerDecoderLayer.forward', @patch('mmdet.models.utils.FFN.forward', _ffn_forward) @force_fp32(apply_to=('bbox_preds', )) @patch('mmdet.apis.multi_gpu_test', MagicMock) @contextlib.contextmanager @patch('mmdet.models.utils.TransformerEncoderLayer.forward', @BACKBONES.register_module() @DATASETS.register_module() @mock.create_autospec @NECKS.register_module() @BBOX_SAMPLERS.register_module() @patch('mmdet.datasets.CustomDataset.load_annotations', MagicMock) @force_fp32(apply_to=('x', )) @force_fp32(apply_to=('all_cls_scores_list', 'all_bbox_preds_list')) @ANCHOR_GENERATORS.register_module() @auto_fp16(apply_to=('x', 'y'), out_fp32=True)",imagecorruptions PIL copy contextlib warnings sys multiprocessing glob paste_masks_in_image test_forward tools onnx __future__ platform torchvision mmcv pathlib re asyncio json collections unittest resource current tempfile torch os pytest matplotlib six scipy functools itertools xml seaborn bisect terminaltables urllib pickle typing sklearn argparse cityscapesscripts mmdet lvis asynctest numpy time shutil abc math kwarray pycocotools setuptools albumentations cv2 random logging subprocess onnxruntime inspect,cs.CV cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/thangvubk_SCNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\thangvubk_SCNet.pdf,SCNet  Softmax  Convolution  RoIAlign  Mask R-CNN  Cascade Mask R-CNN,,yes,SGD,not all,"learning rate, epochs, decay, ",no,,,,
519,https://github.com/willi-menapace/acids-clustering-domain-shift,willi-menapace_acids-clustering-domain-shift.tar.gz,Invariant Information Clustering for Unsupervised Image Classification and Segmentation,@staticmethod,tensorlog PIL multiprocessing sys __future__ MulticoreTSNE torchvision pathlib datetime json torch os matplotlib scipy itertools pickle sklearn argparse tensorboardX numpy shutil codebase random statistics mpl_toolkits,cs.CV cs.LG cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/willi-menapace_acids-clustering-domain-shift.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\willi-menapace_acids-clustering-domain-shift.pdf,,,yes,Adam,no,learning rate,no,,,,
72,https://github.com/hengyicai/ContrastiveLearning4Dialogue,hengyicai_ContrastiveLearning4Dialogue.tar.gz,Group-wise Contrastive Learning for Neural Dialogue Generation,"@skipUnlessBPE @testing_utils.retry(ntries=3, log_retry=True) @lru_cache() @wraps(testfn) @lru_cache(maxsize=10240) @property @staticmethod @testing_utils.skipIfCircleCI @unittest.skip(""Disabled due to LSTM CUDNN bug. (#2436)"") @abstractmethod image_id_to_image_path() must be implemented in subclass @abstractmethod @testing_utils.skipUnlessTorch14 @unittest.skipIf(SKIP_TESTS, ""Missing  Tfidf dependencies."") @testing_utils.retry(ntries=3) @testing_utils.skipUnlessTorch @classmethod @unittest.skipIf(APEX_AVAILABLE, ""Apex is installed, can't test its absence."") @testing_utils.skipIfGPU @contextlib.contextmanager @testing_utils.retry() @testing_utils.skipUnlessGPU @skipUnlessGPU @functools.total_ordering @lru_cache(maxsize=1)",copy csv git multiprocessing warnings sh ctypes torchvision threading re h5py torch importlib rouge constants spacy codecs pickle boto3 numbers tensorboardX numpy extract_and_save_personas traceback random inspect websocket requests yaml signal pdb task_configs __future__ pathlib unittest queue botocore cProfile nltk os pkgutil selenium socket functools hashlib ssl mdx_gfm time abc getpass shutil worlds difflib tornado operator recommonmark subprocess pandas sys regex tqdm tokenizers gensim asyncio parlai_internal datetime tempfile sphinx_rtd_theme itertools torchtext typing shlex subword_nmt joblib setuptools errno zipfile io docformatter PIL pstats contextlib glob markdown fairseq parlai qhoptim base64 platform locale uuid collections json websocket_server concurrent sqlite3 apex projects agents builtins urllib http argparse task_config netrc math logging chromedriver_installer,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hengyicai_ContrastiveLearning4Dialogue.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hengyicai_ContrastiveLearning4Dialogue.pdf,,,yes,Adam Optimizer,yes,"hidden size, group size, learning rate, batch size",no,,,,
367,https://github.com/mjsumpter/remod,mjsumpter_remod.tar.gz,REMOD: Relation Extraction for Modeling Online Discourse,,utils requests pandas csv sys tqdm classify gensim pathlib keras tensorflow re datetime json collections h5py node2vec enum os preproc matplotlib seaborn features pickle typing sklearn argparse joblib numpy config nodevectors shutil math rdflib networkx logging ratelimiter,cs.SI cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mjsumpter_remod.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mjsumpter_remod.pdf,,,no,,,,no,,,,
1295,https://github.com/mehrdadkhani/MMNet,mehrdadkhani_MMNet.tar.gz,Adaptive Neural Signal Detection for Massive MIMO,,utils genSolver loss multiprocessing parser tf_session linear tensorflow json os cvxpy matplotlib gurobipy exp itertools pickle argparse numpy random subprocess sample_generator denoiser detector layer,eess.SP cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/mehrdadkhani_MMNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\mehrdadkhani_MMNet.pdf,,,yes,Adam,not all,"learning rate, batch size",no,,,,
252,https://github.com/dgiova/bert-lm-transferability,dgiova_bert-lm-transferability.tar.gz,Investigating Transferability in Pretrained Language Models,,copy pandas glob tqdm re json collections transformers torch os matplotlib scipy apex itertools ptvsd argparse tensorboardX numpy pytorchtools random logging,cs.CL cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dgiova_bert-lm-transferability.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dgiova_bert-lm-transferability.pdf,Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,"IntroductionDespite the striking success of transfer learning in NLP, remarkably little is understood about how these pretrained models improve downstream task performance. Recent work on understanding deep NLP models has centered on probing, a methodology that involves training classifiers for different tasks on model representations (Alain and Bengio, 2016;Conneau et al., 2018;Hupkes et al., 2018;Liu et al., 2019;Tenney et al., 2019a,b;Goldberg, 2019;Hewitt and Manning, 2019). While probing aims to uncover what a network has already learned, a major goal of machine learning is transfer: systems that build upon what they have learned to expand what they can learn. Given that most † atamkin@stanford.edu recent models are updated end-to-end during finetuning (e.g. Devlin et al., 2019;Howard and Ruder, 2018;Radford et al., 2019), it is unclear how, or even whether, the knowledge uncovered by probing contributes to these models' transfer learning success.In a sense, probing can be seen as quantifying the transferability of representations from one task to another, as it measures how well a simple model (e.g., a softmax classifier) can perform the second task using only features from a model trained on the first. However, when pretrained models are finetuned end-to-end on a downstream task, what is transferred is not the features from each layer of the pretrained model, but its parameters, which define a sequence of functions for processing representations. Critically, these functions and their interactions may shift considerably during training, potentially enabling higher performance despite not initially extracting features correlated with this task. We refer to this phenomenon of how layer parameters from one task can help transfer learning on another task as transferability of parameters.In this work, we investigate a methodology for measuring the transferability of different layer parameters in a pretrained language model to different transfer tasks, using BERT (Devlin et al., 2019) as our subject of analysis. Our methods, described more fully in Section 2 and Figure 1, involve partially reinitializing BERT: replacing different layers with random weights and then observing the change in task performance after finetuning the entire model end-to-end. Compared to possible alternatives like freezing parts of the network or removing layers, partial reinitialization enables fairer comparisons by keeping the network's architecture and capacity constant between trials, changing only the parameters at initialization. Through experiments across different layers, tasks, and dataset sizes, this approach enables us to shed light on multiple dimensions of the transfer learning process: Are the early layers of the network more important than later ones for transfer learning? Do individual layers become more or less critical depending on the task or amount of finetuning data? Does the position of a particular layer within the network matter, or do its parameters aid optimization regardless of where they are in the network?We find that when finetuning on a new task:1. Transferability of BERT layers varies dramatically depending on the amount of finetuning data available. Thus, claims that certain layers are universally responsible or important for learning certain linguistic tasks should be treated with caution. (Figure 2)2. Transferability of BERT layers is not in general predicted by the layer's probing performance for that task. However, as finetuning dataset size decreases, the two quantities exhibit a greater correspondence. (Figure 2, dashed lines)3. Even holding dataset size constant, the most transferable BERT layers differ by task: for some tasks, only the early layers are important, while for others the benefits are more distributed across layers. (Figure 3) 4. Reordering the pretrained BERT layers before finetuning decreases downstream accuracy significantly, confirming that pretraining does not simply provide better-initialized individual layers; instead, transferability through learned interactions across layers is crucial to the success of finetuning. (Figure 4)2 How many pretrained layers are necessary for finetuning?Our first set of experiments aims to uncover how many pretrained layers are sufficient for accurate learning of a downstream task. To do this, we perform a series of incremental reinitialization experiments, where we reinitialize all layers after the kth layer of BERT-Base, for values k ∈ {0, 1, . . . 12}, replacing them with random weights. We then finetune the entire model end-toend on the target task. Note that k = 0 corresponds to a BERT model with all layers reinitialized, while k = 12 is the original BERT model. We do not reinitialize the BERT word embeddings. As BERT uses residual connections (He et al., 2016) around layers, the model can simply learn to ignore any of the reinitialized layers if they are not helpful during finetuning.We use the BERT-Base uncased model, implemented in PyTorch (Paszke et al., 2019) via the Transformers library (Wolf et al., 2019). We finetune the network using Adam (Kingma and Ba, 2015), with a batch size of 8, a learning rate of 2e-5, and default parameters otherwise. More de-tails about reinitialization, training, statistical significance, and other methodological choices can be found in the Appendix. We conduct our experiments on three English language tasks from the GLUE benchmark, spanning the domains of sentiment, reasoning, and syntax (Wang et al., 2018):SST-2 Stanford Sentiment Treebank involves binary classification of a single sentence from a movie review as positive or negative (Socher et al., 2013).QNLI Question Natural Language Inference is a binary classification task derived from SQuAD (Rajpurkar et al., 2016;Wang et al., 2018). The task requires determining whether for a given (QUESTION, ANSWER) pair the QUESTION is answered by the ANSWER.CoLA The Corpus of Linguistic Acceptability is a binary classification task that requires determining whether a single sentence is linguistically acceptable (Warstadt et al., 2019).Because pretraining appears to be especially helpful in the small-data regime (Peters et al., 2018), it is crucial to isolate task-specific effects from data quantity effects by controlling for finetuning dataset size. To do this, we perform our incremental reinitializations on randomly-sampled subsets of the data: 500, 5k, and 50k examples (excluding 50k for CoLA, which contains only 8.5k examples). The 5k subset size is then used as the default for our other experiments. To ensure that an unrepresentative sample is not chosen by chance, we run multiple trials with different subsamples. Confidence intervals produced through multiple trials also demonstrate that trends hold regardless of intrinsic task variability.While similar reinitialization schemes have been explored by Yosinski et al. (2014); Raghu et al. (2019) in computer vision and briefly by Radford et al. (2019) in an NLP context, none investigate these data quantity-and task-specific effects.Figure 2 shows the results of our incremental reinitialization experiments. These results show that the transferability of a BERT layer varies dramatically based on the finetuning dataset size. Across all but the 500 example trials of SST-2, a more specific trend holds: earlier layers provide more of an improvement on finetuning performance when the finetuning dataset is large. This trend suggests that larger finetuning datasets may enable the network to learn a substitute for the parameters in the middle and later layers. In contrast, smaller datasets may leave the network reliant on existing feature processing in those layers. However, across all tasks and dataset sizes, it is clear that the pretrained parameters by themselves do not determine the impact they will have on finetuning performance: instead, a more complex interaction occurs between the parameters, optimizer, and the available data.3 Does probing predict layer transferability?What is the relationship between transferability of representations, measured by probing, and transferability of parameters, measured by partial reinitialization? To compare, we conduct probing experiments for our finetuning tasks on each layer of the pretrained BERT model. Our probing model averages each layer's hidden states, then passes the pooled representation through a linear layer and softmax to produce probabilities for each class. These task-specific components are identical to those in our reinitialization experiments; however, we keep the BERT model's parameters frozen when training our probes.Our results, presented in Figure 2 (dashed lines), show a significant difference between the layers with the highest probing performance and reinitialization curves for the data-rich settings (darkest solid lines). For example, the probing accuracy on all tasks is near chance for the first six layers. Despite this, these early layer parameters exhibit significant transferability to the finetuning tasks: preserving them while reinitializing all other layers enables large gains in finetuning accuracy across tasks. Interestingly, however, we observe that the smallest-data regime's curves are much more similar to the probing curves across all tasks than the larger-data regimes. Smaller finetuning datasets enable fewer updates to the network before overfitting occurs; thus, it may be that finetuning interpolates between the extremes of probing (no data) and fully-supervised learning (enough data to completely overwrite the pretrained parameters). We leave a more in-depth exploration of this connection to future work.4 Which layers are most useful for finetuning?While the incremental reinitializations measure each BERT layer's incremental effect on transfer learning, they do not assess each layer's contribution in isolation, relative to either the full BERT model or an entirely reinitialized model. Measuring this requires eliminating the number of pretrained layers as a possible confounder. To do so, we conduct a series of localized reinitialization experiments, where we take all blocks of three consecutive layers and either 1) reinitialize those layers or 2) preserve those layers while reinitializing the others in the network. 1 These localized reinitializations help determine extent to which BERT's different layers are either necessary (performance decreases when they are removed) or sufficient (performance is higher than random initialization when they are kept) for a specific level of performance. Again, BERT's residual connections permit the model to ignore reinitialized layers' outputs if they harm finetuning performance. These results, shown in Figure 3, demonstrate that the earlier layers appear to be generally more helpful for finetuning relative to the later layers, even when controlling for the amount of finetuning data. However, there are strong task-specific effects: SST-2 appears to be particularly damaged by removing middle layers, while the effects on CoLA are distributed more uniformly. The effects 1 See the Appendix for more discussion and experiments where only one layer is reinitialized. on QNLI appear to be concentrated almost entirely in the first four layers of BERT-suggesting opportunities for future work on whether sparsity of this sort indicates the presence of easy-to-extract features correlated with the task label. These results support the hypothesis that different kinds of feature processing learned during BERT pretraining are helpful for different finetuning tasks, and provide a new way to gauge similarity between different tasks. I.6 HyperparametersWe performed one experiment with a 5x learning rate and implemented early stopping to choose the number of epochs for the probing experiments.For batch size and learning rate, we kept the default parameters for all tasks:• Learning rate: 2e-5• Batch size: 8Figure 1 :1Figure 1: The three experiments we explore. Lighter shades indicate randomly reinitialized layers, while darker shades indicate layers with BERT parameters. For layer permutations, all layers hold BERT parameters, what changes between trials is their order. In all three experiments, the entire model is finetuned end-toend on the GLUE task.",yes,Adam,yes,"batch size, learning rate, adam default parameters otherwise, epochs",no,,,,
654,https://github.com/correlllab/nn4mc,correlllab_nn4mc.tar.gz,Embedded Neural Networks for Robot Autonomy,,"keras tensorflow sys, os pandas sys collections __future__ numpy matplotlib",cs.RO eess.SP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/correlllab_nn4mc.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\correlllab_nn4mc.pdf,,,no,,,,no,,,,
1173,https://github.com/stefanosantaris/DMTKG,stefanosantaris_DMTKG.tar.gz,Distill2Vec: Dynamic Graph Representation Learning with Knowledge Distillation,,utils scipy tensorflow networkx random pickle trimesh argparse os trainer numpy matplotlib time,cs.LG cs.AI cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stefanosantaris_DMTKG.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stefanosantaris_DMTKG.pdf,Knowledge Distillation  Knowledge Distillation,"|B. Distill2Vec-S -Student ModelTo reduce the high online inference latency of the teacher model Distill2Vec-T , we train a compact student model Distill2Vec-S on the online graph snapshots G S . For each time step t = m + 1, . . . , T , the student model Distill2Vec-S computes the structural node representations C t (u), based on Equation 6. To capture the graph evolution over the last l consecutive historical graph snapshots {G S t−l , . . . , G S t }, Distill2Vec-S computes the temporal node representations D t (u) according to Equation 7. The final node representations H S t (u) are calculated based on Equation 8. We employ a knowledge distillation strategy on the student model Distill2Vec-S to transfer the knowledge of the pretrained teacher model Distill2Vec-T . In practice, the student model Distill2Vec-S adopts the following distillation loss function L D during the online training process:min H S L D = (1 − γ)L S + γL F (10)where L S is the binary cross-entropy loss that measures the accuracy error of the student model on the online data, andL F = KL(H S t (u) || H T t (u))is the Kullback-Leibler (KL) divergence between the node embeddings H S t (u) and H T t (u) for each node u ∈ V t [38]. This means that the student model Distill2Vec-S mimics the teacher model Distill2Vec-T during online training, to achieve similar performance with low number of model parameters [19], [22], [23]. Hyperparameter γ ∈ [0, 1] balances the distillation process and the prediction error of the student model Distill2Vec-S on the online data. High values of γ reflect on generating node embeddings H S t (u) similar to the node embeddings H T t (u) of the student model Distill2Vec-T . Instead, low values of γ emphasize on the prediction errors of the student model Distill2Vec-S. This allows the student model to overcome any bias introduced by the teacher and achieve similar or better performance than Distill2Vec-T [19], [20], [22], [23].IV. EXPERIMENTS|",yes,"Cross-Validation, Experimental Tuning, Adam",not all,"Embedding, Window, Heads, learning rate, epochs, ",yes,,,,
696,https://github.com/cruvadom/Convolutional-RNN,cruvadom_Convolutional-RNN.tar.gz,Convolutional RNN: an Enhanced Model for Extracting Features from Sequential Data,,tensorflow,stat.ML cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cruvadom_Convolutional-RNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cruvadom_Convolutional-RNN.pdf,,,yes,Adam,not all,"learning rate, betas",no,,,,
1161,https://github.com/maxzvyagin/crossedwires,maxzvyagin_crossedwires.tar.gz,CrossedWires: A Dataset of Syntactically Equivalent but Semantically Disparate Deep Learning Models,,requests pathlib cross_framework_hpo tensorflow pandas setuptools pickle torch os crossedwires torchvision io,cs.LG cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/maxzvyagin_crossedwires.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\maxzvyagin_crossedwires.pdf,,,yes,"Adam, scikit-optimize, HyperSpace HPO library, Ray Tune SkOptSearch",yes,"learning rate, batch size, epochs",yes,,,,
257,https://github.com/stanleyjzheng/masseyhacksvii,stanleyjzheng_masseyhacksvii.tar.gz,Identifying Melanoma Images using EfficientNet Ensemble: Winning Solution to the SIIM-ISIC Melanoma Classification Challenge,"@app.route('/skintonepredict', methods=['POST']) @app.route('/') @app.route('/predict') @st.cache @app.route('/skintone')",utils PIL pandas sys skimage onnx streamlit tensorflow json src torch os geffnet warmup_scheduler onnx_tf sklearn numpy flask time werkzeug math deepface albumentations cv2 random io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stanleyjzheng_masseyhacksvii.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stanleyjzheng_masseyhacksvii.pdf,,,yes,,yes,"epochs, learning rate, batch size",no,,,,
1431,https://github.com/IssamLaradji/CBStyling,IssamLaradji_CBStyling.tar.gz,Class-Based Styling: Real-time Localized Style Transfer with Semantic Segmentation,@torch.no_grad(),"utils dabnet PIL transformer_net pandas cv2, glob, re cv2 tqdm argparse torch os numpy matplotlib torchvision pylab",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/IssamLaradji_CBStyling.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\IssamLaradji_CBStyling.pdf,,,no,,,,no,,,,
136,https://github.com/Ma-Lab-Berkeley/MCR2,Ma-Lab-Berkeley_MCR2.tar.gz,ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction,@ (test_features.numpy()).T @ (test_features.numpy() - mean).T,utils PIL loss pandas warnings sys glob generate tqdm torchvision cluster evaluate json torch os matplotlib progressbar train_func scipy itertools sklearn argparse numpy time math tarfile corrupt cv2 logging augmentloader architectures,cs.LG cs.CV cs.IT math.IT stat.ML cs.LG cs.CV cs.IT math.IT stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Ma-Lab-Berkeley_MCR2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Ma-Lab-Berkeley_MCR2.pdf,Convolution,,yes,SGD,yes,"batch size, learning rate, momentum, decay, epochs",no,,,,
583,https://github.com/UKPLab/linspector-web,UKPLab_linspector-web.tar.gz,LINSPECTOR WEB: A Multilingual Probing Suite for Word Representations,"@tag('fast', 'core') @receiver(post_delete, sender=Model) @unique @tag('slow', 'core', 'nn', 'contrastive') @receiver(pre_delete, sender=Model) @TokenEmbedder.register('sentence_embedding') @receiver(post_delete, sender=Epoch) @shared_task @register.filter @tag('fast', 'core', 'fixtures') @tag('core') @tag('core', 'nn', 'contrastive') @tag('slow', 'core', 'nn') @tag('fast') @tag('core', 'static') @tag('slow', 'nn', 'consistency') @tag('slow', 'core', 'nn', 'static') @tag('core', 'nn')",copy sys django_celery_results __future__ linspector ast uuid collections json transformers enum tempfile torch os codecs inspector urllib numpy time abc math random celery django inspect allennlp zipfile,cs.CL cs.CL cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/UKPLab_linspector-web.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\UKPLab_linspector-web.pdf,,,no,,,,no,,,,
51,https://github.com/gmum/3d-point-clouds-autocomplete,gmum_3d-point-clouds-autocomplete.tar.gz,HyperPocket: Generative Point Cloud Completion,@classmethod @property @staticmethod @ray.remote,utils requests pandas csv warnings sys glob pptk tqdm re core model datetime json collections h5py torch os ray matplotlib scipy itertools urllib typing sklearn argparse numpy shutil datasets setuptools random logging losses trimesh mpl_toolkits zipfile,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gmum_3d-point-clouds-autocomplete.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gmum_3d-point-clouds-autocomplete.pdf,HyperNetwork,,yes,Adam Optimizer,yes,"learning rate, epochs",no,,,,
161,https://github.com/firojalam/domain-adaptation,firojalam_domain-adaptation.tar.gz,Domain Adaptation with Adversarial Training and Graph Embeddings,,"copy lasagne pandas graph_knn_train warnings sys HTMLParser smart_open ind_model glob performance layers imblearn __future__ trans_model ind_model_domain_adaptation_labeled gensim keras re tensorflow data_process datetime collections cnn_filter nltk os string six scipy optparse shlex sklearn argparse ind_model_cnn numpy performance_semeval base_model time aidrtokenize math cnn os, errno ind_model_domain_adaptation dateutil operator theano random logging subprocess cPickle",cs.LG stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/firojalam_domain-adaptation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\firojalam_domain-adaptation.pdf,,"Domain adaptation with adversarial trainingimproves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1.3. When the network combines domain adversarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.Our source code is available on Github 1 and the data is available on CrisisNLP 2 .The rest of the paper is organized as follows. In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph embedding learning. In Section 3, we present the experimental setup and baselines. The results and analysis are presented in Section 4. In Section 5, we present the works relevant to this study. Finally, conclusions appear in Section 6. Settings for Domain AdaptationTo set a baseline for the domain adaptation experiments, we train a CNN model (i.e., shared components followed by the supervised part in Figure 1) on one event (source) and test it on another event (target). We call this as transfer baseline.To assess the performance of our domain adaptation technique alone, we exclude the semisupervised component from the network. We train and evaluate models with this network configuration using different source and target domains.Finally, we integrate all the components of the network as shown in Figure 1 and run domain adaptation experiments using different source and target domains. In all our domain adaptation experiments, we only use unlabeled instances from the target domain. In domain adaption literature, this is known as unsupervised adaptation. Table 44, we present domain adaptation results.The first block shows event-specific (i.e., train andtest on the same event) results for the supervisedCNN model. These results set the upper boundfor our domain adaptation methods. The trans-fer baselines are shown in the next block, wherewe train a CNN model in one domain and testit on a different domain. Then, the third blockshows the results for the domain adversarial ap-proach without the semi-supervised loss. Theseresults show the importance of domain adversarialcomponent. After that, the fourth block presentsthe performance of the model trained with graph",yes,"SGD, AdaDelta",yes,"learning rate, epochs, droupout",no,,,,
1141,https://github.com/xuuuluuu/SynLSTM-for-NER,xuuuluuu_SynLSTM-for-NER.tar.gz,Better Feature Integration for Named Entity Recognition,,bert_serving tqdm termcolor re model collections transformers enum torch os common typing pickle reader argparse numpy time config math random,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xuuuluuu_SynLSTM-for-NER.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xuuuluuu_SynLSTM-for-NER.pdf,Graph Convolutional Networks  Tanh Activation  Sigmoid Activation  Long Short-Term Memory,,yes,"SGD, Experimental Tuning",not all,"batch size, l2 regularization, learning rate, epochs, hidden size, decay,",yes,,,,
810,https://github.com/umarKarim/cou_stereo,umarKarim_cou_stereo.tar.gz,"Towards Continual, Online, Unsupervised Depth",,PIL pandas dir_dataset regularization __future__ dir_options torchvision warper test_directory collections torch os matplotlib importlib online_train Loss test argparse numpy time resnet_encoder cv2 evaluation,cs.CV cs.AI eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/umarKarim_cou_stereo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\umarKarim_cou_stereo.pdf,,,yes,Adam,yes,"betas, learning rate, batch size, ",no,,,,
341,https://github.com/hyu-cvlab/omnimvs-pytorch,hyu-cvlab_omnimvs-pytorch.tar.gz,OmniMVS: End-to-End Learning for Omnidirectional Stereo Matching,,utils scipy yaml sys multiprocessing dataset traceback tifffile logging skimage easydict os torch mpl_toolkits numpy matplotlib time module,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hyu-cvlab_omnimvs-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hyu-cvlab_omnimvs-pytorch.pdf,,"|IntroductionImage-based depth estimation, including stereo and multi-view dense reconstruction, has been widely studied in the computer vision community for decades. In conventional two-view stereo matching, deep learning methods [12,4] have achieved drastic performance improvement recently. Besides, there are strong needs on omnidirectional or wide FOV depth sensing in autonomous driving and robot navigation to sense the obstacles and surrounding structures. Human drivers watch all directions, not just the front, and holonomic robots need to sense all directions to move freely. However, conventional stereo rigs and algorithms cannot capture or estimate ultra wide FOV (>180 • ) depth maps. Merging depth maps from multiple conventional stereo pairs can be one possibility, but the useful global context information cannot be propagated between * Corresponding author.the pairs and there might be a discontinuity at the seam.Recently, several works have been proposed for the omnidirectional stereo using multiple cameras [29], reflective mirrors [25], or wide FOV fisheye lenses [6]. Nevertheless, very few works utilize deep neural networks for the omnidirectional stereo. In SweepNet [30] a convolutional neural network (CNN) is used to compute the matching costs of equirectangular image pairs warped from the ultrawide FOV images. The result cost volume is then refined by cost aggregation (e.g., Semi-global matching [10]), which is a commonly used approach in conventional stereo matching [5,32,15]. However, such an approach may not be optimal in the wide-baseline omnidirectional setup since the occlusions are more frequent and heavier, and there can be multiple true matches for one ray (Fig. 2b). On the other hand, recent methods for conventional stereo matching such as GC-Net [14] and PSMNet [4] employ the end-to-end deep learning without separate cost aggregation, and achieve better performance compared to the traditional pipeline [32,8,26].We introduce a novel end-to-end deep neural network for estimating omnidirectional depth from multi-view fisheye images. It consists of three blocks, unary feature extraction, spherical sweeping, and cost volume computation as illustrated in Fig. 1. The deep features built from the input images are warped to spherical feature maps for all hypothesized depths (spherical sweeping). Then a 4D feature volume is formed by concatenating the spherical feature maps from all views so that the correlation between multiple views can be learned efficiently. Finally, the 3D encoder-decoder block computes a regularized cost volume in consideration of the global context for omnidirectional depth estimation. While the proposed algorithm can handle various camera layouts, we choose the rig in Fig. 2a because it provides good coverage while it can be easily adopted in the existing vehicles.Large-scale data with sufficient quantity, quality, and diversity are essential to train robust deep neural networks. Nonetheless, acquiring highly accurate dense depth measurements in real-world is very difficult due to the limitations of available depth sensors. Recent works [16,21] have proposed to use realistically rendered synthetic images with ground truth depth maps for conventional stereo methods. Cityscape synthetic datasets in [30] are the only available datasets for the omnidirectional multi-view setup, but the number of data is not enough to train a large network, and they are limited to the outdoor driving scenes with few objects. In this work, we present complementary large-scale synthetic datasets in both indoor and outdoor environments with various objects.The contributions of this paper are summarized as:(i) We propose a novel end-to-end deep learning model to estimate an omnidirectional depth from multiple fisheye cameras. The proposed model directly projects feature maps to the predefined global spheres, combined with the 3D encoder-decoder block enabling to utilize global contexts for computing and regularizing the matching cost.(ii) We offer large-scale synthetic datasets for the omnidirectional depth estimation. The datasets consist of multiple input fisheye images with corresponding omnidirectional depth maps. The experiments on the realworld environments show that our datasets successfully train our network.(iii) We experimentally show that the proposed method outperforms the previous multi-stage methods. We also show that our approaches perform favorably compared to the omnidirectional versions of the state-of-the-art conventional stereo methods through extensive experiments. Related WorkDeep Learning-based Methods for Conventional Stereo Conventional stereo setup assumes a rectified image pair as the input. Most traditional stereo algorithms before deep learning follow two steps: matching cost computation and cost aggregation. As summarized in Hirschmuller et al. [11], sum of absolute differences, filter-based cost, mutual information, or normalized cross-correlation are used to compute the matching cost, and for cost aggregation, local correlation-based methods, global graph cuts [2], and semi-global matching (SGM) [10] are used. Among them, SGM [10] is widely used because of its high accuracy and low computational overhead. Recently, deep learning approaches report much improved performance in the stereo matching. Zagoruyko et al. [31] propose a CNN-based similarity measurement for image patch pairs. Similarly, Zbontar and LeCun [32] introduce MC-CNN that computes matching costs from small image patch pairs. Meanwhile, several papers focus on the cost aggregation or disparity refinement. Güney and Geiger [8] introduce Displets resolving matching ambiguities on reflection or textureless surfaces using objects' 3D models. Seki and Pollefeys [26] propose SGM-Net which predicts the smoothness penalties in SGM [10].On the other hand, there have been several works on endto-end modeling of the stereo pipeline. Kendall et al. [14] propose GC-Net which regularizes the matching cost by 3D convolutional encoder-decoder architecture, and performs disparity regression by the softargmin. Further, PSMNet by Chang and Chen [4] consists of spatial pyramid pooling modules for larger receptive field and multiply stacked 3D encoder-decoder architecture for learning more context information. Also, Mayer et al. [16] develop DispNet, an end-to-end network using correlation layers for disparity estimation, and it is further extended by Pang et al. [18] (CRL) and Ilg et al. [12] (DispNet-CSS). These end-to-end networks have achieved better performance compared to the conventional multi-stage methods. Synthetic Datasets for Learning Stereo Matching For successful training of deep neural networks, an adequate large-scale dataset is essential. In stereo depth estimation, Middlebury [24,11,23] and KITTI datasets [7,17] are most widely used. These databases are faithfully reflecting the real world, but capturing the ground truth depth requires complex calibration and has limited coverage, and more importantly, the number of images is often insufficient for training large networks.Nowadays synthetically rendered datasets are used to complement the real datasets. Mayer et al. [16] introduce a large scale dataset for disparity, optical flow, and scene flow estimation. The proposed dataset consists of 2K scene images and dense disparity maps generated via rendering, which is 10× larger than KITTI [17]. Ritcher et al. [20] provide fully annotated training data by simulating a living city in a realistic 3D game world. For semantic scene completion, SUNGC dataset [27] contains 45K synthetic indoor scenes of 400K rooms and 5M objects with depth and voxel maps. However, almost all datasets use single or stereo pinhole camera models with limited FOV, and there are very few datasets for omnidirectional stereo. Omnidirectional Depth Estimation Various algorithms and systems have been proposed for the omnidirectional depth estimation [6,25,29], but very few use deep neural networks. Schönbein et al. [25] use two horizontally mounted 360°-FOV catadioptric cameras, and estimate the disparity from rectified omnidirectional images. Using two vertically mounted ultra-wide FOV fisheye cameras, Gao and Shen [6] estimate omnidirectional depth by projecting the input images onto four predefined planes. Im et al. [13] propose a temporal stereo algorithm that estimates an all around depth of the static scene from a short motion clip. Meanwhile, purely learning-based approaches Zioulis et al. [33] and Payen et al. [19] have been proposed estimating a 360°depth from a single panoramic image.Recently, Won et al. [30] propose SweepNet with a multi-camera rig system for the omnidirectional stereo. They warp the input fisheye images onto the concentric global spheres, and SweepNet computes matching costs from the warped spherical images pair. Then, the cost volume is refined by applying SGM [10]. However, SGM cannot handle the multiple true matches occurring in such global sweeping approaches as in Fig. 2b.In this paper, we present the first end-to-end deep neural network for the omnidirectional stereo and large-scale datasets to train the network. As shown in the experiments, the proposed method achieves better performance compared to the previous methods and performs favorably in the realworld environment with our new datasets. Omnidirectional Multi-view StereoIn this section, we introduce the multi-fisheye camera rig and the spherical sweeping method, and then describe the proposed end-to-end network architecture for the omnidirectional stereo depth estimation. As shown in Fig. 1 our algorithm has three stages, unary feature extraction, spherical sweeping, and cost volume computation. In the following subsections, the individual stages are described in detail. Spherical sweepingThe rig consists of multiple fisheye cameras mounted at fixed locations. Unlike the conventional stereo which uses the reference camera's coordinate system, we use the rig coordinate system for depth representation, as in [30]. For convenience we set the y-axis to be perpendicular to the plane closest to all camera centers, and the origin at the center of the projected camera centers. A unit ray p for the spherical coordinate θ, φ corresponds to p(θ, φ) = (cos(φ) cos(θ), sin(φ), cos(φ) sin(θ)) . With the intrinsic and extrinsic parameters of the i-th camera (calibrated using [22,28,1]), the image pixel coordinate x i for a 3D point X can be written as a projection functionΠ i ; x i = Π i (X).Thus a point at θ, φ on the sphere of radius ρ is projected to Π i (ρ p(θ, φ)) in the i-th fisheye image.Spherical sweeping generates a series of spheres with different radii and builds the spherical images of each input image. Similar to plane sweeping in conventional stereo, the inverse radius d n is swept from 0 to d max , where 1/d max is the minimum depth to be considered and N is the number of spheres. The pixel value of the equirectangular spherical image warped onto n-th sphere is determined asS n,i (θ, φ) = I i (Π i (p(θ, φ)/d n )),(1)where I i is the input fisheye image captured by i-th camera and d n is the n-th inverse depth. Network ArchitectureThe architecture of the proposed network is detailed in Table 1. The input of the network is a set of grayscale fisheye images. We use the residual blocks [9] for the unary feature extraction, and the dilated convolution for the larger receptive field. The output feature map size is half (r = 2) of the input image. Each feature map is aligned by the spherical sweeping (Sec. 3.2), and transferred to the spherical fea- Unary feature extractionInput ture by a 3 × 3 convolution. The spherical feature maps are concatenated and fused into the 4D initial cost volume by a 3 × 3 × 3 convolution. We then use the 3D encoder-decoder architecture [14] to refine and regularize the cost volume using the global context information. Finally, the inverse depth index n can be computed by the softargmin [14] asH I × W I conv1 5 × 5, 32              1 /2H I × 1 /2W I × 32 conv2 3 × 3, 32 conv3 3 × 3, 32, add conv1 conv4-11 repeat conv2-3 conv12-17 repeat conv2-3 with dilate = 2, 3, 4 Spherical sweeping warp H × W × 1 /2N × 32 transference 3 × 3 × 1, 32 1 /2 × 1 /2 × 1 /2 × 32 concat(4)* 1 /2 × 1 /2 × 1 /2 ×128 fusion 3 × 3 × 3, 64 1 /2 × 1 /2 × 1 /2 × 64 Cost volume computation 3Dconv1-3 3 × 3 × 3, 64 1 /2 × 1 /2 × 1 /2 × 64 3Dconv4-6 from 1, 3 × 3 × 3, 128 1 /4 × 1 /4 × 1 /4 ×128 3Dconv7-9 from 4, 3 × 3 × 3, 128 1 /8 × 1 /8 × 1 /8 ×128 3Dconv10-12 from 7, 3 × 3 × 3, 128 1 /16 × 1 /16 × 1 /16 ×128 3Dconv13-15 from 10, 3 × 3 × 3, 256 1 /32 × 1 /32 × 1 /32 ×256 3Ddeconv1 3 × 3 × 3, 128, add 3Dconv12 1 /16 × 1 /16 × 1 /16 ×128 3Ddeconv2 3 × 3 × 3, 128, add 3Dconv9 1 /8 × 1 /8 × 1 /8 ×128 3Ddeconv3 3 × 3 × 3, 128, add 3Dconv6 1 /4 × 1 /4 × 1 /4 ×128 3Ddeconv4 3 × 3 × 3, 64, add 3Dconv3 1 /2 × 1 /2 × 1 /2 × 64 3Ddeconv5 3 × 3 × 3, 1 H × W × N softargmin H × Wn(θ, φ) = N −1 n=0 n × e −C(φ,θ,n) ν e −C(φ,θ,ν)where C is the (H × W × N ) regularized cost volume.To train the network in an end-to-end fashion, we use the input images and the ground truth inverse depth index asn * (θ, φ) = (N − 1) d * (θ, φ) − d 0 d N −1 − d 0 ,where d * (•) = 1/D * (•) is the ground truth inverse depth, and d 0 and d N −1 are the min and max inverse depth respectively. We use the absolute error loss between the ground truth and predicted index asL(θ, φ) = 1 i M i (θ, φ) n(θ, φ) − round(n * (θ, φ)) .We use the stochastic gradient descent with a momentum to minimize the loss. The overall flow of the proposed network is illustrated in Fig. 1.   Implementation and Training DetailsTo train the network, the input images are converted to grayscale, and the validity mask is set to only contain the pixels within 220°FOV. The intensity values in the valid area are then normalized to zero-mean and unit variance. To prevent the encoder-decoder network from learning only the valid regions in each channel, the order of feature maps to the concatenating stage is randomly permuted (e.g., 1-2-3-4, 2-3-4-1, 3-4-1-2, or 4-1-2-3). Further, we randomly rotate the rig coordinate system (and the GT depth map accordingly) with a small angle, so that the network is not tightly coupled to specific layouts. In all our experiments, the output and GT depth maps are cropped to H = 160 (−π/4 ≤ φ ≤ π/4) and W = 640 since the regions near the poles are highly distorted and less useful. The number of sweep spheres is set to N = 192. We train our network for 30 epochs on the OmniThings dataset from scratch, using 4096 training scenes. The learning rate λ is set to 0.003 for the first 20 epochs and 0.0003 for the remaining 10 epochs. We also test the network fine-tuned on the Sunny and Omni- MC-CNN [32] SweepNet [30] PSMNet [4] PSMNet-ft DispNet-CSS [12] DispNet-CSS-ft OmniMVS OmniMVS-ft Table 4: Datasets used in each methods. For experimental comparisons we use the published pre-trained weights for other methods (up of the dashed line). '-ft' denotes the finetuned versions.House datasets for 16 epochs, with λ = 0.003 for 12 epochs and λ = 0.0003 for the rest. In our system with a Nvidia 1080ti, our OmniMVS takes 1.06s for processing which is quite fast, where MC-CNN [32] takes 1.97s, SweepNet [30] 6.16s, PSMNet [4] 1.79s, and DispNet-CSS [12] 0.57s. Quantitative EvaluationThe error is measured by tie difference of inverse depth index asE(φ, θ) = ||n(φ, θ) − n * (φ, θ)|| N × 100,(3)which is the percent error of estimated inverse depth index from GT compared to all possible indices (N ). We evaluate our approaches quantitatively on the available omnidirectional stereo datasets (Sunny, Cloudy, Sunset [30], Om-niThings and OmniHouse). We compare our method to the previous works of two types. The first type is spherical sweeping-based omnidirectional methods, and the second is stitching conventional stereo results into an omnidirectional one. We use the pretrained weights of other methods in testing, and the training datasets for each method are described in Table 4.Spherical sweeping ZNCC (zero-mean normalized cross correlation) and MC-CNN [32] compute the matching cost from 9 × 9 patches pair in the warped spherical images, and SweepNet [30] estimates the whole matching cost volume from the spherical images pair. Then, SGM [10] regularizes the cost volume with the smoothness penalties P 1 = 0.1 and P 2 = 12.0. As shown in Table 3, our end-to-end networks perform better in all datasets and metrics. Our OmniMVS builds more effective feature maps and learns better matching and regularization. ConclusionsIn this paper we propose a novel end-to-end CNN architecture, OmniMVS for the omnidirectional depth estimation. The proposed network first converts the input fisheye images into the unary feature maps, and builds the 4D feature volume using the calibration and spherical sweeping. The 3D encoder-decoder block computes the matching cost volume, and the final depth estimate is computed by softargmin. Out network can learn the global context information and successfully reconstructs accurate omnidirectional depth estimates even for thin and small objects as well as large textureless surfaces. We also present large-scale synthetic datasets, Omnithings and OmniHouse. The exten-sive experiments show that our method outperforms existing omnidirectional methods and the state-of-the-art conventional stereo methods with stitching.P i = d i (2u i √ 1 − s i , 2v i √ 1 − s i , 1 − 2s i ) ⊤ ,where u i and v i are randomly sampled with the constraint s i = u 2 i + v 2 i < 1, and d i represents a random distance. Collision check of the newly placed object is performed by testing the overlap of the bounding boxes of the objects. For 70% scenes, we generate cuboid rooms with random aspect ratios, textures, and transformations, and for the rest, skies with different weathers at infinite distance for learning the background model.OmniHouse We also propose Omnihouse dataset for learning various indoor environments. Synthesized indoor scenes are reproduced using the models in SUNCG dataset [5] and a few additional models. We collect 451 house models consisting of various rooms and objects (e.g., beds and sofas) with the sunny sky background. To generate more data, multiple images are rendered for each 3D scene at random positions and orientations.Figure 1 shows the effectiveness of using our datasets. The results named 'OmniMVS' are by the model trained on OmniThings dataset only, and 'OmniMVS-ft' is by the model fine-tuned on Sunny [6] and OmniHouse datasets. Both networks perform well on the textured regions whereas OmniMVS-ft performs favorably to OmniMVS on the textureless or reflective surfaces. Additional examples of our proposed datasets are also shown in Fig. 2 and 3. Figure 2 :2Figure 2: (a) Wide-baseline multi-camera rig system. (b) Multiple true matches problem. There can be several observations on a ray in such global sweeping approach. Table 3 :3Quantitative comparison with other methods. The error is defined in Eq. 3. The qualifier '>n' refers to the pixel ratio (%) whose error is larger than n, 'MAE' refers to the mean absolute error, and 'RMS' refers to the root mean squared error. The errors are averaged over all test frames of each datasets. ' * ' of each scores denotes the 2nd place.Dataset Metric>1>3OmniThings >5 MAE RMS>1>3OmniHouse >5MAE RMSSpherical sweeping with regularization ZNCC+SGM [10] 72.56 54.01 MC-CNN [32]+SGM 67.19 47.43 SweepNet [30]+SGM 67.20 47.6345.63 10.51 16.44 39.49 8.65 13.66 39.66 8.87 13.9044.05 38.01 36.6020.64 13.57 15.86 9.46 15.41 9.363.08 2.08 2.077.05 4.15 4.38Stitching conventional stereo PSMNet [4] 86.25 PSMNet-ft 82.69 DispNet-CSS [12] 50.62 DispNet-CSS-ft 67.8663.23 51.98 27.77 48.0844.84 41.74 19.50 38.577.28 11.15 9.09 13.71 4.06 7.98 7.81 12.2763.22 87.56  *  26.56 36.4726.43 15.39 27.01 12.89 11.69 7.16  *  1.54  *  3.18 5.82 13.88 3.51 6.05 14.98 8.29 1.81 3.44OmniMVS OmniMVS-ft47.72  *  50.28  *  22.78  *  15.60  *  3.52  *  7.44 15.12 8.91 2.40 5.2730.53  *  10.29  *  6.27 21.09 4.63 2.581.72 1.044.05 1.97Dataset Metric>1>3Sunny >5MAE RMS>1>3Cloudy >5MAE RMS>1>3Sunset >5MAE RMSSpherical sweeping with regularization ZNCC+SGM 52.00 21.45 10.96 MC-CNN+SGM 39.42 11.73 6.08 SweepNet+SGM 24.82 6.91 4.282.50 1.83 1.315.35 4.56 3.7953.09 22.17 11.50 43.16 11.95 5.82 34.97 9.51 5.092.58 1.85 1.555.45 4.46 3.9652.33 21.90 11.29 39.67 12.82 6.28 24.92 7.25 4.462.53 1.86 1.325.31 4.59 3.80Stitching conventional stereo PSMNet 65.09 30.87 13.13 PSMNet-ft 92.67 31.45 21.32 DispNet-CSS  *  24.80 8.54 5.59 DispNet-CSS-ft 39.02 21.12 14.472.54 4.33 1.44 2.374.03 7.76 4.02  *  25.16 63.62 28.51 10.40 92.92 31.24 20.14 8.47 5.50 4.85 42.29 21.55 14.282.45 4.13 1.43 2.434.26 7.32 3.92  *  24.79 63.83 28.41 10.00 93.24 30.64 19.65 8.29 5.34 4.88 40.21 20.91 14.432.43 4.11 1.38 2.404.11 7.43 3.76 4.88OmniMVS 27.16  OmniMVS-ft 13.932.871.710.792.1212.202.481.460.721.8514.142.881.710.792.04DatasetOmnidirectional stereo Sunny [30] Omni Things Omni HouseConventional stereo Scene Flow [16] KITTI [7, 17]* 6.13 * 3.98 * 1.24 * 3.09 28.13 * 5.37 * 3.54 * 1.17 * 2.83 26.70 * 6.19 * 4.02 * 1.24 * 3.06|",yes,,yes,"learning rate, epochs, ",no,,,,
804,https://github.com/yibinshen/multimath,yibinshen_multimath.tar.gz,Solving Math Word Problems with Multi-Encoders and Multi-Decoders,,pyltp gensim copy math re json src random torch os numpy time,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yibinshen_multimath.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yibinshen_multimath.pdf,Tanh Activation  Sigmoid Activation  Long Short-Term Memory  Sequence to Sequence,,yes,Adam,yes,"learning rate, decay, epochs, batch size, dropout, bearm search",no,,,,
1092,https://github.com/ChangdeDu/DGMM,ChangdeDu_DGMM.tar.gz,Sharing deep generative representation for perceived image reconstruction from human brain activity,,matlab scipy keras sklearn os numpy matplotlib,cs.AI cs.CV q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ChangdeDu_DGMM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ChangdeDu_DGMM.pdf,,,yes,5-fold cross validation,not all,"regularization parameter, ",yes,,,,
645,https://github.com/amirhfarzaneh/lsoftmax-pytorch,amirhfarzaneh_lsoftmax-pytorch.tar.gz,Large-Margin Softmax Loss for Convolutional Neural Networks,,math scipy lsoftmax torch argparse __future__ numpy models matplotlib torchvision,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amirhfarzaneh_lsoftmax-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amirhfarzaneh_lsoftmax-pytorch.pdf,Softmax,"Face VerificationTo further evaluate the learned features, we conduct an experiment on the famous LFW dataset (Huang et al., 2007). The dataset collects 13,233 face images from 5749 persons from uncontrolled conditions. Following the unrestricted with labeled outside data protocol (Huang et al., 2007), we train on the publicly available CASIA-WebFace (Yi et al., 2014) outside dataset (490k labeled face images belonging to over 10,000 individuals) and test on the 6,000 face pairs on LFW. People overlapping between the outside training data and the LFW testing data are excluded. As preprocess-ing, we use IntraFace (Asthana et al., 2014) to align the face images and then crop them based on 5 points. Then we train a single network for feature extraction, so we only compare the single model performance of current state-ofthe-art CNNs. Finally PCA is used to form a compact feature vector. The results are given in Table 5. The generalize softmax loss achieves the current best results while only trained with the CASIA-WebFace outside data, and is also comparable to the current state-of-the-art CNNs with private outside data. Experimental results well validate the conclusion that the L-Softmax loss encourages the intraclass compactness and inter-class separability.",yes,,not all,"batch size, momentum, weight decay",no,,,,
1056,https://github.com/pksvision/Deep-WaveNet-Underwater-Image-Restoration,pksvision_Deep-WaveNet-Underwater-Image-Restoration.tar.gz,Wavelength-based Attributed Deep Neural Network for Underwater Image Restoration,,PIL sys imqual_utils glob tqdm __future__ torchvision re collections get_uiqm torch os scipy uiqm_utils dataset typing argparse numbers numpy time shutil math ntpath measure_ssim_psnr cv2 random misc uqim_utils options ssim vgg models,eess.IV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pksvision_Deep-WaveNet-Underwater-Image-Restoration.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pksvision_Deep-WaveNet-Underwater-Image-Restoration.pdf,Dilated Causal Convolution  Mixture of Logistic Distributions  WaveNet,,yes,Adam,not all,"learning rate, batch size",no,,,,
250,https://github.com/wangxiao5791509/SiamDW_tracker_revised,wangxiao5791509_SiamDW_tracker_revised.tar.gz,Deeper and Wider Siamese Networks for Real-Time Visual Tracking,@engine.fitness_register,utils yaml sys glob test_siamrpn easydict shapely gaft __future__ pprint torchvision matlab pathlib _init_paths core json collections torch os concurrent ray siamrpn tracker scipy dataset test_siamfc argparse tensorboardX numpy time shutil math hyperopt cv2 random logging mpi4py models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/wangxiao5791509_SiamDW_tracker_revised.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\wangxiao5791509_SiamDW_tracker_revised.pdf,1x1 Convolution  Convolution  Local Response Normalization  Grouped Convolution  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  AlexNet,,yes,SGD,yes,"epochs, learning rate, weight decay, momentum, mini batch size",no,,,,
948,https://github.com/zacks417/faster-rcnn-tf,zacks417_faster-rcnn-tf.tar.gz,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,@roidb_handler.setter @property @staticmethod,"utils PIL yaml sys glob nms pdb easydict __future__ pprint ast tensorflow re uuid model json collections cProfile os matplotlib nets six scipy xml pickle argparse numpy time math os,cv2 datasets pycocotools time,os,sys cv2 layer_utils roi_data_layer subprocess cPickle",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/zacks417_faster-rcnn-tf.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\zacks417_faster-rcnn-tf.pdf,VGG-16  Region Proposal Network  Softmax  Convolution  RoIPool  Faster R-CNN  Fast R-CNN,"4-Step Alternating Training.In this paper, we adopt a pragmatic 4-step training algorithm to learn shared features via alternating optimization. In the first step, we train the RPN as described in Section 3.1.3. This network is initialized with an ImageNet-pre-trained model and fine-tuned end-to-end for the region proposal task. In the second step, we train a separate detection network by Fast R-CNN using the proposals generated by the step-1 RPN. This detection network is also initialized by the ImageNet-pre-trained model. At this point the two networks do not share convolutional layers. In the third step, we use the detector network to initialize RPN training, but we fix the shared convolutional layers and only fine-tune the layers unique to RPN. Now the two networks share convolutional layers. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN. As such, both networks share the same convolutional layers and form a unified network. A similar alternating training can be run for more iterations, but we have observed negligible improvements. Experiments on PASCAL VOCWe comprehensively evaluate our method on the PASCAL VOC 2007 detection benchmark [11]. This dataset consists of about 5k trainval images and 5k test images over 20 object categories. We also provide results on the PASCAL VOC 2012 benchmark for a few models. For the ImageNet pre-trained network, we use the ""fast"" version of ZF net [32] that has 5 convolutional layers and 3 fully-connected layers, and the public VGG-16 model 7 [3] that has 13 convolutional layers and 3 fully-connected layers. We primarily evaluate detection mean Average Precision (mAP), because this is the actual metric for object detection (rather than focusing on object proposal proxy metrics).Table 2 (top) shows Fast R-CNN results when trained and tested using various region proposal methods. These results use the ZF net. For Selective Search (SS) [4], we generate about 2000 proposals by the ""fast"" mode. For EdgeBoxes (EB) [6], we generate the proposals by the default EB setting tuned for 0.7 7. www.robots.ox.ac.uk/ ∼ vgg/research/very deep/ IoU. SS has an mAP of 58.7% and EB has an mAP of 58.6% under the Fast R-CNN framework. RPN with Fast R-CNN achieves competitive results, with an mAP of 59.9% while using up to 300 proposals 8 . Using RPN yields a much faster detection system than using either SS or EB because of shared convolutional computations; the fewer proposals also reduce the region-wise fully-connected layers' cost (Table 5).Ablation Experiments on RPN. To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies. First, we show the effect of sharing convolutional layers between the RPN and Fast R-CNN detection network. To do this, we stop after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2). We observe that this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.Next, we disentangle the RPN's influence on training the Fast R-CNN detection network. For this purpose, we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. In these ablation experiments, the RPN does not share features with the detector.Replacing SS with 300 RPN proposals at test-time leads to an mAP of 56.8%. The loss in mAP is because of the inconsistency between the training/testing proposals. This result serves as the baseline for the following comparisons.Somewhat surprisingly, the RPN still leads to a competitive result (55.1%) when using the top-ranked 8. For RPN, the number of proposals (e.g., 300) is the maximum number for an image. RPN may produce fewer proposals after NMS, and thus the average number of proposals is smaller.  Next, we separately investigate the roles of RPN's cls and reg outputs by turning off either of them at test-time. When the cls layer is removed at testtime (thus no NMS/ranking is used), we randomly sample N proposals from the unscored regions. The mAP is nearly unchanged with N = 1000 (55.8%), but degrades considerably to 44.6% when N = 100. This shows that the cls scores account for the accuracy of the highest ranked proposals.On the other hand, when the reg layer is removed at test-time (so the proposals become anchor boxes), the mAP drops to 52.1%. This suggests that the highquality proposals are mainly due to the regressed box bounds. The anchor boxes, though having multiple scales and aspect ratios, are not sufficient for accurate detection.We also evaluate the effects of more powerful networks on the proposal quality of RPN alone. We use VGG-16 to train the RPN, and still use the above detector of SS+ZF. The mAP improves from 56.8% (using RPN+ZF) to 59.2% (using RPN+VGG). This is a promising result, because it suggests that the proposal quality of RPN+VGG is better than that of RPN+ZF. Because proposals of RPN+ZF are competitive with SS (both are 58.7% when consistently used for training and testing), we may expect RPN+VGG to be better than SS. The following experiments justify this hypothesis.Performance of VGG-16. Table 3 shows the results of VGG-16 for both proposal and detection. Using RPN+VGG, the result is 68.5% for unshared features, slightly higher than the SS baseline. As shown above, this is because the proposals generated by RPN+VGG are more accurate than SS. Unlike SS that is predefined, the RPN is actively trained and benefits from better networks. For the feature-shared variant, the result is 69.9%-better than the strong SS baseline, yet with nearly cost-free proposals. We further train the RPN and detection network on the union set of PAS-CAL VOC 2007 trainval and 2012 trainval. The mAP is 73.2%. Figure 5 shows some results on the PASCAL VOC 2007 test set. On the PASCAL VOC 2012 test set (Table 4), our method has an mAP of 70.4% trained on the union set of VOC 2007 trainval+test and VOC 2012 trainval. Table 6 and Table 7 show the detailed numbers.   In Table 5 we summarize the running time of the entire object detection system. SS takes 1-2 seconds depending on content (on average about 1.5s), and Fast R-CNN with VGG-16 takes 320ms on 2000 SS proposals (or 223ms if using SVD on fully-connected layers [2]). Our system with VGG-16 takes in total 198ms for both proposal and detection. With the convolutional features shared, the RPN alone only takes 10ms computing the additional layers. Our regionwise computation is also lower, thanks to fewer proposals (300 per image). Our system has a frame-rate of 17 fps with the ZF net.Sensitivities to Hyper-parameters. In Table 8 we investigate the settings of anchors. By default we use 3 scales and 3 aspect ratios (69.9% mAP in Table 8). If using just one anchor at each position, the mAP drops by a considerable margin of 3-4%. The mAP is higher if using 3 scales (with 1 aspect ratio) or 3 aspect ratios (with 1 scale), demonstrating that using anchors of multiple sizes as the regression references is an effective solution. Using just 3 scales with 1 aspect ratio (69.8%) is as good as using 3 scales with 3 aspect ratios on this dataset, suggesting that scales and aspect ratios are not disentangled dimensions for the detection accuracy. But we still adopt these two dimensions in our designs to keep our system flexible.In Table 9 we compare different values of λ in Equation (1). By default we use λ = 10 which makes the two terms in Equation (1) roughly equally weighted after normalization. Table 9 shows that our result is impacted just marginally (by ∼ 1%) when λ is within a scale of about two orders of magnitude (1 to 100). This demonstrates that the result is insensitive to λ in a wide range.Analysis of Recall-to-IoU. Next we compute the recall of proposals at different IoU ratios with groundtruth boxes. It is noteworthy that the Recall-to-IoU metric is just loosely [19], [20], [21] related to the ultimate detection accuracy. It is more appropriate to use this metric to diagnose the proposal method than to evaluate it.In Figure 4, we show the results of using 300, 1000, and 2000 proposals. We compare with SS and EB, and the N proposals are the top-N ranked ones based on the confidence generated by these methods. The plots show that the RPN method behaves gracefully when the number of proposals drops from 2000 to 300. This explains why the RPN has a good ultimate detection mAP when using as few as 300 proposals. As we analyzed before, this property is mainly attributed to the cls term of the RPN. The recall of SS and EB drops more quickly than RPN when the proposals are fewer.   One-Stage Detection vs. Two-Stage Proposal + Detection. The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic proposals and class-specific detections. In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. These features are used to simultaneously determine the location and category of objects. In RPN, the features are from square (3×3) sliding windows and predict proposals relative to anchors with different scales and aspect ratios. Though both methods use sliding windows, the region proposal task is only the first stage of Faster R-CNN-the downstream Fast R-CNN detector attends to the proposals to refine them. In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faithfully cover the features of the regions. We believe these features lead to more accurate detections.To compare the one-stage and two-stage systems, we emulate the OverFeat system (and thus also circumvent other differences of implementation details) by one-stage Fast R-CNN. In this system, the ""proposals"" are dense sliding windows of 3 scales (128, 256, 512) and 3 aspect ratios (1:1, 1:2, 2:1). Fast R-CNN is trained to predict class-specific scores and regress box locations from these sliding windows. Because the OverFeat system adopts an image pyramid, we also evaluate using convolutional features extracted from 5 scales. We use those 5 scales as in [1], [2].Table 10 compares the two-stage system and two variants of the one-stage system. Using the ZF model, the one-stage system has an mAP of 53.9%. This is lower than the two-stage system (58.7%) by 4.8%. This experiment justifies the effectiveness of cascaded region proposals and object detection. Similar observations are reported in [2], [39], where replacing SS region proposals with sliding windows leads to ∼6% degradation in both papers. We also note that the onestage system is slower as it has considerably more proposals to process.",yes,,not all,"learning rate, momentum, mini batch size, weight decay, iterations",no,,,,
1158,https://github.com/hassancpu/Temporal-Action-Localization,hassancpu_Temporal-Action-Localization.tar.gz,Temporal Action Localization Using Gated Recurrent Units,@property,utils data_loader scipy keras tensorflow json data_loader_prop_LTR h5py random data_loader_prop os numpy matplotlib data_loader_changed,cs.CV cs.MM,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hassancpu_Temporal-Action-Localization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hassancpu_Temporal-Action-Localization.pdf,Gated Recurrent Unit,,yes,Adam,not all,"learning rate, decay, epochs",no,,,,
871,https://github.com/denadai2/bayesian-crime-multiple-cities,denadai2_bayesian-crime-multiple-cities.tar.gz,"Socio-economic, built environment, and mobility conditions associated with crime: A study of multiple cities",,PIL pandas pystan csv yaml spatial_groups sys tqdm psycopg2 sqlalchemy os matplotlib scipy hashlib seaborn pickle sklearn argparse joblib numpy time math logging arviz io,cs.SI cs.CY physics.soc-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/denadai2_bayesian-crime-multiple-cities.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\denadai2_bayesian-crime-multiple-cities.pdf,,,no,,,,no,,,,
209,https://github.com/dgromann/OntologyAlignmentWithEmbeddings,dgromann_OntologyAlignmentWithEmbeddings.tar.gz,Comparing Pretrained Multilingual Word Embeddings on an Ontology Alignment Task,,pandas normalizer polyglotEmbeddings __future__ distutils gensim re collections nltk scipy word2vecEmbeddings decimal pickle sklearn numpy time math semSim fastTextEmbeddings operator polyglot,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dgromann_OntologyAlignmentWithEmbeddings.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dgromann_OntologyAlignmentWithEmbeddings.pdf,,,no,,,,no,,,,
723,https://github.com/spindro/AP-GCN,spindro_AP-GCN.tar.gz,Adaptive Propagation Graph Convolutional Network,@staticmethod,pathlib scipy math warnings numbers networkx seeds typing torch os io_data numpy sparsegraph torch_geometric,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/spindro_AP-GCN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\spindro_AP-GCN.pdf,Graph Convolutional Network,"A. Experimental setupWe used the same experimental setup proposed in [18] which aims to reduce experimental bias. This setup has shown that many advantages reported by recent works vanish under this statistically rigorous evaluation. The first step in this process is the subdivision in visible and invisible sets. The invisible set will serve as a test set and will be used only once to report the final performance. The visible set is subdivided in a training set with N nodes per class and an early stopping set for model selection. A validation set containing the remaining nodes of the visible set is used for hyper-parameters tuning. These splits are determined using the same 20 seeds used in [18] and each experiment is run with 5 different initialization of the weights leading to a total of 100 experiments per dataset.We perform a first evaluation over three citation datasets, Citeseer, Cora-ML, and PubMed, and a co-authorship one, MS-Academic. Then we compare the performances of a subset of selected algorithms on Amazon Computer and Amazon Photo, that are segments of the Amazon co-purchase graph introduced in [35]. All the datasets have a feature vector with a bag-of-words representation associated with the nodes. Other relevant characteristics are summarized in Table I. These features are normalized with an 1 norm and to conclude the preprocessing, which is the same for all the datasets, we select the largest connected component.To be in line with the evaluation of [18] we use the same number of layers (2) and hidden units (64), dropout rate (0.5) on both layers and the adjacency matrix, resampled at each propagation, and Adam optimizer [36] with learning rate 0.01. We choose instead the following hyperparameters for all the datasets: 2 regularization parameter 0.008 on the weights of the first layer, maximum steps  II.of propagation T = 10. For the evaluation on Amazon's dataset we removed the 2 regularization keeping the same learning rate for all the algorithms involved. We adapted the propagation penalty α, controlling the distribution of the propagation steps, to each dataset. The code to test our proposed AP-GCN and replicate our experiments is available on the web. 3 The evaluation in [18] together with their proposed methods PPNP and APPNP included: GCN [14], both optimized and as originally proposed (V.GCN), network of GCNs (N-GCN) [37], graph attention networks (GAT) [30], bootstrapped feature propagation (bt.FP) [38] and jumping knowledge networks with concatenation (JK) [27]. We included in our comparison ARMA [16], with a configuration compatible with the experimental setup.",yes,"Experimental Tuning, Adam",yes,"number of layers, hidden units, dropout, learning rate, l2 regularization, ",yes,,,,
664,https://github.com/yqx7150/ivan,yqx7150_ivan.tar.gz,Variable Augmented Network for Invertible Modality Synthesis-Fusion,,"PIL imageio glob skimage tqdm __future__ torchvision model json os, time, random torch matplotlib scipy rawpy dataset argparse numbers tensorboardX numpy config math os, random, time cv2",eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/yqx7150_ivan.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\yqx7150_ivan.pdf,,,yes,Adam,not all,"epochs, learning rate",no,,,,
1156,https://github.com/jejjohnson/rbig,jejjohnson_rbig.tar.gz,Iterative Gaussianization: from ICA to Random Rotations,,rbig scipy functools warnings sys setuptools typing logging sklearn statsmodels numpy,stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jejjohnson_rbig.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jejjohnson_rbig.pdf,,,yes,3-fold cross validation,no,,yes,,,,
36,https://github.com/Peilun-Li/SG-GAN,Peilun-Li_SG-GAN.tar.gz,Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption,,utils copy sys multiprocessing glob __future__ pprint tensorflow model collections os module scipy argparse numpy time shutil math ops random,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Peilun-Li_SG-GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Peilun-Li_SG-GAN.pdf,,"Ablation studiesEffectiveness of soft gradient-sensitive objective. To demonstrate the effectiveness of soft gradient-sensitive loss L grad , we train a variant of SG-GAN without applying L grad and compare it with SG-GAN-25K. Figure 5 shows an example by inspecting details through a 4X zoom. Compared with SG-GAN-25K, the variant without L grad has coarse semantic boundaries and rough textures, which demonstrates soft gradient-sensitive loss can help generate adapted images with clearer semantic boundaries and smoother textures.Effectiveness of semantic-aware discriminator. We use a variant of SG-GAN without applying semantic-aware discriminator (SD) and compare it with SG-GAN-25K to study the effectiveness of SD. As shown in Figure 6, comparing (g) and (h), the variant without SD lacks for details, e.g., the color of traffic light, and generates coarser textures, e.g., the sky. The difference maps, i.e., (b), (c), (d) in Figure 6, further reveal that semantic-aware discriminator leads to personalized texture rendering for each distinct region with specific semantic meaning.The effect of virtual training image size. Figure 4 compares variants of SG-GAN that use distinct numbers of virtual-world images for training. Generally, SG-GAN-25K generates clearer details than SG-GAN-2K for some images. Further A/B tests between them in Table 1 show SG-GAN-25K is slightly better than SG-GAN-2K because of using more training data. Both qualitative and quantita-tive comparisons indicate more data could help, however, the improved performance may be only notable if dataset difference is in orders of magnitude.Discussion. While SG-GAN generates realistic results for almost all tested images, in very rare case the adapted image is unsatisfactory, as shown in Figure 7. Our model learns the existence of sunlight, however it is unaware of the image is taken in a tunnel and thus sunlight would be abnormal. We attribute this rare unsatisfactory case to the lack of diversity of real-world dataset compared with virtual-world dataset, thus such case could be seen as an outlier.More real-world images could help alleviate such unsatisfactory case, but SG-GAN is restricted by the limited number of fine-grained real-world annotations for training, e.g., Cityscapes dataset only contains a fine-grained training set of 2975 images. However, we foresee a possibility to solve the data insufficient issue by using coarse annotations labeled by human or semantic segmentation models. In our implementation of semantic-aware discriminator, semantic masks are actually clustered to avoid sparse classes, e.g., semantic classes ""building"", ""wall"", ""fence"", ""guard rail"", ""bridge"" and ""tunnel"" are clustered into a single mask indicating ""construction"". Considering such cluster, annotation granularity may not be a vital factor for our model. Thus investigating the trade-off between annotation granularity and dataset size would be a possible next step.",no,,,,no,,,,
926,https://github.com/ymcui/Cross-Lingual-MRC,ymcui_Cross-Lingual-MRC.tar.gz,Cross-Lingual Machine Reading Comprehension,@classmethod,math copy unicodedata tensorflow optimization re modeling json collections random pdb layers os __future__ numpy tokenization six,cs.CL cs.LG cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ymcui_Cross-Lingual-MRC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ymcui_Cross-Lingual-MRC.pdf,Weight Decay  Residual Connection  Adam  Layer Normalization  Softmax  Scaled Dot-Product Attention  Dropout  Gaussian Error Linear Units  Multi-Head Attention  Attention Dropout  WordPiece  Linear Warmup With Linear Decay  Dense Connections  BERT,,yes,Adam,not all,"learning rate, decay, epochs, batch size",no,,,,
1216,https://github.com/ulissigroup/Enabling-Robust-Offline-Active-Learning-for-MLPs,ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.tar.gz,Enabling robust offline active learning for machine learning potentials using simple physics-based priors,,copy amptorch multiprocessing sys random torch os numpy ase,physics.comp-ph cond-mat.mtrl-sci,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ulissigroup_Enabling-Robust-Offline-Active-Learning-for-MLPs.pdf,,,no,,,,no,,,,
446,https://github.com/taimurhassan/anomaly,taimurhassan_anomaly.tar.gz,Unsupervised Anomaly Instance Segmentation for Baggage Threat Recognition,@property,PIL scipy keras tensorflow model os numpy matplotlib config,cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/taimurhassan_anomaly.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\taimurhassan_anomaly.pdf,,"ContributionsThis paper presents a novel unsupervised anomaly instance segmentation framework to detect and extract baggage threats as anomalies. To the best of our knowledge, this is the first approach towards unsupervised anomaly instance segmentation, in a baggage threat detection territory, exhibiting the following distinctive features:-The proposed framework is the first of its kind that is trained only once on the normal baggage X-ray scans. Afterward, it does not require re-training to eliminate the scanner differences to extract anomalous regions (across different datasets).-The proposed framework is built upon a novel Gaussian-Weighted Fourier Stylization (GW-FS) scheme that drastically removes the scanner variations to achieve high generalizability towards extracting the suspicious baggage items as anomalies from the baggage X-ray scans.-A thorough validation on four public X-ray datasets showcases that the proposed framework outperforms its unsupervised and semi-supervised competitors while achieving a competitive performance with the other fully supervised frameworks.The rest of the paper is organized as follows: Section 4 presents the proposed framework, Section 5 showcases the experimental setup, Section 6 presents the detailed evaluation results, and Section 7 discusses the prospects of the proposed framework and concludes the paper. Fig. 3 :3Fig. 3: Unsupervised anomaly instance segmentation framework. Here, we train an encoder-decoder model only once to reconstruct normal baggage X-ray scans. During the inference stage, the trained model recognizes the anomalous regions by exploiting the actual and reconstructed scans' disparities. To eliminate the scanner variations, we propose a GW-FS scheme that mixes the frequency representations within the reference scan and the input scans.",yes,Adam,yes,"epochs, default leraning rate and decays adam",no,,,,
92,https://github.com/ninaqy/PUGeo,ninaqy_PUGeo.tar.gz,PUGeo-Net: A Geometry-centric Network for 3D Point Cloud Upsampling,@ops.RegisterGradient('NnDistance') @tf.RegisterGradient('GatherPoint'),utils sys glob tf_util tqdm __future__ tensorflow re datetime os tf_ops socket sklearn argparse numpy time math random cPickle,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ninaqy_PUGeo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ninaqy_PUGeo.pdf,,,yes,Adam,yes,"learning rate, mini batch, epochs, three parameters were empirically set",no,,,,
350,https://github.com/brynnchernosky/seq2seq,brynnchernosky_seq2seq.tar.gz,Keeping Notes: Conditional Natural Language Generation with a Scratchpad Mechanism,@tf.function,math normal_model scipy tensorflow scratchpad sys attention enhanced_model preprocess numpy matplotlib,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/brynnchernosky_seq2seq.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\brynnchernosky_seq2seq.pdf,Sigmoid Activation  Tanh Activation  Long Short-Term Memory  Sequence to Sequence,"IntroductionThe sequence-to-sequence neural network framework (seq2seq)  has been successful in a wide range of tasks in natural language processing, from machine translation (Bahdanau et al., 2014) and semantic parsing (Dong and Lapata, 2016) to summarization (Nallapati et al., 2016b;See et al., 2017). Despite this success, seq2seq models are known to often exhibit an overall lack of fluency in the natural language output produced: problems include lexical repetition, under-generation in the form of partial phrases and lack of specificity (often caused by the gap between the input and output vocabularies) (Xie, 2017). Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016) * Work performed while at Apple. to deal with repetition and over-copying in summarization, Hua and Wang (2018) introduced a method of attending over keyphrases to improve argument generation, and Kiddon et al. (2016) introduced a method that attends to an agenda of items to improve recipe generation. Perhaps not surprisingly, general-purpose attention mechanisms targeting individual problems from the list above have also begun to be developed. Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015), for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-generation. These techniques, however, often require significant hyperparameter tuning and are purposely limited to fixing a specific problem in the generated text.We present here a general-purpose addition to the standard seq2seq framework that aims to simultaneously tackle all of the above issues. In particular, we propose Scratchpad, a novel write mechanism that allows the decoder to keep notes on its past actions (i.e., generation, attention, copying) by directly modifying encoder states. The Scratchpad mechanism essentially lets the decoder more easily keep track of what the model has focused on and copied from the input in the recent past, as well as what it has produced thus far as output. Thus, Scratchpad can alternatively be viewed as an external memory initialized by the input, or as an input re-encoding step that takes into account past attention and generation.To demonstrate general(izable) improvements on conditional natural language generation problems broadly construed, we instantiate Scratchpad for three well-studied generation tasks -Machine Translation, Question Generation, and Summarization -and evaluate it on a diverse set of datasets. These tasks exhibit a variety of input modalities (structured and unstructured language) and typically have required a variety of computational strategies to perform well (attention, pointing, copying). We find, for each task, that Scratchpad attains improvements over several strong baselines: Sequence-to-Sequence with attention Bahdanau et al., 2014), copy-enhanced approaches (Gu et al., 2016;Vinyals et al., 2015), and coverageenhanced approaches (Tu et al., 2016;See et al., 2017). Scratchpad furthermore obtains state-ofthe-art performance for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT) and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output. ExperimentsIn this section we describe experimental setup and results for Machine Translation, Question Generation, and Summarization tasks which exhibit a variety of input modalities and strategies required to perform well. We work with structured and unstructured language and several sequence to sequences architectures i.e. attention, pointing, and copying. Machine translation is a canonical sequence-to-sequence task where pairwise word-level or phrase-level generation is expected. Question Generation from logical forms requires reasoning about the syntax, parse tree, and vocabulary of the input sequence to infer the meaning of the logical form program and utilize copy-mechanism to copy entities. Lastly, summarization requires understanding both the global and the local context of a sentence within a docu-ment, identifying spans that are informative and diverse, and generating coherent representative summaries. Demonstrating a single mechanism that reaches state of the art on such a diverse set of natural language tasks underscores the generalizability of our technique, particularly given the large range in number of training examples (3k, 56k, 153k, 287k) across datasets.",yes,Adam,yes,"batch size, word vector size, dropout, learning rate, epochs",no,,,,
572,https://github.com/cjx0525/BGCN,cjx0525_BGCN.tar.gz,Bundle Recommendation with Graph Convolutional Networks,@property,utils loss csv setproctitle model json torch os matplotlib string scipy itertools dataset test tensorboardX numpy time config train metric math random,cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cjx0525_BGCN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cjx0525_BGCN.pdf,,,yes,"Adam, Search",yes,"mini batch size, learning rate, l2 regularization term, dropout",yes,,,,
915,https://github.com/manideep2510/eye-in-the-sky,manideep2510_eye-in-the-sky.tar.gz,Pyramid Scene Parsing Network,@functools.wraps(method),PIL scipy math keras re tensorflow functools unet iou libtiff glob cv2 skimage sklearn os numpy pspnet matplotlib,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/manideep2510_eye-in-the-sky.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\manideep2510_eye-in-the-sky.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net  Average Pooling  Auxiliary Classifier  Pyramid Pooling Module  Fully Convolutional Network  Random Gaussian Blur  RandomRotate  Random Horizontal Flip  Weight Decay  SGD with Momentum  Polynomial Rate Decay  PSPNet  Dilated Convolution  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network,,yes,,yes,"learning rate, momentum, weight decay, ",no,,,,
1848,https://github.com/kilianFatras/unbiased_minibatch_sinkhorn_GAN,kilianFatras_unbiased_minibatch_sinkhorn_GAN.tar.gz,Minibatch optimal transport distances; analysis and applications Kilian Fatras,@staticmethod,utils imageio sys mb_utils torchvision pylab re mbot_proba torch os mini_batch_gw matplotlib ot scipy itertools mbot_without_replacement mbot_with_replacement argparse mini_batch_ot numpy random architecture,stat.ML cs.LG stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kilianFatras_unbiased_minibatch_sinkhorn_GAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kilianFatras_unbiased_minibatch_sinkhorn_GAN.pdf,,,yes,SGD,no,,no,,,,
1940,https://github.com/afarahi/kllr,afarahi_kllr.tar.gz,Stellar Property Statistics of Massive Halos from Cosmological Hydrodynamics Simulations: Common Kernel Shapes,,scipy pandas warnings setuptools tqdm sklearn os numpy matplotlib kllr,astro-ph.GA astro-ph.CO astro-ph.CO astro-ph.GA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/afarahi_kllr.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\afarahi_kllr.pdf,,,no,,no,,no,,,,
1518,https://github.com/ukky17/ori_selectivity_CNN,ukky17_ori_selectivity_CNN.tar.gz,Under review as a conference paper at ICLR 2019 CAUSAL IMPORTANCE OF ORIENTATION SELECTIVITY FOR GENERALIZATION IN IMAGE RECOGNITION,,utils grating keras model pickle tqdm sklearn os numpy matplotlib,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ukky17_ori_selectivity_CNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ukky17_ori_selectivity_CNN.pdf,,,yes,"Adam, found empirically",yes,,yes,,,,
1807,https://github.com/maudzung/YOLO3D-YOLOv4-PyTorch,maudzung_YOLO3D-YOLOv4-PyTorch.tar.gz,YOLO3D: End-to-end real-time 3D Oriented Object Bounding Box Detection from LiDAR Point Cloud,,utils copy warnings sys tqdm easydict shapely __future__ torchvision mayavi data_process evaluate torch os matplotlib scipy sklearn argparse numpy time config math torchsummary cv2 random logging models,cs.CV eess.IV cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/maudzung_YOLO3D-YOLOv4-PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\maudzung_YOLO3D-YOLOv4-PyTorch.pdf,Grid Sensitive  RMSProp  Tanh Activation  Softplus  Grouped Convolution  ResNeXt Block  Max Pooling  Spatial Attention Module  Cosine Annealing  Sigmoid Activation  CutMix  Depthwise Convolution  Label Smoothing  DIoU-NMS  Residual Connection  Dilated Convolution  Receptive Field Block  Rectified Linear Units  Depthwise Separable Convolution  BiFPN  Mixup  SGD with Momentum  Weight Decay  Linear Warmup  Polynomial Rate Decay  YOLOv4  Logistic Regression  k-Means Clustering  YOLOv3  Concatenated Skip Connection  Pointwise Convolution  Feature Pyramid Network  Bottom-up Path Augmentation  PAFPN  Region Proposal Network  RoIAlign  Adaptive Feature Pooling  Spatial Pyramid Pooling  Swish  Dropout  Average Pooling  Dense Connections  Inverted Residual Block  EfficientNet  Darknet-53  CSPDarknet53  Softmax  Global Average Pooling  Convolution  1x1 Convolution  Batch Normalization  Leaky ReLU  CSPResNeXt Block  CSPResNeXt  DropBlock  Mish,,no,,no,,no,,,,
1884,https://github.com/CVPRUSFTampa/EventSegmentation,CVPRUSFTampa_EventSegmentation.tar.gz,A Perceptual Prediction Framework for Self Supervised Event Segmentation,,"PIL scipy math tensorflow warnings sys json collections collections, cv2 random sklearn statsmodels os cv2, os, sys, numpy __future__ numpy matplotlib",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/CVPRUSFTampa_EventSegmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\CVPRUSFTampa_EventSegmentation.pdf,,,no,,no,,no,,,,
1552,https://github.com/josedolz/MSL-student-becomes-master,josedolz_MSL-student-becomes-master.tar.gz,Teach me to segment with mixed supervision: Confident students become masters,,utils PIL pandas warnings sys pdb nibabel skimage medicalDataLoader __future__ ctypes torchvision medpy SimpleITK torch os scipy msvcrt argparse numpy progressBar math random logging UNet_Base,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/josedolz_MSL-student-becomes-master.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\josedolz_MSL-student-becomes-master.pdf,,"Baseline methodsTo demonstrate the efficiency of the proposed model, we compared to several baselines. First, we include full-supervised baselines that will act as lower and upper bounds. The lower bound employs only a small set of fully labeled images (either 3, 5 or 10, depending on the setting), whereas the upper bound considers all the available training images. Then, we consider a single-branch network, referred to as Single, which receives both fully and partial labeled images without making distinction between them. To assess the impact of decoupling the branches without further supervision, similar to [26], we modify the baseline network by integrating two independent decoders, while the encoder remains the same. This model, which we refer to as Decoupled, is governed by different types of supervision at each branch. Then, our first model, which we refer to as KL, integrates the KL divergence term presented in Eq. ( 3). Last, KL+Ent corresponds to the whole proposed model, which couples the two important terms in Eq.(3) and Eq. ( 4) in the formulation.Implementation details We employed UNet as backbone architecture for the single branch models. Regarding the dual-branch architectures, we modified the decoding path of the standard UNet to accommodate two separate branches. All the networks are trained during 500 epochs by using Adam optimizer, with a batch size equal to 8. We empirically set the values of λ w , λ kd and λ ent to 0.1, 50 and 1, respectively. We found that our formulation provided the best results when the input distributions to the KL term in eq. (3) were very smooth, which was achieved by applying softmax over the softmax predictions. All the hyperparameters were fixed by using the independent validation set. Fur- Comparison with proposals As mentioned previously, a popular paradigm in weakly and semi-supervised segmentation is to resort to pseudo-masks generated by a trained model, which are used to re-train the network mimicking full supervision. To demonstrate that our model leverages more efficiently the available data, we train a network with the proposals generated by the Lower bound and KL models, whose results are reported in Table 2. We can observe that despite typically improving the base model, minimizing the cross-entropy over proposals does not outperform directly minimizing the entropy on the predictions of the partially labeled images.Ablation study on the importance of the KL term The objective of this ablation study is to assess the effect of balancing the importance of the KL term in our formulation.Particularly, the KL term plays a crucial role in the proposed formulation, as it guides the entropy term during training to avoid degenerate solutions. We note that the value of the KL term is typically 2 orders of magnitude smaller than the entropy objective. Therefore, by setting its weight (λ kl ) to  1, we demonstrate empirically its crucial role during training when coupled with the entropy term, as in this setting the latter strongly dominates the training. In this scenario, we observe that the model is negatively impacted, particularly when fully-labeled images are scarce, i.e., Set-3, significantly outperforming the lower bound model. This confirms our hypothesis that minimizing the entropy alone results in degenerated solutions. Increasing the weight of the KL term typically alleviates this issue. However, if much importance is given to this objective the performance also degrades. This is likely due to the fact that the bottom branch is strongly encouraged to follow the behaviour of the top branch, and the effect of the entropy term is diminished.Qualitative results In addition to the numerical results presented, we also depict qualitative results in Fig. 3 and Fig. 4. Particularly, Fig. 3 depicts the segmentation results for the models evaluated in Table 1. We can observe that segmentation results obtained by models with a single network typically undersegment the object of interest (first row) or generate many false positives (second row). Decoupling the decoding branches might reduce the false positive rate, however, it also tends to undersegment the target. Finally, we observe that both of our formulations achieve qualitatively better segmentation results, with the KL+Ent model yielding segmentations similar to those generated by the upper bound model. Furthermore, in Fig. 4 we illustrate additional qualitative results of our models. We observe that without the entropy term our model produces less confident predictions, which results in more noisy segmentations.  Figure 2 :2Figure 2: Shannon entropy (blue) and min-entropy (red) for a two-class distribution (p, 1 − p), with p ∈ [0, 1].",yes,"Adam, manual tuning",yes,,yes,,,,
1546,https://github.com/ecrows/l2-reddit-experiment,ecrows_l2-reddit-experiment.tar.gz,TOWARDS ETHICAL CONTENT-BASED DETECTION OF ONLINE INFLUENCE CAMPAIGNS,@classmethod,math scipy bert tensorflow pandas csv sys collections json glob errno statistics sklearn os __future__ numpy pprint,cs.CY cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ecrows_l2-reddit-experiment.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ecrows_l2-reddit-experiment.pdf,,,no,,no,,no,,,,
1690,https://github.com/AidanRocke/vertex_prediction,AidanRocke_vertex_prediction.tar.gz,Annotating Object Instances with a Polygon-RNN,,keras tensorflow group_norm unittest image_encoder glob random numpy matplotlib,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AidanRocke_vertex_prediction.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AidanRocke_vertex_prediction.pdf,,"TrainingTo train our model we use cross-entropy at each time step of the RNN. In order to not over-penalize the incorrect predictions that are close to the ground-truth vertex, we smooth our target distribution at each time step. We assign non-zero probability mass to those locations that are within a distance of 2 in our D × D output grid.We follow the typical training regime where we make predictions at each time step but feed in ground-truth vertex information to the next. We train our model using the Adam optimizer [12] with a batch size b = 8 and an initial learning rate of λ = 1e − 4. We decay the learning rate after 10 epochs by a factor of 10 and use the default values of β 1 = 0.9 and β 2 = 0.999.For the task of first vertex prediction, we train another CNN using a multi-task loss. In particular, we use the logistic loss for every location in the grid. As groundtruth for the object boundaries, we draw the edges of the ground-truth polygon, and use the vertices of the polygon as ground-truth for the vertex layer. Our full model takes approximately a day to train on a Nvidia Titan-X GPU. Prediction ModeWe first sanity check the performance of our model without any interaction from the annotator, i.e., we predict the full polygon automatically. We will refer to this setting as the prediction mode.Baselines: We use the recently proposed DeepMask [20] and SharpMask [21] as state-of-the-art baselines. Given an input image patch, DeepMask uses a CNN to output a pixel labeling of an object, and does so agnostic to the class. Sharpmask extends Deepmask by clever upsampling of the output to obtain the labeling at a much higher resolution (160 vs 56). Note that in their original approach, [20,21] exhaustively sample patches at different scales over the entire image. Here, we use ground-truth boxes when reporting performance for their approach. Further, DeepMask and SharpMask use a 50 layer ResNet [10] architecture, which has been trained on the COCO [16] dataset. We fine-tune this network on our Cityscapes split in two steps. In the first step, we fine-tune the feed-forward ResNet architecture for 150 epochs, followed by fine-tuning the weights for the Sharpmask's upsampling layers, for 70 epochs. This two step process is in the same spirit as that suggested in the paper. Note that while these two approaches perform well in labeling the pixels, their output cannot easily be corrected by an annotator in cases when mistakes occur. This is in contrast to our approach, which efficiently integrates a human in the loop in order to get high quality annotations.We use two additional baselines, SquareBox and Dila-tion10. SquareBox is a simple baseline where the full box is labeled as the object. Instead of taking the tight-fit box, we reduce the dimensions of the box, keeping the same aspect ratio. Based on the validation set, we get the best results by choosing 80% of the original box. If an instance has multiple components, we fit a box for each individual component as opposed to using the full box. This baseline mimics the scenario, in which the object is modeled simply as a box rather than a polygon. For the Dilation10 baseline, we use the segmentation results from [33], which was trained on the Cityscapes segmentation dataset. For each bounding box, we consider the pixels belonging to the respective object category as the instance mask.Quantitative Results: We report the IoU metric in Table 3. We outperform the baselines in 6 out of 8 categories, as well as in the average across all classes. We perform particularly well in car, person, and rider, outperforming Sharpmask by 12%, 7%, and 6%, respectively. This is particularly impressive since Sharpmask uses a more powerful ResNet architecture (we use VGG).Effect of object size: In Fig. 4, we see how our model performs w.r.t baselines on different instance sizes. For small instances our model performs significantly better than the baselines. For larger objects, the baselines have an ad- vantage due to larger output resolution. This effect is most notable for classes such as bus and train, in which our model obtains lower IOU compared to the baselines.",yes,Adam,yes,"batch size, epochs, momentum, learning rate, weight decay",no,,,,
1568,https://github.com/brain-research/conv-sv,brain-research_conv-sv.tar.gz,The Singular Values of Convolutional Layers,,math tensorflow re sys absl matplotlib2tikz conv2d_singular_values os numpy matplotlib time,cs.LG cs.AI stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/brain-research_conv-sv.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\brain-research_conv-sv.pdf,Batch Normalization,,yes,"SGD, manual tuning",yes,,yes,,,,
1643,https://github.com/lephong/mulrel-nel,lephong_mulrel-nel.tar.gz,Improving Entity Linking by Modeling Latent Relations between Mentions,@staticmethod,re nel sys json random torch argparse numpy pprint time io,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lephong_mulrel-nel.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lephong_mulrel-nel.pdf,,,yes,"Adam, manually selected",yes,,yes,,,,
1742,https://github.com/ytx21cn/CDCL-human-seg,ytx21cn_CDCL-human-seg.tar.gz,Cross-Domain Complementary Learning Using Pose for Multi-Person Part Segmentation,,utils model_simulated_RGB101_cdcl_pascal PIL copy sys code glob skimage tqdm imutils configobj util keras tensorflow model_simulated_RGB101 human_seg os scipy IPython mylayers segmentation argparse numpy time math config_reader cv2 io,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ytx21cn_CDCL-human-seg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ytx21cn_CDCL-human-seg.pdf,,,yes,Adam,yes,"learning rate, batch size, momentum",yes,,,,
1883,https://github.com/nnkkmto/item2vec-rec-test,nnkkmto_item2vec-rec-test.tar.gz,,,tarfile gensim pandas lib logging sklearn argparse os,cs.LG cs.AI cs.IR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nnkkmto_item2vec-rec-test.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nnkkmto_item2vec-rec-test.pdf,,"ITEM2VEC: NEURAL ITEM EMBEDDING FOR COLLABORATIVE FILTERINGOren Barkan^* and Noam Koenigstein* ^Tel Aviv University *MicrosoftSingle item recommendations based on item similarities are used also for a variety of other recommendation tasks: In ""candy rank"" recommendations for similar items (usually of lower price) are suggested at the check-out page right before the payment. In ""bundle"" recommendations a set of several items are grouped and recommended together. Finally, item similarities are used in online stores for better exploration and discovery and improve the overall user experience. It is unlikely that a user-item CF method, that learns the connections between items implicitly by defining slack variables for users, would produce better item representations than a method that is optimized to learn the item relations directly. Item similarities are also at the heart of item-based CF algorithms that aim at learning the representation directly from the item-item relations [4,5]. There are several scenarios where item-based CF methods are desired: in a large scale dataset, when the number of users is significantly larger than the number of items, the computational complexity of methods that model items solely is significantly lower than methods that model both users and items simultaneously. For example, online music services may have hundreds of millions of enrolled users with just tens of thousands of artists (items).In certain scenarios, the user-item relations are not available. For instance, a significant portion of today's online shopping is done without an explicit user identification process.Instead, the available information is per session. Treating these sessions as ""users"" would be prohibitively expensive as well as less informative.Recent progress in neural embedding methods for linguistic tasks have dramatically advanced state-ofthe-art NLP capabilities [6,7,8,12]. These methods attempt to map words and phrases to a low dimensional vector space that captures semantic relations between words. Specifically, Skip-gram with Negative Sampling (SGNS), known also as word2vec [8], set new records in various NLP tasks [7,8] and its applications have been extended to other domains beyond NLP [9,10].In this paper, we propose to apply SGNS to itembased CF. Motivated by its great success in other domains, we suggest that SGNS with minor modifications may capture the relations between different items in collaborative filtering datasets. To this end, we propose a modified version of SGNS named item2vec. We show that item2vec can induce a similarity measure that is competitive with an itembased CF using SVD, while leaving the comparison to other more complex methods to a future research.The rest of the paper is organized as follows: Section 2 overviews the SGNS method. Section 3 describes how to apply SGNS to item-based CF. In Section 4, we describe the experimental setup and present qualitative and quantitative results.",yes,SGD,no,,no,,,,
1514,https://github.com/chirag126/nifty,chirag126_nifty.tar.gz,Towards a Unified Framework for Fair and Stable Graph Representation Learning,,utils gc copy pandas warnings tqdm fairgnn_utils torch os matplotlib torch_geometric dgl scipy ipdb seaborn aif360 sklearn argparse numpy deeprobust time math networkx random models,cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/chirag126_nifty.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\chirag126_nifty.pdf,Weight Normalization,"|DATASETS AND EXPERIMENTAL SETUPWe first describe datasets designed to study fair and stable network embeddings and then outline experimental setup.Datasets. We construct three new datasets. 1) The German credit graph has 1,000 nodes representing clients in a German bank that are connected based on the similarity of their credit accounts. The task is to classify clients into good vs. bad credit risks considering clients' gender as the sensitive attribute [Dua and Graff, 2017]. 2) The Recidivism graph has 18,876 nodes representing defendants who got released on bail at the U.S state courts during 1990-2009[Jordan and Freiburger, 2015. Defendants are connected based on the similarity of past criminal records and demographics. The goal is to classify defendants into bail (i.e., unlikely to commit a violent crime if released) vs. no bail (i.e., likely to commit a violent crime) considering race information as the protected attribute. 3) The Credit defaulter graph has 30,000 nodes representing individuals that we connected based on the similarity of their spending and payment patterns [Yeh and Lien, 2009]. The task is to predict whether an individual will default on the credit card payment or not while considering age as the sensitive attribute. See Appendix for details on dataset construction.Performance evaluation. To measure predictive performance of downstream binary node classification, we use AUROC and F1-score. To quantify group fairness, we use statistical parity (SP) [Dwork et al., 2012], defined as: ∆ SP =||P (ŷ u =1||s=0)−P (ŷ u =1||s=1)||, and equal opportunity (EO) [Hardt et al., 2016], defined as: ∆ EO =||P (ŷ u =1||y u =1, s=0)−P (ŷ u =1||y u =1, s=1)||, where probabilities are estimated on the test set [Dai and Wang, 2021]. To measure counterfactual fairness, we define the unfairness score as the percentage of test nodes for which predicted label changes when the node's sensitive attribute is flipped. Finally, the instability score represents the percentage of test nodes for which predicted label changes when random noise is added to node attributes.GNN methods. To investigate the flexibility of NIFTY, we incorporate it into five estabished and state-of-the-art GNN methods: GCN [Kipf and Welling, 2017], Graph-SAGE [Hamilton et al., 2017], Jumping Knowledge (JK) [Xu et al., 2018], GIN [Xu et al., 2019], and Info-Max [Veličković et al., 2019].Baseline methods and implementation. We consider two baseline methods: FairGCN [Dai and Wang, 2021] and Ro-bustGCN [Zhu et al., 2019]; all hyperparameters are set  following the authors' guidelines. We use stop-gradient operation for training the Siamese networks [Chen and He, 2020]. We set regularization coefficient to λ = 0.6 in all our experiments and conduct a sensitivity analysis into the effect of λ on NIFTY's performance. See Appendix for details.|",yes,Adam,yes,,no,,,,
1649,https://github.com/kencan7749/pytorch_iCNN,kencan7749_pytorch_iCNN.tar.gz,Understanding Deep Image Representations by Inverting Them,,utils PIL scipy loss copy icnn_torch datetime utils_ME utils_hts sys cv2 torch os numpy merge_utils,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kencan7749_pytorch_iCNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kencan7749_pytorch_iCNN.pdf,,,yes,SGD,no,,no,,,,
1934,https://github.com/nrdg/fracridge,nrdg_fracridge.tar.gz,"A reparameterization of Ridge Regression Fractional ridge regression: a fast, interpretable reparameterization of ridge regression","@pytest.mark.parametrize(""bb"", [(1), (2), (1000)]) @pytest.mark.parametrize(""fit_intercept"", [False]) @njit(fastmath=True, nogil=True, cache=True) @pytest.mark.parametrize(""fit_intercept"", [(False), (True)]) @pytest.mark.parametrize(""jit"", [True, False]) @pytest.mark.parametrize(""fit_intercept"", [True, False]) @pytest.mark.parametrize(""nn"", [(1000), (10), (284)]) @pytest.mark.parametrize(""nn, pp"", [(1000, 10), (10, 100), (284, 50)]) @pytest.mark.parametrize(""frac"", [0.1, 0.23, 1])",fuzzywuzzy warnings sys fracridge distutils pathlib json collections sphinx_gallery setuptools_scm os pytest sphinx matplotlib sphinx_rtd_theme string scipy functools numba sklearn numpy shutil setuptools subprocess,stat.ME cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nrdg_fracridge.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nrdg_fracridge.pdf,,"PythonThe Python implementation of FRR depends on Scipy (Virtanen et al., 2020) and Numpy (van der Walt et al., 2011). The object-oriented interface provided conforms with the API of the popular Scikit-Learn library (Pedregosa et al., 2011;Buitinck et al., 2013), including automated tests that verify compliance with this API. Unit tests are implemented using pytest (Krekel et al., 2004-). Documentation is automatically compiled using sphinx, with sphinx-gallery examples ( Òscar Nájera et al., 2020). The Python implementation also optionally uses Numba (Lam et al., 2015) for just-in-time compilation of a few of the underlying numerical routines used in the implementation. This functionality relies on an implementation provided in the hyperlearn library (Han-Chen, 2020) and copied into the fracridge source-code, together with its license, which is compatible with the fracridge license. In addition to its release on GitHub, the software is available to install through the Python Package Index (PyPI) through the standard Python Package Installer (pip install fracridge). For Python, we did not implement standardization procedures, as those are implemented as a part of Scikit-Learn. ); coef3 = inv(X'*X + alphaI)*X'*y; norm(coef) norm(coef2) norm(coef2) ./ norm(coef) norm(coef2-coef3)Python import numpy as np from numpy.linalg import inv, norm from fracridge import fracridge from fracridge import FracRidge y = np.random.randn(100) X = np.random.randn(100, 10) # Calculate coefficients with naive OLS coef = inv(X.T @ X) @ X.T @ y # Call fracridge function: coef2, alpha = fracridge(X, y, 0.3) # Calculate coefficients with naive RR alphaI = alpha * np.eye(X.shape[1]) coef3 = inv(X.T @ X + alphaI) @ X.T @ y print(norm(coef)) print(norm(coef2)) print(norm(coef2) / norm(coef)) print(norm(coef2 -coef3))# sklearn-compatible object-oriented API: fr = FracRidge(fracs=0.3) fr.fit(X, y) coef_oo = fr.coef_ alpha_oo = fr.alpha_ print(norm(coef_oo) / norm(coef))",no,,no,,no,,,,
1798,https://github.com/petrovskaia/maxvol-for-soil,petrovskaia_maxvol-for-soil.tar.gz,Optimal soil sampling design based on the maxvol algorithm,,scipy maxvol_cut maxvolpy sklearn numpy matplotlib,cs.CY stat.AP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/petrovskaia_maxvol-for-soil.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\petrovskaia_maxvol-for-soil.pdf,,,no,,no,,no,,,,
1802,https://github.com/therooler/pennylane-qllh,therooler_pennylane-qllh.tar.gz,Implementing perceptron models with qubits,@staticmethod,tensorflow itertools setuptools pennylane rockyraccoon typing os numpy matplotlib,quant-ph,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/therooler_pennylane-qllh.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\therooler_pennylane-qllh.pdf,,,no,,no,,no,,,,
1779,https://github.com/sccfnad/Sparse-convolutional-coding-for-neuronal-assembly-detection,sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.tar.gz,Sparse convolutional coding for neuronal assembly detection,,"Plotting copy sys __future__ sys, os json h5py os matplotlib Sorting sys, re, os scipy itertools sklearn argparse numpy time shutil math Convensembles",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\sccfnad_Sparse-convolutional-coding-for-neuronal-assembly-detection.pdf,,,no,,no,,no,,,,
1787,https://github.com/siat-nlp/CpRec,siat-nlp_CpRec.tar.gz,A Generic Network Compression Framework for Sequential Recommender Systems,,utils math tensorflow BlockWiseEmbedding generator_recsys argparse os ops_compress numpy time shutil Data_loader,cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/siat-nlp_CpRec.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\siat-nlp_CpRec.pdf,Softmax,"EXPERIMENTAL SETUPis section introduces our experimental setup including datasets, baseline models, our implementation details and evaluation metrics.",yes,Adam,yes,"batch size, learning rate",yes,,,,
1748,https://github.com/MoMe36/BranchingDQN,MoMe36_BranchingDQN.tar.gz,Action Branching Architectures for Deep Reinforcement Learning,,utils scipy pandas model random tqdm torch argparse os numpy matplotlib time gym,cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MoMe36_BranchingDQN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MoMe36_BranchingDQN.pdf,,,yes,Adam,yes,"minibatch size, epochs, learning rate, momentum, discount factor",no,,,,
1672,https://github.com/dongkwonjin/Semantic-Line-MWCS,dongkwonjin_Semantic-Line-MWCS.tar.gz,Harmonious Semantic Line Detection via Maximal Weight Clique Selection,@property,PIL torchvision libs tests torch os evaluation_DHT matplotlib visualizes post_processes pickle networks sklearn numbers numpy trains shutil math datasets cv2 random options evaluation,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dongkwonjin_Semantic-Line-MWCS.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dongkwonjin_Semantic-Line-MWCS.pdf,,,no,,no,,no,,,,
1955,https://github.com/team-approx-bayes/dl-with-bayes,team-approx-bayes_dl-with-bayes.tar.gz,Practical Deep Learning with Bayesian Principles,,importlib torchvision math imageio json torchsso pickle sklearn argparse os torch numpy matplotlib mpi4py inspect shutil,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/team-approx-bayes_dl-with-bayes.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\team-approx-bayes_dl-with-bayes.pdf,Adam,"19:µ ← µ − αm/(s + δ + γ). 20: until stopping criterion is met w (8) < l a t e x i t s h a 1 _ b a s e 6 4 = "" 0 AX 6 C I J d p G 7 l x M 3 d Z N Y U g X Y X y Y A = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y V R w S 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r W 9 s b h W 3 S z u 7 e / s H 9 u F R S 4 W x J L R J Q h 7 K j o c V 5 U z Q p m a a 0 0 4 k K Q 4 8 T t v e 5 D b z 2 1 M q F Q v F g 5 5 F t B / g k W A + I 1 g b a W D b v Q D r s e c n T + l j U q m d p w O 7 7 F S d O d A q c X N S h h y N g f 3 V G 4 Y k D q j Q h G O l u q 4 T 6 X 6 C p W a E 0 7 T U i x W N M J n g E e 0 a K n B A V T + Z J 0 / R m V G G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z T r X s Z e J / X j f W f q 2 f M B H F m g q y O O T H H O k Q Z T W g I Z O U a D 4 z B B P J T F Z E x l h i o k 1 Z J V O C u / z l V d K 6 q L q X V e f + q l y / y e s o w g m c Q g V c u I Y 6 3 E E D m k B g C s / w C m 9 W Y r 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w 9 G n J N p < /l a t e x i t > w (7) < l a t e x i t s h a 1 _ b a s e 6 4 = "" w d y E a r qC E e V H b D 9 Y S b b I d 9 H 4 C 4 Y = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y V R o S 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r W 9 s b h W 3 S z u 7 e / s H 9 u F R S 4 W x J L R J Q h 7 K j o c V 5 U z Q p m a a 0 0 4 k K Q 4 8 T t v e 5 D b z 2 1 M q F Q v F g 5 5 F t B / g k W A + I 1 g b a W D b v Q D r s e c n T + l j U q m d p w O 7 7 F S d O d A q c X N S h h y N g f 3 V G 4 Y k D q j Q h G O l u q 4 T 6 X 6 C p W a E 0 7 T U i x W N M J n g E e 0 a K n B A V T + Z J 0 / R m V G G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z T r X s Z e J / X j f W / n U / Y S K K N R V k c c i P O d I h y m p A Q y Y p 0 X x m C C a S m a y I j L H E R J u y S q Y E d / n L q 6 R 1 U X U v q 8 7 9 V b l + k 9 d R h B M 4 h Q q 4 U I M 6 3 E E D m k B g C s / w C m 9 W Y r 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w 9 F F p N o < / l a t e x i t > w (6) < l a t e x i t s h a 1 _ b a s e 6 4 = "" Z R r N h 8 W v B s e 2 C T o E T / I V G F U d S X g = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y V R U Z d F N y 4 r 2 A e 0 t U y m k 3 b o Z B J m J p U S 8 i d u X C j i 1 j 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 X s S Z 0 o 7 z b R V W V t f W N 4 q b p a 3 t n d 0 9 e / + g q c J Y E t o g I Q 9 l 2 8 O K c i Z o Q z P N a T u S F A c e p y 1 v f J v 5 r Q m V i o X i Q U 8 j 2 g v w U D C f E a y N 1 L f t b o D 1 y P O T p / Q x q V y e p n 2 7 7 F S d G d A y c X N S h h z 1 v v 3 V H Y Q k D q j Q h G O l O q 4 T 6 V 6 C p W a E 0 7 T U j R W N M B n j I e 0 Y K n B A V S + Z J U / R i V E G y A + l e U K j m f p 7 I 8 G B U t P A M 5 N Z T r X o Z e J / X i f W / n U v Y S K K N R V k f s i P O d I h y m p A A y Y p 0 X x q C Ca S m a y I j L D E R J u y S q Y E d / H L y 6 R 5 V n X P q 8 7 9 R b l 2 k 9 d R h C M 4 h g q 4 c A U 1 u I M 6 N I D A B J 7 h F d 6 s x H q x 3 q 2 P + W j B y n c O 4 Q + s z x 9 D k J N n < / l a t e x i t > w (5) < l a t e x i t s h a 1 _ b a s e 6 4 = ""B 2 L n C C B s D G x 8 M 7 K w k i P 5 4 C t p I 2 E = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y X x g S 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 9 v 5 B U 4 W x J L R B Q h 7 K t o c V 5 U z Q h m a a 0 3 Y k K Q 4 8 T l v e + D b z W x M q F Q v F g 5 5 G t B f g o W A + I 1 g b q W / b 3 Q D r k e c n T + l j U r k 8 T f t 2 2 a k 6 M 6 B l 4 u a k D D n q f f u r O w h J H F C h C c d K d V w n 0 r 0 E S 8 0 I p 2 m p G y s a Y T L G Q 9 o x V O C A q l 4 y S 5 6 i E 6 M M k B 9 K 8 4 R G M / X 3 R o I D p a a B Z y a z n G r R y 8 T / v E 6 s / e t e w k Q U a y r I / J A f c 6 R D l N W A B k x S o v n U E E w k M 1 k R G W G J i T Z l l U w J 7 u K X l 0 n z r O q e V 5 3 7 i 3 L t J q + j C E d w D B V w 4 Q p q c A d 1 a A C B C T z D K 7 x Z i f V i v V s f 8 9 G Cl e 8 c w h 9 Y n z 9 C C p N m < / l a t e x i t > w (4) < l a t e x i t s h a 1 _ b a s e 6 4 = "" J j h+ r o V 7 R X C t H v 1 5 j i K B S 8 Q 2 k V 8 = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y X R g i 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r W 9 s b h W 3 S z u 7 e / s H 9 u F R S 4 W x J L R J Q h 7 K j o c V 5 U z Q p m a a 0 0 4 k K Q 4 8 T t v e 5 D b z 2 1 M q F Q v F g 5 5 F t B / g k W A + I 1 g b a W D b v Q D r s e c n T + l j U q m d p w O 7 7 F S d O d A q c X N S h h y N g f 3 V G 4 Y k D q j Q h G O l u q 4 T 6 X 6 C p W a E 0 7 T U i x W N M J n g E e 0 a K n B A V T + Z J 0 / R m V G G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z T r X s Z e J / X j f W / n U / Y S K K N R V k c c i P O d I h y m p A Q y Y p 0 X x m C C a S m a y I j L H E R J u y S q Y E d / n L q 6 R 1 U X U v q 8 5 9 r V y / y e s o w g m c Q g V c u I I 6 3 E E D m k B g C s / w C m 9 W Y r 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w 9 A h J N l < / l a t e x i t > w (3) < l a t e x i t s h a 1 _ b a s e 6 4 = "" h v h n f 9 u G B x M R 9 D / 7 h k T L Q S q 9 t 6 g = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y W x g i 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r W 9 s b h W 3 S z u 7 e / s H 9 u F R S 4 W x J L R J Q h 7 K j o c V 5 U z Q p m a a 0 0 4 k K Q 4 8 T t v e 5 D b z 2 1 M q F Q v F g 5 5 F t B / g k W A + I 1 g b a W D b v Q D r s e c n T + l j U q m d p w O 7 7 F S d O d A q c X N S h h y N g f 3 V G 4 Y k D q j Q h G O l u q 4 T 6 X 6 C p W a E 0 7 T U i x W N M J n g E e 0 a K n B A V T + Z J 0 / R m V G G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z T r X s Z e J / X j f W / n U / Y S K K N R V k c c i P O d I h y m p A Q y Y p 0 X x m C C a S m a y I j L H E R J u y S q Y E d / n L q 6 R 1 U X V r V e f + s l y / y e s o w g m c Q g V c u I I 6 3 E E D m k B g C s / w C m 9 W Y r 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w 8 + / p N k < / l a t e x i t >w (2) < l a t e x i t s h a 1 _ b a s e 6 4 = "" p e Z M H p n h s nV K m u 0 F y h J J G 0 t z l O 4 = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o t Q N y W p g i 6 L b l x W s A 9 o a 5 l M J + 3 Q y S T M T C o l 5 E / c u F D E r X / i z r 9 x 0 m a h r Q c G D u f c y z 1 z v I g z p R 3 n 2 y q s r W 9 s b h W 3 S z u 7 e / s H 9 u F R S 4 W x J L R J Q h 7 K j o c V 5 U z Q p m a a 0 0 4 k K Q 4 8 T t v e 5 D b z 2 1 M q F Q v F g 5 5 F t B / g k W A + I 1 g b a W D b v Q D r s e c n T + l j U q m d p w O 7 7 F S d O d A q c X N S h h y N g f 3 V G 4 Y k D q j Q h G O l u q 4 T 6 X 6 C p W a E 0 7 T U i x W N M J n g E e 0 a K n B A V T + Z J 0 / R m V G G y A + l e U K j u f p 7 I 8 G B U r P A M 5 N Z T r X s Z e J / X j f W / n U / Y S K K N R V k c c i P O d I h y m p A Q y Y p 0 X x m C Ca S m a y I j L H E R J u y S q Y E d / n L q 6 R V q 7 o X V e f + s l y / y e s o w g m c Q g V c u I I 6 3 E E D m k B g C s / w C m 9 W Y r 1 Y 7 9 b H Y r R g 5 T v H 8 A f W 5 w 8 9 e J N j < / l a t e x i t > w (1) < l a t e x i t s h a 1 _ b a s e 6 4 = "" D Z V r i W H h A A B p / V I r x p V e / A k N e 7 4 = "" > AA A B + X i c b V D L S s N A F L 3 x W e s r 6 t L N Y B H q p i Q q 6 L L o x m U F + 4 A 2 l s l 0 0 g 6 d T M L M p F J C / s S N C 0 X c + i f u / B s n b R b a e m D g c M 6 9 3 D P H j z l T 2 n G + r Z X V t f W N z d J W e X t n d 2 / f P j h s q S i R h D Z J x C P Z 8 b G i n A n a 1 E x z 2 o k l x a H P a d s f 3 + Z + e 0 K l Y p F 4 0 N O Y e i E e C h Y w g r W R + r b d C 7 E e + U H 6 l D 2 m V f c s 6 9 s V p + b M g J a J W 5 A K F G j 0 7 a / e I C J J S I U m H C v V d Z 1 Y e y m W m h F O s 3 I v U T T G Z I y H t G u o w C F V X j p L n q F T o w x Q E E n z h E Y z 9 f d G i k O l p q F v J v O c a t H L x f + 8 b q K D a y 9 l I k 4 0 F W R + K E g 4 0 h H K a 0 A D J i n R f G o I J p K Z r I i M s M R E m 7 L K p g R 3 8 c v L p H V e c y 9 q z v 1 l p X 5 T 1 F G C Y z i B K r h w B X W 4 g w Y 0 g c A E n u E V 3 q z U e r H e r Y / 5 6 I p V 7 B z B H 1 i f P z v y k 2 I = < / l a t e x i t > w (i) ⇠ q(w) < l a t e x i t s h a 1 _ b a s e 6 4 = "" D v 2 Z W T 7 a V b 6 y C M x 0 A I W 4 B 6 z A H B Y = "" > A A A C C 3 i c b V D L S s N A F J 3 U V 4 2 v q E s 3 Q 4 v Q b k q i g i 6 L b l x W s A 9 o Y p l M J + 3 Q y S T O T J Q S u n f j r 7 h x o Y h b f 8 C d f + O k D a i t B y 4 c z r m X e + / x Y 0 a l s u 0 v o 7 C 0 v L K 6 V l w 3 N z a 3 t n e s 3 b 2 W j B K B S R N H L B I d H 0 n C K C d N R R U j n V g Q F P q M t P 3 R R e a 3 7 4 i Q N O L X a h w T L 0 Q D T g O K k d J S z y q 5 I V J D P 0 j v J z d p h V Y n r q S h e V v 5 k a s 9 q 2 z X 7 C n g I n F y U g Y 5 G j 3 r 0 + 1 H O A k J V 5 g h K b u O H S s v R U J R z M j E d B N J Y o R H a E C 6 m n I U E u m l 0 1 8 m 8 F A r f R h E Q h d X c K r + n k h R K O U 4 9 H V n d q K c 9 z L x P 6 + b q O D M S y m P E 0 U 4 n i 0 K E g Z V B L N g Y J 8 K g h U b a 4 K w o P p W i I d I I K x 0 f K Y O w Z l / e Z G 0 j m r O c c 2 + O i n X z / M 4 i u A A l E A F O O A U 1 M E l a I A m w O A B P I E X 8 G o 8 G s / G m / E + a y 0 Y + c w + + A P j 4 x v 4 C Z r 8 < / l a t e x i t > M < l a t e x i t s h a 1 _ b a s e 6 4 = "" u Z v z E R Q P K M M R 9 N i L G A c S / T 5 q G + 0 = "" > A A A B 8 n i c b V B N S 8 N A F H y p X 7 V + V T 1 6 W S y C p 5 K o o M e i F y 9 C B W s L a S i b 7 b Z d u t m E 3 R e h h P 4 M L x 4 U 8 e q v 8 e a / c d P m o K 0 D C 8 P M e + y 8 C R M p D L r u t 1 N a W V 1 b 3 y h v V r a 2 d 3 b 3 q v s H j y Z O N e M t F s t Y d 0 J q u B S K t 1 C g 5 J 1 E c x q F k r f D 8 U 3 u t 5 + 4 N i J W D z h J e B D R o R I D w S h a y e 9 G F E e M y u x u 2 q v W 3 L o 7 A 1 k m X k F q U K D Z q 3 5 1 + z F L I 6 6 Q S W q M 7 7 k J B h n V K J j k 0 0 o 3 N T y h b E y H 3 L d U 0 Y i b I J t F n p I T q / T J I N b 2 K S Q z 9 f d G R i N j J l F o J / O I Z t H L x f 8 8 P 8 X B V Z A J l a T I F Z t / N E g l w Z j k 9 5 O + 0 J y h n F h C m R Y 2 K 2 E j q i l D 2 1 L F l u A t n r x M H s / q 3 n n d v b + o N a 6 L O s p w B M d w C h 5 c Q g N u o Q k t Y B D D M 7 z C m 4 P O i / P u f M x H S 0 6 x c w h / 4 H z + A I M 0 k W U = < / l a t e x i t > M local < l a t e x i t s h a 1 _ b a s e 6 4 = "" Z w i U Q G l f N I i A P S g L d N t g j p + c W 9 4 = "" > A A A B / H i c b V D L S s N A F L 3 x W e s r 2 q W b w S K 4 K o k K u i y 6 c S N U s A 9 o Q 5 h M J + 3 Q y S T M T I Q Q 6 q + 4 c a G I W z / E n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B A l n S j v O t 7 W y u r a + s V n Z q m 7 v 7 O 7 t 2 w e H H R W n k t A 2 i X k s e w F W l D N B 2 5 p p T n u J p D g K O O 0 G k 5 v C 7 z 5 S q V g s H n S W U C / C I 8 F C R r A 2 k m / X B h H W Y 4 J 5 f j f 1 c x 4 b N v X t u t N w Z k D L x C 1 J H U q 0 f P t r M I x J G l G h C c d K 9 V 0 n 0 V 6 O p W a E 0 2 l 1 k C q a Y D L B I 9 o 3 V O C I K i + f h Z + i E 6 M M U R h L 8 4 R G M / X 3 R o 4 j p b I o M J N F V L X o F e J / X j / V 4 Z W X M 5 G k m g o y P x S m H O k Y F U 2 g I Z O U a J 4 Z g o l k J i s i Y y w x 0 a a v q i n B X f z y M u m c N d z z h n N / U W 9 e l 3 V U 4 A i O 4 R R c u I Q m 3 E I L 2 k A g g 2 d 4 h T fr y X q x 3 q 2 P + e i K V e 7 U 4 A + s z x 9 z 0 J V I < / l a t e x i t > ĝ < l a t e x i t s h a 1 _ b a s e 6 4 = "" M l F g S W D u h H u l 1 v p 7 7 q 5 w m 5 H aD K U = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q q 6 L L o x m U F + 4 A m l M l 0 0 g 6 d P J i 5 K Z T Q P 3 H j Q h G 3 / o k 7 / 8 Z J m 4 V W D w w c z r m X e + Y E q R Q a H e f L q q y t b 2 x u V b d r O 7 t 7 + w f 2 4 V F H J 5 l i v M 0 S m a h e Q D W X I u Z t F C h 5 L 1 W c R o H k 3 W B y V / j d K V d a J P E j z l L u R 3 Q U i 1 A w i k Y a 2 L Y 3 p p h 7 E c V x E O a j + X x g 1 5 2 G s w D 5 S 9 y S 1 K F E a 2 B / e s O E Z R G P k U m q d d 9 1 U v R z q l A w y e c 1 L 9 M 8 p W x C R 7 x v a E w j r v 1 8 k X x O z o w y J G G i z I u R L N S f G z m N t J 5 F g Z k s I u p V r x D / 8 / o Z h j d + L u I 0 Q x 6 z 5 a E w k w Q T U t R A h k J x h n J m C G V K m K y E j a m i D E 1 Z N V O C u / r l v 6 R z 0 X A v G 8 7 D V b 1 5 W 9 Z R h R M 4 h X N w 4 R q a c A 8 t a A O D K T z B C 7 x a u f V s v V n v y 9 G K V e 4 c w y 9 Y H 9 8 8 y Z Q L < / l a t e x i t > ĥ < l a t e x i t s h a 1 _ b a s e 6 4 = "" g o o R a E z m A k h 1 J P Q L 6 5 z W V o I q Q z M = "" > A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I V d F l 0 4 7 K C f U A T y m Q 6 a Y Z O J m H m p l B C / s S N C 0 X c + i f u / B s n b R Z a P T B w O O d e 7 p k T p I J r c J w v q 7 a 2 v r G 5 V d 9 u 7 O z u 7 R / Y h 0 c 9 n W S K s i 5 N R K I G A d F M c M m 6 w E G w Q a o Y i Q P B + s H 0 r v T 7 M 6 Y 0 T + Q j z F P m x 2 Q i e c g p A S O N b N u L C O R e T C A K w j w q i p H d d F r O A v g v c S v S R B U 6 I / v T G y c 0 i 5 k E K o j W Q 9 d J w c + J A k 4 F K x p e p l l K 6 J R M 2 N B Q S W K m / X y R v M B n R h n j M F H m S c A L 9 e d G T m K t 5 3 F g J s u I e t U r x f + 8 Y Q b h j Z 9 z m W b A J F 0 e C j O B I c F l D X j M F a M g 5 o Y Q q r j J i m l E F K F g y m q Y E t z V L / 8 l v Y u W e 9 l y H q 6 a 7 d u q j j o 6 Q a f o H L n o G r X R P e q g L q J o h p 7 Q C 3 q 1 c u v Z e r P e l 6 M 1 q 9 o 5 R r 9 g f X w D P k + U D A = = < / l a t e x i t > M local < l a t e x i t s h a 1 _ b a s e 6 4 = "" Z w i U Q G l f N I i A P S g L d N t g j p + c W 9 4 = "" > A A A B / H i c b V D L S s N A F L 3 x W e s r 2 q W b w S K 4 K o k K u i y 6 c S N U s A 9 o Q 5 h M J + 3 Q y S T M T I Q Q 6 q + 4 c a G I W z / E n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B A l n S j v O t 7 W y u r a + s V n Z q m 7 v 7 O 7 t 2 w e H H R W n k t A 2 i X k s e w F W l D N B 2 5 p p T n u J p D g K O O 0 G k 5 v C 7 z 5 S q V g s H n S W U C / C I 8 F C R r A 2 k m / X B h H W Y 4 J 5 f j f 1 c x 4 b N v X t u t N w Z k D L x C 1 J H U q 0 f P t r M I x J G l G h C c d K 9 V 0 n 0 V 6 O p W a E 0 2 l 1 k C q a Y D L B I 9 o 3 V O C I K i + f h Z + i E 6 M M U R h L 8 4 R G M / X 3 R o 4 j p b I o M J N F V L X o F e J / X j / V 4 Z W X M 5 G k m g o y P x S m H O k Y F U 2 g I Z O U a J 4 Z g o l k J i s i Y y w x 0 a a v q i n B X f z y M u m c N d z z h n N / U W 9 e l 3 V U 4 A i O 4 R R c u I Q m 3 E I L 2 k A g g 2 d 4 h T f r y X q x 3 q 2 P + e i K V e 7 U 4 A + s z x 9 z 0 J V I < / l a t e x i t > M local < l a t e x i t s h a 1 _ b a s e 6 4 = "" Z w i U Q G l f N I i A P S g L d N t g j p + c W 9 4 = "" > A A A B / H i c b V D L S s N A F L 3 x W e s r 2 q W b w S K 4 K o k K u i y 6 c S N U s A 9 o Q 5 h M J + 3 Q y S T M T I Q Q 6 q + 4 c a G I W z / E n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B A l n S j v O t 7 W y u r a + s V n Z q m 7 v 7 O 7 t 2 w e H H R W n k t A 2 i X k s e w F W l D N B 2 5 p p T n u J p D g K O O 0 G k 5 v C 7 z 5 S q V g s H n S W U C / C I 8 F C R r A 2 k m / X B h H W Y 4 J 5 f j f 1 c x 4 b N v X t u t N w Z k D L x C 1 J H U q 0 f P t r M I x J G l G h C c d K 9 V 0 n 0 V 6 O p W a E 0 2 l 1 k C q a Y D L B I 9 o 3 V O C I K i + f h Z + i E 6 M M U R h L 8 4 R G M / X 3 R o 4 j p b I o M J N F V L X o F e J / X j / V 4 Z W X M 5 G k m g o y P x S m H O k Y F U 2 g I Z O U a J 4 Z g o l k J i s i Y y w x 0 a a v q i n B X f z y M u m c N d z z h n N / U W 9 e l 3 V U 4 A i O 4 R R c u I Q m 3 E I L 2 k A g g 2 d 4 h T f r y X q x 3 q 2 P + e i K V e 7 U 4 A + s z x 9 z 0 J V I < / l a t e x i t > M local < l a t e x i t s h a 1 _ b a s e 6 4 = "" Z w i U Q G l f N I i A P S g L d N t g j p + c W 9 4 = "" > A A A B / H i c b V D L S s N A F L 3 x W e s r 2 q W b w S K 4 K o k K u i y 6 c S N U s A 9 o Q 5 h M J + 3 Q y S T M T I Q Q 6 q + 4 c a G I W z / E n X / j p M 1 C W w 8 M H M 6 5 l 3 v m B A l n S j v O t 7 W y u r a + s V n Z q m 7 v 7 O 7 t 2 w e H H R W n k t A 2 i X k s e w F W l D N B 2 5 p p T n u J p D g K O O 0 G k 5 v C 7 z 5 S q V g s H n S W U C / C I 8 F C R r A 2 k m / X B h H W Y 4 J 5 f j f 1 c x 4 b N v X t u t N w Z k D L x C 1 J H U q 0 f P t r M I x J G l G h C c d K 9 V 0 n 0 V 6 O p W a E 0 2 l 1 k C q a Y D L B I 9 o 3 V O C I K i + f h Z + i E 6 M M U R h L 8 4 R G M / X 3 R o 4 j p b I o M J N F V L X o F e J / X j / V 4 Z W X M 5 G k m g o y P x S m H O k Y F U 2 g I Z O U a J 4 Z g o l k J i s i Y y w x 0 a a v q i n B X f z y M u m c N d z z h n N / U W 9 e l 3 V U 4 A i O 4 R R c u I Q m 3 E I L 2 k A g g 2 d 4 h T fr y X q x 3 q 2 P + e i K V e 7 U 4 A + s z x 9 z 0 J V I < / l a t e x i t > ĝ < l a t e x i t s h a 1 _ b a s e 6 4 = "" M l F g S W D u h H u l 1 v p 7 7 q 5 w m 5 H aD K U = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q q 6 L L o x m U F + 4 A m l M l 0 0 g 6 d P J i 5 K Z T Q P 3 H j Q h G 3 / o k 7 / 8 Z J m 4 V W D w w c z r m X e + Y E q R Q a H e f L q q y t b 2 x u V b d r O 7 t 7 + w f 2 4 V F H J 5 l i v M 0 S m a h e Q D W X I u Z t F C h 5 L 1 W c R o H k 3 W B y V / j d K V d a J P E j z l L u R 3 Q U i 1 A w i k Y a 2 L Y 3 p p h 7 E c V x E O a j + X x g 1 5 2 G s w D 5 S 9 y S 1 K F E a 2 B / e s O E Z R G P k U m q d d 9 1 U v R z q l A w y e c 1 L 9 M 8 p W x C R 7 x v a E w j r v 1 8 k X x O z o w y J G G i z I u R L N S f G z m N t J 5 F g Z k s I u p V r x D / 8 / o Z h j d + L u I 0 Q x 6 z 5 a E w k w Q T U t R A h k J x h n J m C G V K m K y E j a m i D E 1 Z N V O C u / r l v 6 R z 0 X A v G 8 7 D V b 1 5 W 9 Z R h R M 4 h X N w 4 R q a c A 8 t a A O D K T z B C 7 x a u f V s v V n v y 9 G K V e 4 c w y 9 Y H 9 8 8 y Z Q L < / l a t e x i t > ĥ < l a t e x i t s h a 1 _ b a s e 6 4 = "" g o o R a E z m A k h 1 J P Q L 6 5 z W V o I q Q z M = "" > A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I V d F l 0 4 7 K C f U A T y m Q 6 a Y Z O J m H m p l B C / s S N C 0 X c + i f u / B s n b R Z a P T B w O O d e 7 p k T p I J r c J w v q 7 a 2 v r G 5 V d 9 u 7 O z u 7 R / Y h 0 c 9 n W S K s i 5 N R K I G A d F M c M m 6 w E G w Q a o Y i Q P B + s H 0 r v T 7 M 6 Y 0 T + Q j z F P m x 2 Q i e c g p A S O N b N u L C O R e T C A K w j w q i p H d d F r O A v g v c S v S R B U 6 I / v T G y c 0 i 5 k E K o j W Q 9 d J w c + J A k 4 F K x p e p l l K 6 J R M 2 N B Q S W K m / X y R v M B n R h n j M F H m S c A L 9 e d G T m K t 5 3 F g J s u I e t U r x f + 8 Y Q b h j Z 9 z m W b A J F 0 e C j O B I c F l D X j M F a M g 5 o Y Q q r j J i m l E F K F g y m q Y E t z V L / 8 l v Y u W e 9 l y H q 6 a 7 d u q j j o 6 Q a f o H L n o G r X R P e q g L q J o h p 7 Q C 3 q 1 c u v Z e r P e l 6 M 1 q 9 o 5 R r 9 g f X w D P k + U D A = = < / l a t e x i t > ĝ < l a t e x i t s h a 1 _ b a s e 6 4 = "" M l F g S W D u h H u l 1 v p 7 7 q 5 w m 5 H a D K U = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q q 6 L L o x m U F + 4 A m l M l 0 0 g 6 d P J i 5 K Z T Q P 3 H j Q h G 3 / o k 7 / 8 Z J m 4 V W D w w c z r m X e + Y E q R Q a H e f L q q y t b 2 x u V b d r O 7 t 7 + w f 2 4 V F H J 5 l i v M 0 S m a h e Q D W X I u Z t F C h 5 L 1 W c R o H k 3 W B y V / j d K V d a J P E j z l L u R 3 Q U i 1 A w i k Y a 2 L Y 3 p p h 7 E c V x E O a j + X x g 1 5 2 G s w D 5 S 9 y S 1 K F E a 2 B / e s O E Z R G P k U m q d d 9 1 U v R z q l A w y e c 1 L 9 M 8 p W x C R 7 x v a E w j r v 1 8 k X x O z o w y J G G i z I u R L N S f G z m N t J 5 F g Z k s I u p V r x D / 8 / o Z h j d + L u I 0 Q x 6 z 5 a E w k w Q T U t R A h k J x h n J m C G V K m K y E j a m i D E 1 Z N V O C u / r l v 6 R z 0 X A v G 8 7 D V b 1 5 W 9 Z R h R M 4 h X N w 4 R q a c A 8 t a A O D K T z B C 7 x a u f V s v V n v y 9 G K V e 4 c w y 9 Y H 9 8 8 y Z Q L < / l a t e x i t > ĥ < l a t e x i t s h a 1 _ b a s e 6 4 = "" g o o R a E z m A k h 1 J P Q L 6 5 z W V o I q Q z M = "" > A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I V d F l 0 4 7 K C f U A T y m Q 6 a Y Z O J m H m p l B C / s S N C 0 X c + i f u / B s n b R Z a P T B w O O d e 7 p k T p I J r c J w v q 7 a 2 v r G 5 V d 9 u 7 O z u 7 R / Y h 0 c 9 n W S K s i 5 N R K I G A d F M c M m 6 w E G w Q a o Y i Q P B + s H 0 r v T 7 M 6 Y 0 T + Q j z F P m x 2 Q i e c g p A S O N b N u L C O R e T C A K w j w q i p H d d F r O A v g v c S v S R B U 6 I / v T G y c 0 i 5 k E K o j W Q 9 d J w c + J A k 4 F K x p e p l l K 6 J R M 2 N B Q S W K m / X y R v M B n R h n j M F H m S c A L 9 e d G T m K t 5 3 F g J s u I e t U r x f + 8 Y Q b h j Z 9 z m W b A J F 0 e C j O B I c F l D X j M F a M g 5 o Y Q q r j J i m l E F K F g y m q Y E t z V L / 8 l v Yu W e 9 l y H q 6 a 7 d u q j j o 6 Q a f o H L n o G r X R P e q g L q J o h p 7 Q C 3 q 1 c u v Z e r P e l 6 M 1 q 9 o 5 R r 9 g f X w D P k + U D A = = < / l a t e x i t > ĝ < l a t e x i t s h a 1 _ b a s e 6 4 = "" M l F g S W D u h H u l 1 v p 7 7 q 5 w m 5 H aD K U = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q q 6 L L o x m U F + 4 A m l M l 0 0 g 6 d P J i 5 K Z T Q P 3 H j Q h G 3 / o k 7 / 8 Z J m 4 V W D w w c z r m X e + Y E q R Q a H e f L q q y t b 2 x u V b d r O 7 t 7 + w f 2 4 V F H J 5 l i v M 0 S m a h e Q D W X I u Z t F C h 5 L 1 W c R o H k 3 W B y V / j d K V d a J P E j z l L u R 3 Q U i 1 A w i k Y a 2 L Y 3 p p h 7 E c V x E O a j + X x g 1 5 2 G s w D 5 S 9 y S 1 K F E a 2 B / e s O E Z R G P k U m q d d 9 1 U v R z q l A w y e c 1 L 9 M 8 p W x C R 7 x v a E w j r v 1 8 k X x O z o w y J G G i z I u R L N S f G z m N t J 5 F g Z k s I u p V r x D / 8 / o Z h j d + L u I 0 Q x 6 z 5 a E w k w Q T U t R A h k J x h n J m C G V K m K y E j a m i D E 1 Z N V O C u / r l v 6 R z 0 X A v G 8 7 D V b 1 5 W 9 Z R h R M 4 h X N w 4 R q a c A 8 t a A O D K T z B C 7 x a u f V s v V n v y 9 G K V e 4 c w y 9 Y H 9 8 8 y Z Q L < / l a t e x i t > ĥ < l a t e x i t s h a 1 _ b a s e 6 4 = "" g o o R a E z m A k h 1 J P Q L 6 5 z W V o I q Q z M = "" > A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I V d F l 0 4 7 K C f U A T y m Q 6 a Y Z O J m H m p l B C / s S N C 0 X c + i f u / B s n b R Z a P T B w O O d e 7 p k T p I J r c J w v q 7 a 2 v r G 5 V d 9 u 7 O z u 7 R / Y h 0 c 9 n W S K s i 5 N R K I G A d F M c M m 6 w E G w Q a o Y i Q P B + s H 0 r v T 7 M 6 Y 0 T + Q j z F P m x 2 Q i e c g p A S O N b N u L C O R e T C A K w j w q i p H d d F r O A v g v c S v S R B U 6 I / v T G y c 0 i 5 k E K o j W Q 9 d J w c + J A k 4 F K x p e p l l K 6 J R M 2 N B Q S W K m / X y R v M B n R h n j M F H m S c A L 9 e d G T m K t 5 3 F g J s u I e t U r x f + 8 Y Q b h j Z 9 z m W b A J F 0 e C j O B I c F l D X j M F a M g 5 o Y Q q r j J i m l E F K F g y m q Y E t z V L / 8 l v Yu W e 9 l y H q 6 a 7 d u q j j o 6 Q a f o H L n o G r X R P e q g L q J o h p 7 Q C 3 q 1 c u v Z e r P e l 6 M 1 q 9 o 5 R r 9 g f X w D P k + U D A = = < / l a t e x i t > ĝ < l a t e x i t s h a 1 _ b a s e 6 4 = "" M l F g S W D u h H u l 1 v p 7 7 q 5 w m 5 Momentum and initialisation: It is well known that both momentum and good initialisation can improve the speed of convergence for SG methods in deep learning [51]. Since VOGN is similar to Adam, we can implement momentum in a similar way. This is shown in step 17 of Algorithm 1, where β 1 is the momentum rate. We initialise the mean µ in the same way the weights are initialised in Adam (we use init.xavier_normal in PyTorch [11]). For the momentum term m, we use the same initialisation as Adam (initialised to 0). VOGN requires an additional initialisation for the variance σ 2 . For this, we first run a forward pass through the first minibatch, calculate the average of the squared gradients and initialise the scale s 0 with it (see step 1 in Algorithm 1). This implies that the variance is initialised to σ 2 0 = τ /(N (s 0 + δ)). For the tempering parameter τ , we use a schedule where it is increased from a small value (e.g., 0.1) to 1. With these initialisation protocols, VOGN is able to mimic the convergence behaviour of Adam in the beginning.H a D K U = "" > A A A B + X i c b V D L S s N A F L 2 p r 1 p f U Z d u B o v g q i Q q 6 L L o x m U F + 4 A m l M l 0 0 g 6 d P J i 5 K Z T Q P 3 H j Q h G 3 / o k 7 / 8 Z J m 4 V W D w w c z r m X e + Y E q R Q a H e f L q q y t b 2 x u V b d r O 7 t 7 + w f 2 4 V F H J 5 l i v M 0 S m a h e Q D W X I u Z t F C h 5 L 1 W c R o H k 3 W B y V / j d K V d a J P E j z l L u R 3 Q U i 1 A w i k Y a 2 L Y 3 p p h 7 E c V x E O a j + X x g 1 5 2 G s w D 5 S 9 y S 1 K F E a 2 B / e s O E Z R G P k U m q d d 9 1 U v R z q l A w y e c 1 L 9 M 8 p W x C R 7 x v a E w j r v 1 8 k X x O z o w y J G G i z I u R L N S f G z m N t J 5 F g Z k s I u p V r x D / 8 / o Z h j d + L u I 0 Q x 6 z 5 a E w k w Q T U t R A h k J x h n J m C G V K m K y E j a m i D E 1 Z N V O C u / r l v 6 R z 0 X A v G 8 7 D V b 1 5 W 9 Z R h R M 4 h X N w 4 R q a c A 8 t a A O D K T z B C 7 x a u f V s v V n v y 9 G K V e 4 c w y 9 Y H 9 8 8 y Z Q L < / l a t e x i t > ĥ < l a t e x i t s h a 1 _ b a s e 6 4 = "" g o o R a E z m A k h 1 J P Q L 6 5 z W V o I q Q z M = "" > A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I V d F l 0 4 7 K C f U A T y m Q 6 a Y Z O J m H m p l B C / s S N C 0 X c + i f u / B s n b R Z a P T B w O O d e 7 p k T p I J r c J w v q 7 a 2 v r G 5 V d 9 u 7 O z u 7 R / Y h 0 c 9 n W S K s i 5 N R K I G A d F M c M m 6 w E G w Q a o Y i Q P B + s H 0 r v T 7 M 6 Y 0 T + Q j z F P m x 2 Q i e c g p A S O N b N u L C O R e T C A K w j w q i p H d d F r O A v g v c S v S R B U 6 I / v T G y c 0 i 5 k E K o j W Q 9 d J w c + J A k 4 F K x p e p l l K 6 J R M 2 N B Q S W K m / X y R v M B n R h n j M F H m S c A L 9 e d G T m K t 5 3 F g J s u I e t U r x f + 8 Y Q b h j Z 9 z m W b A J F 0 e C j O B I c F l D X j M F a M g 5 o Y Q q r j J i m l E F K F g y m q Y E t z V L / 8 l v Y u W e 9 l y H q 6 a 7 d u q j j o 6 Q a f o H L n o G r X R P e q g L q J o h p 7 Q C 3 q 1 c u v Z e r P e l 6 M 1 q 9 o 5 R r 9 g f X w D P k + U D A = = < / l a tLearning rate scheduling: A common approach to quickly achieve high validation accuracies is to use a specific learning rate schedule [13]. The learning rate (denoted by α in Algorithm 1) is regularly decayed by a factor (typically a factor of 10). The frequency and timings of this decay are usually pre-specified. In VOGN, we use the same schedule used for Adam, which works well.Distributed training: We also employ distributed training for VOGN to perform large experiments quickly. We can parallelise computation both over data and Monte-Carlo (MC) samples. Data parallelism is useful to split up large minibatch sizes. This is followed by averaging over multiple MC samples and their losses on a single GPU. MC sample parallelism is useful when minibatch size is small, and we can copy the entire minibatch and process it on a single GPU. Algorithm 1 and Figure 2 illustrate our distributed scheme. We use a combination of these two parallelism techniques with different MC samples for different inputs. This theoretically reduces the variance during training (see Equation 5 in Kingma et al. [26]), but sometimes requires averaging over multiple MC samples to get a sufficiently low variance in the early iterations. Overall, we find that this type of distributed training is essential for fast training on large problems such as ImageNet.Implementation of the Gauss-Newton update in VOGN: As discussed earlier, VOGN uses the Gauss-Newton approximation, which is fundamentally different from Adam. In this approximation, the gradients on individual data examples are first squared and then averaged afterwards (see step 12 in Algorithm 1 which implements the update for s t shown in ( 4)). We need extra computation to get access to individual gradients, due to which, VOGN is slower Adam or SGD (e.g., in Fig. 1). However, this is not a theoretical limitation and this can be improved if a framework enables an easy computation of the individual gradients. Details of our implementation are described in Appendix B. This implementation is much more efficient than a naive one where gradients over examples are stored and the sum over the square is computed sequentially. Our implementation usually brings the running time of VOGN to within 2-5 times of the time that Adam takes.Tuning VOGN: Currently, there is no common recipe for tuning the algorithmic hyperparameters for VI, especially for large-scale tasks like ImageNet classification. One key idea we use in our experiments is to start with Adam hyperparameters and then make sure that VOGN training closely follows an Adam-like trajectory in the beginning of training. To achieve this, we divide the tuning into an optimisation part and a regularisation part. In the optimisation part, we first tune the hyperparameters of a deterministic version of VOGN, called the online Gauss-Newton (OGN) method. This method, described in Appendix C, is more stable than VOGN since it does not require MC sampling, and can be used as a stepping stone when moving from Adam/SGD to VOGN. After reaching a competitive performance to Adam/SGD by OGN, we move to the regularisation part, where we tune the prior precision δ, the tempering parameter τ , and the number of MC samples K for VOGN. We initialise our search by setting the prior precision δ using the L2-regularisation parameter used for OGN, as well as the dataset size N . Another technique is to warm-up the parameter τ towards τ = 1 (also see the ""momentum and initialisation"" part). Setting τ to smaller values usually stabilises the training, and increasing it slowly also helps during tuning. We also add an external damping factor γ > 0 to the moving average s t . This increases the lower bound of the eigenvalues of the diagonal covariance Σ t and prevents the noise and the step size from becoming too large. We find that a mix of these techniques works well for the problems we considered. ExperimentsIn this section, we present experiments on fitting several deep networks on CIFAR-10 and Ima-geNet. Our experiments demonstrate practical training using VOGN on these benchmarks and show performance that is competitive with Adam and SGD. We also assess the quality of the posterior approximation, finding that benefits of Bayesian principles are preserved. CIFAR-10 [28] contains 10 classes with 50,000 images for training and 10,000 images for validation. For ImageNet, we train with 1.28 million training examples and validate on 50,000 examples, classifying between 1,000 classes. We used a large minibatch size M = 4, 096 and parallelise them across 128 GPUs (NVIDIA Tesla P100). We compare the following methods on CIFAR-10: Adam, MC-dropout [9]. For ImageNet, we also compare to SGD, K-FAC, and Noisy K-FAC. We do not consider Noisy K-FAC for other comparisons since tuning is difficult. We compare 3 architectures: LeNet-5, AlexNet, ResNet-18. We only compare to Bayes by Backprop (BBB) [4] for CIFAR-10 with LeNet-5 since it is very slow to converge for larger-scale experiments. We carefully set the hyperparameters of all methods, following the best practice of large distributed training [13] as the initial point of our hyperparameter tuning. The full set of hyperparameters is in Appendix D. Performance on a Continual-learning taskThe goal of continual learning is to avoid forgetting of old tasks while sequentially observing new tasks. The past tasks are never visited again, making it difficult to remember them. The field of continual learning has recently grown, with many approaches proposed to tackle this problem [27,33,43,48,50]. Most approaches consider a simple setting where the tasks (such as classifying a subset of classes) arrive sequentially, and all the data from that task is available. We consider the same setup in our experiments.We compare to Elastic Weight Consolidation (EWC) [27] and a VI-based approach called Variational Continual Learning (VCL) [43]. VCL employs BBB for each task, and we expect to boost its performance by replacing BBB by VOGN. Figure 3b shows results on a common benchmark called Permuted MNIST. We use the same experimental setup as in Swaroop et al. [52]. In Permuted MNIST, each task consists of the entire MNIST dataset (10-way classification) with a different fixed random permutation applied to the input images' pixels. We run each method 20 times, with different random seeds for both the benchmark's permutations and model training. See Appendix D.2 for hyperparameter settings and further details. We see that VOGN performs at least as well as VCL, and far better than a popular approach called EWC [27]. Additionally, as found in the batch learning setting, VOGN is much quicker than BBB: we run VOGN for only 100 epochs per task, whereas VCL requires 800 epochs per task to achieve best results [52]. I Out-of-distribution experimental setup and additional resultsWe use experiments from the out-of-distribution tests literature [16,31,8,32], comparing VOGN to Adam and MC-dropout. Using trained architectures (LeNet-5, AlexNet and ResNet-18) on CIFAR-10, we test on SVHN, LSUN (crop) and LSUN (re-size) as out-of-distribution datasets, with the in-distribution data given by the validation set of CIFAR-10 (10,000 images). The entire training set of SVHN (73,257 examples, 10 classes) [42] is used. The test set of LSUN (Large-scale Scene UNderstanding dataset [55], 10,000 images from 10 different scenes) is randomly cropped to obtain LSUN (crop), and is down-sampled to obtain LSUN (re-size). These out-of-distribution datasets have no similar classes to CIFAR-10.Similar to the literature [16,30], we use 3 metrics to test performance on out-of-distribution data. Firstly, we plot histograms of predictive entropy for the in-distribution and out-of-distribution datasets, seen in Figure 5, 15, 16 and 17. Predictive entropy is given by K k=1 −p ik log pik . Ideally, on outof-distribution data, a model would have high predictive entropy, indicating it is unsure of which class the input image belongs to. In contrast, for in-distribution data, good models should have many examples with low entropy, as they should be confident of many input examples' (correct) class. We also compare AUROC and FPR at 95% TPR, also reported in the figures. By thresholding the most   Also shown are the AUROC metric (higher is better) and FPR at 95% TPR metric (lower is better), averaged over 3 runs. The standard deviations are very small and so not reported here.Figure 17: Histograms of predictive entropy for out-of-distribution tests for LeNet-5 trained on CIFAR-10 without data augmentation. Going from left to right, the inputs are: the in-distribution dataset (CIFAR-10), followed by out-of-distribution data: SVHN, LSUN (crop), LSUN (resize). Also shown are the AUROC metric (higher is better) and FPR at 95% TPR metric (lower is better), averaged over 3 runs. The standard deviations are very small and so not reported here.Figure 1 :1Figure 1: Comparing VOGN [24], a natural-gradient VI method, to Adam and SGD, training ResNet-18 on ImageNet. The two left plots show that VOGN and Adam have similar convergence behaviour and achieve similar performance in about the same number of epochs. VOGN achieves 67.38% on validation compared to 66.39% by Adam and 67.79% by SGD. Run-time of VOGN is 76 seconds per epoch compared to 44 seconds for Adam and SGD. The rightmost figure shows the calibration curve. VOGN gives calibrated predictive probabilities (the diagonal represents perfect calibration).",yes,Adam,yes,"learning rate, batch size, epochs",yes,,,,
1642,https://github.com/open-mmlab/mmsegmentation,open-mmlab_mmsegmentation.tar.gz,PSANet: Point-wise Spatial Attention Network for Scene Parsing,"@auto_fp16() @SEGMENTORS.register_module() @functools.wraps(loss_func) @patch.multiple(BaseDecodeHead, __abstractmethods__=set()) @pytest.mark.parametrize('dataset, classes', [ @HEADS.register_module() @force_fp32(apply_to=('seg_logit', )) @UPSAMPLE_LAYERS.register_module() @PIPELINES.register_module() @LOSSES.register_module() @property @staticmethod @deprecated_api_warning({'flip_ratio': 'prob'}, cls_name='RandomFlip') @abstractmethod @patch('mmseg.apis.multi_gpu_test', multi_gpu_test) @patch('mmseg.datasets.CustomDataset.load_annotations', MagicMock) @pytest.mark.parametrize('separate_eval', [True, False]) @auto_fp16(apply_to=('img', )) @classmethod @PIXEL_SAMPLERS.register_module() @BACKBONES.register_module() @patch('mmseg.datasets.CustomDataset.__getitem__', @DATASETS.register_module() @NECKS.register_module() @patch('torch.nn.modules.batchnorm._BatchNorm._check_input_dim', @pytest.mark.parametrize('block', [BasicBlock, Bottleneck]) @patch('torch.distributed.get_world_size', get_world_size)",requests PIL packaging copy timm oss2 yaml warnings sys prettytable model_archiver glob io onnx base64 mmseg platform mmcv ts pathlib re pytorch_sphinx_theme json collections unittest detail resource tempfile torch os pytest tensorrt matplotlib scipy functools itertools gzip hashlib bisect typing argparse lxml gather_models cityscapesscripts numpy time shutil abc math tarfile setuptools random logging cv2 subprocess onnxruntime zipfile,cs.CV cs.CV cs.LG cs.NE cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.AI cs.LG cs.CV cs.CV cs.LG cs.CV cs.CV cs.CV cs.CV cs.CL cs.LG cs.CV cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/open-mmlab_mmsegmentation.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\open-mmlab_mmsegmentation.pdf,Average Pooling  Auxiliary Classifier  Pyramid Pooling Module  Fully Convolutional Network  Random Gaussian Blur  RandomRotate  Random Horizontal Flip  Weight Decay  SGD with Momentum  Polynomial Rate Decay  PSPNet  Dilated Convolution  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Concatenated Skip Connection  Auxiliary Classifier  Synchronized Batch Normalization  Polynomial Rate Decay  Random Gaussian Blur  Random Horizontal Flip  SGD with Momentum  Weight Decay  Point-wise Spatial Attention  PSANet  Average Pooling  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Conditional Random Field  Feedforward Network  Weight Decay  SGD with Momentum  DeepLab  Dropout  Rectified Linear Units  Max Pooling  Convolution  Dense Connections  Softmax  VGG  Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net  Dual Attention Network  Average Pooling  Fully Convolutional Network  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Average Pooling  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Synchronized Batch Normalization  Average Pooling  Fully Convolutional Network  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Criss-Cross Network  Residual Connection  Non-Local Operation  1x1 Convolution  Non-Local Block  Cosine Annealing  Non-Local Operation  Non-Local Block  Sigmoid Activation  Average Pooling  ResNeXt Block  Squeeze-and-Excitation Block  Dense Connections  SENet  Softmax  Mask R-CNN  Layer Normalization  Cascade Mask R-CNN  RoIAlign  Region Proposal Network  Cascade R-CNN  Deformable Convolution  Linear Warmup With Cosine Annealing  Random Horizontal Flip  Random Resized Crop  Step Decay  SGD with Momentum  Weight Decay  Max Pooling  Bottleneck Residual Block  Residual Block  Residual Network  Feature Pyramid Network  GCNet  Global Context Block  Grouped Convolution  Global Average Pooling  Residual Connection  Rectified Linear Units  Kaiming Initialization  1x1 Convolution  Convolution  Batch Normalization  ResNeXt  Rectified Linear Units  Batch Normalization  Convolution  Average Pooling  Pyramid Pooling Module  Auxiliary Classifier  Dilated Convolution  PSPNet  Residual Connection  Non-Local Operation  1x1 Convolution  Non-Local Block  Dense Connections  Feedforward Network  Group Normalization  Panoptic FPN  Spatial Pyramid Pooling  Random Scaling  Random Horizontal Flip  Linear Warmup  SGD with Momentum  Weight Decay  Batch Normalization  Dilated Convolution  Atrous Spatial Pyramid Pooling  DeepLabv3  Sigmoid Activation  Rectified Linear Units  1x1 Convolution  Feature Pyramid Network  Convolution  Softmax  RoIAlign  Mask R-CNN  PointRend  1x1 Convolution  Residual Connection  Non-Local Operation  Non-Local Block  Softmax  Group Normalization  Random Scaling  ResNeXt Block  Grouped Convolution  ResNeXt  Panoptic FPN  Feature Pyramid Network  Average Pooling  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Deep Layer Aggregation  Average Pooling  Center Pooling  Cascade Corner Pooling  Cascade R-CNN  CenterNet  Region Proposal Network  RoIPool  Faster R-CNN  Softmax  RoIAlign  Mask R-CNN  HRNet  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Residual Network  Convolution  Average Pooling  Residual Connection  Rectified Linear Units  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Average Pooling  Atrous Spatial Pyramid Pooling  Dilated Convolution  Batch Normalization  Depthwise Convolution  Pointwise Convolution  DeepLabv3  Pyramid Pooling Module  Spatial Pyramid Pooling  Global Average Pooling  Depthwise Separable Convolution  Residual Connection  Dense Connections  Rectified Linear Units  1x1 Convolution  Max Pooling  Softmax  Convolution  Xception,,yes,Caffe,yes,,no,,,,
1789,https://github.com/baumgach/acdc_segmenter,baumgach_acdc_segmenter.tar.gz,An Exploration of 2D and 3D Deep Learning Techniques for Cardiac MR Image Segmentation,,utils gc pandas glob acdc_data nibabel skimage metrics_acdc tfwrapper threading medpy tensorflow re model queue h5py os socket image_utils matplotlib importlib scipy background_generator seaborn argparse numpy time config shutil model_zoo experiments cv2 logging,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/baumgach_acdc_segmenter.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\baumgach_acdc_segmenter.pdf,,,yes,Adam,yes,"learning rate, momentum",yes,,,,
1679,https://github.com/haofanwang/accurate-head-pose,haofanwang_accurate-head-pose.tar.gz,HYBRID COARSE-FINE CLASSIFICATION FOR HEAD POSE ESTIMATION,,"utils PIL math datasets, hopenet, utils scipy sys, os, argparse cv2 datasets, hopenet torch os sys, os, argparse, time numpy matplotlib torchvision",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/haofanwang_accurate-head-pose.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\haofanwang_accurate-head-pose.pdf,,,yes,Adam,yes,"batch size, epochs, momentum, learning rate",no,,,,
1875,https://github.com/dawnranger/pytorch-AGNN,dawnranger_pytorch-AGNN.tar.gz,Attention-based Graph Neural Network for Semi-supervised Learning,,utils scipy model networkx argparse torch __future__ numpy time,stat.ML cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dawnranger_pytorch-AGNN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dawnranger_pytorch-AGNN.pdf,,,yes,Adam,yes,"dropout rate, learning rate, regularization factor",yes,,,,
1613,https://github.com/AlamiMejjati/GeneratingObjectStamps,AlamiMejjati_GeneratingObjectStamps.tar.gz,Generating Object Stamps,"@deprecated(""Please use GANTrainer and set num_gpu"", ""2019-01-31"") @staticmethod @auto_reuse_variable_scope @memoized",PIL sys glob tqdm pylab GAN_bicycle tensorflow datetime os matplotlib six scipy GAN argparse numpy pycocotools cv2 random tensorpack dataloader,cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AlamiMejjati_GeneratingObjectStamps.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AlamiMejjati_GeneratingObjectStamps.pdf,Convolution  Generative Adversarial Network,,yes,Adam,yes,,no,,,,
1526,https://github.com/583748495/psift,583748495_psift.tar.gz,PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation,@tf.RegisterGradient('ThreeInterpolate') @tf.RegisterGradient('GroupPoint') @staticmethod @tf.RegisterGradient('GatherPoint'),tensorflow sys tf_utils tf_interpolate h5py show3d_balls pickle argparse os __future__ numpy models scannet_dataset time tf_grouping,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/583748495_psift.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\583748495_psift.pdf,,,no,,no,,no,,,,
1619,https://github.com/THUNLP-MT/Document-Transformer,THUNLP-MT_Document-Transformer.tar.gz,Improving the Transformer Translation Model with Document-Level Context,@property @staticmethod,math copy thumt tensorflow itertools sys collections datetime operator random argparse os __future__ numpy cPickle six,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/THUNLP-MT_Document-Transformer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\THUNLP-MT_Document-Transformer.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,,yes,Adam,yes,,no,,,,
1942,https://github.com/TeaPearce/Bayesian_NN_Ensembles,TeaPearce_Bayesian_NN_Ensembles.tar.gz,Uncertainty in Neural Networks: Approximately Bayesian Ensembling,,utils module_VI pandas module_gp __future__ keras tensorflow module_last_layer datetime os matplotlib importlib scipy pymc3 module_HMC hyperparams module_HMC_orig pickle sklearn edward numpy module_NN_ens math theano mpl_toolkits tensorflow_probability DataGen,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/TeaPearce_Bayesian_NN_Ensembles.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\TeaPearce_Bayesian_NN_Ensembles.pdf,,"C.3 Regression BenchmarkingTables 3 & 4 show all experiments run on the regression benchmarking datasets. The below discussion focuses on NLL results in table 4.ERF GP refers to the equivalent GP for an infinite width, single-layer BNN with ERF activations. It was tuned and implemented as for the ReLU GP. We were interested to discover how different activation functions would affect uncertainty estimates. In general the ReLU GP performed better than the ERF GP, with some exceptions, such as for Wine. The target variable for Wine is ordinal, containing five factors, it is therefore understandable that the ReLU GP, which extrapolates linearly, is at a slight disadvantage.10x 50 NNs refers to an anchored ensemble of ten NNs with 50 hidden nodes. We find that these results fall in between the 5x 50 NNs and the ReLU GP. This agrees with the convergence analysis done in section 5.2.We also implemented an anchored ensemble of five two-layer NNs, 5x 50-50 NNs. Even with minimal hyperparameter tuning (section E) we found an extra layer gave a performance boost over the 5x 50 NNs. We expect with more careful tuning this margin would increase.Single 50 NN refers to a single regularised NN, of one hidden layer with 50 hidden nodes, for which we used a constant value of predictive variance. Although this performs poorly in several cases, e.g. Boston and Yacht, the results are surprisingly close to those achieved by both our method and Deep Ensembles, even surpassing them on the Energy dataset. A method outputting constant predictive variance should not perform well in experiments designed to test uncertainty quantification, and this raises questions over the validity of the benchmarks.  We trained a three-layer NN on eight of ten classes of Fashion MNIST. We trained on 48,000 examples, tested on 8,000.Experiments were repeated 5 times with a different random seed for each run.Data categories were created as suggested by their name in table 6. Examples are shown in figure 12.Train Edge CIFAR MNIST Distort Noise • Distort comprised of rotations, vertical flips, and pixel value inversions.• Noise comprised of iid Gaussian noise, mean = 0.0, standard deviation = 2.0.• Sparse comprised of iid Bernoulli noise, pixles were given a value of 50.0 with p = 0.005, else 0.0.Hyperparameters: activation = ReLU, optimiser = adam, epochs = 30, learning rate = 0.005, batch size = 256, hidden layers = 3, hidden units = 100 E.7.2 CIFAR-10 CIFAR-10 contains 50,000 32x32 color training images, labelled over 10 categories, and 10,000 test images.We removed 2 categories during training (ships, dogs) so trained over 40,000 examples.OOD data classes are as show in the images in table 2.• Scramble permuted each row of pixels in a given image.• Invert took the negative of the pixel values.• Noise sampled pixels from bernoulli distribution (p=0.005) of large magnitude (pixel value=50).NN architecture: A convolutional NN was used, with the following structure, 64-64-maxpool-128-128-maxpool-256-256-256-maxpool-512-512-512-maxpool-flatten-2048fc-softmax. All convolutional kernels were [3 x 3 x number of channels in previous layer]. All maxpooling kernels were [2 x 2]. The total number of parameters was 8,689,472.Hyperparameters: activation = ReLU, optimiser = adam, learning rate = 0.001 decreasing to 0.0005 after 10 epochs and to 0.0001 after 20 epochs, batch size = 300.In order to bring test accuracies and confidence on the training dataset roughly in line, it was necessary to train for a different number of training epochs for each method (this effectively applies early stopping to the unconstrained case). Anchored eps = 25, Regularise eps = 30, Unconstrained eps = 15.Experiments were repeated 3 times with a different random seed for each run.",yes,Adam,yes,"batch size, learn rate σ, variance, epochs, decay rate",yes,,,,
1938,https://github.com/gunnxx/rove,gunnxx_rove.tar.gz,Robust-to-Noise Models in Natural Language Processing Tasks,@property,utils torchtext model json random typing logging tqdm argparse os dill torch,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/gunnxx_rove.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\gunnxx_rove.pdf,fastText,"MethodologyWe do experiments on two corpora: Airline Twitter Sentiment 2 and Movie Review (Maas et al., 2011), which are marked up for sentiment analysis task.We conduct three types of experiments: (a) the train-and testsets are spell-checked and artificial noise in inserted; (b) the train-and testsets are not changed (with the above mentioned exception for Russian corpus) and no artificial noise is added; and (c) the trainset is spell-checked and noised, the testset is unchanged.These experimental setups are meant to demonstrate the robustness of tested architectures to artificial and natural noise.As baselines we use architectures based on fasttext word embedding model (Bojanowski et al., 2016) and an architecture which follows (Kim et al., 2016). Another baseline, which is purely character-level, will be adopted from the work (Kim, 2014). MethodologyWe conduct three types of experiments: (a) the trainset and testset are not changed and no artificial noise is induced; (b) the artificial noise is inserted into trainset and testset simultaneously; and (c) the trainset is being noised, the testset is unchanged.These experimental setups are meant to demonstrate the robustness of tested architectures to artificial and natural noise (i.e. typos).The proposed corpora to use are: English and Russian news corpora, CoNLL'03 (Tjong Kim Sang and De Meulder, 2003) and Persons-1000 (Mozharova and Loukachevitch, 2016) respectively, and French social media corpus CAp'2017 (Lopez et al., 2017).We investigate variations of the state of the art architecture for Russian (Anh et al., 2017) and English (Lample et al., 2016) languages and apply the same architecture to the French language corpus.",no,,no,,no,,,,
1607,https://github.com/histocartography/histocartography,histocartography_histocartography.tar.gz,HistoCartography: A Toolkit for Graph Analytics in Digital Pathology,@classmethod @abstractmethod @property @staticmethod,requests PIL copy pandas mlflow csv yaml warnings sys multiprocessing glob skimage tqdm better_apidoc torchvision distutils pathlib re collections json unittest histocartography h5py torch os matplotlib importlib dgl scipy functools itertools hashlib bisect typing pickle sklearn numpy time abc shutil math setuptools traceback networkx cv2 logging random subprocess io,cs.CV eess.IV cs.CV cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/histocartography_histocartography.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\histocartography_histocartography.pdf,,,no,,no,,no,,,,
1731,https://github.com/justinlovelace/Dynamic-Problem-Lists,justinlovelace_Dynamic-Problem-Lists.tar.gz,Dynamically Extracting Outcome-Specific Problem Lists from Clinical Notes with Guided Multi-Headed Attention,@property,"utils tabulate copy pandas yaml sys clean_text tqdm gensim, logging gensim re evaluate model json collections nltk torch os matplotlib sklearn argparse numpy shutil random logging inspect",cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/justinlovelace_Dynamic-Problem-Lists.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\justinlovelace_Dynamic-Problem-Lists.pdf,,"Related WorkThere has been a large body of prior work utilizing natural language processing (NLP) techniques to extract information from clinical narratives. Blecker et al. (2016) demonstrated that unstructured clinical notes could be used to effectively identify patients with heart failure in real time. Their methods that involved data from clinical notes outperformed those using only structured data, demonstrating the importance of effectively utilizing the rich source of information contained within the clinical narrative. Prior work has found success predicting ICD code assignment using clinical notes within MIMIC-III and has found that deep learning techniques outperform traditional methods (Baumel et al., 2018;Mullenbach et al., 2018;Sadoughi et al., 2018;Xu et al., 2018). Mullenbach et al. (2018) augmented a convolutional model with a per-label attention mechanism and found that it led to both improved performance and greater interpretability as measured by a qualitative, expert evaluation. Sadoughi et al. (2018) later improved upon their model by utilizing multiple convolutions of different widths and then max-pooling across the channels before the attention mechanism.There has also been work done demonstrating that machine learning models can effectively leverage the unstructured clinical narrative for the prediction of clinical outcomes (Ghassemi et al., 2014;.  augmented long short-term memory networks (LSTMs) with an attention mechanism and applied it to predict clinical outcomes such as mortality and ICU readmission. However, when defining readmission, they treated both ICU readmissions and deaths as positive examples. The clinical work by Krumholz et al. (2013) has demonstrated that these are orthogonal outcomes, and thus modeling them jointly as a single outcome does not make sense from a clinical perspective. By treating them as separate outcomes in this work, we are able to independently explore the risk factors for these two distinct outcomes.  also raised some questions about the interpretability of attention in their work with clinical notes, repeating the experiments introduced by Jain and Wallace (2019) to evaluate the explanatory capabilities of attention. However, Wiegreffe and Pinter (2019) explored some of the problems with their underlying assumptions and experimental setup and demonstrated that their experiment failed to fully explore their premise, and thus failed to support their claim.  Comparison Against OracleWe conduct an additional experiment to explore the effectiveness of our problem extraction model. In this experiment we train a logistic regression oracle to predict the outcomes directly from the ground truth labels derived from ICD codes. It is important to note that because ICD codes are associated with entire hospital stays in our dataset, this experiment involves using future information compared to the clinically useful application setting of our other models. Not only are ICD codes themselves unavailable at the time of ICU discharge,  Nevertheless, this experiment can provide some insight into the effectiveness of our problem extraction model and whether it is currently a performance bottleneck. We report results for this logistic regression oracle across two of our problem configurations in Table 4. We find that using the ground truth labels leads to notably improved performance compared to our framework for the readmission outcomes, but actually leads to worse performance for most of the mortality outcomes. While the improvement for readmission outcomes can likely be attributed in part to the use of future information, the improvement likely also results from the improved accuracy of the problem labels, suggesting that the efficacy of our problem extraction model is a limiting factor in our framework's performance. However, our framework is not reliant on any particular architecture for problem extraction and this experiment demonstrates that as advances continue to be made on the task of automated ICD coding, our framework will become increasingly viable. The worse performance for mortality outcomes again suggests that the problem space doesn't perfectly represent all of the relevant information contained within the notes and highlights the importance of our end-to-end training regime which allows for some adaptation to the outcome of interest.& F-ICD Proc F-Phe & R-ICD Proc R-ICD Diag & R-ICD Proc R-Phe & R-ICD Proc Micro AU-ROC Macro AU-ROC Micro AU-ROC Macro AU-ROC Micro AU-ROC Macro AU-ROC Micro AU-",yes,Adam,yes,"learning rate, dropour, hidden units, beatch size, epochs",no,,,,
1660,https://github.com/userbehavioranalysis/SR-GNN_PyTorch-Geometric,userbehavioranalysis_SR-GNN_PyTorch-Geometric.tar.gz,Session-based Recommendation with Graph Neural Networks,@property,train math csv model datetime dataset operator pickle logging tqdm argparse os torch tensorboardX numpy time torch_geometric,cs.IR cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/userbehavioranalysis_SR-GNN_PyTorch-Geometric.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\userbehavioranalysis_SR-GNN_PyTorch-Geometric.pdf,,,yes,Adam,yes,Adam to optimize hyperparameters,yes,,,,
1907,https://github.com/snakeztc/NeuralDialog-CVAE,snakeztc_NeuralDialog-CVAE.tar.gz,Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,@staticmethod,utils decoder_fn_lib tensorflow re data_apis collections sys beeprint pickle nltk config_utils os __future__ numpy models time,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/snakeztc_NeuralDialog-CVAE.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\snakeztc_NeuralDialog-CVAE.pdf,,,yes,Adam,yes,"word embedding size, hidden size, latent variable z, context window, minibatch size, learning rate, gradient clipping",no,,,,
1603,https://github.com/openskynetwork/aircraft-localization,openskynetwork_aircraft-localization.tar.gz,LocaRDS: A Localization Reference Data Set,@__check_index @jit(nopython=True) @__check_station @property @staticmethod,gc copy pandas sensorsparams warnings sys multiprocessing pdb tqdm lightgbm ast distutils libs Cython collections json src pwlf datetime pytorch_lightning splineaircraftpos pathos torch os csaps optimize matplotlib datautils common scipy itertools IPython numba aircraftpos pickle typing sklearn argparse joblib numpy time graph_tool round2_mlat math networkx random learnfilter pyproj bayes_opt,cs.NI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/openskynetwork_aircraft-localization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\openskynetwork_aircraft-localization.pdf,,,no,,no,,no,,,,
1704,https://github.com/hazeld/rank-aware-attention-network,hazeld_rank-aware-attention-network.tar.gz,The Pros and Cons: Rank-aware Temporal Attention for Skill Determination in Long Videos,@property,pandas model dataset losses argparse torch os opts tensorboardX numpy subprocess time shutil,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hazeld_rank-aware-attention-network.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hazeld_rank-aware-attention-network.pdf,Temporal attention  Softmax,"Related WorkIn this section, we first review skill determination works in video, both task-specific and widely applicable methods. We then review works proposing attention modules, specifically temporal attention, for a variety of problems.Skill Determination. Several seminal works attempted skill determination in video [13,14,37]. Gordon [13] was the first to explore the viability of automated skill assessment from videos, as well as identifying appropriate tasks for analysis, with a case study on skill assessment of gymnastic vaults from skeleton trajectories. Despite the importance of automatic skill assessment from video for training and guidance [5,1], following works remain limited [2,7,23,27,29,35,38,40,41]. These works demonstrate good performance by focusing on features specific to the task, such as skeleton trajectory in diving [27] or entropy between repeated sutures in surgery [40]. Parallel efforts instead perform skill determination from non-visual sensors such as inertial measurement units [8,9,21,33,39].Several datasets have been introduced in prior work [7,11,23,27,35]. MIT Dive [27] and UNLV datasets [23] only include short video clips (< 5s), whilst the remaining [11,7,27] are small scale datasets. Fis-V [35] contains 500 figure skating videos, however this is not publicly available. We test on our previous dataset, EPIC-Skills [7], as this includes the JIGSAWS [11] dataset re-annotated for ranking alongside 3 other tasks. We also present a new dataset for skill assessment from longer videos (avg length = 188s), consisting of 500 videos across 5 daily-living tasks.To assess skill in long videos, different approaches have been proposed. One is to first localize pre-selected events specific to the task [2], such as shooting or passing the ball in a basketball game. Alternatively, global features from the entire video have been used [27,29,38,40], such as skeleton trajectories [27], features averaged across the video [23], or from randomly sampled segments in our previous work [7]. The only work to use attention in long videos is [35] for figure-skating. They use a self-attentive LSTM and a multi-scale skip LSTM to learn local (technical movements) and global (performance of players) scores respectively. This method uses a regression framework specifically for predicting the components of figure skating scores, not appropriate for common tasks.We differ from all previous works in that we train a model to attend to skill-relevant parts of a video; learnable thus applicable to any task. We use a convolutional network with temporal segments and propose a novel rank-aware loss function. We do not use LSTMs due to the reported issues with maintaining information over longer videos [30,32], and inferior performance compared to non-recurrent networks in many sequence-based tasks [3,12,32].Attention Modules. Attention is increasingly used in finegrained recognition, as intelligently weighting input is key to distinguishing between similar categories. This is a common problem in image recognition [10,31] where attention can localize discriminative attributes in the object of interest. For instance, Fu et al. [10] present RA-CNN to recursively zoom into the most discrimative image region with an inter-scale ranking loss. x Singh et al. [31] adapt the spatial transformer network [15] into a Siamese network to perform relevant attribute ranking. Similarly, in person reidentification from video, attention [16,19,34] is utilized to select the frames with the best view of identifying attributes.Attention has also been adopted in the video domain for action recognition [25,26] and localization [17,28,22,24], including for weakly supervised localization from videolevel label [22,24]. Pei et al. [25] combine an attention module with a gated recurrent network to classify actions in untrimmed video. Piergeovanni et al. [26] present temporal attention filters to discover latent sub-events in activities. Nguyen et al. [22] use attention filters within a CNN to identify a sparse set of video segments which minimize a video's classification loss. They use this in combination with class-specific attention from the activations to localize target actions. We build on the class-agnostic attention filters used in this work for our rank-aware attention (Sec 3.3).Using class-specific attention is a common technique in existing temporal attention works [22,24]. In this work, we propose the first model to train rank-specific (which we call rank-aware) attention, and demonstrate that it outperforms rank-agnostic attention and existing methods.  2. Rank-Aware Attention Network. Given a ranked pair of videos (pi, pj) where pi exhibits higher skill: each video is uniformly split into segments. Extracted features (I3D) are passed into a pair of attention modules to produce video-level representations for the ranking functions (FC layers). Each ranking function produces a score s + (green) or s − (red). Additionally, a uniformly weighted video representation produces a third ranking score u (blue). Three types of losses are defined: the ranking loss maximizes the margin (greento-green, red-to-red, blue-to-blue) between the pair of ranked videos, the disparity loss ensures attention branches outperform uniform (green-to-blue, red-to-blue) and the final loss optimizes the attention modules to become rank-aware (green-to-red).",yes,Adam,yes,"batch size, epochs, learning rate",no,,,,
1971,https://github.com/cmhungsteve/SSTDA,cmhungsteve_SSTDA.tar.gz,Action Segmentation with Joint Self-Supervised Temporal Domain Adaptation,@staticmethod,train batch_gen copy math centroid itertools loss model random argparse torch os tensorboardX numpy time predict,cs.CV cs.LG eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cmhungsteve_SSTDA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cmhungsteve_SSTDA.pdf,Self-Supervised Temporal Domain Adaptation,,yes,Adam,no,,no,,,,
1698,https://github.com/ictnlp/TLAT-NMT,ictnlp_TLAT-NMT.tar.gz,Token-level Adaptive Training for Neural Machine Translation,"@register_optimizer('adagrad') @register_optimizer('sgd') @register_task('language_modeling') @register_optimizer('adadelta') @register_lr_scheduler('reduce_lr_on_plateau') @register_model_architecture('transformer', 'transformer_iwslt_de_en') @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_de_big') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_model_architecture('masked_lm', 'masked_lm') @register_model_architecture('transformer_lm', 'transformer_lm') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_optimizer('lamb') @register_model_architecture('transformer', 'transformer_wmt_en_de') @register_model('transformer_lm') @register_task('translation_moe') @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_model_architecture('masked_lm', 'bert_large') @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @register_model_architecture('transformer', 'transformer_wmt_en_de_big_t2t') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_model_architecture('transformer_lm', 'transformer_lm_big') @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @s3_request @register_task('cross_lingual_lm') @register_model('lightconv') @register_model('transformer') @register_task('multilingual_translation') @register_model_architecture('transformer', 'transformer') @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @register_model('fconv_lm') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture('transformer', 'transformer_wmt_en_de_big') @register_criterion('masked_lm_loss') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_optimizer('nag') @property @register_criterion('adaptive_loss') @register_criterion('cross_entropy') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @wraps(func) @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @register_model('masked_lm') @contextlib.contextmanager @register_model_architecture('fconv', 'fconv') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_model_architecture('lightconv', 'lightconv') @register_model('lightconv_lm') @register_model('lstm') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @register_task('masked_lm') @register_lr_scheduler('triangular') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_optimizer('adam') @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @staticmethod @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_model('fconv') @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_model_architecture('transformer', 'transformer_vaswani_wmt_en_fr_big') @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_criterion('label_smoothed_cross_entropy') @register_task('translation') @register_model_architecture('fconv', 'fconv_iwslt_de_en')",requests copy scripts contextlib warnings sys multiprocessing types sacremoses generate fairseq pdb tqdm __future__ ctypes fileinput generator pathlib re collections json unittest botocore tests fnmatch tempfile torch os socket eval_lm html importlib struct interactive apex functools itertools gzip hashlib bisect builtins urllib typing pickle boto3 argparse subword_nmt urlparse numbers tensorboardX numpy shlex preprocess time shutil sentencepiece train math tarfile setuptools traceback sacrebleu random logging operator subprocess inspect io,cs.CL cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ictnlp_TLAT-NMT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ictnlp_TLAT-NMT.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,,no,,no,,no,,,,
1739,https://github.com/samyak0210/saliency,samyak0210_saliency.tar.gz,Tidying Deep Saliency Prediction Architectures,,"utils PIL loss cv2, os sys tqdm PNASnet torchvision model pytorch_ssim collections os, cv2 torch os matplotlib operations scipy pickle argparse numpy time genotypes torchsummary cv2 logging glob, os dataloader",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/samyak0210_saliency.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\samyak0210_saliency.pdf,Interpretability  Softmax  Max Pooling  Rectified Linear Units  1x1 Convolution  Convolution  Batch Normalization  SimpleNet,,yes,Adam,yes,"epochs, learning rate",no,,,,
1601,https://github.com/TinfoilHat0/BiFair,TinfoilHat0_BiFair.tar.gz,BiFair: Training Fair Models with Bilevel Optimization,,my_utils pandas tqdm my_models torchvision torch os ray matplotlib functools IPython fair_algs sklearn argparse numpy time torchsummary options higher,cs.LG cs.CY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/TinfoilHat0_BiFair.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\TinfoilHat0_BiFair.pdf,,"Synthetic Dataset ExperimentsWe create a synthetic biased dataset with two features as follows: for points with favorable label (y = 1), we draw the first feature as x 1 ∼ N (1, 1), and for points with unfavorable label (y = 0), we draw it as x 1 ∼ N (0, 1). We then designate the second feature x 2 as the sensitive feature, and introduce bias by ensuring 2/3 of favorable labels have x 2 = 1, and ensuring 2/3 of unfavorable labels have x 2 = 0 in the dataset. So, x 2 = 1 has a higher association with the favorable label, and x 2 = 0 has a higher association with the unfavorable label in the dataset. We create a training dataset of size 6000, and a validation and a test dataset of size 2000, where in each partition, we have the same number favorable and unfavorable labels.In our first experiment, we train a logistic regression model on this dataset by (i) cross-entropy minimization, and by (ii) BiFair. We illustrate the resulting models' prediction on the test dataset in Figure 2. As is seen, in case (i), the resulting model assigns a high weight to x 2 , even higher than the weight of x 1 . Consequently, it exhibits high bias. For case (ii), the weight on x 2 is much smaller than of x 1 , and this reduces the bias: averaged over three runs, (i) yields a mean accuracy of 73.1%, and a mean AOD of 41.3%, and BiFair yields a mean accuracy of 69.6%, and a mean AOD of 2.4%.  As can be seen, in baseline setting, model assigns a high weight to the sensitive attribute x 2 . Due to this, privileged unfavorable points are likely to be predicted as favorable, even more than unprivileged favorable points. This can be seen by looking at the mean probability of each group: mean probability of privileged unfavorable points are higher. On the other hand, with BiFair, model assigns a much smaller weight to the sensitive attribute. This results in favorable points having similar distributions regardless of their sensitive attribute value. Also, their mean values are now overlapping. The same observation applies for unfavorable points as well. CE stands for cross-entropy. Results are averaged over three runs, and reported as mean ± std. All losses are measured over the validation data. Excluding the baseline setting, best results are highlighted in bold. Even for the case T in = 1, we improve the fairness of model a lot with a small drop in accuracy. This is also the setting which gives best BAcc value. For AOD, we reach the best value at T in = 16, and performance deteriorates afterwards due to overfitting on the training data. These results suggest that BiFair can work well even for small values of T in , and may improve fairness significantly with relatively modest drops in the accuracy.In our second experiment, we illustrate how the performance of the model trained by BiFair changes with respect to the number of inner iterations taken per outer iteration (T in of Algorithm 1). The results are presented in Table 2. As can be seen, even with a single iteration, the bias of the model is significantly reduced. Increasing the value of T in gives different trade-offs between accuracy and bias until T in = 16, after that point the performance degrades for both metrics as the model begins to overfit on the training data. Real-world Dataset ExperimentsWe now compare our algorithm with straightforward regularization based approach, as well as with three other fair training algorithms sampled from the AIF360 toolkit [5]. In particular, we have chosen one fair training algorithm for each one of the pre-processing, in-processing, and post-processing categories. When deciding on which particular algorithm to choose, we have taken benchmarks presented in [5] into account, and have chosen one of the best performing algorithm in its respective category. When the results between two algorithms were too close to call for a clear winner, we have chosen the algorithm that we deem as simpler to implement. We briefly describe each algorithm we compare against below.Regularization simply corresponds to adding the fairness loss L f as a regularization term to the utility loss L u in the training objective, i.e., the training objective is to minimizeL u + λ • L f .Kamiran Reweighing [8] is a pre-processing technique that aims to ensure statistical independence between the label and the sensitive attribute by assigning weights to data points. Particularly, the technique first computes the expected probability, pr exp , for each combination of sensitive attribute and label under the assumption that the sensitive attribute and the label are independent. Then, they measure the observed probability, pr obs , for each combination of sensitive attribute and label in the training dataset. Finally, each data point is assigned the weight pr exp /pr obs based on the value of their sensitive attribute and their label.Prejudice Remover [12] is an in-processing technique that tries to ensure statistical independence between the model's prediction, and the sensitive attribute. To do so, the empirical mutual information between the model's prediction and the sensitive attribute is added as a regularization term in the training objective.Reject Option Classifier (ROC) [18] is a post-processing technique that aims to reduce bias by modifying the models' predictions around a symmetric confidence interval of a given decision threshold. For example, let the decision threshold be 0.5, and the confidence interval be 0.1. Then, for a particular point, if the probability of belonging to the favorable label assigned by the model lies in [0.4, 0.6], the prediction for this point is determined based on whether it is privileged, or unprivileged: if it is privileged, it is is predicted as unfavorable, otherwise, as favorable.As for our datasets, we both use tabular and image data. For tabular setting, we do logistic regression over Adult income prediction [34], and Compas recidivism datasets [2]. In the Adult dataset, the goal is to predict whether a person's income is greater than of $50k USD a year, the sensitive feature is gender, and men are privileged. For Compas, the goal is to predict whether a criminal will reoffend, race is the sensitive attribute, and white is the privileged group. For image data, we train a ResNet-18 [35] over a subset of CelebA [36], a dataset of celebrity faces, for smile detection task where ""smiling"" is the favorable outcome. Particularly, we randomly sample 40k images from CelebA, designate gender as the sensitive attribute, and to create bias, we ensure a higher association between women and the smiling label, which makes women the privileged group. For each dataset, we ensure a train/validation/test split of 60%/20%/20%. We provide more details on the datasets we use, such as the applied pre-processing techniques and data augmentation methods in the Appendix.In our experiments, we train every model using Adam optimizer [37]. We also use Adam to update the training dataset weights in case of BiFair. Each model is trained until the validation loss stagnates for 10 epochs, and the decision threshold is adjusted on the validation dataset to maximize BAcc as in [5].The general setting of our experiments are as follows: we run each algorithm over a set of hyperparameters using grid-search, and for each algorithm other than BiFair, we report the configuration that minimizes the AOD on the test dataset. For BiFair, we report a configuration in which its AOD is lower than any other fair training algorithm, and its BAcc is either equal or higher, than the fair algorithm with the highest BAcc. The fact that we can find such a configuration for BiFair indicates that it is strictly better than every other fair training algorithm that we compare against. As the results shows, for Compas and CelebA datasets, BiFair improves both BAcc, and AOD, compared to every other fair training algorithm. For Adult dataset, we reduce the bias more than others by maintaining the same accuracy on the average. We provide more details related to our hyperparameter search process, and report the particular configurations in which we obtained the presented results in the Appendix. 61.3 ± 0.4 1.5 ± 0.1 Prejudice Remover [12] 61.0 ± 0.5 2.1 ± 1.1 ROC Post-processing [18]  80.5 ± 0 6.9 ± 0.4 Prejudice Remover [12] 80.7 ± 0.1 6.7 ± 0.1 ROC Post-processing [18]  91.5 ± 0.3 3.3 ± 0.4 Prejudice Remover [12] 91.3 ± 0.1 3.0 ± 0.3 ROC Post-processing [18] 91.4 ± 0.5 2.9 ± 0.1 BiFair 91.9 ± 0.2 2.5 ± 0.1 Results are averaged over three runs, and reported as mean ± std. Baseline corresponds to training with only cross-entropy minimization.",yes,"Adam, grid search",yes,,yes,,,,
1970,https://github.com/benedekrozemberczki/PDN,benedekrozemberczki_PDN.tar.gz,Pathfinder Discovery Networks for Neural Message Passing,,utils texttable param_parser tqdm argparse torch sklearn numpy pdn torch_geometric,cs.LG cs.AI cs.SI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/benedekrozemberczki_PDN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\benedekrozemberczki_PDN.pdf,,"Edge feature importanceModel interpretability is an important part of developing deep neural networks. Architectures with interpretable weights can provide novel insights on the structure of data, while also making validation, inspection, and debugging significantly easier. We believe that PDNs can add significant interpretability to graph learning tasks when we frame the learned weights as an attention mechanism over the input graphs. In this set of experiments we discuss two scenarios when learned PDN weights have direct interpretations. Attention on proximity. In this experiment, we use the multiscale model described in Section 5.2. We utilize the first 5 normalized adjacency matrix powers as input similarity graphs and apply the hyperparameters described in Section 6.3. We train this model on 100-shot learning tasks, and report the mean weight for each adjacency power from 100 repetitions (see Figure 8).Based on these learned weights, we observe that the model has learned to prioritize messages that come from the first order neighbourhoods of vertices -in other words, the PDN attends to closer neighbors more. We note that the Cora and Pubmed graphs exhibit high homophily between nodes, suggesting that the model's weighting scheme is well motivated. Interestingly, we also observe that the importance of information coming from the second hop starts to decline between 50-100 epochs; not coincidentally, this is around when peak accuracy is reached, which we observe a decline (test accuracy not shown). This implies that graph neural network models overfit to information coming from the first order proximity of individual data points. More importantly, the added interpretability from PDN allows us to observe exactly where the overfitting is occurring.  Attention on neighbourhood similarity. Using the similarity scores listed in Table 6 of Appendix G we train a Linear PDN model with the hyperparameter settings described in Appendix E. As a reminder, this implementation of the pathfinder layer uses softmax activations and does not have a hidden layer -in this setting, the weights can be interpreted as attention. For the citation graph and social network datasets we plot the average attention score (calculated from 10 training runs) for a selected subset of edge scores in Figure 9.The results show that unnormalized edge similarity scores such as the degree product and common neighbours tend to receive low attention when the edge weight aggregation happens. On most datasets similarity metrics which are normalized and do not consider the degree of shared neighbors (e.g. association strength and minimal overlap) receive relatively high attention.",yes,,no,epochs,yes,,,,
1610,https://github.com/Harry24k/adversarial-attacks-pytorch,Harry24k_adversarial-attacks-pytorch.tar.gz,Torchattacks: A PyTorch Repository for Adversarial Attacks,@property,math scipy copy sys collections torch os __future__ numpy matplotlib time torchvision six,cs.LG cs.AI cs.CR,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Harry24k_adversarial-attacks-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Harry24k_adversarial-attacks-pytorch.pdf,,,yes,Adam,no,,no,,,,
1703,https://github.com/tdozat/Parser-v2,tdozat_Parser-v2.tar.gz,CEA LIST : Processing low-resource languages for CoNLL 2018 Shared Task,@error.setter @preds.setter @property @staticmethod @classmethod,gc warnings sys multiprocessing cif_lstm_cell glob ngram_multivocab multibucket cnn_embed parser gama_parser recur_cells __future__ sgd_optimizer re tensorflow tagger subtoken_vocab fish_parser collections ngram_vocab network parsers os radam_optimizer index_vocab lstm_cell matplotlib multivocab mlp_embed configparser optimizers nn xbar_parser scipy backports gzip pretrained_vocab codecs bucket argparse rnn_embed numpy xtagger gru_cell time configurable token_vocab taggers ConfigParser rnn_cell cPickle models bin_parser,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tdozat_Parser-v2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tdozat_Parser-v2.pdf,,,yes,,no,,no,,,,
1772,https://github.com/nuannuanhcc/ps_reppoint,nuannuanhcc_ps_reppoint.tar.gz,RepPoints: Point Set Representation for Object Detection,"@auto_fp16() @DETECTORS.register_module @BACKBONES.register_module @force_fp32(apply_to=('cls_scores', 'bbox_preds', 'centernesses')) @weighted_loss @force_fp32(apply_to=('cls_scores', 'bbox_preds')) @force_fp32(apply_to=('feats', ), out_fp16=True) @functools.wraps(loss_func) @functools.wraps(func) @torch.no_grad() @NECKS.register_module @SHARED_HEADS.register_module @property @staticmethod @once_differentiable @abstractmethod @force_fp32(apply_to=('mask_pred', )) @LOSSES.register_module @master_only @force_fp32( @functools.wraps(old_func) @DATASETS.register_module @auto_fp16(apply_to=('img', )) @HEADS.register_module @force_fp32(apply_to=('mask_iou_pred', )) @ROI_EXTRACTORS.register_module","requests imagecorruptions PIL copy gc pandas warnings multiprocessing sys roi_align tqdm faiss __future__ platform torchvision mmcv threading os, sys re datetime collections mmdetection Cython json resource enum tempfile torch os pkgutil socket matplotlib six importlib scipy functools xml seaborn robustness_eval terminaltables sklearn argparse mmdet roi_pool tensorboardX numpy time abc getpass shutil math pycocotools setuptools cv2 logging random subprocess inspect",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/nuannuanhcc_ps_reppoint.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\nuannuanhcc_ps_reppoint.pdf,RepPoints  Feature Pyramid Network  Average Pooling  ResNeXt Block  Deformable Convolution  Stochastic Gradient Descent  Random Horizontal Flip  RPDet  Grouped Convolution  Bottleneck Residual Block  Global Average Pooling  Residual Block  Residual Connection  Rectified Linear Units  Kaiming Initialization  Max Pooling  1x1 Convolution  Convolution  Batch Normalization  Residual Network  ResNeXt,,yes,SGD,no,,no,,,,
1606,https://github.com/facebookresearch/BLINK,facebookresearch_BLINK.tar.gz,Scalable Zero-shot Entity Linking with Dense Entity Retrieval,@staticmethod,utils requests prettytable sys multiprocessing evaluator pdb regex tqdm termcolor faiss bs4 pprint bert_reranking flair elq re evaluate datetime json collections blink transformers nltk torch os segtok sqlite3 matplotlib importlib scipy xml pytorch_transformers candidate_generators urllib pickle emoji typing argparse pysolr numpy time shutil math setuptools traceback bz2 random logging subprocess colorama io,cs.CL cs.AI cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/facebookresearch_BLINK.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\facebookresearch_BLINK.pdf,Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,,yes,Adam,yes,,yes,,,,
1776,https://github.com/MasazI/cnn_depth_tensorflow,MasazI_cnn_depth_tensorflow.tar.gz,Depth Map Prediction from a Single Image using a Multi-Scale Deep Network,,math PIL tensorflow model datetime train_operation dataset wget h5py random os numpy model_part,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/MasazI_cnn_depth_tensorflow.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\MasazI_cnn_depth_tensorflow.pdf,,,yes,SGD,yes,"batch size, learning rate, momentum",yes,,,,
1729,https://github.com/ZiyuanMa/DHC,ZiyuanMa_DHC.tar.gz,Distributed Heuristic Multi-Agent Path Finding with Communication,"@autocast() @ray.remote(num_cpus=1) @ray.remote(num_cpus=1, num_gpus=1) @torch.no_grad()",threading copy worker model multiprocessing environment random pickle typing tqdm torch os ray buffer numpy matplotlib time configs,cs.RO cs.AI cs.MA,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZiyuanMa_DHC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZiyuanMa_DHC.pdf,Q-Learning,"C. Multi-Agent Distributed Prioritized Experience ReplayIn MAPF, agents has their individual goals instead of a common goal, thus IQL is more suitable for this problem compared with centralized MARL. The appealing merit of IQL is that it avoids the scalability problem in centralized training, which requires to learn a Q-function for joint actions over all agents. The joint action space grows exponentially as the number of agents increases. On the other hand, IQL is naturally appropriate to learn decentralized policy for partially observable settings, because each agent makes decision only based on its own observation. As each agent in the MAPF environment plays the same role as others, to simplify the training process, instead of learning multiple policies for multiple agents, we train a single model from a single agent's perspective while treating others as part of its environment. The final trained policy can be applied to each agent for decentralized execution.To speed up deep RL with the advancement of computational resources, many RL algorithms have been promoted by distributed training, such as A3C [32] and Distributed PPO [39]. The idea behind these distributed versions of algorithms is parallelizing the gradient computation so as to facilitate parameter optimization. Specifically, PRIMAL is distributed using A3C as a backend. Another direction to distributed RL is parallelizing experience data generation and selection with a shared replay memory, such that more high priority data can be gathered to benefit the model. This is referred to as Ape-X architecture, where multiple actors generate experience and a single learner updates the network [19]. Our IQL from a single agent's perspective method can be naturally distributed by Ape-X framework.Fig. 3 illustrates our system flow. In the experiments, we setup sixteen independent actors running on CPUs to generate data, and a single learner on GPU to train. Each actor has a copy of the environment with current Q-network and keep generating new transitions from multiple agents and initializing priorities for them. The transitions from all actors are fed into a shared prioritized replay buffer. Then learner samples the most useful experiences from the buffer and updates the network and priorities of the experience. Note that although the model is trained for a single agent, the transitions of all the agents need to be stored for communication purpose, and the priorities are initialized and updated from that agent's perspective. As priorities are shared, the good experiences explored by any actor can improve the learner.The final loss function is a multi-step TD errorL(θ) = Huber(R t − Q(s t , a t , θ))(3)with R t = r t + γr t+1 + . . . + γ n Q(s t+n , a t+n , θ), where R t is the total return of the agent we care about, s t and a t are the state and action of that agent, and θ denotes the parameters of the target network, a periodical copy of the online parameters θ.",yes,,yes,"batch size, learning rate, trainin step",no,,,,
1855,https://github.com/netpaladinx/DPMPN,netpaladinx_DPMPN.tar.gz,DYNAMICALLY PRUNED MESSAGE PASSING NET-WORKS FOR LARGE-SCALE KNOWLEDGE GRAPH REASONING,@classmethod @property,utils copy csv sys glob tensorflow model collections json os matplotlib functools itertools argparse numpy time config shutil datasets networkx dataenv,cs.AI cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/netpaladinx_DPMPN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\netpaladinx_DPMPN.pdf,,"|EXPERIMENTSDatasets. We use six large KG datasets: FB15K, FB15K-237, WN18, WN18RR, NELL995, and YAGO3-10. FB15K-237 (Toutanova & Chen, 2015) is sampled from FB15K (Bordes et al., 2013) with redundant relations removed, and WN18RR (Dettmers et al., 2018) is a subset of WN18 (Bordes et al., 2013) removing triples that cause test leakage. Thus, they are both considered more challenging. NELL995 (Xiong et al., 2017) has separate datasets for 12 query relations each corresponding to a single-query-relation KBC task. YAGO3-10 (Mahdisoltani et al., 2014) contains the largest KG with millions of edges. Their statistics are shown in Table 1. We find some statistical differences between train and validation (or test). In a KG with all training triples as its edges, a triple (head, rel, tail) is considered as a multi-edge triple if the KG contains other triples that also connect head and tail ignoring the direction. We notice that FB15K-237 is a special case compared to the others, as there are no edges in its KG directly linking any pair of head and tail in validation (or test). Therefore, when using training triples as queries to train our model, given a batch, for FB15K-237, we cut off from the KG all triples connecting the head-tail pairs in the given batch, ignoring relation types and edge directions, forcing the model to learn a composite reasoning pattern rather than a single-hop pattern, and for the rest datasets, we only remove the triples of this batch and their inverse from the KG to avoid information leakage before training on this batch. This can be regarded as a hyperparameter tuning whether to force a multi-hop reasoning or not, leading to a performance boost of about 2% in HITS@1 on FB15-237.Experimental settings. We use the same data split protocol as in many papers (Dettmers et al., 2018;Xiong et al., 2017;Das et al., 2018). We create a KG, a directed graph, consisting of all train triples and their inverse added for each dataset except NELL995, since it already includes reciprocal relations. Besides, every node in KGs has a self-loop edge to itself. We also add inverse relations into the validation and test set to evaluate the two directions. For evaluation metrics, we use HITS@1,3,10 and the mean reciprocal rank (MRR) in the filtered setting for FB15K-237, WN18RR,  (Dettmers et al., 2018), [♥] from (Shen et al., 2018), [♦] from (Sun et al., 2018), [ ] from (Das et al., 2018), and [ ] from (Lacroix et al., 2018). Some collected results only have a metric score while some including ours take the form of ""mean (std)"". FB15K, WN18, and YAGO3-10, and use the mean average precision (MAP) for NELL995's singlequery-relation KBC tasks. For NELL995, we follow the same evaluation procedure as in (Xiong et al., 2017;Das et al., 2018;Shen et al., 2018), ranking the answer entities against the negative examples given in their experiments. We run our experiments using a 12G-memory GPU, TITAN X (Pascal), with Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz. Our code is written in Python based on TensorFlow 2.0 and NumPy 1.16 and can be found by the link 3 below. We run three times for each hyperparameter setting per dataset to report the means and standard deviations. See hyperparameter details in the appendix.Baselines. We compare our model against embedding-based approaches, including TransE (Bordes et al., 2013), TransR (Lin et al., 2015b), DistMult (Yang et al., 2015), ConvE (Dettmers et al., 2018), ComplE (Trouillon et al., 2016), HolE (Nickel et al., 2016), RotatE (Sun et al., 2018), and ComplEx-N3 (Lacroix et al., 2018), and path-based approaches that use RL methods, including DeepPath (Xiong et al., 2017), MINERVA (Das et al., 2018), and M-Walk (Shen et al., 2018), and also that uses learned neural logic, NeuralLP .Comparison results and analysis. We report comparison on FB15K-23 and WN18RR in Table 2.Our model DPMPN significantly outperforms all the baselines in HITS@1,3 and MRR. Compared to the best baseline, we only lose a few points in HITS@10 but gain a lot in HITS@1,3. We speculate that it is the reasoning capability that helps DPMPN make a sharp prediction by exploiting graph-structured composition locally and conditionally. When a target becomes too vague to predict, reasoning may lose its advantage against embedding-based models. However, path-based baselines, with a certain ability to do reasoning, perform worse than we expect. We argue that it might be inappropriate to think of reasoning, a sequential decision process, equivalent to a sequence of nodes.The average lengths of the shortest paths between heads and tails as shown in Table 1 suggests a very short path, which makes the motivation of using a path almost useless. The reasoning pattern should be modeled in the form of dynamical local graph-structured pattern with nodes densely connected (A) Convergence Analysis (eval on test) H@1 H@3 H@10 MMR H@1 H@3 H@10 MMR 40.0 with each other to produce a decision collectively. We also run our model on FB15K, WN18, and YAGO3-10, and the comparison results in the appendix show that DPMPN achieves a very competitive position against the best state of the art. We summarize the comparison on NELL995's tasks in the appendix. DPMPN performs the best on five tasks, also being competitive on the rest.Convergence analysis. Our model converges very fast during training. We may use half of training queries to train model to generalize as shown in Figure 4(A). Compared to less expensive embedding-based models, our model need to traverse a number of edges when training on one input, consuming much time per batch, but it does not need to pass a second epoch, thus saving a lot of training time. The reason may be that training queries also belong to the KG's edges and some might be exploited to construct subgraphs during training on other queries.Component analysis. Given the stacked GNN architecture, we want to examine how much each GNN component contributes to the performance. Since IGNN is input-agnostic, we cannot rely on its node representations only to predict a tail given an input query. However, AGNN is inputdependent, which means it can be carried out to complete the task without taking underlying node representations from IGNN. Therefore, we can arrange two sets of experiments: (1) AGNN + IGNN, and (2) AGNN-only. In AGNN-only, we do not run message passing in IGNN to compute H v,: but instead use node embeddings as H v,: , and then we run pruned message passing in AGNN as usual. We want to be sure whether IGNN is actually useful. In this setting, we compare the first set which runs IGNN for two steps against the second one which totally shuts IGNN down. The results in Figure 4(B) (and Figure 7(B) in Appendix) show that IGNN brings an amount of gains in each metric on WN18RR , indicating that representations computed by full-graph message passing indeed help subgraph-based message passing.Horizon analysis. The sampling, attending-to, attending-from and searching (i.e., propagation steps) horizons determine how large area a subgraph can expand over. These factors affect computation complexity as well as prediction performance. Intuitively, enlarging the exploring area by sampling more, attending more, and searching longer, may increase the chance of hitting a target to gain some performance. However, the experimental results in Figure 4(C)(D) show that it is not always the case. In Figure 4(E), we can see that increasing the maximum number of attending-  from nodes per step is useful. That also explains why we call nodes in the attending-from horizon the core nodes, as they determine where subgraphs can be expanded and how attention will be propagated to affect the final probability distribution on the tail prediction. However, GPUs with a limited memory do not allow for a too large number of sampled or attended nodes especially for Max-attending-from-per-step. The detailed explanations can be found in attention strategies in Section 2 where the upper bound is controlled by N 1 N 2 and N 3 (Max-attending-from-per-step corresponding to N 1 , Max-sampling-per-node to N 2 , and Max-attending-to-per-step to N 3 ). In N 1 N 2 , Section 3.2 suggests that we should sample more by a large N 2 but attend less by a small N 1 . Figure 4(F) suggests that the propagation steps of AGNN should not go below four.Attention flow analysis. If the flow-style attention really captures the way we reason about the world, its process should be conducted in a diverging-converging thinking pattern. Intuitively, first, for the diverging thinking phase, we search and collect ideas as much as we can; then, for the converging thinking phase, we try to concentrate our thoughts on one point. To check whether the attention flow has such a pattern, we measure the average entropy of attention distributions changing along steps and also the proportion of attention concentrated at the top-1,3,5 nodes. As we expect, attention is more focused at the final step and the beginning. CONCLUSIONWe introduce Dynamically Pruned Message Passing Networks (DPMPN) and apply it to large-scale knowledge graph reasoning tasks. We propose to learn an input-dependent local subgraph which is progressively and selectively constructed to model a sequential reasoning process in knowledge graphs. We use graphical attention expression, a flow-style attention mechanism, to guide and prune the underlying message passing, making it scalable for large-scale graphs and also providing clear graphical interpretations. We also take the inspiration from the consciousness prior to develop a two-GNN framework to boost experimental performances.Proposition. Given a graph G (undirected or directed in both directions), we assume the probability of the degree of an arbitrary node being less than or equal to d is larger than p, i.e., P (deg(v) ≤ d) > p, ∀v ∈ V . Considering a sequence of consecutively expanding subgraphs (G 0 , G 1 , . . . , G T ), starting with G 0 = {v}, for all t ≥ 1, we can ensureP ||V G t || ≤ d(d − 1) t − 2 d − 2 > p d(d−1) t−1 −2 d−2 . (6)Proof. We consider the extreme case of greedy consecutive expansion, whereG t = G t−1 ∪ ∆G t = G t−1 ∪ ∂G t−1, since if this case satisfies the inequality, any case of consecutive expansion can also satisfy it. By definition, all the subgraphs G t are a connected graph. Here, we use ∆V t to denote V ∆G t for short. In the extreme case, we can ensure that the newly added nodes ∆V t at step t only belong to the neighborhood of the last added nodes ∆V t−1 . Since for t ≥ 2 each node in ∆V t−1 already has at least one edge within G t−1 due to the definition of connected graphs, we can haveP ||∆V t || ≤ ||∆V t−1 ||(d − 1) > p ||∆V t−1 || . (7)For t = 1, we have P (||∆V 1 || ≤ d) > p and thusP ||V G 1 || ≤ 1 + d > p. (8) For t ≥ 2, based on ||V G t || = 1 + ||∆V 1 || + . . . + ||∆V t ||, we obtain P ||V G t || ≤ 1 + d + d(d − 1) + . . . + d(d − 1) t−1 > p 1+d+d(d−1)+...+d(d−1) t−2 ,(9)which isP ||V G t || ≤ d(d − 1) t − 2 d − 2 > p d(d−1) t−1 −2 d−2 . (10)We can find that t = 1 also satisfies this inequality. The hyperparameters can be categorized into three groups:• Normal hyperparameters, including batch size, n dims att, n dims, learning rate, grad clipnorm, and n epochs. We set smaller dimensions, n dims att, for computation in the attention module, as it uses more edges than the message passing uses in AGNN, and also intuitively, it does not need to propagate high-dimensional messages but only compute scalar scores over a sampled neighborhood, in concert with the idea in the key-value mechanism (Bengio, 2017). We set n epochs = 1 in most cases, indicating that our model can be trained well by one epoch only due to its fast convergence. • The hyperparameters in charge of the sampling-attending horizon, including max sampling per step that controls the maximum number to sample edges per step in IGNN, and max sampling per node, max attending from per step and max attending to per step that control the maximum number to sample neighbors of each selected node per step per input, the maximum number of selected nodes for attending-from per step per input, and the maximum number of selected nodes in a sampled neighborhood for attending-to per step per input in AGNN. • The hyperparameters in charge of the searching horizon, including n steps in IGNN representing the number of propagation steps to run standard message passing in IGNN, and n steps in AGNN representing the number of propagation steps to run pruned message passing in AGNN.Note that we tune these hyperparameters according to not only their performances but also the computation resources available to us. In some cases, to deal with a very large knowledge graph with limited resources, we need to make a trade-off between efficiency and effectiveness. For example, each of NELL995's single-query-relation tasks has a small training set, though still with a large graph, so we can reduce the batch size in favor of affording larger dimensions and a larger samplingattending horizon without any concern for waiting too long to finish one epoch.|",yes,Adam,yes,"batch size, dimentions, learning rate, epochs, sampling-attending horizon, searching horizon",yes,,,,
1712,https://github.com/eserie/wax-ml,eserie_wax-ml.tar.gz,WAX-ML: A Python library for machine learning and feedback loops on streaming data,"@eager_function @pytest.mark.parametrize(""use_jit"", [False, True]) @final @pytest.fixture @pytest.mark.parametrize(""dtype"", [""float32"", ""float64""]) @functools.wraps(f) @pytest.mark.parametrize(""p"", [0, 1, 2, 3, 4, ep.inf]) @partial(HaikuEnv, iter(raw_observations_array()), name=""Env"") @jax.jit @overload @jax.value_and_grad @pytest.mark.parametrize(""tensor_type"", [""numpy"", ""jax"", ""tensorflow"", ""torch""]) @compare_allclose @pytest.mark.parametrize( @functools.wraps(func) @overload  # noqa: F811 (waiting for pyflakes > 2.1.1) @compare_allclose(rtol=1e-6) @hk.transform @pytest.fixture(scope=""session"", params=[""t1"", ""t2""]) @overload  # noqa: F811 @common_dtype @partial(HaikuEnv, data_generator) @partial(HaikuEnv, iter(raw_observations_dataarray()), name=""Env"") @pytest.mark.parametrize(""f"", [ep.logical_and, ep.logical_or]) @dataclass @partial(unroll_transform_with_state, dynamic=False) @pytest.mark.parametrize(""astensor"", [False, True]) @unroll_transform_with_state @property @unroll @partial(HaikuAgent, name=""Agent"") @pytest.mark.parametrize(""i"", [0, 1]) @pytest.mark.parametrize(""format"", [""dataframe""]) @pytest.mark.parametrize(""assume_centered"", [False, True]) @staticmethod @partial(jax.jit, device=gpus[0]) @partial(HaikuEnv, iter(raw_observations_dataframe()), name=""Env"") @pytest.mark.parametrize(""format"", [""dataarray"", ""dataframe"", ""series""]) @samedevice @jit_init_apply @pytest.mark.parametrize(""t1, t2"", TEST_INT_LIST) @pytest.mark.parametrize(""dtype"", [""<M8[us]"", ""<M8[ns]""]) @pytest.mark.parametrize(""value"", [1, -1, 2]) @abstractmethod @pytest.mark.parametrize(""seed"", [onp.int32(10), onp.int64(10), 10]) @contextmanager @pytest.mark.parametrize(""fill_value"", [0.0, 0, ""0""]) @partial(HaikuEnv, iter(raw_observations()), name=""Env"") @add_batch @pytest.mark.parametrize(""axis"", [None, 0, 1, (0, 1)]) @hk.transform_with_state @pytest.mark.parametrize(""seed"", [10.0]) @pytest.mark.parametrize(""keepdims"", [False, True]) @HaikuAgent @classmethod @abstractmethod  # noqa: F811 (waiting for pyflakes > 2.1.1) @pytest.mark.parametrize(""static"", [True, False]) @compare_all @pytest.fixture(scope=""session"") @pytest.mark.parametrize(""axis"", [0, 1, -1]) @partial(unroll_transform_with_state, skip_first=skip_first) @partial(add_batch, take_mean=False) @pytest.mark.parametrize(""adjust"", [False, True, ""linear""]) @pytest.mark.parametrize(""indexing"", [""ij"", ""xy""]) @compare_allclose(rtol=1e-5) @pytest.mark.parametrize(""others_astensors"", [False, True]) @pytest.mark.parametrize(""p"", [0, 1, 2, ep.inf]) @tf.function @tf.autograph.experimental.do_not_convert @compare_equal @dataclass(frozen=True)",requests packaging pandas warnings sys types glob plotnine optax tqdm pathlib tensorflow haiku datetime collections dataclasses torch os pytest matplotlib jax importlib functools itertools seaborn typing sklearn numbers numpy abc setuptools typing_extensions logging xarray wax inspect io,cs.LG cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eserie_wax-ml.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eserie_wax-ml.pdf,,,no,,no,,no,,,,
1564,https://github.com/huiqu18/TDGAN-PyTorch,huiqu18_TDGAN-PyTorch.tar.gz,Learn distributed GAN with Temporary Discriminators,@classmethod @abstractmethod @staticmethod @property,"requests PIL pandas warnings sys nibabel skimage tqdm bs4 __future__ torchvision util pathlib datetime collections json h5py data visdom torch os tmp matplotlib dominate importlib scipy functools itertools argparse numpy time abc math tarfile ntpath parse_config random cv2 logging os, json, glob operator options subprocess models zipfile",cs.CV eess.IV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/huiqu18_TDGAN-PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\huiqu18_TDGAN-PyTorch.pdf,,"SettingsThe training data of Multi-Organ dataset is divided into four subsets according to organs: liver, breast, kidney and prostate. Each subset is assumed to be in a local health entity, consisting of 64 images of size 256×256. We perform the following experiments: (1) TDGAN. The health entities are temporarily available, we train a generator using the proposed TDGAN method.(2) Sequential fine-tuning. The health entities are temporarily available, the generator is fine-tuned in a sequential manner. Because previous discriminators are offline, only the generator is initialized using parameters from the generator trained/fine-tuned on the previous tasks. The new local discriminators are randomly initialized. (3) Joint learning. The data in each health center can be collected together to train a regular GAN model. (4) Local GAN. A local GAN is trained using the local data for each health entity.To evaluate the performance of generators, synthetic images are generated using labels from both previous data and current data. Then they are used to train a segmentation model. The higher accuracy the trained segmentation model performs on the test set, the better quality of the synthetic images have. To make fair comparison, the number of synthetic images keeps the same for each segmentation model. We only use the same organ test set for evaluation.Besides, we set the number of online health entities at each time to 1 and 2, corresponding to single-entity and multiple-entity cases, respectively. There are four tasks in the former case and two tasks in the latter one.",yes,Adam,yes,,no,,,,
1688,https://github.com/pmcgrath249/DeepCVLab,pmcgrath249_DeepCVLab.tar.gz,Scalability in Perception for Autonomous Driving: Waymo Open Dataset,,copy warnings sys tqdm easydict torchvision pathlib re tensorflow datetime collections json torch os waymo_open_dataset six pickle numpy setuptools logging,cs.CV cs.LG cs.CV cs.LG stat.ML cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/pmcgrath249_DeepCVLab.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\pmcgrath249_DeepCVLab.pdf,Concatenated Skip Connection  Convolution  Average Pooling  Global Average Pooling  Kaiming Initialization  1x1 Convolution  Batch Normalization  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  Nesterov Accelerated Gradient  Weight Decay  Step Decay  DenseNet  Dense Block  Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,no,,no,,no,,,,
1781,https://github.com/hbahadirsahin/gmvae,hbahadirsahin_gmvae.tar.gz,Under review as a conference paper at ICLR 2017 DEEP UNSUPERVISED CLUSTERING WITH GAUSSIAN MIXTURE VARIATIONAL AUTOENCODERS,@property @staticmethod @tf.function,utils copy loss tensorflow model sys ujson logging sklearn dataloader numpy,cs.LG cs.NE stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hbahadirsahin_gmvae.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hbahadirsahin_gmvae.pdf,AutoEncoder,,yes,Adam,yes,"batch size, learning rate, momentum",yes,,,,
1728,https://github.com/GewelsJI/SINet-V2,GewelsJI_SINet-V2.tar.gz,Concealed Object Detection,,"utils PIL torch, time lib os, argparse, imageio torchvision datetime libtiff torch os os, argparse os, random scipy os, argparse, cv2 argparse tensorboardX numpy time shutil thop math jittor cv2 logging random",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/GewelsJI_SINet-V2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\GewelsJI_SINet-V2.pdf,,,yes,Adam,yes,"batch size, epochs, learning rate",no,,,,
1984,https://github.com/dtriepke/Graph_Convolutional_Network,dtriepke_Graph_Convolutional_Network.tar.gz,Graph Convolutional Networks for Text Classification,,math keras itertools pandas get_graph tensorflow collections networkx pickle sklearn tqdm os __future__ numpy,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dtriepke_Graph_Convolutional_Network.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dtriepke_Graph_Convolutional_Network.pdf,Graph Convolutional Network,,yes,Adam,yes,"learning rate, dropout rate, epochs",no,,,,
1743,https://github.com/BFTrainer/BFTrainer,BFTrainer_BFTrainer.tar.gz,BFTrainer: Low-Cost Training of Neural Networks on Unfillable Supercomputer Nodes,@hvd.elastic.run,utils prog pandas psutil managerOperations torchvision threading stat collections manager enum torch os socket gurobipy horovod scipy itertools jobInfo argparse persistqueue numpy time DBOperations timeit subprocess msgOperations sys_admin,cs.DC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/BFTrainer_BFTrainer.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\BFTrainer_BFTrainer.pdf,,"INTRODUCTIONSupercomputers typically service requests for computational resources with policies that adhere roughly to first come first serve (FCFS) semantics. This approach inevitably results in idle nodes as larger tasks block subsequent tasks from executing. Backfilling [38], a strategy by which later tasks are promoted to run sooner if doing so does not delay other tasks, can improve efficiency, but substantial idle resources inevitably remain. For example, if tasks T1, T2, and T3 request 20%, 90%, and 50% of all nodes, respectively, FCFS will run these tasks sequentially, in that order, with the result that 80% and 10% of the machine is idle as T1 and T2 execute, respectively. If T3 requires less time than T1, then backfilling can promote it to run concurrently with T1, but substantial idle resources remain.Such unfillable nodes are common on leadership-class supercomputers that operate under policies that prioritize large applications that cannot run on small clusters or would take so long as to be impractical: what is known as capability computing [3,23,28]. We refer to a node that the main scheduler does not use for (regular or backfilled) jobs as an idle node, and denote the set of all idle nodes at a particular time as N .We describe here how N can be used effectively for DNN training, an activity that consumes growing numbers of compute resources in the cloud [30] and at supercomputer centers [45]. DNN training is malleable due to the data parallelism mechanism used to process a batch of data for gradient calculation, and can easily be rescaled as one needs to checkpoint only the model weights and (in the case of stateful optimizers) optimizer state. Indeed, deep learning frameworks such as AdaptDL [30], TorchElastic of Py-Torch [27], and Elastic Horovod [34] enable scaling up and down the number of workers dynamically at runtime with slight cost, without requiring a restart or resuming from checkpoints saved to durable storage.Our proposed BFTrainer leverages this malleability of DNN training to make optimal use of non-backfilled idle supercomputer nodes. Basically, BFTrainer collects idle nodes into a resource pool, N , from which it reallocates them for DNN training jobs (referred to as Trainers in the rest of the paper). The core idea is that because nodes in N can come and leave without any commitment, the use of BFTrainer does not affect jobs submitted to the main scheduler. That same characteristic makes it difficult for conventional HPC applications that are not malleable, and are expensive to migrate or checkpoint and restart, to use N effectively.BFTrainer may be used in two ways. In the first, a single user runs multiple Trainers: for example, for hyperparameter optimization (HPO) or neural architecture search (NAS). Here each user has an isolated BFTrainer instance and specifies a metric (e.g., aggregated throughput) for BFTrainer to allocate nodes optimally for their Trainers. To support multiple users or if a single user cannot consume all idle nodes, we can divide the entire supercomputer into several small clusters logically and have one BFTrainer for each small cluster to manage idle nodes for that cluster. In the second scenario, all users submit to a single BFTrainer instance; in this case, an administrator needs to propose a priority score (e.g., efficiency) for BFTrainer to plan resource allocation optimally.Specific benefits of BFTrainer include the following:• Zero cost to jobs in the main scheduler: All Trainers running on idle nodes are fully preemptable at all times; thus, BFTrainer has no impact on other jobs.• Optimal elastic scheduling: A mathematical optimization model is used to identify node allocations for Trainers that are optimal given available information. • Customizable objective: Administrators or users can assign a metric, for example, throughput, scaling efficiency, or priority score, and BFTrainer will allocate resources to different Trainers to optimize the metric.The use of BFTrainer can deliver benefits to both supercomputer centers and supercomputer users. For a supercomputer center, BFTrainer can increase the amount of computing performed; for users, assuming that their supercomputer center incentivizes the use of BFTrainer by reduced charges, it can reduce their costs if they are prepared to accept a less certain scheduling approach (e.g., AWS spot instances provide a similar tradeoff). Note that depending on relative demand, BFTrainer may run slower or faster than the main queue.We next use Summit supercomputer logs to obtain quantitative data on the frequency of unfillable nodes ( §2); define an MILP model for allocating nodes in N to Trainers ( §3); introduce our experimental setup and evaluation metrics ( §4); evaluate our resource allocation algorithm by replaying real traces with real DNN models ( §5); review related work ( §6); and conclude ( §7).",no,,no,,no,,,,
1722,https://github.com/jiahaoLjh/PlaneSweepPose,jiahaoLjh_PlaneSweepPose.tar.gz,Multi-View Multi-Person 3D Pose Estimation with Plane Sweep Stereo,@staticmethod,utils copy yaml sys glob tqdm easydict pprint pathlib _init_paths core json collections torch os scipy dataset pickle argparse numpy time random logging models,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jiahaoLjh_PlaneSweepPose.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jiahaoLjh_PlaneSweepPose.pdf,,"Back Projection ProjectionReference View 1 Reference View 2 counterpart, the fusion of information from multi-view images with multiple persons is more challenging since the identity of the 2D poses from each camera view is unknown. Previous works such as [5,7,11] address this problem in three steps. The 2D poses are first estimated for each camera view independently. Subsequently, the 2D poses from different views that correspond to the same person are identified and grouped together. Finally, the 3D pose of each person is estimated with triangulation or optimization-based pictorial structure models using the set of grouped 2D pose detections from multiple views.The establishment of cross-view correspondences is critical for multi-view multi-person 3D pose estimation. Traditional methods use either greedy matching approach [11] for fast inference speed, or optimization-based approach [5,7,8] for better global consistency. Recently, Voxel-Pose [25] is proposed to jointly solve the challenging crossview matching and 3D pose estimation problems in an object-detection paradigm. Instead of explicitly searching for 2D pose correspondences, VoxelPose projects the 2D pose heatmaps from multiple views to a common 3D space, and performs both 3D pose detection and estimation in the 3D volumetric space. The 3D object detection formulation avoids the explicit cross-view matching step, thus effectively reduces the impact from incorrectly established cross-view correspondences. Despite its effectiveness, several limitations exist for the object-detection-based pipeline: 1) Prior knowledge of the common 3D space dimension according to the multi-camera settings is needed to define the volumetric space for 3D object detection. 2) The back-projection of the 2D pose detections on each 3D voxel is not scalable to larger scenes. 3) 3D convolution that is applied to all voxel locations incurs unnecessary heavy computations, especially for large sparse scenes.In this work, we present our plane-sweep-based approach for multi-view multi-person 3D pose estimation. Our approach avoids explicit cross-view matching and aggregates multiple views for 3D pose estimation in a single shot. Specifically, we build our framework upon the concept of plane sweep stereo [6] to estimate the depth for each joint of each person in a target camera view. As illustrated in Figure 1, 2D poses are first back-projected to successive virtual depth planes, and then warped to the respective reference camera views. We measure the cross-view consistency at each depth level, which is then used to regress the depths from standard convolutional neural networks. Our depth regression adopts a two-stage coarse-to-fine scheme. Personlevel depth is first estimated for each 2D pose. Joint-level relative depth with respect to the person-level depth within a much smaller depth range is then regressed for each joint. The two stages can be trained together in an end-to-end manner. During inference, we obtain the 3D poses by backprojecting the 2D poses with the estimated depths. Multiple 3D poses of the same person from different views can be easily merged via a simple distance-based clustering.We evaluate our plane-sweep-based framework on three benchmark datasets, i.e., the Campus and the Shelf datasets, and CMU Panoptic dataset, where we outperform existing state-of-the-arts. In addition to the removal of explicit cross-view matching and triangulation compared to the traditional three-step approaches, our method is also more efficient compared to VoxelPose in two aspects: 1) In contrast to VoxelPose that builds voxels in the 3D space, we leverage on the plane sweep algorithm that is proportional to only the number of virtual depth planes. 2) Instead of performing 3D convolution on all voxel locations, we utilize the much faster 1D convolutions for each 2D pose. Furthermore, our method is more generablizable to scenarios with no prior knowledge of the multi-camera settings since only the range of virtual depth planes needs to be pre-defined for each camera view.Our contributions in this work are:• We present a plane-sweep-based approach to perform multi-view multi-person 3D pose estimation without the need for explicit cross-view matching.• Our approach outperforms existing state-of-the-arts on benchmark datasets, while being much more efficient compared to existing works. ! 𝑑Joint Depth Regressor ! 𝑑 (""#$)+ Score Matrix ∈ ℝ !×# Score Matrix ∈ ℝ ! (""#$) ×# (a) (b)Figure 2: Overview of our approach. 2D pose estimation is first performed for each camera view. We then use the plane sweep algorithm to aggregate the cross-view consistency score for the target person highlighted with jet colormap. Personlevel depth is regressed first in (a). Joint-level relative depth is then estimated in (b) and combined with the person-level depth to reconstruct the 3D pose.person. Kadkhodamohammadi et al. [18] propose to compute a distance between each pair of 2D poses from different views based on the epipolar constraints, and then find the cross-view correspondences with the lowest distance.Instead of directly performing triangulation, the matched 2D poses from all camera views are stacked together and passed into a regression neural network to estimate the 3D pose. Dong et al. [7] enhance the cross-view consistency with appearance features. They utilize a person Re-ID network [27] to get the appearance features for each person. These features are then used to compute the appearancebased distance. They also formulate a convex optimization problem to solve for the optimal correspondence matrix, and use a rank constraint to enforce cycle-consistency. Chen et al. [5] propose to match cross-view 2D poses by applying the epipolar constraints on feet joints instead of the entire 2D pose. They perform bipartite matching for each pair of views on the pairwise affinities defined on feet joints. A maximum a posteriori (MAP) estimator is adopted for the 3D pose reconstruction. Huang et al. [11] propose a greedy bottom-up matching approach for 2D pose grouping. Candidate 3D poses are first obtained from triangulation of each pair of 2D poses. These candidate 3D poses form a 3D pose subspace that are then used with a distance-based greedy clustering approach to group the cross-view poses.Triangulation with learnable weights inspired by [16] is applied for each group to obtain the 3D pose estimates.The aforementioned methods are multi-stage pipelines, where incorrect correspondences can cause large errors in the subsequent 3D pose estimation step. A recent work, VoxelPose [25], presents a novel pipeline that avoids the explicit cross-view matching and performs 3D pose estimation directly from the multi-view input. This work is inspired by the volumetric approach presented in [16] that generates 3D volumes from 2D detections. To identify multiple persons in the common 3D volumetric space, VoxelPose utilizes a 3D object detection formulation to localize each 3D pose, followed by a per-person 3D pose estimation. VoxelPose shows promising results since cross-view consistency is implicitly enforced in the 3D pose estimation. However, the 3D convolution used on the volumetric space is computationally expensive, thus not scalable for larger scenes.In this work, we present our multi-view 3D pose estimation approach. Inspired by plane sweep stereo [6,12] for dense depth regression, our approach utilizes a pose-aware geometric consistency metric to aggregate multi-view information and performs depth regression for 2D poses without explicitly establishing correspondences. Our approach demonstrates higher 3D pose estimation precision, while being much more efficient compared to previous works. Our MethodOur task is to estimate the 3D poses for all persons in a common 3D space from multi-view images captured by a set of synchronized and calibrated cameras. The overview of our framework is shown in Figure 2. We first perform 2D pose estimation for each camera view independently using a top-down multi-person pose estimation approach, e.g., HR-Net [23]. Subsequently, we perform depth regression for each candidate 2D pose with J joints under a target camera view by utilizing 2D pose detections from multiple reference views. Finally, the 3D poses can be reconstructed from back-projections of the candidate 2D poses with the estimated depths. In this section, we present our multi-view depth regression approach based on plane sweep stereo. A coarse person-level depth regression module is introduced first in Section 3.1, followed by a per-person joint-level relative depth regression module in Section 3.2. Person-Level Depth RegressionOur framework is inspired by the plane sweep stereo for dense depth estimation. The basic idea of plane sweep stereo is to back-project the target view image to a set of successive virtual depth planes, and then warp these projections to the reference view images so that photometric consistency can be measured to determine the depth of each target view pixel. We adopt the concept of plane sweep in our framework for the person-and joint-level depth regression. In contrast to the dense depth estimation in the standard plane sweep stereo that relies on photometric consistency, we measure a pose-aware geometric consistency for the depth regression of 2D human poses instead. In this section, we present a person-level depth regression module to coarsely localize the depth for each candidate 2D pose. The person-level depth is defined to be the depth of the center hip joint of each person in our implementation. ConclusionIn this work, we present our plane-sweep-based approach to regress 2D pose depths for the task of multiview multi-person 3D pose estimation. Our method uses the plane sweep algorithm to aggregate multi-view information based on a pose-aware geometric consistency and effectively estimates the depths for each 2D pose in a target camera view without explicitly establishing cross-view correspondences. Depth regression is performed in a coarse-tofine scheme, where we first regress the person-level depth followed by the joint-level relative depth estimation. Our framework is computationally more efficient and shows superior performance compared to previous state-of-the-arts.Figure 1 :1Figure 1: Our method is based on plane sweep stereo to regress depths for 2D pose detections. 2D poses are backprojected to successive depth planes and warped to reference views for consistency measurement which is utilized for depth regression.",no,,no,,no,,,,
1721,https://github.com/otiliastr/coper,otiliastr_coper.tar.gz,Contextual Parameter Generation for Knowledge Graph Link Prediction,@abc.abstractmethod @property @staticmethod,"requests copy yaml warnings glob tqdm __future__ os, sys tensorflow qa_cpg json collections src torch os matplotlib six functools itertools pickle argparse numbers numpy abc shutil tarfile math operator logging random",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/otiliastr_coper.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\otiliastr_coper.pdf,,,no,,no,,no,,,,
1990,https://github.com/andompesta/nodeembedding-to-communityembedding,andompesta_nodeembedding-to-communityembedding.tar.gz,From Node Embedding To Community Embedding *,@staticmethod,utils multiprocessing psutil distutils threading Cython collections queue os concurrent matplotlib scipy itertools ADSCModel pickle sklearn numpy time timeit networkx random logging,cs.SI cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/andompesta_nodeembedding-to-communityembedding.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\andompesta_nodeembedding-to-communityembedding.pdf,,,yes,,no,,no,,,,
1615,https://github.com/giovanniguidi/FCN-keras,giovanniguidi_FCN-keras.tar.gz,Fully Convolutional Networks for Semantic Segmentation,,utils PIL yaml io keras tensorflow trainers datetime json os data_generators string scipy predictors sklearn argparse numpy preprocessing cv2 random losses models base,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/giovanniguidi_FCN-keras.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\giovanniguidi_FCN-keras.pdf,Average Pooling  Dropout  Global Average Pooling  Dense Connections  Rectified Linear Units  Max Pooling  Softmax  Convolution  VGG  Residual Network,"Segmentation ArchitectureWe cast ILSVRC classifiers into FCNs and augment them for dense prediction with in-network upsampling and a pixelwise loss. We train for segmentation by fine-tuning. Next, we build a novel skip architecture that combines coarse, semantic and local, appearance information to refine prediction.For this investigation, we train and validate on the PAS-CAL VOC 2011 segmentation challenge [7]. We train with a per-pixel multinomial logistic loss and validate with the standard metric of mean pixel intersection over union, with the mean taken over all classes, including background. The training ignores pixels that are masked out (as ambiguous or difficult) in the ground truth.",yes,SGD,yes,,no,,,,
1745,https://github.com/LINGYUNFDU/MSSU-Net,LINGYUNFDU_MSSU-Net.tar.gz,Multi-Scale Supervised 3D U-Net for Kidneys and Kidney Tumor Segmentation,@abstractmethod @property @staticmethod,"copy pandas multiprocessing sys nibabel skimage torchvision medpy SimpleITK datetime collections json os,sys nnunet torch os pkgutil os,sys,glob,shutil matplotlib importlib scipy apex hashlib _warnings pickle sklearn argparse numpy batchgenerators time shutil abc math tensorwatch subprocess inspect",eess.IV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/LINGYUNFDU_MSSU-Net.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\LINGYUNFDU_MSSU-Net.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,yes,Adam,yes,"learning rate, epochs",no,,,,
1830,https://github.com/stanford-futuredata/selection-via-proxy,stanford-futuredata_selection-via-proxy.tar.gz,SELECTION VIA PROXY: EFFICIENT DATA SELECTION FOR DEEP LEARNING,"@dataset_options @click.option('--subset', type=int, default=512_466, show_default=True, @click.option('--dataset', '-d', type=click.Choice(DATASETS), @click.option('rounds', '--round', '-r', multiple=True, type=int, @click.option('--min-count', default=1, show_default=True, @abc.abstractmethod @miscellaneous_options @click.option('--eval-target-at', multiple=True, type=int, @click.option('--track-test-acc/--no-track-test-acc', @property @click.option('--precomputed-selection', @click.option('--selection-method', type=click.Choice(coreset_methods), @click.option('--learning-rate', default=0.05, show_default=True, @click.option('--selection-method', type=click.Choice(active_learning_methods), @click.group() @click.option('--ngrams', '-n', default=2, show_default=True, @click.option('--train-target/--no-train-target', @click.option('--initial-subset', type=int, default=1_000, show_default=True, @cli.command() @click.option('--dim', default=10, show_default=True, @click.option('--bucket', default=10_000_000, show_default=True, @click.option('--initial-subset', type=int, default=25_623, show_default=True, @computing_options @click.option('sizes', '--size', multiple=True, type=int, @click.option('--initial-subset', type=int, default=72_000, show_default=True, @click.option('--datasets-dir', default='./data', show_default=True, @click.option('--threads', default=4, show_default=True, @click.argument('executable') @training_options @click.option('--seed', '-s', type=int, @click.option('--run-dir', default='./run', show_default=True, @click.option('--epochs', default=5, show_default=True, @click.option('--subset', type=int, default=10_000, show_default=True, @click.option('--subset', type=int, default=1_800_000, @proxy_training_overrides @click.option('--selection-method',",pandas contextlib glob tqdm torchvision re click datetime collections json torch os svp gzip functools apex scipy typing numpy time abc math setuptools typing_extensions subprocess,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/stanford-futuredata_selection-via-proxy.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\stanford-futuredata_selection-via-proxy.pdf,,,no,,yes,"batch size, learning rate, epochs",no,,,,
1813,https://github.com/lennelov/endd-reproduce,lennelov_endd-reproduce.tar.gz,,@abstractmethod,utils PIL sys pathlib tensorflow datetime collections json uncertainty_metrics os matplotlib scipy settings seaborn pickle sklearn numpy time math random operator models,stat.ML cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/lennelov_endd-reproduce.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\lennelov_endd-reproduce.pdf,VGG-16,,yes,Adam,yes,"batch size, dropout rate, epochs, momentum",yes,,,,
1646,https://github.com/hoppiece/song,hoppiece_song.tar.gz,Self Organizing Nebulous Growths for Robust and Incremental Data Visualization,,copy pickle sklearn os song_ numpy matplotlib time,cs.CV cs.HC cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hoppiece_song.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hoppiece_song.pdf,,,yes,"SGD, manually selected",yes,detailed experiment setup description,yes,,,,
1683,https://github.com/abdkhanstd/COVID-19,abdkhanstd_COVID-19.tar.gz,Blockchain-Federated-Learning and Deep Learning Models for COVID-19 detection using CT Imaging,@threadsafe_generator @staticmethod,"utils copy csv sys multiprocessing glob nibabel skimage tqdm __future__ processor torchvision threading capsuels pathlib keras tensorflow collections h5py data torch os scipy functools sklearn argparse tensorboardX numpy time numpy, scipy extractor runstats random operator models model_caps_iv3",eess.IV cs.CV cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/abdkhanstd_COVID-19.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\abdkhanstd_COVID-19.pdf,Concatenated Skip Connection  Dense Block  1x1 Convolution  Dense Connections  Dropout  Batch Normalization  Residual Connection  Softmax  Rectified Linear Units  Bottleneck Residual Block  DenseNet  Max Pooling  Average Pooling  Convolution  Residual Block  Global Average Pooling  Kaiming Initialization  Residual Network,,no,,no,,no,,,,
1736,https://github.com/facebookresearch/GDT,facebookresearch_GDT.tar.gz,Multi-modal Self-Supervision from Generalized Data Transformations,@property @staticmethod @torch.no_grad(),utils pandas csv multiprocessing sys signal ffmpeg glob torchvision re model datetime collections src torch os fractions matplotlib scipy apex gdt_helper bisect kornia pickle video_transforms argparse joblib decoder numpy time shutil audio_utils math datasets librosa spec_augment_pytorch errno random logging subprocess sparse_image_warp_pytorch log_utils python_speech_features av,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/facebookresearch_GDT.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\facebookresearch_GDT.pdf,,"ExperimentsWe compare self-supervised methods on pretraining audio-visual representations. Quality is assessed based on how well the pretrained representation transfers to other (supervised) downstream tasks. We first study the model in order to determine the best learning transformations and setup. Then, we use the latter to train for longer and compare them to the state of the art.Self-supervised pretraining. For pretraining, we consider the standard audio-visual pretraining datasets, Kinetics-400 [50] and AudioSet [31], and additionally, the recently released, VGG-Sound dataset [17]. Finally, we also explore how our algorithm scales to even larger, less-curated datasets and train on IG65M [32] as done in XDC [5].Our method learns a pair of representations f = (f v , f a ) for visual and audio information respectively. We use R(2+1)D-18 [101] as the visual encoder f v and ResNet [40] with 9 layers as the audio encoder f a unless otherwise noted; both encoders produce a fixed-dimensional output (512-D) after global spatio-temporal average pooling. Both vectors are then passed through two fully-connected layers with intermediate size of 512 to produce 256-D embeddings as in [10] which are normalized by their L2-norm [106]. The embedding is used for computing the contrastive loss, while for downstream tasks, a linear layer after the global spatio-temporal average pooling is randomly intialized. Further implementation details are given in the Appendix. 4 Donwstream tasks. To assess the visual representation f v , we consider standard action recognition benchmark datasets, UCF-101 [94] and HMDB-51 [57]. We test the performance of our pretrained models on the tasks of finetuning the pretrained representation, conducting few-shot learning and video action retrieval. To assess the audio representation f a , we train a linear classifier on frozen features for the common ESC-50 [82] and DCASE2014 [95] benchmarks and finetune for VGG-Sound [17]. Details are given in appendix A.4. In this section, we conduct an extensive study on each parameter of the GDT transformation studied here, T = (i, τ, m, g), and evaluate the performance by finetuning our network on the UCF-101 and HMDB-51 action recognition benchmarks, see Appendix for implementation details. Few-shot classificationWe follow the protocol in [49] and evaluate our our GDT pretrained network using few-shot classification on the UCF-101 dataset, and additionally on HMDB-51. We randomly sample n videos per class from the train set, average the encoder's global average pooling features from ten clips per training sample and measure classification performance on the validation set after training a SVM with 'one-vs-all' and C = 1.Retrieval We follow the standard protocol as outlined in [109]. We use the split 1 of UCF101, and additionally HMDB-51. We uniformly sample 10 clips per video, and average the max-pooled features after the last residual block for each clip per video. We use these averaged features from the validation set to query the videos in the training set. The cosine distance of representations between the query clip and all clips in the training set are computed. When the class of a test clip appears in the classes of k nearest training clips, it is considered to be correctly predicted. We report accuracies for k = 1, 5, 10, 20, 50 and compare with the other self-supervised methods on UCF101 and HMDB-51 in table A.2.Audio We extract 10 equally spaced 2-second sub-clips from each full audio sample of ESC-50 [82] and 60 1-second sub-clips from each full sample of DCASE2014 [95]. ESC-50 is an environmental sound classification dataset which has 2K clips of 50 different audio classes. DCASE2014 is an acoustic scenes and event classification dataset which has 100 training clips of 10 different audio classes. We save the activations that result from the audio encoder to quickly train the linear classifiers. We use activations after the last convolutional layer of the ResNet-18 and apply a max pooling with kernelsize (1,3) and stride of (1,2) without padding to the output. For both datasets, we then optimize a L2 regularized linear layer with batch size 512 using the Adam optimizer [54] with learning rate 1 • 10 −4 , weight-decay set to 5 • 10 −4 and the default parameters. The classification score for each audio sample is computed by averaging the sub-clip scores in the sample, and then predicting the class with the highest score. The mean top-1 accuracy is then taken across all audio clips and averaged across all official folds. For VGG-Sound [17], we follow their evaluation metrics but follow a much shorter training schedule as our model is pretrained. We optimize the network with batch size 128 using the Adam optimizer [54] with learning rate 1 • 10 −4 for the pretrained backbone and 1 • 10 −3 for the newly randomly initialized linear layer, weight-decay set to 1 • 10 −5 and the default parameters. We drop the learning rate once at 10 epochs and train for 30 epochs, which takes less than 10h on a single Nvidia GTX 1080 Titan GPU.Fig. 1 :1Fig. 1: Schematic overview of our framework. A: Hierarchical sampling process of generalized transformations T = t M • ... • t 1 for the multi-modal training study case. B: With generalized data transformations (GDT), the network learns a meaningful embedding via learning desirable invariances and distinctiveness to transformations (realigned here for clarity) across modalities and time. The embedding is learned via noise contrastive estimation against clips of other source videos. C: Subset of the c(T, T ) contrast matrix which shows which pairs are repelling (0) and attracting (1) (see text for details). Illustrational videos taken from [1].",yes,Adam,yes,"batch size, epochs, learning rate, wieght decay",no,,,,
1812,https://github.com/C-puqing/DIP-GM,C-puqing_DIP-GM.tar.gz,Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?,@input_to_numpy @abstractmethod @staticmethod,utils PIL copy pandas warnings sys glob easydict ast torchvision pathlib datetime json collections SinkhornModule data tempfile torch os matplotlib gurobipy module torch_geometric scipy functools itertools xml pickle tensorboardX numpy time abc shutil networkx random subprocess eval,cs.CV cs.LG math.OC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/C-puqing_DIP-GM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\C-puqing_DIP-GM.pdf,,,yes,Adam,yes,lambda,yes,,,,
1995,https://github.com/eliotheinrich/MLPhaseBoundary,eliotheinrich_MLPhaseBoundary.tar.gz,Machine learning out-of-equilibrium phases of matter,,math keras plot_lattice pickle sklearn mpl_toolkits numpy matplotlib config,cond-mat.dis-nn cond-mat.str-el,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/eliotheinrich_MLPhaseBoundary.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\eliotheinrich_MLPhaseBoundary.pdf,,,yes,,no,We found all the results we report below to be  nsensitive to the exact parameter values used for training (so-called “hyper-parameters”).,no,,,,
1819,https://github.com/birdortyedi/image-retrieval-with-capsules,birdortyedi_image-retrieval-with-capsules.tar.gz,Fashion Image Retrieval with Capsule Networks,@staticmethod,utils TripletDirectoryIterator keras re tensorflow csv blocks models random layers tqdm argparse os config numpy colorama time shutil,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/birdortyedi_image-retrieval-with-capsules.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\birdortyedi_image-retrieval-with-capsules.pdf,,,no,,no,,no,,,,
1824,https://github.com/Takuya-Shuto-engineer/PokemonGAN,Takuya-Shuto-engineer_PokemonGAN.tar.gz,Analyzing and Improving the Image Quality of StyleGAN,@tf.custom_gradient @staticmethod,requests PIL copy pwd dnnlib sys types glob projector ctypes pprint platform distutils threading pathlib tensorflow re uuid datetime json collections h5py enum tempfile fnmatch os html six importlib gzip scipy hashlib pretrained_networks training lmdb pickle typing sklearn argparse numpy time shutil tensorboard traceback cv2 logging metrics inspect io,cs.CV cs.LG cs.NE eess.IV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Takuya-Shuto-engineer_PokemonGAN.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Takuya-Shuto-engineer_PokemonGAN.pdf,R1 Regularization  Leaky ReLU  Residual Connection  Adam  Path Length Regularization  Weight Demodulation  StyleGAN2  Convolution,"D. Projection method detailsGiven a target image x, we seek to find the corresponding w ∈ W and per-layer noise maps denoted n i ∈ R ri×ri where i is the layer index and r i denotes the resolution of the ith noise map. The baseline StyleGAN generator in 1024×1024 resolution has 18 noise inputs, i.e., two for each resolution from 4×4 to 1024×1024 pixels. Our improved architecture has one fewer noise input because we do not add noise to the learned 4×4 constant (Figure 2).Before optimization, we compute µ w = E z f (z) by running 10 000 random latent codes z through the mapping network f . We also approximate the scale of W by computing σ 2 w = E z f (z) − µ w 2 2 , i.e., the average square Euclidean distance to the center.At the beginning of optimization, we initialize w = µ w and n i = N (0, I) for all i. The trainable parameters are  1.the components of w as well as all components in all noise maps n i . The optimization is run for 1000 iterations using Adam optimizer [25] with default parameters. Maximum learning rate is λ max = 0.1, and it is ramped up from zero linearly during the first 50 iterations and ramped down to zero using a cosine schedule during the last 250 iterations. In the first three quarters of the optimization we add Gaussian noise to w when evaluating the loss function as w = w + N (0, 0.05 σ w t 2 ), where t goes from one to zero during the first 750 iterations. This adds stochasticity to the optimization and stabilizes finding of the global optimum.Given that we are explicitly optimizing the noise maps, we must be careful to avoid the optimization from sneaking actual signal into them. Thus we include several noise map regularization terms in our loss function, in addition to an image quality term. The image quality term is the LPIPS [50] distance between target image x and the synthesized image: L image = D LPIPS [x, g( w, n 0 , n 1 , . . .)]. For increased performance and stability, we downsample both images to 256×256 resolution before computing the LPIPS distance. Regularization of the noise maps is performed on  1.In the subsequent rows, we enable spectral normalization in the generator (SN-G) and in the discriminator (SN-D). We also test the training without weight demodulation (Demod) and path length regularization (P.reg). All of these configurations are highly detrimental to FID, as well as to Recall. ↑ indicates that higher is better, and ↓ that lower is better.multiple resolution scales. For this purpose, we form for each noise map greater than 8×8 in size a pyramid down to 8×8 resolution by averaging 2×2 pixel neighborhoods and multiplying by 2 at each step to retain the expected unit variance. These downsampled noise maps are used for regularization only and have no part in synthesis.Let us denote the original noise maps by n i,0 = n i and the downsampled versions by n i,j>0 . Similarly, let r i,j be the resolution of an original (j = 0) or downsampled (j > 0) noise map so that r i,j+1 = r i,j /2. The regularization term for noise map n i,j is thenL i,j = 1 r2i,j E. Results with spectral normalizationSince spectral normalization (SN) is widely used in GANs [31], we investigated its effect on StyleGAN2. Table 4 gives the results for a variety of configurations where spectral normalization is enabled in addition to our techniques (weight demodulation, path length regularization) or instead of them. Table 5. Computational effort expenditure and electricity consumption data for this project. The unit for computation is GPUyears on a single NVIDIA V100 GPU -it would have taken approximately 51 years to execute this project using a single GPU.See the text for additional details about the computation and energy consumption estimates. Initial exploration includes all training runs after the release of StyleGAN [24] that affected our decision to start this project. Paper exploration includes all training runs that were done specifically for this project, but were not intended to be used in the paper as-is. FFHQ config F refers to the training of the final network. This is approximately the cost of training the network for another dataset without hyperparameter tuning. Other runs in paper covers the training of all other networks shown in the paper. Backup runs left out includes the training of various networks that could potentially have been shown in the paper, but were ultimately left out to keep the exposition more focused. Video, figures, etc. includes computation that was spent on producing the images and graphs in the paper, as well as on the result video. Public release covers testing, benchmarking, and large-scale image dumps related to the public release.Interestingly, adding spectral normalization to our generator is almost a no-op. On an implementation level, SN scales the weight tensor of each layer with a scalar value 1/σ(w). The effect of such scaling, however, is overridden by Equation 3 for the main convolutional layers as well as the affine transformation layers. Thus, the only thing that SN adds on top of weight demodulation is through its effect on the tRGB layers.When we enable spectral normalization in the discriminator, FID is slightly compromised. Enabling it in the generator as well leads to significantly worse results, even though its effect is isolated to the tRGB layers. Leaving SN enabled, but disabling a subset of our contributions does not improve the situation. Thus we conclude that StyleGAN2 gives better results without spectral normalization.",yes,Adam,yes,"batch size, learning rate, momentum",yes,,,,
1534,https://github.com/Farahn/AES,Farahn_AES.tar.gz,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,,"gc pandas csv sys bert_serving __future__ random, itertools re tensorflow json collections nltk os codecs sklearn argparse numpy unicodedata random io",cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/Farahn_AES.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\Farahn_AES.pdf,Residual Connection  Attention Dropout  Linear Warmup With Linear Decay  Weight Decay  Gaussian Error Linear Units  Dense Connections  Adam  WordPiece  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  BERT,,yes,Adam,yes,"batch size, learning rate, epochs",no,,,,
1804,https://github.com/bigvideoresearch/SCC,bigvideoresearch_SCC.tar.gz,Webly Supervised Image Classification with Self-Contained Confidence,@contextmanager @runner.patch_pipeline('OnlineSelfContainedConfPipeline') @runner.patch_dataset('OnlineLabelerDataset') @staticmethod @runner.patch_pipeline('ReWeightLearnPipeline') @runner.patch_pipeline('OfflineSelfContainedConfPipeline'),"utils ae_toolbox PIL pandas contextlib sys multiprocessing runner_master signal pdb tqdm os, io, logging faiss ctypes torchvision threading pathlib atexit re model datetime collections json queue torch os importlib scipy functools itertools rulers builtins networks sklearn argparse pipelines numpy Queue time shutil math datasets traceback json5 random logging oyaml labelers petrel_tool io",cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/bigvideoresearch_SCC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\bigvideoresearch_SCC.pdf,Mixup,,yes,SGD,yes,"batch size, epochs, learning rate, momentum, weight decay",yes,,,,
1507,https://github.com/marload/ConvNets-TensorFlow2,marload_ConvNets-TensorFlow2.tar.gz,VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION,@staticmethod,utils tensorflow argparse os models,cs.CV cs.CV cs.CV cs.CV cs.LG cs.CV cs.LG cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.CV cs.AI cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/marload_ConvNets-TensorFlow2.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\marload_ConvNets-TensorFlow2.pdf,VGG-16  Dropout  Step Decay  Weight Decay  SGD with Momentum  Xavier Initialization  Color Jitter  Random Horizontal Flip  Random Resized Crop  Dense Connections  Rectified Linear Units  Max Pooling  Softmax  VGG  Convolution  Convolution  Average Pooling  Local Response Normalization  Auxiliary Classifier  1x1 Convolution  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  Weight Decay  Polyak Averaging  Random Horizontal Flip  Random Resized Crop  SGD with Momentum  Step Decay  GoogLeNet  Inception Module  Average Pooling  Random Horizontal Flip  Random Resized Crop  Step Decay  Weight Decay  SGD with Momentum  Global Average Pooling  Rectified Linear Units  Kaiming Initialization  Max Pooling  1x1 Convolution  Batch Normalization  Bottleneck Residual Block  Residual Block  Convolution  Residual Connection  Residual Network  Concatenated Skip Connection  Convolution  Average Pooling  Global Average Pooling  Kaiming Initialization  1x1 Convolution  Batch Normalization  Rectified Linear Units  Dropout  Dense Connections  Max Pooling  Softmax  Nesterov Accelerated Gradient  Weight Decay  Step Decay  DenseNet  Dense Block  Average Pooling  Affine Coupling  Normalizing Flows  Rectified Linear Units  1x1 Convolution  Batch Normalization  Random Resized Crop  Random Horizontal Flip  Step Decay  SGD with Momentum  Weight Decay  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Residual Connection  Convolution  Residual Network  Max Pooling  Residual Connection  Convolution  Average Pooling  Reduction-A  Inception-ResNet-v2-A  Inception-ResNet-v2 Reduction-B  Inception-ResNet-v2-B  Softmax  1x1 Convolution  Rectified Linear Units  Dropout  Inception-ResNet-v2-C  Inception-ResNet-v2  squeeze-and-excitation networks  Average Pooling  Global Average Pooling  Max Pooling  Softmax  Kaiming Initialization  Step Decay  SGD with Momentum  Random Horizontal Flip  Random Resized Crop  SENet  Dense Connections  Rectified Linear Units  Sigmoid Activation  Squeeze-and-Excitation Block  Convolution  1x1 Convolution  Residual Connection  Region Proposal Network  Average Pooling  Local Response Normalization  Global Average Pooling  Grouped Convolution  Batch Normalization  Rectified Linear Units  Dropout  Depthwise Convolution  Pointwise Convolution  Dense Connections  Max Pooling  Softmax  AlexNet  RoIPool  Faster R-CNN  Step Decay  Weight Decay  Random Horizontal Flip  Random Resized Crop  ShuffleNet  Channel Shuffle  ShuffleNet Block  Convolution  1x1 Convolution  Average Pooling  Auxiliary Classifier  Convolution  Dense Connections  Max Pooling  Inception-v3 Module  Dropout  Softmax  Inception-v3  Label Smoothing  Gradient Clipping  Exponential Decay  RMSProp  SGD with Momentum  Auxiliary Classifier  Inception-v3 Module  Residual Connection  Dense Connections  Label Smoothing  Inception-v3  Rectified Linear Units  1x1 Convolution  Batch Normalization  Inception-ResNet-v2 Reduction-B  Inception-ResNet-v2-A  Inception-ResNet-v2-B  Inception-ResNet-v2-C  Inception-ResNet-v2  Exponential Decay  RMSProp  Reduction-B  Inception-C  Inception-B  Average Pooling  Dropout  Softmax  Inception-v4  Reduction-A  Inception-A  Bottleneck Residual Block  Global Average Pooling  Residual Block  Kaiming Initialization  Max Pooling  Convolution  Residual Network  Convolution  Average Pooling  Global Average Pooling  1x1 Convolution  Batch Normalization  Rectified Linear Units  Depthwise Convolution  Pointwise Convolution  Dense Connections  Softmax  Exponential Decay  Random Resized Crop  Random Horizontal Flip  Depthwise Separable Convolution  MobileNetV1  Average Pooling  Inception Module  Residual Connection  Pointwise Convolution  Dropout  Weight Decay  Step Decay  RMSProp  SGD with Momentum  1x1 Convolution  Softmax  Dense Connections  Global Average Pooling  Max Pooling  Depthwise Separable Convolution  Rectified Linear Units  Xception  Depthwise Convolution  Convolution  Spatial Pyramid Pooling  Atrous Spatial Pyramid Pooling  Dilated Convolution  DeepLabv3  Depthwise Convolution  Pointwise Convolution  Depthwise Separable Convolution  Rectified Linear Units  Average Pooling  1x1 Convolution  Convolution  Batch Normalization  ReLU6  Dropout  Weight Decay  RMSProp  MobileNetV2  Residual Block  Inverted Residual Block  Residual Connection  Residual Connection  Convolution  Average Pooling  Global Average Pooling  1x1 Convolution  Rectified Linear Units  Dropout  Xavier Initialization  Max Pooling  Softmax  Weight Decay  SGD with Momentum  SqueezeNet  Fire Module  Residual Attention Network  Channel & Spatial attention  Channel Attention Module  Convolution  Spatial Attention Module  1x1 Convolution  Pyramidal Bottleneck Residual Unit  Convolution  Average Pooling  Global Average Pooling  Kaiming Initialization  Batch Normalization  Rectified Linear Units  Random Resized Crop  Weight Decay  Step Decay  Nesterov Accelerated Gradient  Random Horizontal Flip  Zero-padded Shortcut Connection  Pyramidal Residual Unit  PyramidNet,,yes,"SGD, validation set",yes,,yes,,,,
1667,https://github.com/WenmuZhou/PAN.pytorch,WenmuZhou_PAN.pytorch.tar.gz,Efficient and Accurate Arbitrary-Shaped Text Detection with Pixel Aggregation Network,@staticmethod,utils PIL copy sys glob skimage tqdm __future__ pprint torchvision pathlib re zipfile json collections queue torch os trainer matplotlib scipy itertools codecs post_processing pyclipper colorlog sklearn numbers numpy time shutil math data_loader Polygon traceback cv2 random logging subprocess models predict base,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/WenmuZhou_PAN.pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\WenmuZhou_PAN.pytorch.pdf,,"Implementation DetailsWe use the ResNet [15] or VGG16 [44] pre-trained on ImageNet [4] as our backbone. The dimension of the similarity vector is set to 4. All the networks are optimized by using stochastic gradient descent (SGD). The pre-trained model is trained on SynthText for 50K iterations with a fixed learning rate of 1 × 10 −3 . Two training strategies are adopted in other experiments: i) Training from scratch. ii) Fine-tuning on SynthText pre-trained model. When training from scratch, we train PAN with batch size 16 on 4 GPUs for 36K iterations, and the initial learning rate is set to 1 × 10 −3 . Similar to [56], we use the ""poly"" learning rate strategy in which the initial rate is multiplied by (1− iter max iter ) power , and the power is set to 0.9 in all experiments. When fine-tuning on SynthText pre-trained model, the number of iterations is 36K, and the initial learning rate is 1 × 10 −3 . We use a weight decay of 5 × 10 −4 and a Nesterov momentum [45] of 0.99. We adopt the weight initialization introduced by [13].In the training phase, we ignore the blurred text regions labeled as DO NOT CARE in all datasets. The negativepositive ratio of OHEM is set to 3. We apply random scale, random horizontal flip, random rotation and random crop on training images. On ICDAR 2015 and MSRA-TD500, we fit a minimal area rectangle for each predicted text instance. The shrink ratio r of the kernels is set to 0.5 on ICDAR 2015 and 0.7 on other datasets. In the testing phase, the distance threshold d is set to 6. To further demonstrate the robustness of the proposed PAN, we evaluate the model by training on one dataset and testing on other datasets. Based on the annotation level, we divide the datasets into two groups which are word level and text line level datasets. SynthText, ICDAR 2015 and Total-Text are annotated at word level, while CTW1500 and MSRA-TD500 are annotated at text line level. For fair comparisons, we train all model without any external dataset, and the short side of test images in ICDAR 2015, MSRA-TD500, CTW1500 and Total-Text are set to 736, 736, 640, 640 respectively.",yes,SGD,yes,,no,,,,
1674,https://github.com/cagatayyildiz/oderl,cagatayyildiz_oderl.tar.gz,Continuous-Time Model-Based Reinforcement Learning,@abstractmethod @property @staticmethod,"utils envs math torch, pickle, io torch, os torch, copy, numpy torch TorchDiffEqPack copy, math, os, collections numpy ctrl matplotlib abc",cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cagatayyildiz_oderl.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cagatayyildiz_oderl.pdf,,,yes,Adam,yes,"batch size, momentum, learning rate",no,,,,
1663,https://github.com/seijimaekawa/NAGC,seijimaekawa_NAGC.tar.gz,Non-linear Attributed Graph Clustering by Symmetric NMF with PU Learning,,math scipy VU_init evaluate build_graph csv sys glob random sklearn argparse numpy time,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/seijimaekawa_NAGC.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\seijimaekawa_NAGC.pdf,,,no,,no,"extensive experiment details, manual search?",yes,,,,
1741,https://github.com/cansu97/PixelAI,cansu97_PixelAI.tar.gz,End-to-End Pixel-Based Deep Active Inference for Body Perception and Action,@staticmethod,copy actionlib_msgs csv sys pixelAI conv_decoder glob nao_footstep_clipping sensor_msgs rospy actionlib __future__ catkin_pkg gazebo_msgs tf distutils std_msgs threading trajectory_msgs collections motion nao_apps humanoid_nav_msgs torch os start_walk_pose cv_bridge naoqi_driver_py roslib matplotlib naoqi_driver naoqi_bridge_msgs naoqi optparse dynamic_reconfigure pickle argparse geometry_msgs almath numpy process_data time dbus math cv2 random subprocess diagnostic_msgs Conv_decoder_model process_data_sim std_srvs,cs.CV cs.AI cs.LG cs.RO q-bio.NC,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/cansu97_PixelAI.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\cansu97_PixelAI.pdf,,,yes,Adam,yes,"mini-batch, learning rate, exponential decay, kernel size, ratio of drop-out",yes,,,,
1831,https://github.com/midas-research/DECA,midas-research_DECA.tar.gz,Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition,,utils loss_datagen PIL sys glob pprint torchvision dataloader_utils collections torch os scipy pickle imresize argparse data_model_loader numpy time shutil config math random cv2 models,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/midas-research_DECA.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\midas-research_DECA.pdf,,,no,,no,,no,,,,
1697,https://github.com/oaraque/moral-foundations,oaraque_moral-foundations.tar.gz,MoralStrength: Exploiting a Moral Lexicon and Embedding Similarity for Moral Foundations Prediction,@pytest.fixture,math moralstrength pandas spacy gsitk setuptools glob pickle data sklearn os pytest numpy io,cs.CL cs.CY,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/oaraque_moral-foundations.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\oaraque_moral-foundations.pdf,Logistic Regression,"Experimental DesignTo evaluate the potentials of the MoralStrength lexicon, we postulate the problem as a classification task. In particular, we employ the three approaches previously described, namely Moral Freq, Moral Stats, and SIMON, to predict the moral rhetoric in each of the aforementioned datasets (see section 3.4).In our experimental design, we include a basic Bag-of-Words (unigram) model providing a standardized way of obtaining a baseline in the computational linguistics field. We also report as a baseline the frequency counts employing the original MFD. We built a series of logistic regression models; firstly, we assess 6 The original corpus is available at https://github.com/t-davidson/ hate-speech-and-offensive-language/tree/master/data.  the predictive power of the unigrams, Moral Freq, Moral Stats, and SIMON lexicon methods alone. Then, we train logistic regression models concatenating the features extracted by the above approaches 8 . In this way, we examine the effective performance of both engineered and word embedding features in analyzing user-generated text. We also combine the unigrams to the proposed lexicon approaches described above. Hence, for each dataset and moral dimension, we train a series of logistic regression models following a 10-fold cross-validation scheme.We then report the F1-score as the evaluation metric per moral dimension since this is the once employed in the majority of the related studies.To directly compare our proposed framework with the current state-of-theart approach of Lin et al. [40], we replicated their same configuration. Namely, we perform over-sampling on the original dataset to overcome the highly imbalanced nature of the benchmark data (see Section 3.4). After over-sampling on the Hurricane Sandy data, we resulted with an average number of training examples, N = 6, 128, instead of the original dataset size, N = 3, 478 (see Table 2).Since over-sampling implies ""artificial"" data samples, we propose an alternative methodology; more specifically, we performed under-sampling, which also deals with the issue of unbalanced classes, however, in doing so, it randomly excludes data points of the most populated class. In this way, for the Hurricane Sandy we had N = 824 data points (see Table 2. By reporting the score for both methods, we ensure the results are not biased by the technique used to address the class imbalance.For all experiments, we report the performance in terms of F1-score, which is the metric also employed by Lin et al. [40], as well as the average F1-score over all moral dimensions. Moreover, to compare the improvement of the simplest model, which for this study we consider being the Moral Freq model, we employ the Friedman statistical test [47], which yields a ranking of the proposed method ordered by their performance. To obtain further insights on the statistical significance of our obtained results for the baseline model the Bonferroni-Dunn [47] post-hoc statistical test is performed with α = 0.05.",no,,no,,no,,,,
1869,https://github.com/AlexMoreo/pydci,AlexMoreo_pydci.tar.gz,REVISITING DISTRIBUTIONAL CORRESPONDENCE INDEXING: A PYTHON REIMPLEMENTATION AND NEW EXPERIMENTS,@classmethod,pandas classification feature_selection util model tempfile data os scipy itertools urllib pickle sklearn joblib numpy time tarfile math experiments random subprocess zipfile,cs.CL cs.LG stat.ML cs.LG cs.IR stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AlexMoreo_pydci.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AlexMoreo_pydci.pdf,,|,yes,GridSearch,no,,no,,,,
1570,https://github.com/avivga/lord-pytorch,avivga_lord-pytorch.tar.gz,DEMYSTIFYING INTER-CLASS DISENTANGLEMENT,@abstractmethod @staticmethod,imageio tqdm torchvision re model h5py torch os assets itertools dataset pickle argparse numpy dlib abc config shutil cv2,cs.LG cs.CV stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/avivga_lord-pytorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\avivga_lord-pytorch.pdf,,"|LATENT OPTIMIZATION FOR CLASS SUPERVISIONWe make explicit the assumption that inter-class variation is significantly larger than intra-class variation. This allows us to model images as a combination of class and content codes:x i = G θ (e yi , 0, c i )(5)Shared Latent Optimization: We model the class representation as an embedding e y that is shared between all images belonging to the same class {x i ||y i = y}. Instead of using amortized inference (learning a mapping from the image to the class codes using an encoder), we optimize over the class embeddings directly using latent optimization. This has several important benefits: i) As the code is shared exactly between all images belonging to the same class (each having different content), it is impossible to include any content information in the class code. ii) As we learn per-class representations directly rather than using previous techniques as group averaging, each mini-batch can contain images randomly sampled from all classes allowing maximal diversity.We learn the content representation by optimizing over per-sample content embeddings directly using latent optimization and not in an amortized fashion using an image to content encoder. As we show in the experimental section, a model trained with latent optimization preserves a very high degree of disentanglement along the training and is less sensitive to hyperparameter choices.Asymmetric Noise Regularization: Latent optimization over the class embeddings ensures that no content information is present in the class representation. To ensure that class information does not leak into the content representation, we regularize the content code to enforce minimality of information. Previous approaches attempted to minimize content information by setting a bottleneck of a small content code or by matching the content distribution to a prior normal distribution using KLdivergence. Using a small noiseless bottleneck, does not however reduce information significantly.A continuous variable may in fact store an infinite amount of information (although the amount of information the generator may extract is limited by other factors). In our experiments, we found that regularizing with KL-divergence (as done by previous works) led to a partial posterior collapse i.e. nearly all means and standard deviations learned by the encoder defaulted to 0 and 1 respectively, satisfying a perfect standard normal distribution. For a few components, the encoder learned large means and very small standard deviations. The KL-divergence therefore learned behavior similar to a small-size bottleneck. This phenomenon implies that regularizing the distribution of the content codes with KL-divergence may require additional attention and a careful hyperparameter tuning. We present the experimental evidence in the Appendix A.3.In our approach, we regularize the content code with an additive Gaussian noise of a fixed variance, and an activation decay penalty. In contrast to a variational auto-encoder, we do not learn the variance, but rather keep it fixed. This prevents the possibility of the variance decreasing to a small value, ensuring that noise is applied equally on all components. Our objective function becomes: L = n i=1 G θ (e yi , 0, c i + z i ) − x i + λ c i 2 z i ∼ N (0, σ 2 I)(6)The first loss terms uses a VGG perceptual loss as implemented by Hoshen & Malik (2019). Unless stated otherwise, we optimize over class and content codes (e yi and c i ) directly using latent optimization. All latent codes and the parameters of the generator are learned end-to-end using stochastic gradient descent:{e * 1 , .., e * k , c * 1 .., c * n , θ * } = arg min e,c,θ L (7) 3.2 AMORTIZATION FOR ONE-SHOT INFERENCELatent optimization, which is used effectively for training, requires optimization for every image (including at inference time). In the training set, a class embedding is shared across multiple images, which prevents the embedding from including content information. However, at inference time, a single image from an unknown class is observed. Optimizing over the latent codes for a single image leads to overfitting which results in entangled representations. Moreover, it requires iterative test-time inference since it does not perform amortized inference.To this end, we introduce a second stage which learns class and content encoders that directly infer class e yi and content c i representations from a single image x i . The second stage effectively amortizes the results of the first stage and generalizes well to unseen classes and images. We train encoders E y : X − → Y and E c : X − → C, which take as input an image x i and output its class and content embeddings that were learned by our method in the first stage. We also use a reconstruction loss, to ensure the representations learned in the second stage must reconstruct the original image x i . The optimization objective is presented in Eq. 8. The optimization is over the parameters of encoders E y and E c (which are randomly initialized) and the parameters of the generator G (which are initialized from the first stage). Note that the given e yi and c i are the representations we have learned during the previous stage. tions and regenerating them as follows:L E = n i=1 G θ (E y (x i ), 0, E c (x i )) − x i + α 1 • E y (x i ) − e yi 2 + α 2 • E c (x i ) − c i 2(x1← −2 = G θ (E y ( x1 ), 0, E c ( x2 ))(9) Quantitative Experiments:Content transfer experiments: To test the quality of disentanglement, we measure the quality of content transfer in terms of perceptual similarity by LPIPS (Zhang et al., 2018). We use the content labels available in the Cars3D and SmallNorb datasets as ground truth for content transfer. For given test images x i and x j , we measure the similarity between x 1← −2 = G θ (E y (x i ), 0, E c (x j )) and another image from the same class of x i matching the same content of x j . For CelebA, given Classification experiments: To assess the disentanglement of our learned representations, we follow the protocol in Harsh Jha et al. ( 2018) and train a classifier to classify class labels from content codes and vice versa. Results can be seen in Tab. 2. On all datasets, our model achieves near perfect disentanglement as the classifier could barely guess class labels from content codes by a random chance (same for the other direction). All the baselines fail to zero out the mutual information between the two representations. To conclude, our method is able to learn the most disentangled features without introducing adversarial constraints. For CelebA, in order to test if the content of an image is predictable from the class code we train a linear regression model to regress the position of 68 facial landmarks. It can be seen that the linear regression results in the highest error on our class representations, indicating the highest degree of disentanglement of our method. It should be noted that all methods could classify class labels from class codes and content labels from content codes very accurately (not shown). ABLATION ANALYSISWe perform a careful ablation analysis on the components on our method, a summary of this study is presented in Tab.   Figure 6: Examples of translations between anime and faces from CelebA using our method.high degree of disentanglement. We hypothesize that achieving similar degree of disentanglement by amortization requires a more sophisticated objective and a more careful hyperparameter tuning. An extended study is presented in Appendix A.4. It should be noted that latent optimization requires more iterations than optimizing an amortized encoder and leads to a slower convergence (the number of iterations increased by ×2 in our experiments). In both the amortized and semi-amortized models, we find that the KL-divergence fails to regularize the information leakage from the class representation into the content representations. A visualization of the partial posterior collapse can be found in the Appendix A.3. We finally demonstrate the importance of our second stage by assessing the performance after the first stage only. This can be done by optimizing over the latent codes of a new test image while keeping the rest of the model frozen. As can be seen, this approach suffers from low performance in all metrics. The effect of the asymmetric noise regularization can be observed from the inferior performance of training our model without regularization. A qualitative visualization of this analysis is provided in Appendix A.6. A.3 KL-DIVERGENCE POSTERIOR COLLAPSEWe provide evidence for the partial posterior collapse we experienced when regularizing the content codes with KL-divergence. Fig. 8 shows the mean and standard deviations of each of the 128 components of the content code (averaged over all samples in the dataset) in a model trained on SmallNorb. It can be seen that 126 out of 128 components of the content code collapse to match a perfect standard normal distribution, while in the remaining 2 components the standard deviation is reduced dramatically along with a substantial increase in the mean. This phenomenon implies that regularizing the distribution of the content codes with KL-divergence may require additional attention and a careful hyperparameter tuning. We find in our experiments that the asymmetric regularization introduced in our method results in better disentanglement.|",yes,"Adam, SGD",yes,,yes,,,,
1535,https://github.com/megagonlabs/opiniondigest,megagonlabs_opiniondigest.tar.gz,OPINIONDIGEST: A Simple Framework for Opinion Summarization,@staticmethod,utils pandas csv sys tqdm sumeval gensim re collections json nltk torch os beam_search torchtext typing argparse dill numpy commentjson sentencepiece time math random models,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/megagonlabs_opiniondigest.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\megagonlabs_opiniondigest.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,"Summary GenerationOur goal is to generate a natural language summary which articulates S e , the set of selected opinions.To achieve this, we need a natural language generation (NLG) model which takes a set of opinion phrases as input and produces a fluent, review-like summary as output. Because we cannot rely on gold-standard summaries for training, we train an NLG model that encodes the extracted opinion phrases of a single review and then attempts to reconstruct the review's full text. Then, the trained model can be used to generate summaries. The {T (O r ), r} pairs are used to train a Transformer model (Vaswani et al., 2017)  Summarization: At summarization time, we use the textualization of the selected opinions, T (S e ), as input to the trained Transformer, which generates a natural language summary s e as output (Figure 1, Step 3b). We order the selected opinions by frequency (i.e., their respective cluster's size), but any desired ordering may be used.",yes,SGD,yes,,yes,,,,
1964,https://github.com/hcorinna/gradual-compatibility,hcorinna_gradual-compatibility.tar.gz,Gradual (In)Compatibility of Fairness Criteria,,utils scipy pandas evaluate_regularizers warnings pickle data sklearn lr pyitlib numpy it matplotlib config,cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/hcorinna_gradual-compatibility.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\hcorinna_gradual-compatibility.pdf,,"IntroductionSince the publication of Angwin et al. (2016), statistical fairness measures such as independence (also known as statistical parity and demographic parity), sufficiency and separation have been much discussed. There has been interest in the relation between these fairness measures (see, e.g., Kleinberg, Mullainathan, and Raghavan (2016); Chouldechova (2017); Mitchell et al. (2021); Heidari et al. (2019); Leben (2020); Baer, Gilbert, and Wells (2019)), and also in the relation between fairness measures and accuracy (see, e.g., Corbett-Davies et al. (2017); Menon and Williamson (2018); Cooper and Abrams (2021)).It is well known that strict satisfaction of some combinations of these fairness measures is not possible; impossibility theorems establish that satisfaction of one fairness measure may prevent satisfaction of another under certain conditions (Kleinberg, Mullainathan, and Raghavan 2016;Chouldechova 2017). This is a fundamental limitation if one wishes to satisfy different fairness measures at the same time. However, the situation may be different if we only require a partial, or gradual, fulfillment of fairness measures. * These authors contributed equally.In the present paper, we will show that it is possible to simultaneously improve the degree to which some of these fairness measures are satisfied.To show this, we first define a notion of partial satisfaction of fairness measures via information theory. The fairness measures have an exact information-theoretic counterpart, but the information-theoretic formulation also generalizes the fairness measures in providing degrees of fairness. An information-theoretic decomposition of separation and sufficiency reveals quantitative relations between the three measures that have not been explored so far: The expression of separation can be viewed as interpolating between sufficiency and independence. These theoretical relations suggest that enforcing one of the measures may quantitatively improve others.We then put these theoretical considerations to work using so-called fairness regularization. The information-theoretic formulations of independence, separation and sufficiency serve as regularization terms during training to create fairness-regularized predictors for three standard datasets. We first examine whether fairness regularization directly increases the respective fairness measure after training, showing that this works reasonably well. We then explore whether enforcing one fairness measure indirectly increases the degree to which other fairness measures hold, in keeping with our theoretical hypotheses. We show that enforcing independence indirectly increases separation, and vice versa. We also discover that enforcing sufficiency will not increase separation, and vice versa. We offer a diagnosis of why this is the case on the basis of our theoretical findings.It is also known that increasing fairness may depreciate accuracy (Corbett-Davies et al. 2017), and that sufficiency and separation may be better aligned with accuracy than independence -this is true at least for perfect accuracy (Hardt, Price, and Srebro 2016). For predictors that are not perfectly accurate, it has been shown that increases of accuracy and fairness need not go hand in hand (Räz 2021). It would be desirable to better understand how accuracy and fairness are related when neither one is perfectly satisfied. The information-theoretic decompositions of separation and sufficiency suggest that accuracy is a natural ingredient of these fairness measures.The paper is structured as follows: In Section 2, we introduce the information-theoretic formulations of statistical fairness measures and decompose them in order to examine how the fairness measures are related. We also formulate empirical hypotheses about the effect of using these information-theoretic formulations in fairness regularization. Section 3 situates our contribution in the broader recent discussion of fair-ML while Section 4 discusses the relation of our contribution to similar works. In Section 5, we experimentally test our previously stated hypotheses. For this, we provide descriptions of the experimental setup and discuss our empirical results on direct and indirect fairness regularization. In Section 6 we summarize our findings and highlight open questions as well as avenues for future research. Description of ExperimentsWe use a logistic regression model that is optimized by scipy's (Virtanen et al. 2020) minimize function with L-BFGS-B (Byrd et al. 1995;Zhu et al. 1997) as the solver.The loss function takes the following form:L = l f it + λ • l 2 + µ • l f air .(3)The algorithm optimizes the cross-entropy loss (dubbed l f it ) with l 2 regularization and fairness regularization terms (dubbed l f air ). The parameter λ is chosen using 5-fold cross validation, for each dataset and before fairness regularization. The value of λ was left unchanged during fairness regularization. We used the following values for λ: 0.00001 (""Adult income""), 0.001 (""German credit""), and 0.001 (""ProPublica recidivism""). 5 The fairness regularization terms l f air take values in the range {IND, SEP, SUF, BAL, -ACC}. We regularize (and thus try to minimize) negative accuracy, which is the same as maximizing accuracy because it is desirable to obtain an accurate predictor, and because negative accuracy features in the fairness decompositions. For each fairness regularization term, we choose values for the fairness parameter µ in the range [0, 100]. 6 The model is trained and evaluated separately for each µ and l f air using 5-fold cross validation. In the evaluation step, we evaluate the three normalized fairness measures independence, sufficiency, and separation, as well as the negative accuracy, and the non-normalized balance term. 7 Normalization was only used during evaluation. More details on the experimental setup can be found in Appendix A.2.",yes,,no,,no,,,,
1923,https://github.com/roholazandie/EmpTransfo,roholazandie_EmpTransfo.tar.gz,EmpTransfo: A Multi-head Transformer Architecture for Creating Empathetic Dialog Systems,@wraps(func) @s3_request @lru_cache() @abc.abstractmethod @property @classmethod,utils requests copy sys glob regex tqdm __future__ pprint pathlib re tensorflow json collections botocore fnmatch tempfile nltk torch os ftfy apex functools itertools hashlib spacy ignite urllib pickle boto3 argparse urlparse numpy config shutil abc math tarfile unicodedata pytorch_pretrained_bert logging random cPickle io,cs.CL cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/roholazandie_EmpTransfo.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\roholazandie_EmpTransfo.pdf,Absolute Position Encodings  Position-Wise Feed-Forward Layer  Residual Connection  Byte Pair Encoding  Dense Connections  Label Smoothing  Rectified Linear Units  Adam  Softmax  Dropout  Multi-Head Attention  Layer Normalization  Scaled Dot-Product Attention  Transformer,,yes,Adam,yes,"loss function coefficients ci, learning rate, epochs, decay",yes,,,,
1912,https://github.com/xk-huang/yet-another-vectornet,xk-huang_yet-another-vectornet.tar.gz,VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized Representation,@property,utils pandas sys pdb tqdm pprint re modeling torch os matplotlib torch_geometric argoverse dataset typing pickle numpy time shutil,cs.CV cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xk-huang_yet-another-vectornet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xk-huang_yet-another-vectornet.pdf,,,yes,Adam,yes,"learning rate, epochs",no,,,,
1788,https://github.com/violet-zct/group-conditional-DRO,violet-zct_group-conditional-DRO.tar.gz,Examining and Combating Spurious Features under Distribution Shift,"@register_optimizer('adagrad') @register_optimizer('sgd') @register_model_architecture(""transformer"", ""transformer"") @register_optimizer('adadelta') @register_model_architecture('dummy_model', 'dummy_model') @register_model('dummy_model') @register_lr_scheduler('reduce_lr_on_plateau') @lru_cache() @register_bpe('byte_bpe') @register_model_architecture('hf_gpt2', 'hf_gpt2') @register_model_architecture(""wav2vec"", ""wav2vec"") @register_model_architecture('fconv', 'fconv_wmt_en_de') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_big') @register_lr_scheduler('cosine') @register_model(""transformer_from_pretrained_xlm"") @register_model_architecture('roberta', 'roberta_base') @register_model('xlmr') @register_model_architecture('roberta', 'roberta') @register_optimizer('adamax') @register_criterion('masked_lm') @register_model_architecture('lightconv', 'lightconv_wmt_en_fr_big') @register_model('multilingual_transformer') @register_model_architecture('lightconv_lm', 'lightconv_lm') @register_optimizer('apollo') @register_bpe('hf_byte_bpe') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron_11b') @register_model('camembert') @register_model_architecture('masked_lm', 'masked_lm') @register_task('sentence_ranking') @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta_large') @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_fr_big"") @register_model_architecture('transformer_lm', 'transformer_lm') @include_dirs.setter @register_tokenizer('space') @register_model_architecture('lightconv_lm', 'lightconv_lm_gbw') @register_task('multilingual_masked_lm') @torch.jit.unused @register_model('model_parallel_transformer_lm') @register_optimizer('lamb') @torch.jit.export @register_model('model_parallel_roberta') @register_task('denoising') @register_model('transformer_lm') @register_model_architecture('fconv', 'fconv_wmt_en_fr') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_medium') @register_model_architecture(""transformer_align"", ""transformer_align"") @register_bpe('bert') @register_model_architecture('masked_lm', 'bert_large') @register_model(""iterative_nonautoregressive_transformer"") @register_task(""language_modeling"") @register_lr_scheduler('inverse_sqrt') @register_model_architecture('lstm', 'lstm') @register_criterion('cross_entropy_group_dro') @metrics.aggregate(""train"") @register_model_architecture(""transformer_align"", ""transformer_wmt_en_de_big_align"") @register_task('multilingual_denoising') @register_model('lstm_lm') @register_model(""wav2vec_seq2seq"") @register_tokenizer('moses') @register_model_architecture('transformer_lm', 'transformer_lm_gpt2_small') @register_criterion('sentence_prediction') @register_bpe('subword_nmt') @register_model_architecture('model_parallel_transformer_lm', 'transformer_lm_megatron') @register_model_architecture('fconv_self_att', 'fconv_self_att') @register_model_architecture('transformer_lm', 'transformer_lm_big') @metrics.aggregate(""valid"") @register_model_architecture('lstm', 'lstm_wiseman_iwslt_de_en') @register_model(""wav2vec2"") @register_model(""wav2vec_ctc"") @register_model('model_parallel_transformer') @register_criterion('label_smoothed_cross_entropy_with_alignment') @s3_request @register_model_architecture('bart', 'bart_base') @ensemble_encoder @register_task('cross_lingual_lm') @register_model('lightconv') @register_criterion(""ctc"") @register_model_architecture(""levenshtein_transformer"", ""levenshtein_transformer"") @register_bpe('fastbpe') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big_t2t"") @register_bpe('gpt2') @register_task('multilingual_translation') @register_model_architecture('lightconv', 'lightconv_wmt_zh_en_big') @register_lr_scheduler('polynomial_decay') @unittest.skipIf(torch.cuda.device_count() < 2, ""test requires 2 GPUs"") @register_model_architecture('roberta', 'roberta_large') @register_model('fconv_lm') @unittest.skipIf(not torch.cuda.is_available(), ""test requires a GPU"") @register_model_architecture('hf_gpt2', 'hf_gpt2_large') @register_model_architecture('lstm_lm', 'lstm_lm') @register_bpe('characters') @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer"") @register_model_architecture(""transformer"", ""transformer_vaswani_wmt_en_de_big"") @register_model_architecture(""transformer"", ""transformer_iwslt_de_en"") @register_model(""insertion_transformer"") @register_model_architecture('bart', 'bart_large') @register_model_architecture('transformer_lm', 'transformer_lm_gpt') @register_optimizer('adafactor') @register_model('hf_gpt2') @register_model(""transformer_align"") @register_model_architecture(""cmlm_transformer"", ""cmlm_transformer_wmt_en_de"") @register_model_architecture('fconv_lm', 'fconv_lm') @register_model_architecture(""insertion_transformer"", ""insertion_transformer"") @register_criterion('sentence_ranking') @register_criterion('wav2vec') @register_model_architecture('fconv', 'fconv_wmt_en_ro') @register_optimizer('nag') @property @register_model('roberta') @register_criterion('adaptive_loss') @ensemble_decoder @register_model_architecture('roberta', 'xlm') @register_task('translation_from_pretrained_bart') @register_criterion('cross_entropy') @unittest.skipIf(not torch.cuda.is_available(), 'test requires a GPU') @register_bpe('bytes') @register_model_architecture('bart', 'mbart_base') @register_model_architecture('transformer_lm', 'transformer_lm_wiki103') @register_model_architecture('masked_lm', 'xlm_base') @classmethod @register_model_architecture('masked_lm', 'bert_base') @register_model_architecture('multilingual_transformer', 'multilingual_transformer') @register_task('dummy_masked_lm') @register_task('translation_multi_simple_epoch') @register_task(""audio_pretraining"") @register_model_architecture(""nacrf_transformer"", ""nacrf_transformer"") @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_gbw') @wraps(func) @register_criterion('legacy_masked_lm_loss') @register_model('masked_lm') @register_model_architecture(""wav2vec_ctc"", ""wav2vec_ctc"") @register_bpe('sentencepiece') @register_model_architecture(""transformer"", ""transformer_wmt_en_de"") @contextlib.contextmanager @register_model_architecture('bart', 'mbart_large') @register_model_architecture('fconv', 'fconv') @register_task('legacy_masked_lm') @register_task('sentence_prediction') @register_task('translation_lev') @register_model_architecture('transformer_lm', 'transformer_lm_gbw') @register_model_architecture('lightconv', 'lightconv') @register_model_architecture('lightconv', 'lightconv_wmt_en_de_big') @register_model('lightconv_lm') @register_model('lstm') @register_task('semisupervised_translation') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_gbw') @torch.jit.script @register_task('masked_lm') @register_model_architecture('hf_gpt2', 'hf_gpt2_xl') @with_incremental_state @register_criterion('vocab_parallel_cross_entropy') @register_model_architecture('multilingual_transformer', 'multilingual_transformer_iwslt_de_en') @register_optimizer('adam') @register_lr_scheduler('triangular') @register_scoring(""wer"") @register_tokenizer('nltk') @register_task('dummy_mt') @register_model(""nacrf_transformer"") @register_lr_scheduler('tri_stage') @lru_cache(maxsize=8) @register_model(""nonautoregressive_transformer"") @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta') @unittest.skipIf( @torch.no_grad() @register_model_architecture('lightconv', 'lightconv_iwslt_de_en') @register_model_architecture('lightconv', 'lightconv_wmt_en_de') @register_criterion('composite_loss') @register_model('fconv_self_att') @register_model_architecture(""wav2vec2"", ""wav2vec2"") @register_model(""wav2vec"") @staticmethod @register_model_architecture('fconv_lm', 'fconv_lm_dauphin_wikitext103') @register_model(""transformer"") @register_model_architecture(""wav2vec_seq2seq"", ""wav2vec_seq2seq"") @register_optimizer('radam') @register_model_architecture('bart', 'mbart_base_wmt20') @register_model(""levenshtein_transformer"") @register_model('bart') @register_model('fconv') @register_model_architecture(""transformer"", ""transformer_wmt_en_de_big"") @register_model_architecture( @register_task(""translation_from_pretrained_xlm"") @contextmanager @register_model_architecture('fconv_self_att', 'fconv_self_att_wp') @register_lr_scheduler('step') @register_model_architecture('lstm', 'lstm_luong_wmt_en_de') @register_criterion(""nat_loss"") @register_model_architecture('model_parallel_roberta', 'model_parallel_roberta_base') @register_lr_scheduler('milestone') @register_lr_scheduler('fixed') @register_model_architecture('transformer_lm', 'transformer_lm_baevski_wiki103') @register_task('dummy_lm') @register_scoring(""bleu"") @register_criterion('label_smoothed_cross_entropy') @register_model(""cmlm_transformer"") @register_scoring(""sacrebleu"") @register_model_architecture('hf_gpt2', 'hf_gpt2_medium') @register_task('translation') @register_model_architecture('fconv', 'fconv_iwslt_de_en')","copy loss warnings multiprocessing types ctypes jsonlines torchvision threading re amp_C torch importlib spacy fairseq_cli xentropy_cuda pickle boto3 numbers tensorboardX numpy sentencepiece tarfile process traceback random inspect requests utils yaml pdb faiss __future__ examples pathlib unittest queue botocore tests transformers fnmatch nltk os socket string gzip functools hashlib gzip, csv time shutil gossip operator subprocess gc pandas sys regex tqdm tokenizers atexit datetime cython enum tempfile lightconv_cuda matplotlib struct fastBPE scipy itertools typing subword_nmt urlparse setuptools sacrebleu io fvcore scripts contextlib sacremoses fairseq fileinput soundfile uuid collections json pyarrow data apex bisect urllib palaas argparse editdistance math logging torch_xla dynamicconv_cuda",cs.LG cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/violet-zct_group-conditional-DRO.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\violet-zct_group-conditional-DRO.pdf,,,yes,"Adam, ablation studies",yes,"weighted moving average EMA, batch size, epochs, learning rate",yes,,,,
1844,https://github.com/ModelBunker/MORAN-PyTorch,ModelBunker_MORAN-PyTorch.tar.gz,MORAN: A Multi-Object Rectified Attention Network for Scene Text Recognition,,PIL sys collections colour lmdb tools cv2 random torch argparse os __future__ numpy models matplotlib time torchvision six,cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ModelBunker_MORAN-PyTorch.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ModelBunker_MORAN-PyTorch.pdf,,,yes,AdaDelta,yes,"learning rate, batch size, iterations",yes,,,,
1549,https://github.com/AsahiLiu/PointDetectron,AsahiLiu_PointDetectron.tar.gz,Group Contextual Encoding for 3D Point Clouds,@staticmethod,"PIL csv sys multiprocessing glob nms model_util_scannet pdb sunrgbd_detection_dataset __future__ eval_det plyfile scannet_detection_dataset backbone_module model_util_sunrgbd encoding tensorflow backbone_module_SA2_denseaspp3_6 enc_layer datetime json torch os dump_helper matplotlib StringIO metric_util importlib scannet_utils tf_visualizer voting_module scipy pointnet2_modules IPython backbone_module_enc_complex_FP2_K8_G12_C3 gzip builtins loss_helper_boxnet typing pc_util pickle argparse numpy os, sys, argparse time sunrgbd_utils load_scannet_data pointnet2 ap_helper math tf_logger backbone_module_enc_FP2_K8_G12_C3 loss_helper setuptools box_util proposal_module cv2 nn_distance pytorch_utils trimesh pointnet2_utils backbone_module_SA2_denseaspp3_6_12 inspect __builtin__ io",cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/AsahiLiu_PointDetectron.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\AsahiLiu_PointDetectron.pdf,PointNet,,no,,no,,no,,,,
1936,https://github.com/amoliu/CayleyNet,amoliu_CayleyNet.tar.gz,CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters,,"gensim scipy sklearn sklearn, sklearn numpy matplotlib time, re",cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/amoliu_CayleyNet.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\amoliu_CayleyNet.pdf,CayleyNet,,no,,no,,no,,,,
1890,https://github.com/tohtsky/myFM,tohtsky_myFM.tar.gz,On the Difficulty of Evaluating Baselines A Study on Recommender Systems,@abstractmethod @property @abstractclassmethod @classmethod @abstractproperty,requests pandas warnings sys tqdm zipfile datetime json myfm collections unittest os scipy urllib pickle typing sklearn argparse numbers numpy abc setuptools io,cs.IR cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/tohtsky_myFM.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\tohtsky_myFM.pdf,,"Tuned HyperparametersOne of our central arguments is that it is not easy to run a machine learning method properly. In most research papers, it is common practice to search over the hyperparameter space (e.g., learning rates, embedding dimension, regularization) and report the results for the ""best"" setting. However, Section 2 indicates that this still does not solve the problem, and reported results can vary substantially from a proper setup. We speculate that hyperparameter search spaces are often incomplete and do not replace experience with a method. For example, interpreting and acting on the results of different hyperparameter settings is non-trivial, e.g., should the boundary be extended, or refined? What is the right search grid? Can we search hyperparameters on a small model and transfer the results to a larger one? All these questions make it hard for setting up an unknown ""black-box"".A second source are knobs that are not even considered during hyperparameter search. For example, a method might require to recenter the data before running it, or that the training data is shuffled, or to stop training early, or to use a certain initialization. Such knobs might be trivial and not even worth reporting for someone with experience in a method, but will make it almost impossible for others to set comparisons up properly. This becomes a problem when the method is run by a non-expert on a different dataset or experimental setup.",yes,"ALS, SGD",yes,"learning rate, embedding dimension, regularization",yes,,,,
1878,https://github.com/kleag/labforsims2-corpus,kleag_labforsims2-corpus.tar.gz,A French Medical Conversations Corpus Annotated for a Virtual Patient Dialogue System,,"spacy spacy, string sys collections sklearn",,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/kleag_labforsims2-corpus.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\kleag_labforsims2-corpus.pdf,,,yes,AdamDelta,yes,"kernel size, embedding dimension, dropout, learning rate",yes,,,,
1631,https://github.com/xamyzhao/brainstorm,xamyzhao_brainstorm.tar.gz,U-Net: Convolutional Networks for Biomedical Image Segmentation,,main PIL sys neuron pynd glob keras tensorflow re json src os scipy functools medipy ext textwrap argparse numpy time math cv2 logging,cs.CV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/xamyzhao_brainstorm.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\xamyzhao_brainstorm.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,,yes,SGD,no,,no,,,,
1961,https://github.com/fepegar/resseg,fepegar_resseg.tar.gz,Simulation of Brain Resection for Cavity Segmentation Using Self-Supervised and Semi-Supervised Learning,"@click.argument('output-path', type=OUTPUT_FILE_TYPE) @click.option( @click.argument('input-path', type=INPUT_FILE_TYPE) @click.argument('input-path', type=click.Path(exists=True, dir_okay=False)) @property @click.command() @click.argument('output-dir', type=click.Path(dir_okay=True)) @click.argument(",ants unet pandas warnings sys nibabel tqdm pathlib click model unittest torch os matplotlib urllib resseg torchio numpy setuptools,eess.IV cs.CV,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/fepegar_resseg.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\fepegar_resseg.pdf,,,yes,Adam,yes,"learning rate, weight decay, batch size, epochs",no,,,,
1768,https://github.com/ram-g-athreya/RNN-Question-Answering,ram-g-athreya_RNN-Question-Answering.tar.gz,TEMPLATE-BASED QUESTION ANSWERING USING RECURSIVE NEURAL NETWORKS,,main copy pandas csv glob tqdm treelstm __future__ json nltk torch os matplotlib seaborn sklearn argparse numpy config math random logging fastText,cs.CL cs.DB cs.LG,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ram-g-athreya_RNN-Question-Answering.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ram-g-athreya_RNN-Question-Answering.pdf,,,yes,Adam,yes,"batch size, epochs, learning rate, momentum, weight decaybatch",yes,,,,
1757,https://github.com/craigmacartney/Wave-U-Net-For-Speech-Enhancement,craigmacartney_Wave-U-Net-For-Speech-Enhancement.tar.gz,WAVE-U-NET: A MULTI-SCALE NEURAL NETWORK FOR END-TO-END AUDIO SOURCE SEPARATION,@config_ingredient.capture @config_ingredient.named_config @config_ingredient.config @ex.automain @staticmethod @classmethod @ex.config,"multiprocessing os, re, subprocess glob Validation Config tensorflow soundfile multistreamworkers os fractions scipy functools OutputLayer Datasets Estimate_Sources Input scikits lxml Sample numpy multistreamcache Models Metadata librosa subprocess sacred Utils cPickle",cs.SD eess.AS stat.ML cs.SD cs.LG cs.NE eess.AS eess.SP,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/craigmacartney_Wave-U-Net-For-Speech-Enhancement.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\craigmacartney_Wave-U-Net-For-Speech-Enhancement.pdf,Concatenated Skip Connection  Rectified Linear Units  Max Pooling  Convolution  U-Net,"Training procedureDuring training, audio excerpts are sampled randomly and inputs padded accordingly for models with input context. As loss, we use the mean squared error (MSE) over all source output samples in a batch. We use the ADAM optimizer with learning rate 0.0001, decay rates β 1 = 0.9 and β 2 = 0.999 and a batch size of 16. We define 2000 iterations as one epoch, and perform early stopping after 20 epochs of no improvement on the validation set, measured by the MSE loss. Afterwards, the last model is fine-tuned further, with the batch size doubled and the learning rate lowered to 0.00001, again until 20 epochs without improvement in validation loss. Finally, the model with the best validation loss is selected. To determine the impact of the model improvements described in Section 3.2, we train a baseline model M1 as described in Section 3.1 and models M2 to M5 which add the difference output layer from Section 3.2.1 (M2), the input context and resampling from Section 3.2.2 (M3), stereo channels from Section 3.2.3 (M4), and learned upsampling from Section 3.2.4 (M5), and also contain all features of the respectively previous model. We apply the best model of the above (M4) to multi-instrument separation (M6). Models with input context (M3 to M6) have L m = 147443 input and L s = 16389 output samples.",yes,Adam,yes,"batch size, epochs, learning rate, momentum, weight decay",no,,,,
1517,https://github.com/dlej/adaptive-dropout,dlej_adaptive-dropout.tar.gz,The Flip Side of the Reweighted Coin: Duality of Adaptive Dropout and Regularization,@abstractmethod @torch.no_grad(),numpy scipy torch abc,cs.LG stat.ML,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/dlej_adaptive-dropout.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\dlej_adaptive-dropout.pdf,Adaptive Dropout  Variational Dropout  Dropout,,yes,"Adam, found empirically",yes,,yes,,,,
1592,https://github.com/jkkummerfeld/irc-disentanglement,jkkummerfeld_irc-disentanglement.tar.gz,A Large-Scale Corpus for Conversation Disentanglement,,math unicodedata re sys collections traceback dynet dynet_config random logging reserved_words argparse sklearn string ortools __future__ numpy time,cs.CL,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/jkkummerfeld_irc-disentanglement.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\jkkummerfeld_irc-disentanglement.pdf,,,no,,no,,yes,,,,
1758,https://github.com/toru34/rush_emnlp_2015,toru34_rush_emnlp_2015.tar.gz,A Neural Attention Model for Abstractive Sentence Summarization,@staticmethod,utils math _dynet collections pickle layers tqdm argparse os sklearn numpy time,cs.CL cs.AI,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/toru34_rush_emnlp_2015.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\toru34_rush_emnlp_2015.pdf,,"Data SetThe standard sentence summarization evaluation set is associated with the DUC-2003 and DUC-2004 shared tasks (Over et al., 2007). The data for this task consists of 500 news articles from the New York Times and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html.For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004). To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths. The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report.In addition to the standard DUC-2014 evalu-ation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length.For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003;Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014). Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades.For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs. We therefore prune training based on the following heuristic filters:(1) Are there no non-stop-words in common? (2) Does the title contain a byline or other extraneous editing marks? (3) Does the title have a question mark or colon? After applying these filters, the training set consists of roughly J = 4 million title-article pairs. We apply a minimal preprocessing step using PTB tokenization, lower-casing, replacing all digit characters with #, and replacing of word types seen less than 5 times with UNK. We also remove all articles from the time-period of the DUC evaluation. release.The complete input training vocabulary consists of 119 million word tokens and 110K unique word types with an average sentence size of 31.3 words. The headline vocabulary consists of 31 million tokens and 69K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries). On average there are 4.6 overlapping word types between the headline and the input; although only 2.6 in the first 75-characters of the input. BaselinesDue to the variety of approaches to the sentence summarization problem, we report a broad set of headline-generation baselines.From the DUC-2004 task we include the PRE-FIX baseline that simply returns the first 75characters of the input as the headline. We also report the winning system on this shared task, TOPIARY (Zajic et al., 2004). TOPIARY merges a compression system using linguisticallymotivated transformations of the input (Dorr et al., 2003) with an unsupervised topic detection (UTD) algorithm that appends key phrases from the full article onto the compressed output. Woodsend et al. (2010) (described above) also report results on the DUC dataset.The DUC task also includes a set of manual summaries performed by 8 human summarizers each summarizing half of the test data sentences (yielding 4 references per sentence). We report the average inter-annotater agreement score as REF-ERENCE. For reference, the best human evaluator scores 31.7 ROUGE-1.We also include several baselines that have access to the same training data as our system. The first is a sentence compression baseline COM-PRESS (Clarke and Lapata, 2008). This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver.To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)).Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, MOSES+ (Koehn et al., 2007). To improve the baseline for this task, we augment the phrase table with ""deletion"" rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit. We also explicitly tune the model using MERT to target the 75byte capped ROUGE score as opposed to standard BLEU-based tuning. Unfortunately, one remaining issue is that it is non-trivial to modify the translation decoder to produce fixed-length outputs, so we tune the system to produce roughly the expected length. Table 1: Experimental results on the main summary tasks on various ROUGE metrics . Baseline models are described in detail in Section 7.2. We report the percentage of tokens in the summary that also appear in the input for Gigaword as Ext %.",yes,SGD,yes,"mini-batch size, epochs, learning rate, weight decay",no,,,,
1551,https://github.com/luithw/decolle-quantization,luithw_decolle-quantization.tar.gz,HESSIAN AWARE QUANTIZATION OF SPIKING NEURAL NETWORKS,@parse_args('v') @staticmethod @property,pyhessian copy yaml warnings tqdm onnx qtorch torchvision datetime collections usps_loader torch decolle os socket matplotlib importlib itertools codecs argparse tensorboardX numpy shutil setuptools onnxruntime mpl_toolkits,cs.NE,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/luithw_decolle-quantization.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\luithw_decolle-quantization.pdf,,,yes,,yes,,no,,,,
1579,https://github.com/ZhaofengWu/MEDIQA_WTMED,ZhaofengWu_MEDIQA_WTMED.tar.gz,WTMED at MEDIQA 2019: A Hybrid Approach to Biomedical Natural Language Inference,@classmethod @staticmethod,copy data_utils csv warnings sys multiprocessing pdb tqdm pprint re scispacy model json collections datetime mt_dnn nltk torch os concurrent string module rouge scipy functools spacy pickle sklearn argparse numpy time shutil math unicodedata traceback random logging pytorch_pretrained_bert subprocess,,file:///mnt/ceph/storage/data-tmp/2021/liju1602/paperswithcode_crawl_pdf/ZhaofengWu_MEDIQA_WTMED.pdf,\\nfs.ceph.dw.webis.de\cephfsstorage\data-tmp\2021\liju1602\paperswithcode_crawl_pdf\ZhaofengWu_MEDIQA_WTMED.pdf,,,yes,AdaMax,yes,,no,,,,
