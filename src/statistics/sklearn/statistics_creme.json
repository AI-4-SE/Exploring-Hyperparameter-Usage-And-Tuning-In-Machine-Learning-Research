{
    "benchmarks/logistic_regression/run.py": {
        "Pipeline_97": {
            "steps": "StandardScaler()",
            "memory": "LogisticRegression(optimizer=SGD(LEARNING_RATE))"
        },
        "Pipeline_101": {
            "steps": "StandardScaler()",
            "memory": "SKL2RiverClassifier(SGDClassifier(loss='log', learning_rate='constant', eta0=LEARNING_RATE, penalty='none'), classes=[False, True])"
        },
        "LogisticRegression_99": {
            "optimizer": "SGD(LEARNING_RATE)"
        },
        "SGDClassifier_104": {
            "loss": "log",
            "learning_rate": "constant",
            "eta0": "LEARNING_RATE",
            "penalty": "none"
        }
    },
    "river/anomaly/test_svm.py": {
        "OneClassSVM_28": {},
        "SGDOneClassSVM_29": {}
    },
    "river/checks/__init__.py": {
        "OneHotEncoder_84": {
            "params": "default"
        }
    },
    "river/compat/river_to_sklearn.py": {
        "Pipeline_62": {
            "steps": "[(name, convert_river_to_sklearn(step)) for (name, step) in estimator.steps.items()]"
        },
        "LabelEncoder_271": {
            "params": "default"
        }
    },
    "river/compat/sklearn_to_river.py": {},
    "river/compat/test_sklearn.py": {
        "SGDRegressor_28": {
            "params": "default"
        },
        "LinearRegression_13": {
            "params": "default"
        },
        "LogisticRegression_14": {
            "params": "default"
        },
        "StandardScaler_15": {
            "params": "default"
        },
        "KMeans_16": {
            "n_clusters": "3",
            "seed": "42"
        }
    },
    "river/conftest.py": {},
    "river/feature_selection/k_best.py": {},
    "river/linear_model/pa.py": {},
    "river/linear_model/test_glm.py": {
        "StandardScaler_78": {
            "params": "default"
        },
        "LinearRegression_126": {
            "params": "default"
        },
        "LinearRegression_130": {
            "params": "default"
        },
        "LinearRegression_144": {
            "params": "default"
        },
        "LinearRegression_148": {
            "params": "default"
        },
        "LinearRegression_163": {
            "params": "default"
        },
        "StandardScaler_218": {
            "params": "default"
        },
        "LinearRegression_219": {},
        "SGDRegressor_220": {},
        "StandardScaler_242": {
            "params": "default"
        },
        "LinearRegression_243": {},
        "SGDRegressor_244": {},
        "StandardScaler_328": {
            "params": "default"
        },
        "LogisticRegression_329": {},
        "SGDClassifier_330": {},
        "StandardScaler_368": {
            "params": "default"
        },
        "Perceptron_369": {},
        "Perceptron_370": {},
        "StandardScaler_396": {
            "params": "default"
        },
        "LinearRegression_398": {},
        "SGDRegressor_406": {},
        "StandardScaler_444": {
            "params": "default"
        },
        "LogisticRegression_446": {},
        "SGDClassifier_453": {}
    },
    "river/metrics/test_cross_entropy.py": {},
    "river/metrics/test_fbeta.py": {},
    "river/metrics/test_log_loss.py": {},
    "river/metrics/test_metrics.py": {},
    "river/metrics/test_r2.py": {},
    "river/multioutput/chain.py": {
        "LogisticRegression_97": {
            "params": "default"
        },
        "LinearRegression_214": {
            "params": "default"
        }
    },
    "river/naive_bayes/test_naive_bayes.py": {
        "Pipeline_67": {
            "steps": "('tokenize', feature_extraction.BagOfWords(lowercase=False))",
            "memory": "('model', model(alpha=alpha))"
        },
        "Pipeline_97": {
            "steps": "('tokenize', feature_extraction.BagOfWords(lowercase=False))",
            "memory": "('model', model(alpha=alpha))"
        },
        "Pipeline_101": {
            "steps": "('tokenize', feature_extraction.BagOfWords(lowercase=False))",
            "memory": "('model', model(alpha=alpha))"
        },
        "Pipeline_173": {
            "steps": "('tokenize', feature_extraction.BagOfWords(lowercase=False))",
            "memory": "('model', model(alpha=alpha))"
        },
        "Pipeline_200": {
            "steps": "('tokenize', feature_extraction.BagOfWords(lowercase=False))",
            "memory": "('model', model(alpha=alpha))"
        }
    },
    "river/stream/iter_sklearn.py": {}
}