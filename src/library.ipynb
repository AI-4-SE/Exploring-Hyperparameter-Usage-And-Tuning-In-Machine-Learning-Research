{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from collections import Counter\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def df_to_latex(df: pd.DataFrame) -> None:\n",
    "    print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlrrl}\n",
      "\\toprule\n",
      "             Class &  Count &      Category &  \\#HP &  AvgOptionsUsed &        Most Used HP \\\\\n",
      "\\midrule\n",
      "    StandardScaler &    134 & preprocessing &    3 &            0.10 &             default \\\\\n",
      "            KMeans &     93 &       cluster &    9 &            1.89 &          n\\_clusters \\\\\n",
      "LogisticRegression &     76 &  linear\\_model &   15 &            2.00 &                   C \\\\\n",
      "               PCA &     70 & decomposition &    9 &            1.03 &        n\\_components \\\\\n",
      "  LinearRegression &     54 &  linear\\_model &    5 &            0.09 &             default \\\\\n",
      "      MinMaxScaler &     48 & preprocessing &    3 &            0.40 &             default \\\\\n",
      "   GaussianMixture &     47 &       mixture &   14 &            2.23 &        n\\_components \\\\\n",
      "               SVC &     46 &           svm &   15 &            1.65 &                   C \\\\\n",
      "      LabelEncoder &     42 & preprocessing &    0 &            0.00 &             default \\\\\n",
      "     OneHotEncoder &     39 & preprocessing &    7 &            0.85 &              sparse \\\\\n",
      "          Variable &   1665 &    tensorflow &   12 &            1.97 &       initial\\_value \\\\\n",
      "             Dense &   1296 &         keras &   11 &            2.70 &               units \\\\\n",
      "            DEFINE &   1037 &        compat &    9 &            3.01 & name, default, help \\\\\n",
      "           Session &    817 &        compat &    3 &            0.47 &             default \\\\\n",
      "           Dropout &    661 &         keras &    4 &            1.04 &                rate \\\\\n",
      "             Saver &    489 &        compat &   15 &            0.80 &            var\\_list \\\\\n",
      "     AdamOptimizer &    403 &        compat &    6 &            1.37 &       learning\\_rate \\\\\n",
      "             Graph &    378 &    tensorflow &    0 &            0.00 &             default \\\\\n",
      "            Conv2D &    376 &         keras &   17 &            4.98 &             filters \\\\\n",
      "LayerNormalization &    362 &         keras &   11 &            2.01 &             epsilon \\\\\n",
      "            Linear &  11716 &            nn &    5 &            2.16 &         in\\_features \\\\\n",
      "            Conv2d &   9716 &            nn &   11 &            4.80 &         in\\_channels \\\\\n",
      "        Sequential &   7493 &            nn &    1 &            0.94 &               *args \\\\\n",
      "              ReLU &   6081 &            nn &    1 &            0.66 &             inplace \\\\\n",
      "       BatchNorm2d &   5048 &            nn &    7 &            1.16 &        num\\_features \\\\\n",
      "         Parameter &   3694 &            nn &    2 &            1.13 &                data \\\\\n",
      "        DataLoader &   3402 &         utils &   15 &            4.02 &             dataset \\\\\n",
      "           Dropout &   3272 &            nn &    2 &            0.99 &                   p \\\\\n",
      "        ModuleList &   2966 &            nn &    1 &            0.55 &             modules \\\\\n",
      "              Adam &   1589 &         optim &    7 &            1.57 &             default \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_library_classes(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                classes.append(class_name)\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options and categories\n",
    "    class_options = []\n",
    "    categories = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "            parts = class_data[\"full_name\"].split(\".\")\n",
    "            if len(parts) <= 2:\n",
    "                category = parts[0]\n",
    "            else:\n",
    "                category = parts[1]\n",
    "            categories.append(category)\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class, library_name)\n",
    "            class_options.append(0)\n",
    "            categories.append(\"Unknown\")\n",
    "            continue\n",
    "\n",
    "    df_classes[\"Category\"] = categories\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if  param not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "                                \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            option_str = \", \".join(options)\n",
    "            most_used_class_option.append(option_str)\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "    \n",
    "    return df_classes\n",
    "\n",
    "\n",
    "df_sklearn_classes = get_library_classes(\"sklearn\", \"modules/sklearn_default_values.json\" , \"data/statistics/*\")\n",
    "df_sklearn_classes = df_sklearn_classes[:10]\n",
    "df_tf_classes = get_library_classes(\"tensorflow\", \"modules/tensorflow_default_values.json\" , \"data/statistics/*\")\n",
    "df_tf_classes = df_tf_classes[:10]\n",
    "df_pytorch_classes = get_library_classes(\"torch\", \"modules/torch_default_values.json\" , \"data/statistics/*\")\n",
    "df_pytorch_classes = df_pytorch_classes[:10]\n",
    "df_all_classes = pd.concat([df_sklearn_classes, df_tf_classes, df_pytorch_classes])\n",
    "\n",
    "df_to_latex(df=df_all_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlrrl}\n",
      "\\toprule\n",
      "                 Method &  Count &        Category &  \\#Args &  AvgArgsUsed &  Most Used Args \\\\\n",
      "\\midrule\n",
      "       train\\_test\\_split &    231 & model\\_selection &      6 &         3.76 &       *arrays\\_0 \\\\\n",
      "         accuracy\\_score &    162 &         metrics &      4 &         2.02 &  y\\_true, y\\_pred \\\\\n",
      "               f1\\_score &    160 &         metrics &      7 &         2.81 &  y\\_true, y\\_pred \\\\\n",
      "          roc\\_auc\\_score &    114 &         metrics &      7 &         2.12 & y\\_true, y\\_score \\\\\n",
      "                shuffle &     90 &           utils &      3 &         2.24 &       *arrays\\_0 \\\\\n",
      "              normalize &     86 &   preprocessing &      5 &         1.71 &               X \\\\\n",
      "            check\\_array &     82 &           utils &     13 &         2.12 &           array \\\\\n",
      "average\\_precision\\_score &     78 &         metrics &      5 &         2.18 & y\\_true, y\\_score \\\\\n",
      "      cosine\\_similarity &     73 &         metrics &      3 &         1.90 &               X \\\\\n",
      "        check\\_is\\_fitted &     69 &           utils &      4 &         1.26 &       estimator \\\\\n",
      "                reshape &   4203 &      tensorflow &      3 &         2.01 &   tensor, shape \\\\\n",
      "         variable\\_scope &   4191 &          compat &     13 &         1.28 &   name\\_or\\_scope \\\\\n",
      "                   cast &   2994 &      tensorflow &      3 &         2.00 &        x, dtype \\\\\n",
      "            placeholder &   2867 &          compat &      3 &         2.30 &           dtype \\\\\n",
      "               constant &   2699 &      tensorflow &      4 &         1.66 &           value \\\\\n",
      "                 concat &   2580 &      tensorflow &      3 &         2.02 &          values \\\\\n",
      "             reduce\\_sum &   2085 &            math &      4 &         1.76 &    input\\_tensor \\\\\n",
      "            reduce\\_mean &   1981 &            math &      4 &         1.41 &    input\\_tensor \\\\\n",
      "            expand\\_dims &   1939 &      tensorflow &      3 &         2.00 &           input \\\\\n",
      "                  shape &   1844 &      tensorflow &      3 &         1.00 &           input \\\\\n",
      "                    cat &  12487 &           torch &      3 &         1.84 &         tensors \\\\\n",
      "                 tensor &  10250 &           torch &      5 &         1.71 &            data \\\\\n",
      "                  zeros &   6496 &           torch &      6 &         2.01 &           *size \\\\\n",
      "             from\\_numpy &   4451 &           torch &      1 &         1.00 &         ndarray \\\\\n",
      "                no\\_grad &   4012 &           torch &      0 &         0.00 &         default \\\\\n",
      "                    sum &   3650 &           torch &      2 &         1.53 &           input \\\\\n",
      "                   load &   3602 &           torch &      4 &         1.34 &               f \\\\\n",
      "                   ones &   3536 &           torch &      6 &         2.12 &           *size \\\\\n",
      "                  stack &   3259 &           torch &      3 &         1.61 &         tensors \\\\\n",
      "                   relu &   3179 &              nn &      2 &         1.08 &           input \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_library_methods(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    methods = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, _ in module_data.items():\n",
    "                            if key[0].islower():\n",
    "                                method_name_parts = key.split(\"_\")[:-1]\n",
    "                                method_name = \"_\".join(method_name_parts)\n",
    "                                #for item in library_data:\n",
    "                                #    if item[\"name\"] == method_name:\n",
    "                                methods.append(method_name)\n",
    "\n",
    "    method_data = Counter(methods)\n",
    "    df_methods = pd.DataFrame.from_dict(method_data, orient=\"index\").reset_index()\n",
    "    df_methods = df_methods.rename(columns={'index':'Method', 0:'Count'})\n",
    "    df_methods = df_methods.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Compute number of args that can be set regarding the API data\n",
    "    method_options = []\n",
    "    methods = df_methods[\"Method\"].to_list()\n",
    "    categories = []\n",
    "\n",
    "    for method in methods:\n",
    "        try:\n",
    "            method_data = next(filter(lambda x: x[\"name\"] == method, library_data))\n",
    "            method_options.append(len(method_data[\"params\"]))\n",
    "            parts = method_data[\"full_name\"].split(\".\")\n",
    "            if len(parts) <= 2:\n",
    "                category = parts[0]\n",
    "            else:\n",
    "                category = parts[1]\n",
    "            categories.append(category)\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", method, library_name)\n",
    "            method_options.append(0)\n",
    "            categories.append(\"Unknown\")\n",
    "            continue\n",
    "\n",
    "    df_methods[\"Category\"] = categories\n",
    "    df_methods[\"#Args\"] = method_options\n",
    "\n",
    "    # Compute average number of args used per method\n",
    "    avg_method_args = []\n",
    "    most_used_method_args = []\n",
    "\n",
    "    for method in methods:\n",
    "        avg_method_args_used = []\n",
    "        method_args = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].islower():\n",
    "                                    method_name_parts = module_name.split(\"_\")[:-1]\n",
    "                                    method_name = \"_\".join(method_name_parts)\n",
    "                                    if method == method_name:\n",
    "                                        counter = 0\n",
    "                                        for arg in data.keys():\n",
    "                                            if arg == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if arg not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if arg == \"params\":\n",
    "                                                method_args.append(\"default\")\n",
    "                                            else:\n",
    "                                                method_args.append(arg)\n",
    "                                        \n",
    "                                        avg_method_args_used.append(counter)\n",
    "                                \n",
    "        avg_method_args.append(round((sum(avg_method_args_used) / len(avg_method_args_used)),2))\n",
    "        method_arg_data = Counter(method_args)\n",
    "        try:\n",
    "            most_common_num = method_arg_data.most_common(1)[0][1]\n",
    "            args = [x[0] for x in method_arg_data.most_common() if x[1] == most_common_num]\n",
    "            arg_str = \", \".join(args)\n",
    "            most_used_method_args.append(arg_str)\n",
    "        except IndexError:\n",
    "            most_used_method_args.append(\"None\")\n",
    "\n",
    "    df_methods[\"AvgArgsUsed\"] = avg_method_args\n",
    "    df_methods[\"Most Used Args\"] = most_used_method_args\n",
    "\n",
    "    return df_methods\n",
    "\n",
    "df_sklearn_methods = get_library_methods(\"sklearn\", \"modules/sklearn_default_values.json\", \"data/statistics/*\")\n",
    "df_sklearn_methods = df_sklearn_methods[:10]\n",
    "df_tf_methods = get_library_methods(\"tensorflow\", \"modules/tensorflow_default_values.json\", \"data/statistics/*\")\n",
    "df_tf_methods = df_tf_methods[:10]\n",
    "df_torch_methods = get_library_methods(\"torch\", \"modules/torch_default_values.json\", \"data/statistics/*\")\n",
    "df_torch_methods = df_torch_methods[:10]\n",
    "df_all_methods = pd.concat([df_sklearn_methods, df_tf_methods, df_torch_methods])\n",
    "\n",
    "df_to_latex(df=df_all_methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "                     Class &  Count &  Init With Params &  Init Without Params &  \\#HP &  AvgOptionsUsed \\\\\n",
      "\\midrule\n",
      "            StandardScaler &    134 &                 9 &                  125 &    3 &            0.10 \\\\\n",
      "                    KMeans &     93 &                86 &                    7 &    9 &            1.89 \\\\\n",
      "        LogisticRegression &     76 &                64 &                   12 &   15 &            2.00 \\\\\n",
      "                       PCA &     70 &                65 &                    5 &    9 &            1.03 \\\\\n",
      "          LinearRegression &     54 &                 4 &                   50 &    5 &            0.09 \\\\\n",
      "              MinMaxScaler &     48 &                18 &                   30 &    3 &            0.40 \\\\\n",
      "           GaussianMixture &     47 &                47 &                    0 &   14 &            2.23 \\\\\n",
      "                       SVC &     46 &                40 &                    6 &   15 &            1.65 \\\\\n",
      "              LabelEncoder &     42 &                 0 &                   42 &    0 &            0.00 \\\\\n",
      "             OneHotEncoder &     39 &                27 &                   12 &    7 &            0.85 \\\\\n",
      "                      TSNE &     38 &                37 &                    1 &   16 &            2.24 \\\\\n",
      "              GridSearchCV &     35 &                35 &                    0 &   10 &            3.66 \\\\\n",
      "   AgglomerativeClustering &     34 &                22 &                   12 &    8 &            2.12 \\\\\n",
      "      KNeighborsClassifier &     33 &                22 &                   11 &    8 &            0.97 \\\\\n",
      "     DecisionTreeRegressor &     29 &                26 &                    3 &   11 &            1.62 \\\\\n",
      "          NearestNeighbors &     28 &                28 &                    0 &    8 &            1.96 \\\\\n",
      "    RandomForestClassifier &     26 &                10 &                   16 &   18 &            1.04 \\\\\n",
      "                  Pipeline &     26 &                26 &                    0 &    3 &            1.04 \\\\\n",
      "                 LinearSVC &     24 &                12 &                   12 &   12 &            0.92 \\\\\n",
      "           TfidfVectorizer &     23 &                22 &                    1 &   21 &            2.74 \\\\\n",
      "                     Ridge &     20 &                14 &                    6 &    9 &            1.20 \\\\\n",
      "                    DBSCAN &     20 &                16 &                    4 &    8 &            1.95 \\\\\n",
      "           CountVectorizer &     20 &                19 &                    1 &   17 &            5.75 \\\\\n",
      "                 KernelPCA &     18 &                18 &                    0 &   16 &            4.00 \\\\\n",
      "             MLPClassifier &     18 &                14 &                    4 &   23 &            2.39 \\\\\n",
      "        PolynomialFeatures &     16 &                 4 &                   12 &    4 &            0.25 \\\\\n",
      "                GaussianNB &     15 &                 0 &                   15 &    2 &            0.00 \\\\\n",
      "GradientBoostingClassifier &     12 &                 9 &                    3 &   20 &            2.75 \\\\\n",
      "           MiniBatchKMeans &     10 &                10 &                    0 &   12 &            2.10 \\\\\n",
      "     RandomForestRegressor &     10 &                 9 &                    1 &   17 &            1.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def get_algorithms(library_name: str, library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == library_name:\n",
    "                        module_data = file_data[library]\n",
    "                        for key, value in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                for item in library_data:\n",
    "                                    if item[\"name\"] == class_name:\n",
    "                                        classes.append(class_name)\n",
    "\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "    # Get Number of API Options and category\n",
    "    categories = []\n",
    "    class_options = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        try:\n",
    "            class_data = next(filter(lambda x: x[\"name\"] == ml_class, library_data))\n",
    "            class_options.append(len(class_data[\"params\"]))\n",
    "            category = class_data[\"full_name\"].split(\".\")[1]\n",
    "            categories.append(category)\n",
    "        except StopIteration:\n",
    "            print(\"Could not find: \", ml_class)\n",
    "            continue\n",
    "            #raise StopIteration()\n",
    "\n",
    "    df_classes[\"Category\"] = categories\n",
    "    df_classes[\"#HP\"] = class_options\n",
    "\n",
    "    # Compute average number of options used per class and most used option\n",
    "    avg_class_options = []\n",
    "    most_used_class_option = []\n",
    "    # compute how often a algorithm is initialized with and without params\n",
    "    init_with_params = []\n",
    "    init_without_params = []\n",
    "\n",
    "    for ml_class in classes:\n",
    "        with_params = 0\n",
    "        without_params = 0\n",
    "        avg_class_options_used = []\n",
    "        class_options = []\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == library_name:\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        counter = 0\n",
    "\n",
    "                                        if \"params\" in data:\n",
    "                                            without_params += 1\n",
    "                                        else:\n",
    "                                            with_params += 1\n",
    "\n",
    "                                        for param in data.keys():\n",
    "                                            if param == \"variable\":\n",
    "                                                continue\n",
    "\n",
    "                                            if param not in (\"variable\", \"params\", \"class\"):\n",
    "                                                counter += 1\n",
    "\n",
    "                                            if param == \"params\":\n",
    "                                                class_options.append(\"default\")\n",
    "                                            else:\n",
    "                                                class_options.append(param)\n",
    "            \n",
    "                                        avg_class_options_used.append(counter)\n",
    "\n",
    "\n",
    "        init_with_params.append(with_params)\n",
    "        init_without_params.append(without_params)                  \n",
    "        avg_class_options.append(round((sum(avg_class_options_used) / len(avg_class_options_used)),2))\n",
    "        class_option_data = Counter(class_options)\n",
    "        try:\n",
    "            #most_common_number = class_option_data.most_common(1)[0][1]\n",
    "            #options = [elem[0] for elem in class_option_data.most_common() if elem[1] == most_common_number]\n",
    "            #option_str = \", \".join(options)\n",
    "            most_used_class_option.append(class_option_data.most_common(1)[0][0])\n",
    "        except IndexError:\n",
    "            most_used_class_option.append(\"None\")\n",
    "\n",
    "    df_classes[\"Init With Params\"] = init_with_params\n",
    "    df_classes[\"Init Without Params\"] = init_without_params\n",
    "    df_classes[\"AvgOptionsUsed\"] = avg_class_options\n",
    "    df_classes[\"Most Used HP\"] = most_used_class_option\n",
    "\n",
    "    return df_classes\n",
    "\n",
    "df_sklearn_ml_algo = get_algorithms(\"sklearn\", \"modules/sklearn_estimators.json\", \"data/statistics/*\")\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo[:30]\n",
    "\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo[[\"Class\", \"Count\", \"Init With Params\", \"Init Without Params\", \"#HP\", \"AvgOptionsUsed\"]]\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "df_to_latex(df=df_sklearn_ml_algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrlrrll}\n",
      "\\toprule\n",
      "                    Class &  Count &       Category &  \\#HP &  AvgOptionsUsed & Most Used HP & Common Value Types \\\\\n",
      "\\midrule\n",
      "                   KMeans &     93 &        cluster &    9 &            1.83 &   n\\_clusters &               Call \\\\\n",
      "                      PCA &     68 &  decomposition &    9 &            1.03 & n\\_components &           Constant \\\\\n",
      "       LogisticRegression &     57 &   linear\\_model &   15 &            2.14 &       solver &           Constant \\\\\n",
      "          GaussianMixture &     52 &        mixture &   14 &            2.25 & n\\_components &               Call \\\\\n",
      "         LinearRegression &     44 &   linear\\_model &    5 &            0.14 &      default &                  - \\\\\n",
      "         NearestNeighbors &     44 &      neighbors &    8 &            1.95 &  n\\_neighbors &           Constant \\\\\n",
      "  AgglomerativeClustering &     38 &        cluster &    8 &            2.21 &   n\\_clusters &           Constant \\\\\n",
      "     KNeighborsClassifier &     29 &      neighbors &    8 &            0.93 &  n\\_neighbors &           Constant \\\\\n",
      "                      SVC &     28 &            svm &   15 &            1.54 &            C &           Constant \\\\\n",
      "    DecisionTreeRegressor &     28 &           tree &   11 &            1.64 &    criterion &           Constant \\\\\n",
      "                   DBSCAN &     23 &        cluster &    8 &            1.96 &  min\\_samples &     MethodArgument \\\\\n",
      "                LinearSVC &     22 &            svm &   12 &            1.23 &      default &                  - \\\\\n",
      "   RandomForestClassifier &     22 &       ensemble &   18 &            0.91 &      default &                  - \\\\\n",
      "                 Pipeline &     22 &       pipeline &    3 &            1.05 &        steps &               List \\\\\n",
      "                KernelPCA &     18 &  decomposition &   16 &            4.00 & n\\_components &           Constant \\\\\n",
      "            MLPClassifier &     14 & neural\\_network &   23 &            2.14 &        alpha &           Constant \\\\\n",
      "          MiniBatchKMeans &     14 &        cluster &   12 &            1.71 &   n\\_clusters &     MethodArgument \\\\\n",
      "      OneVsRestClassifier &      9 &     multiclass &    3 &            1.44 &    estimator &               Call \\\\\n",
      "                   KDTree &      9 &      neighbors &    4 &            1.67 &            X &               Call \\\\\n",
      "                    Ridge &      9 &   linear\\_model &    9 &            1.33 &        alpha &           Constant \\\\\n",
      "            KernelDensity &      8 &      neighbors &    9 &            1.75 &       kernel &           Constant \\\\\n",
      "    RandomForestRegressor &      6 &       ensemble &   17 &            1.17 & n\\_estimators &           Constant \\\\\n",
      "LatentDirichletAllocation &      6 &  decomposition &   16 &            4.33 &     n\\_topics &               Name \\\\\n",
      "                    Lasso &      6 &   linear\\_model &   11 &            1.17 &        alpha &           Constant \\\\\n",
      "      KNeighborsRegressor &      5 &      neighbors &    8 &            1.00 &  n\\_neighbors &               Call \\\\\n",
      "             TruncatedSVD &      5 &  decomposition &    7 &            1.40 & n\\_components &           Constant \\\\\n",
      "GradientBoostingRegressor &      4 &       ensemble &   21 &            0.25 &      default &                  - \\\\\n",
      "       SpectralClustering &      4 &        cluster &   15 &            1.75 &   n\\_clusters &           Constant \\\\\n",
      "             SGDRegressor &      4 &   linear\\_model &   19 &            8.50 &         loss &           Constant \\\\\n",
      "     LogisticRegressionCV &      4 &   linear\\_model &   17 &            7.50 &       n\\_jobs &          Attribute \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_value_types(df, library_name, project_dir) -> pd.DataFrame:\n",
    "    value_type = []\n",
    "\n",
    "    for _, row in df_sklearn_ml_algo.iterrows():\n",
    "        class_value_types = []\n",
    "\n",
    "        df_class_name = row[\"Class\"]\n",
    "        df_class_option_name = row[\"Most Used HP\"].split(\",\")[0]\n",
    "\n",
    "        for project in glob.glob(project_dir):\n",
    "                with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                    project_data = json.load(project_file)\n",
    "\n",
    "                    for file in project_data.keys():\n",
    "                        file_data = project_data[file]\n",
    "                        for library in file_data.keys():\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if library == library_name:\n",
    "                                    module_data = file_data[library]\n",
    "                                    for module_name, data in module_data.items():\n",
    "                                        if module_name[0].isupper():\n",
    "                                            name = module_name.split(\"_\")[0]\n",
    "                                            if df_class_name == name:\n",
    "                                                for key, value in data.items():\n",
    "                                                    if key == df_class_option_name:\n",
    "                                                        #print(df_class_name, df_class_option_name, key, value)\n",
    "                                                        possible_values = value[\"possible_values\"]\n",
    "                                                        if possible_values:\n",
    "                                                            for x in possible_values:\n",
    "                                                                class_value_types.append(x[1])\n",
    "                                                        else:\n",
    "                                                            class_value_types.append(value[\"type\"])\n",
    "\n",
    "        value_type_data = Counter(class_value_types)    \n",
    "        top_types = value_type_data.most_common(1)\n",
    "        try:\n",
    "            if top_types:\n",
    "                types = top_types[0][0]\n",
    "                #types = [str(x[0][0]) for x in top_types]\n",
    "                #types = \", \".join(types)\n",
    "            else:\n",
    "                types = \"-\"\n",
    "        except Exception:\n",
    "            print(row[\"Class\"], row[\"Most Used HP\"], value_type_data.most_common(3))\n",
    "            print(top_types)\n",
    "            types = \"None\"\n",
    "\n",
    "        value_type.append(types)                                                   \n",
    "    \n",
    "    df[\"Common Value Types\"] = value_type\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_value_types(df_sklearn_ml_algo, \"sklearn\", \"data/statistics/*\")\n",
    "df_to_latex(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrl}\n",
      "\\toprule\n",
      "                 Class &             Category &  Count &  \\#HP &  AvgOptionsUsed & Most Used HP \\\\\n",
      "\\midrule\n",
      "        StandardScaler &        preprocessing &    115 &    3 &            0.10 &      default \\\\\n",
      "          MinMaxScaler &        preprocessing &     51 &    3 &            0.20 &      default \\\\\n",
      "                  TSNE &             manifold &     40 &   16 &            2.23 & n\\_components \\\\\n",
      "         OneHotEncoder &        preprocessing &     40 &    7 &            0.82 &       sparse \\\\\n",
      "          LabelEncoder &        preprocessing &     35 &    0 &            0.00 &      default \\\\\n",
      "       CountVectorizer &   feature\\_extraction &     21 &   17 &            5.29 &    tokenizer \\\\\n",
      "       TfidfVectorizer &   feature\\_extraction &     21 &   21 &            2.67 &  ngram\\_range \\\\\n",
      "        LabelBinarizer &        preprocessing &     10 &    3 &            0.10 &      default \\\\\n",
      "   FunctionTransformer &        preprocessing &      7 &    8 &            1.86 &     validate \\\\\n",
      "      TfidfTransformer &   feature\\_extraction &      6 &    4 &            0.83 &   smooth\\_idf \\\\\n",
      "         SimpleImputer &               impute &      5 &    6 &            0.80 &     strategy \\\\\n",
      "                Isomap &             manifold &      5 &   12 &            1.60 & n\\_components \\\\\n",
      "          MaxAbsScaler &        preprocessing &      5 &    1 &            0.00 &      default \\\\\n",
      "                   MDS &             manifold &      5 &    9 &            4.60 & n\\_components \\\\\n",
      "   MultiLabelBinarizer &        preprocessing &      5 &    2 &            0.60 &      classes \\\\\n",
      "        OrdinalEncoder &        preprocessing &      4 &    5 &            0.00 &      default \\\\\n",
      "        DictVectorizer &   feature\\_extraction &      3 &    4 &            0.00 &      default \\\\\n",
      "     SpectralEmbedding &             manifold &      3 &    7 &            3.00 & n\\_components \\\\\n",
      "LocallyLinearEmbedding &             manifold &      2 &   12 &            3.00 &  n\\_neighbors \\\\\n",
      "    PolynomialFeatures &        preprocessing &      2 &    4 &            1.00 &       degree \\\\\n",
      "      KBinsDiscretizer &        preprocessing &      2 &    6 &            2.50 &       encode \\\\\n",
      "     VarianceThreshold &    feature\\_selection &      1 &    1 &            1.00 &    threshold \\\\\n",
      "     HashingVectorizer &   feature\\_extraction &      1 &   16 &            0.00 &      default \\\\\n",
      "              Nystroem & kernel\\_approximation &      1 &    8 &            2.00 &        gamma \\\\\n",
      "            RBFSampler & kernel\\_approximation &      1 &    3 &            2.00 &        gamma \\\\\n",
      "SparseRandomProjection &    random\\_projection &      1 &    6 &            2.00 & n\\_components \\\\\n",
      "            Normalizer &        preprocessing &      1 &    2 &            1.00 &         copy \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sklearn_exp_settings = get_algorithms(\"sklearn\", \"modules/sklearn_experimental_settings.json\", \"data/statistics/*\")\n",
    "#df_sklearn_ml_algo = df_sklearn_ml_algo[:30]\n",
    "\n",
    "df_sklearn_exp_settings = df_sklearn_exp_settings[[\"Class\", \"Category\", \"Count\", \"#HP\", \"AvgOptionsUsed\", \"Most Used HP\"]]\n",
    "df_sklearn_exp_settings = df_sklearn_exp_settings.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "#df_sklearn_exp_settings = get_value_types(df_sklearn_exp_settings, \"sklearn\", \"statistics/*\")\n",
    "\n",
    "df_to_latex(df=df_sklearn_exp_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "                  Class &  Count &  Init With Params &  Init Without Params \\\\\n",
      "\\midrule\n",
      "         StandardScaler &    115 &                 7 &                  108 \\\\\n",
      "                 KMeans &     93 &                86 &                    7 \\\\\n",
      "                    PCA &     68 &                64 &                    4 \\\\\n",
      "     LogisticRegression &     57 &                47 &                   10 \\\\\n",
      "        GaussianMixture &     52 &                52 &                    0 \\\\\n",
      "           MinMaxScaler &     51 &                10 &                   41 \\\\\n",
      "       LinearRegression &     44 &                 5 &                   39 \\\\\n",
      "       NearestNeighbors &     44 &                44 &                    0 \\\\\n",
      "          OneHotEncoder &     40 &                27 &                   13 \\\\\n",
      "                   TSNE &     40 &                38 &                    2 \\\\\n",
      "AgglomerativeClustering &     38 &                26 &                   12 \\\\\n",
      "           LabelEncoder &     35 &                 0 &                   35 \\\\\n",
      "   KNeighborsClassifier &     29 &                18 &                   11 \\\\\n",
      "                    SVC &     28 &                22 &                    6 \\\\\n",
      "  DecisionTreeRegressor &     28 &                25 &                    3 \\\\\n",
      "                 DBSCAN &     23 &                19 &                    4 \\\\\n",
      "              LinearSVC &     22 &                12 &                   10 \\\\\n",
      " RandomForestClassifier &     22 &                 8 &                   14 \\\\\n",
      "               Pipeline &     22 &                22 &                    0 \\\\\n",
      "        CountVectorizer &     21 &                18 &                    3 \\\\\n",
      "        TfidfVectorizer &     21 &                20 &                    1 \\\\\n",
      "           GridSearchCV &     18 &                18 &                    0 \\\\\n",
      "              KernelPCA &     18 &                18 &                    0 \\\\\n",
      "          MLPClassifier &     14 &                10 &                    4 \\\\\n",
      "        MiniBatchKMeans &     14 &                14 &                    0 \\\\\n",
      "         LabelBinarizer &     10 &                 1 &                    9 \\\\\n",
      "                  Ridge &      9 &                 8 &                    1 \\\\\n",
      "    OneVsRestClassifier &      9 &                 9 &                    0 \\\\\n",
      "          KernelDensity &      8 &                 8 &                    0 \\\\\n",
      "    FunctionTransformer &      7 &                 7 &                    0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_init_type_of_algorithms(library_dir: str, project_dir: str) -> pd.DataFrame:\n",
    "    with open(library_dir, \"r\", encoding=\"utf-8\") as library_file:\n",
    "        library_data = json.load(library_file)\n",
    "\n",
    "    # Get Most used Class\n",
    "    classes = []\n",
    "\n",
    "    for project in glob.glob(project_dir):\n",
    "        with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "            project_data = json.load(project_file)\n",
    "\n",
    "            for file in project_data.keys():\n",
    "                file_data = project_data[file]\n",
    "                for library in file_data.keys():\n",
    "                    if library == \"sklearn\":\n",
    "                        module_data = file_data[library]\n",
    "                        for key, value in module_data.items():\n",
    "                            if key[0].isupper():\n",
    "                                class_name = key.split(\"_\")[0]\n",
    "                                for item in library_data:\n",
    "                                    if item[\"name\"] == class_name:\n",
    "                                        classes.append(class_name)\n",
    "                                        \n",
    "\n",
    "\n",
    "    class_data = Counter(classes)\n",
    "    df_classes = pd.DataFrame.from_dict(class_data, orient=\"index\").reset_index()\n",
    "    df_classes = df_classes.rename(columns={'index':'Class', 0:'Count'})\n",
    "    df_classes = df_classes.sort_values(by=['Count'], ascending=False)\n",
    "    \n",
    "\n",
    "    init_with_params = []\n",
    "    init_without_params = []\n",
    "    classes = df_classes[\"Class\"].to_list()\n",
    "\n",
    "    for ml_class in classes:\n",
    "        with_params = 0\n",
    "        without_params = 0\n",
    "        for project in glob.glob(project_dir):\n",
    "            with open(project, \"r\", encoding=\"utf-8\") as project_file:\n",
    "                project_data = json.load(project_file)\n",
    "\n",
    "                for file in project_data.keys():\n",
    "                    file_data = project_data[file]\n",
    "                    for library in file_data.keys():\n",
    "                        if library == \"sklearn\":\n",
    "                            module_data = file_data[library]\n",
    "                            for module_name, data in module_data.items():\n",
    "                                if module_name[0].isupper():\n",
    "                                    name = module_name.split(\"_\")[0]\n",
    "                                    if ml_class == name:\n",
    "                                        if \"params\" in data:\n",
    "                                            without_params += 1\n",
    "                                        else:\n",
    "                                            with_params += 1\n",
    "        \n",
    "\n",
    "        init_with_params.append(with_params)\n",
    "        init_without_params.append(without_params)\n",
    "\n",
    "    \n",
    "    df_classes[\"Init With Params\"] = init_with_params\n",
    "    df_classes[\"Init Without Params\"] = init_without_params\n",
    "\n",
    "    return df_classes\n",
    "\n",
    "df_sklearn_ml_algo = count_init_type_of_algorithms(\"modules/sklearn_estimators.json\", \"data/statistics/*\")\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo[:30]\n",
    "df_sklearn_ml_algo = df_sklearn_ml_algo.sort_values(by=['Count'], ascending=False)\n",
    "\n",
    "df_to_latex(df=df_sklearn_ml_algo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e13cb00df3a1965131e2646c4b79d07cf1fea213fe4be5c4b37e6d4cb11002e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
